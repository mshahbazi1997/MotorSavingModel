{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1: create an environment and select the kernel\n",
    "Here are the command I use for creating an env:\n",
    "\n",
    "`conda create --name MotorSavings python=3.10`\n",
    "\n",
    "Once you done that make sure that your notebook is connected to your env. You may need to close and reopen your program.\n",
    "# Install all required packages\n",
    "`pip install -r ../requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.8.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: seaborn==0.13.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (0.13.0)\n",
      "Requirement already satisfied: pandas==2.1.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: numpy==1.26.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (1.26.0)\n",
      "Requirement already satisfied: torch<2.3,>=2.2 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: scipy==1.11.3 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn==1.3.2 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (4.66.1)\n",
      "Requirement already satisfied: gymnasium==1.0.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from matplotlib==3.8.0->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from pandas==2.1.1->-r ../requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from pandas==2.1.1->-r ../requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r ../requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r ../requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from gymnasium==1.0.0->-r ../requirements.txt (line 10)) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from gymnasium==1.0.0->-r ../requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from gymnasium==1.0.0->-r ../requirements.txt (line 10)) (0.0.4)\n",
      "Requirement already satisfied: filelock in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (3.17.0)\n",
      "Requirement already satisfied: sympy in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (2025.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.8.0->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from jinja2->torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mahdiyarshahbazi/miniconda3/envs/savings/lib/python3.10/site-packages (from sympy->torch<2.3,>=2.2->-r ../requirements.txt (line 5)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from get_utils import return_ignore, get_loss\n",
    "from plot import *\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_rate\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "#%matplotlib qt\n",
    "#%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory info\n",
    "This is the format that I use for generating directory when I train models. I use the network size of 128 for all analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'Sim_fixed_128_temp'\n",
    "n_model = 40 # number of simulated networks\n",
    "# sizes = [16,32,64,128,256]\n",
    "sizes = [128]\n",
    "\n",
    "# root_dir = '/Volumes/T9/MotorNet'\n",
    "root_dir = os.path.join(os.path.expanduser('~'),'Documents/Data/MotorNet')\n",
    "base_dir = root_dir\n",
    "save_fig = os.path.join(os.path.expanduser('~'),'Dropbox/Projects/papers/MotorSavings/paper/figures_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running all models for all phases of training and for different network sizes\n",
    "This could take almost one day, depending on your system. \n",
    "### Skip this if you already have your networks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53943) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model00...\n",
      "model01...\n",
      "model02...\n",
      "model03...\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...\n",
      "model13...\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training FF2:   3%|▌                     | 88/3201 [02:41<1:24:42,  1.63s/batch]Batch 100/3201 Done, mean position loss: 28.634330840110778\n",
      "Training FF2:   3%|▋                     | 94/3201 [02:44<1:18:37,  1.52s/batch]Batch 100/3201 Done, mean position loss: 28.532140123844147\n",
      "Training FF2:   3%|▋                     | 99/3201 [02:46<1:34:18,  1.82s/batch]Batch 100/3201 Done, mean position loss: 28.592647414207455\n",
      "Training FF2:   3%|▋                    | 101/3201 [02:47<1:24:44,  1.64s/batch]Batch 100/3201 Done, mean position loss: 29.404684541225432\n",
      "Training FF2:   3%|▋                     | 96/3201 [02:48<1:19:51,  1.54s/batch]Batch 100/3201 Done, mean position loss: 29.134295716285706\n",
      "Training FF2:   3%|▋                     | 99/3201 [02:49<1:20:33,  1.56s/batch]Batch 100/3201 Done, mean position loss: 28.446683402061463\n",
      "Training FF2:   3%|▋                     | 96/3201 [02:49<1:38:44,  1.91s/batch]Batch 100/3201 Done, mean position loss: 28.280916085243224\n",
      "Training FF2:   3%|▋                    | 101/3201 [02:50<1:32:20,  1.79s/batch]Batch 100/3201 Done, mean position loss: 28.717613904476167\n",
      "Training FF2:   3%|▋                    | 104/3201 [02:51<1:10:45,  1.37s/batch]Batch 100/3201 Done, mean position loss: 29.033025119304657\n",
      "Training FF2:   3%|▋                     | 98/3201 [02:48<1:24:45,  1.64s/batch]Batch 100/3201 Done, mean position loss: 28.052222907543182\n",
      "Training FF2:   3%|▋                     | 97/3201 [02:49<1:17:15,  1.49s/batch]Batch 100/3201 Done, mean position loss: 28.94268371582031\n",
      "Training FF2:   3%|▋                    | 100/3201 [02:49<1:23:19,  1.61s/batch]Batch 100/3201 Done, mean position loss: 28.227682683467865\n",
      "Training FF2:   3%|▋                    | 101/3201 [02:50<1:21:10,  1.57s/batch]Batch 100/3201 Done, mean position loss: 28.943860721588134\n",
      "Training FF2:   3%|▋                     | 99/3201 [02:51<1:22:46,  1.60s/batch]Batch 100/3201 Done, mean position loss: 27.987858142852787\n",
      "Training FF2:   3%|▋                    | 101/3201 [02:51<1:21:36,  1.58s/batch]Batch 100/3201 Done, mean position loss: 28.732665627002717\n",
      "Training FF2:   3%|▋                     | 97/3201 [02:51<1:30:48,  1.76s/batch]Batch 100/3201 Done, mean position loss: 28.477309312820438\n",
      "Training FF2:   3%|▋                     | 99/3201 [02:52<1:27:27,  1.69s/batch]Batch 100/3201 Done, mean position loss: 29.201753828525547\n",
      "Training FF2:   3%|▋                    | 103/3201 [02:52<1:20:28,  1.56s/batch]Batch 100/3201 Done, mean position loss: 28.14122413635254\n",
      "Training FF2:   3%|▋                    | 100/3201 [02:52<1:17:57,  1.51s/batch]Batch 100/3201 Done, mean position loss: 28.947109704017638\n",
      "Training FF2:   3%|▋                    | 103/3201 [02:54<1:13:43,  1.43s/batch]Batch 100/3201 Done, mean position loss: 28.480390214920043\n",
      "Training FF2:   3%|▋                    | 100/3201 [02:53<1:35:42,  1.85s/batch]Batch 100/3201 Done, mean position loss: 27.853842706680297\n",
      "Training FF2:   3%|▋                     | 98/3201 [02:53<1:32:39,  1.79s/batch]Batch 100/3201 Done, mean position loss: 27.818379583358762\n",
      "Training FF2:   3%|▋                    | 107/3201 [02:53<1:19:43,  1.55s/batch]Batch 100/3201 Done, mean position loss: 28.76573683977127\n",
      "Training FF2:   3%|▋                     | 99/3201 [02:53<1:29:28,  1.73s/batch]Batch 100/3201 Done, mean position loss: 28.210832238197327\n",
      "Training FF2:   3%|▋                    | 104/3201 [02:55<1:15:40,  1.47s/batch]Batch 100/3201 Done, mean position loss: 28.494750418663024\n",
      "Training FF2:   3%|▋                    | 103/3201 [02:55<1:21:21,  1.58s/batch]Batch 100/3201 Done, mean position loss: 29.140353405475615\n",
      "Training FF2:   3%|▋                    | 102/3201 [02:54<1:12:08,  1.40s/batch]Batch 100/3201 Done, mean position loss: 28.157387931346893\n",
      "Training FF2:   3%|▋                    | 105/3201 [02:56<1:13:07,  1.42s/batch]Batch 100/3201 Done, mean position loss: 29.65637907266617\n",
      "Training FF2:   3%|▋                    | 104/3201 [02:55<1:13:30,  1.42s/batch]Batch 100/3201 Done, mean position loss: 28.861903805732727\n",
      "Training FF2:   3%|▋                    | 102/3201 [02:56<1:13:59,  1.43s/batch]Batch 100/3201 Done, mean position loss: 28.90854964971542\n",
      "Training FF2:   3%|▋                    | 108/3201 [02:55<1:15:53,  1.47s/batch]Batch 100/3201 Done, mean position loss: 28.697361226081846\n",
      "Training FF2:   3%|▋                    | 101/3201 [02:55<1:21:53,  1.58s/batch]Batch 100/3201 Done, mean position loss: 28.752028963565827\n",
      "Training FF2:   3%|▋                    | 105/3201 [02:55<1:17:56,  1.51s/batch]Batch 100/3201 Done, mean position loss: 28.85254388332367\n",
      "Training FF2:   3%|▋                    | 105/3201 [02:57<1:25:50,  1.66s/batch]Batch 100/3201 Done, mean position loss: 28.72984300374985\n",
      "Training FF2:   3%|▋                    | 109/3201 [02:56<1:10:40,  1.37s/batch]Batch 100/3201 Done, mean position loss: 28.829287467002867\n",
      "Training FF2:   3%|▋                    | 103/3201 [02:57<1:22:05,  1.59s/batch]Batch 100/3201 Done, mean position loss: 27.575967950820925\n",
      "Training FF2:   3%|▋                    | 106/3201 [02:57<1:26:37,  1.68s/batch]Batch 100/3201 Done, mean position loss: 28.208952894210817\n",
      "Training FF2:   3%|▋                    | 104/3201 [02:58<1:17:12,  1.50s/batch]Batch 100/3201 Done, mean position loss: 27.208686861991882\n",
      "Training FF2:   3%|▋                    | 102/3201 [02:58<1:17:05,  1.49s/batch]Batch 100/3201 Done, mean position loss: 29.202988121509552\n",
      "Training FF2:   3%|▋                    | 106/3201 [03:02<1:16:10,  1.48s/batch]Batch 100/3201 Done, mean position loss: 28.755295600891113\n",
      "Training FF2:   6%|█▎                   | 191/3201 [05:14<1:17:17,  1.54s/batch]Batch 200/3201 Done, mean position loss: 25.3841868185997\n",
      "Training FF2:   6%|█▎                   | 195/3201 [05:21<1:23:31,  1.67s/batch]Batch 200/3201 Done, mean position loss: 26.24719353199005\n",
      "Training FF2:   6%|█▎                   | 195/3201 [05:24<1:26:44,  1.73s/batch]Batch 200/3201 Done, mean position loss: 25.688883607387545\n",
      "Training FF2:   6%|█▎                   | 201/3201 [05:24<1:15:52,  1.52s/batch]Batch 200/3201 Done, mean position loss: 25.859548506736758\n",
      "Training FF2:   6%|█▎                   | 192/3201 [05:24<1:29:27,  1.78s/batch]Batch 200/3201 Done, mean position loss: 25.807157604694368\n",
      "Training FF2:   6%|█▎                   | 199/3201 [05:24<1:34:11,  1.88s/batch]Batch 200/3201 Done, mean position loss: 25.720389761924746\n",
      "Training FF2:   6%|█▎                   | 194/3201 [05:27<1:30:24,  1.80s/batch]Batch 200/3201 Done, mean position loss: 24.35719907283783\n",
      "Training FF2:   6%|█▎                   | 193/3201 [05:26<1:14:37,  1.49s/batch]Batch 200/3201 Done, mean position loss: 25.596127190589904\n",
      "Training FF2:   6%|█▎                   | 192/3201 [05:27<1:15:49,  1.51s/batch]Batch 200/3201 Done, mean position loss: 25.440623116493224\n",
      "Training FF2:   6%|█▎                   | 198/3201 [05:28<1:18:21,  1.57s/batch]Batch 200/3201 Done, mean position loss: 25.413262434005738\n",
      "Batch 200/3201 Done, mean position loss: 25.65024684667587\n",
      "Training FF2:   6%|█▎                   | 194/3201 [05:28<1:27:49,  1.75s/batch]Batch 200/3201 Done, mean position loss: 24.775232050418854\n",
      "Training FF2:   6%|█▎                   | 204/3201 [05:30<1:18:26,  1.57s/batch]Batch 200/3201 Done, mean position loss: 25.899872176647186\n",
      "Training FF2:   6%|█▎                   | 195/3201 [05:29<1:24:07,  1.68s/batch]Batch 200/3201 Done, mean position loss: 24.767551820278168\n",
      "Training FF2:   6%|█▎                   | 201/3201 [05:30<1:24:35,  1.69s/batch]Batch 200/3201 Done, mean position loss: 25.534981622695923\n",
      "Training FF2:   6%|█▎                   | 202/3201 [05:29<1:17:47,  1.56s/batch]Batch 200/3201 Done, mean position loss: 24.91822200536728\n",
      "Training FF2:   6%|█▎                   | 204/3201 [05:31<1:25:27,  1.71s/batch]Batch 200/3201 Done, mean position loss: 25.52863242149353\n",
      "Training FF2:   6%|█▎                   | 205/3201 [05:32<1:21:47,  1.64s/batch]Batch 200/3201 Done, mean position loss: 25.706372680664064\n",
      "Training FF2:   6%|█▎                   | 202/3201 [05:33<1:18:01,  1.56s/batch]Batch 200/3201 Done, mean position loss: 25.724898059368137\n",
      "Training FF2:   6%|█▎                   | 203/3201 [05:32<1:20:07,  1.60s/batch]Batch 200/3201 Done, mean position loss: 25.531982741355897\n",
      "Training FF2:   6%|█▎                   | 202/3201 [05:32<1:14:08,  1.48s/batch]Batch 200/3201 Done, mean position loss: 24.361980714797973\n",
      "Training FF2:   6%|█▎                   | 200/3201 [05:33<1:23:47,  1.68s/batch]Batch 200/3201 Done, mean position loss: 25.16016990661621\n",
      "Training FF2:   6%|█▎                   | 197/3201 [05:33<1:18:19,  1.56s/batch]Batch 200/3201 Done, mean position loss: 25.71941453933716\n",
      "Training FF2:   6%|█▎                   | 205/3201 [05:33<1:22:16,  1.65s/batch]Batch 200/3201 Done, mean position loss: 24.336750440597534\n",
      "Training FF2:   6%|█▎                   | 202/3201 [05:35<1:10:14,  1.41s/batch]Batch 200/3201 Done, mean position loss: 25.510284607410433\n",
      "Training FF2:   6%|█▎                   | 202/3201 [05:34<1:24:59,  1.70s/batch]Batch 200/3201 Done, mean position loss: 25.655421459674834\n",
      "Training FF2:   6%|█▎                   | 203/3201 [05:34<1:19:46,  1.60s/batch]Batch 200/3201 Done, mean position loss: 25.296252946853635\n",
      "Training FF2:   6%|█▎                   | 200/3201 [05:34<1:21:29,  1.63s/batch]Batch 200/3201 Done, mean position loss: 24.57017135381699\n",
      "Training FF2:   6%|█▎                   | 204/3201 [05:34<1:29:48,  1.80s/batch]Batch 200/3201 Done, mean position loss: 25.322563412189485\n",
      "Training FF2:   6%|█▎                   | 204/3201 [05:35<1:48:06,  2.16s/batch]Batch 200/3201 Done, mean position loss: 25.49758072376251\n",
      "Training FF2:   6%|█▎                   | 207/3201 [05:36<1:18:23,  1.57s/batch]Batch 200/3201 Done, mean position loss: 25.23886658191681\n",
      "Training FF2:   7%|█▍                   | 216/3201 [05:37<1:18:18,  1.57s/batch]Batch 200/3201 Done, mean position loss: 25.935454795360567\n",
      "Training FF2:   6%|█▎                   | 205/3201 [05:38<1:14:44,  1.50s/batch]Batch 200/3201 Done, mean position loss: 25.844423842430114\n",
      "Training FF2:   6%|█▎                   | 204/3201 [05:38<1:27:13,  1.75s/batch]Batch 200/3201 Done, mean position loss: 25.481467370986937\n",
      "Training FF2:   6%|█▎                   | 208/3201 [05:39<1:35:44,  1.92s/batch]Batch 200/3201 Done, mean position loss: 26.209599647521973\n",
      "Training FF2:   6%|█▎                   | 200/3201 [05:38<1:22:34,  1.65s/batch]Batch 200/3201 Done, mean position loss: 25.660209805965426\n",
      "Training FF2:   6%|█▎                   | 200/3201 [05:40<1:18:49,  1.58s/batch]Batch 200/3201 Done, mean position loss: 25.671079494953155\n",
      "Training FF2:   6%|█▎                   | 201/3201 [05:39<1:17:07,  1.54s/batch]Batch 200/3201 Done, mean position loss: 26.00862806081772\n",
      "Training FF2:   6%|█▎                   | 206/3201 [05:40<1:27:24,  1.75s/batch]Batch 200/3201 Done, mean position loss: 25.669897253513337\n",
      "Training FF2:   6%|█▎                   | 201/3201 [05:40<1:31:12,  1.82s/batch]Batch 200/3201 Done, mean position loss: 25.21751947402954\n",
      "Training FF2:   9%|█▉                   | 292/3201 [07:49<1:16:40,  1.58s/batch]Batch 300/3201 Done, mean position loss: 24.012502019405368\n",
      "Training FF2:   9%|█▉                   | 295/3201 [08:01<1:30:10,  1.86s/batch]Batch 300/3201 Done, mean position loss: 24.64332240343094\n",
      "Training FF2:   9%|█▉                   | 298/3201 [08:02<1:17:02,  1.59s/batch]Batch 300/3201 Done, mean position loss: 24.450872457027437\n",
      "Training FF2:   9%|█▉                   | 289/3201 [08:04<1:29:47,  1.85s/batch]Batch 300/3201 Done, mean position loss: 24.33292368888855\n",
      "Training FF2:   9%|█▉                   | 296/3201 [08:05<1:17:45,  1.61s/batch]Batch 300/3201 Done, mean position loss: 25.363400399684906\n",
      "Training FF2:   9%|█▉                   | 301/3201 [08:07<1:22:50,  1.71s/batch]Batch 300/3201 Done, mean position loss: 24.539405841827396\n",
      "Training FF2:   9%|█▉                   | 290/3201 [08:06<1:25:21,  1.76s/batch]Batch 300/3201 Done, mean position loss: 24.047670993804932\n",
      "Training FF2:   9%|█▉                   | 293/3201 [08:07<1:31:56,  1.90s/batch]Batch 300/3201 Done, mean position loss: 24.341035010814664\n",
      "Training FF2:   9%|█▉                   | 301/3201 [08:07<1:06:41,  1.38s/batch]Batch 300/3201 Done, mean position loss: 22.828718161582948\n",
      "Training FF2:   9%|█▉                   | 297/3201 [08:07<1:20:01,  1.65s/batch]Batch 300/3201 Done, mean position loss: 24.43065050840378\n",
      "Training FF2:  10%|██                   | 314/3201 [08:09<1:11:22,  1.48s/batch]Batch 300/3201 Done, mean position loss: 24.410345945358277\n",
      "Training FF2:  10%|██                   | 305/3201 [08:08<1:18:42,  1.63s/batch]Batch 300/3201 Done, mean position loss: 24.16653177499771\n",
      "Training FF2:   9%|█▉                   | 298/3201 [08:10<1:14:55,  1.55s/batch]Batch 300/3201 Done, mean position loss: 24.450487985610962\n",
      "Training FF2:   9%|█▉                   | 300/3201 [08:09<1:10:17,  1.45s/batch]Batch 300/3201 Done, mean position loss: 23.41732822418213\n",
      "Training FF2:   9%|█▉                   | 293/3201 [08:10<1:22:46,  1.71s/batch]Batch 300/3201 Done, mean position loss: 23.83618540048599\n",
      "Training FF2:   9%|█▉                   | 299/3201 [08:10<1:09:57,  1.45s/batch]Batch 300/3201 Done, mean position loss: 24.266154601573945\n",
      "Training FF2:   9%|█▉                   | 297/3201 [08:11<1:27:55,  1.82s/batch]Batch 300/3201 Done, mean position loss: 23.524520032405853\n",
      "Training FF2:  10%|██                   | 305/3201 [08:12<1:03:53,  1.32s/batch]Batch 300/3201 Done, mean position loss: 24.559144036769865\n",
      "Training FF2:   9%|█▉                   | 304/3201 [08:13<1:21:32,  1.69s/batch]Batch 300/3201 Done, mean position loss: 24.219481573104858\n",
      "Training FF2:   9%|█▉                   | 303/3201 [08:13<1:11:47,  1.49s/batch]Batch 300/3201 Done, mean position loss: 24.272977030277254\n",
      "Training FF2:   9%|█▉                   | 301/3201 [08:14<1:14:52,  1.55s/batch]Batch 300/3201 Done, mean position loss: 23.71400229215622\n",
      "Training FF2:   9%|█▉                   | 304/3201 [08:15<1:35:02,  1.97s/batch]Batch 300/3201 Done, mean position loss: 23.467666409015656\n",
      "Training FF2:   9%|█▉                   | 299/3201 [08:14<1:23:02,  1.72s/batch]Batch 300/3201 Done, mean position loss: 25.20764427423477\n",
      "Training FF2:   9%|█▉                   | 299/3201 [08:15<1:13:02,  1.51s/batch]Batch 300/3201 Done, mean position loss: 24.425357570648195\n",
      "Training FF2:   9%|█▉                   | 304/3201 [08:14<1:15:43,  1.57s/batch]Batch 300/3201 Done, mean position loss: 23.757317204475402\n",
      "Training FF2:   9%|█▉                   | 300/3201 [08:16<1:24:32,  1.75s/batch]Batch 300/3201 Done, mean position loss: 24.15870517015457\n",
      "Training FF2:  10%|██                   | 309/3201 [08:15<1:20:17,  1.67s/batch]Batch 300/3201 Done, mean position loss: 24.504279491901396\n",
      "Training FF2:   9%|█▉                   | 296/3201 [08:15<1:22:05,  1.70s/batch]Batch 300/3201 Done, mean position loss: 23.946818516254424\n",
      "Training FF2:   9%|█▉                   | 302/3201 [08:16<1:18:36,  1.63s/batch]Batch 300/3201 Done, mean position loss: 24.102175030708313\n",
      "Training FF2:  10%|██                   | 310/3201 [08:17<1:15:12,  1.56s/batch]Batch 300/3201 Done, mean position loss: 23.43626635789871\n",
      "Training FF2:   9%|█▉                   | 297/3201 [08:18<1:27:23,  1.81s/batch]Batch 300/3201 Done, mean position loss: 24.37655054092407\n",
      "Training FF2:   9%|█▉                   | 298/3201 [08:18<1:20:03,  1.65s/batch]Batch 300/3201 Done, mean position loss: 23.430584733486175\n",
      "Training FF2:   9%|█▉                   | 300/3201 [08:19<1:24:59,  1.76s/batch]Batch 300/3201 Done, mean position loss: 24.185747809410095\n",
      "Training FF2:   9%|█▉                   | 304/3201 [08:20<1:29:07,  1.85s/batch]Batch 300/3201 Done, mean position loss: 24.47183545589447\n",
      "Training FF2:  10%|██                   | 309/3201 [08:21<1:41:05,  2.10s/batch]Batch 300/3201 Done, mean position loss: 24.210427675247193\n",
      "Training FF2:  10%|██                   | 307/3201 [08:21<1:26:53,  1.80s/batch]Batch 300/3201 Done, mean position loss: 23.85310496091843\n",
      "Training FF2:  10%|██                   | 306/3201 [08:23<1:30:25,  1.87s/batch]Batch 300/3201 Done, mean position loss: 24.490485973358155\n",
      "Training FF2:  10%|██                   | 310/3201 [08:23<1:17:57,  1.62s/batch]Batch 300/3201 Done, mean position loss: 24.59022219657898\n",
      "Training FF2:   9%|█▉                   | 304/3201 [08:24<1:12:40,  1.51s/batch]Batch 300/3201 Done, mean position loss: 24.41371936798096\n",
      "Training FF2:  10%|██                   | 311/3201 [08:30<1:17:31,  1.61s/batch]Batch 300/3201 Done, mean position loss: 24.13798600435257\n",
      "Training FF2:  12%|██▌                  | 383/3201 [10:33<1:23:41,  1.78s/batch]Batch 400/3201 Done, mean position loss: 22.97754034280777\n",
      "Training FF2:  13%|██▋                  | 408/3201 [10:43<1:11:19,  1.53s/batch]Batch 400/3201 Done, mean position loss: 23.737797389030455\n",
      "Training FF2:  12%|██▌                  | 394/3201 [10:44<1:14:21,  1.59s/batch]Batch 400/3201 Done, mean position loss: 23.801486263275144\n",
      "Training FF2:  12%|██▌                  | 390/3201 [10:44<1:09:38,  1.49s/batch]Batch 400/3201 Done, mean position loss: 24.9009320640564\n",
      "Training FF2:  12%|██▌                  | 393/3201 [10:44<1:19:45,  1.70s/batch]Batch 400/3201 Done, mean position loss: 23.890435564517976\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:45<1:08:59,  1.48s/batch]Batch 400/3201 Done, mean position loss: 23.591636872291566\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:49<1:23:42,  1.79s/batch]Batch 400/3201 Done, mean position loss: 23.550654368400572\n",
      "Training FF2:  12%|██▌                  | 397/3201 [10:50<1:18:08,  1.67s/batch]Batch 400/3201 Done, mean position loss: 22.22427656888962\n",
      "Training FF2:  12%|██▌                  | 393/3201 [10:50<1:21:48,  1.75s/batch]Batch 400/3201 Done, mean position loss: 22.69609838485718\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:49<1:15:38,  1.62s/batch]Batch 400/3201 Done, mean position loss: 23.82432957649231\n",
      "Training FF2:  13%|██▋                  | 413/3201 [10:51<1:09:07,  1.49s/batch]Batch 400/3201 Done, mean position loss: 23.884365372657776\n",
      "Training FF2:  13%|██▋                  | 403/3201 [10:51<1:03:35,  1.36s/batch]Batch 400/3201 Done, mean position loss: 23.64591934919357\n",
      "Training FF2:  13%|██▋                  | 405/3201 [10:51<1:14:21,  1.60s/batch]Batch 400/3201 Done, mean position loss: 23.74705106258392\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:52<1:19:08,  1.69s/batch]Batch 400/3201 Done, mean position loss: 23.562988221645355\n",
      "Training FF2:  13%|██▋                  | 402/3201 [10:54<1:18:04,  1.67s/batch]Batch 400/3201 Done, mean position loss: 23.68645166397095\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:54<1:10:55,  1.52s/batch]Batch 400/3201 Done, mean position loss: 23.33633695602417\n",
      "Training FF2:  13%|██▋                  | 401/3201 [10:53<1:08:05,  1.46s/batch]Batch 400/3201 Done, mean position loss: 22.96782875776291\n",
      "Batch 400/3201 Done, mean position loss: 23.576295313835146\n",
      "Training FF2:  12%|██▌                  | 398/3201 [10:53<1:21:14,  1.74s/batch]Batch 400/3201 Done, mean position loss: 23.097962708473204\n",
      "Training FF2:  12%|██▌                  | 400/3201 [10:55<1:24:33,  1.81s/batch]Batch 400/3201 Done, mean position loss: 23.69904629945755\n",
      "Training FF2:  13%|██▋                  | 404/3201 [10:56<1:13:09,  1.57s/batch]Batch 400/3201 Done, mean position loss: 24.50606385707855\n",
      "Batch 400/3201 Done, mean position loss: 22.994030125141144\n",
      "Training FF2:  13%|██▋                  | 403/3201 [10:56<1:12:11,  1.55s/batch]Batch 400/3201 Done, mean position loss: 23.343051624298095\n",
      "Training FF2:  13%|██▋                  | 407/3201 [10:55<1:07:36,  1.45s/batch]Batch 400/3201 Done, mean position loss: 22.87668999195099\n",
      "Training FF2:  13%|██▋                  | 403/3201 [10:56<1:05:35,  1.41s/batch]Batch 400/3201 Done, mean position loss: 23.49394926071167\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:57<1:27:15,  1.87s/batch]Batch 400/3201 Done, mean position loss: 23.366170032024385\n",
      "Training FF2:  13%|██▋                  | 404/3201 [10:58<1:08:39,  1.47s/batch]Batch 400/3201 Done, mean position loss: 23.726694698333738\n",
      "Training FF2:  12%|██▌                  | 400/3201 [10:56<1:13:46,  1.58s/batch]Batch 400/3201 Done, mean position loss: 23.81580680370331\n",
      "Training FF2:  13%|██▋                  | 404/3201 [10:57<1:14:57,  1.61s/batch]Batch 400/3201 Done, mean position loss: 23.6182053732872\n",
      "Training FF2:  12%|██▌                  | 399/3201 [10:58<1:15:05,  1.61s/batch]Batch 400/3201 Done, mean position loss: 23.3200581240654\n",
      "Training FF2:  13%|██▋                  | 409/3201 [10:58<1:08:06,  1.46s/batch]Batch 400/3201 Done, mean position loss: 23.427269413471222\n",
      "Training FF2:  13%|██▋                  | 419/3201 [11:00<1:11:04,  1.53s/batch]Batch 400/3201 Done, mean position loss: 23.14838582754135\n",
      "Training FF2:  13%|██▋                  | 406/3201 [11:00<1:13:35,  1.58s/batch]Batch 400/3201 Done, mean position loss: 22.691485998630522\n",
      "Training FF2:  13%|██▋                  | 412/3201 [11:01<1:17:15,  1.66s/batch]Batch 400/3201 Done, mean position loss: 23.098330008983613\n",
      "Training FF2:  13%|██▋                  | 407/3201 [11:02<1:03:51,  1.37s/batch]Batch 400/3201 Done, mean position loss: 23.864176673889162\n",
      "Training FF2:  13%|██▋                  | 404/3201 [11:03<1:29:10,  1.91s/batch]Batch 400/3201 Done, mean position loss: 23.669498414993285\n",
      "Training FF2:  13%|██▋                  | 408/3201 [11:04<1:09:01,  1.48s/batch]Batch 400/3201 Done, mean position loss: 23.51755813360214\n",
      "Training FF2:  13%|██▋                  | 403/3201 [11:04<1:07:34,  1.45s/batch]Batch 400/3201 Done, mean position loss: 23.77947875738144\n",
      "Training FF2:  13%|██▋                  | 416/3201 [11:09<1:22:01,  1.77s/batch]Batch 400/3201 Done, mean position loss: 23.472884972095493\n",
      "Training FF2:  13%|██▋                  | 415/3201 [11:11<1:24:31,  1.82s/batch]Batch 400/3201 Done, mean position loss: 23.936103808879853\n",
      "Training FF2:  15%|███▏                 | 484/3201 [13:16<1:20:34,  1.78s/batch]Batch 500/3201 Done, mean position loss: 22.552644860744476\n",
      "Training FF2:  15%|███▏                 | 495/3201 [13:19<1:22:07,  1.82s/batch]Batch 500/3201 Done, mean position loss: 23.148792915344238\n",
      "Training FF2:  15%|███▏                 | 489/3201 [13:20<1:05:22,  1.45s/batch]Batch 500/3201 Done, mean position loss: 23.59008353948593\n",
      "Training FF2:  16%|███▎                 | 503/3201 [13:22<1:02:52,  1.40s/batch]Batch 500/3201 Done, mean position loss: 24.5188266992569\n",
      "Training FF2:  15%|███▏                 | 489/3201 [13:24<1:13:07,  1.62s/batch]Batch 500/3201 Done, mean position loss: 23.044157066345214\n",
      "Training FF2:  16%|███▎                 | 504/3201 [13:25<1:14:59,  1.67s/batch]Batch 500/3201 Done, mean position loss: 23.01450786113739\n",
      "Training FF2:  15%|███▏                 | 492/3201 [13:25<1:10:35,  1.56s/batch]Batch 500/3201 Done, mean position loss: 23.487882010936737\n",
      "Training FF2:  15%|███▎                 | 496/3201 [13:26<1:09:28,  1.54s/batch]Batch 500/3201 Done, mean position loss: 22.938480985164645\n",
      "Training FF2:  15%|███▏                 | 485/3201 [13:26<1:19:47,  1.76s/batch]Batch 500/3201 Done, mean position loss: 21.95212647676468\n",
      "Training FF2:  16%|███▎                 | 505/3201 [13:30<1:18:59,  1.76s/batch]Batch 500/3201 Done, mean position loss: 23.390793173313142\n",
      "Training FF2:  15%|███▏                 | 491/3201 [13:29<1:07:47,  1.50s/batch]Batch 500/3201 Done, mean position loss: 22.9383563876152\n",
      "Training FF2:  16%|███▎                 | 507/3201 [13:30<1:14:10,  1.65s/batch]Batch 500/3201 Done, mean position loss: 22.12577509880066\n",
      "Training FF2:  16%|███▎                 | 498/3201 [13:30<1:16:50,  1.71s/batch]Batch 500/3201 Done, mean position loss: 22.559836814403532\n",
      "Training FF2:  16%|███▎                 | 506/3201 [13:31<1:08:04,  1.52s/batch]Batch 500/3201 Done, mean position loss: 23.28081286430359\n",
      "Training FF2:  16%|███▎                 | 500/3201 [13:33<1:17:23,  1.72s/batch]Batch 500/3201 Done, mean position loss: 23.51003133535385\n",
      "Training FF2:  15%|███▏                 | 495/3201 [13:34<1:16:27,  1.70s/batch]Batch 500/3201 Done, mean position loss: 23.36769524335861\n",
      "Training FF2:  15%|███▏                 | 493/3201 [13:33<1:06:06,  1.46s/batch]Batch 500/3201 Done, mean position loss: 24.137265255451204\n",
      "Training FF2:  16%|███▎                 | 501/3201 [13:34<1:14:24,  1.65s/batch]Batch 500/3201 Done, mean position loss: 23.043182330131533\n",
      "Training FF2:  16%|███▎                 | 510/3201 [13:35<1:17:17,  1.72s/batch]Batch 500/3201 Done, mean position loss: 23.124174954891206\n",
      "Training FF2:  16%|███▎                 | 500/3201 [13:34<1:15:44,  1.68s/batch]Batch 500/3201 Done, mean position loss: 23.25197344303131\n",
      "Training FF2:  16%|███▎                 | 500/3201 [13:35<1:11:03,  1.58s/batch]Batch 500/3201 Done, mean position loss: 23.06857069730759\n",
      "Training FF2:  16%|███▎                 | 502/3201 [13:36<1:09:21,  1.54s/batch]Batch 500/3201 Done, mean position loss: 23.36893896341324\n",
      "Training FF2:  16%|███▎                 | 501/3201 [13:36<1:17:38,  1.73s/batch]Batch 500/3201 Done, mean position loss: 22.72870410680771\n",
      "Training FF2:  16%|███▎                 | 504/3201 [13:37<1:15:26,  1.68s/batch]Batch 500/3201 Done, mean position loss: 22.518194801807404\n",
      "Training FF2:  16%|███▎                 | 503/3201 [13:37<1:06:44,  1.48s/batch]Batch 500/3201 Done, mean position loss: 23.31381723403931\n",
      "Training FF2:  16%|███▎                 | 506/3201 [13:37<1:05:05,  1.45s/batch]Batch 500/3201 Done, mean position loss: 22.70779883146286\n",
      "Training FF2:  16%|███▎                 | 503/3201 [13:38<1:16:18,  1.70s/batch]Batch 500/3201 Done, mean position loss: 22.87560704946518\n",
      "Training FF2:  16%|███▎                 | 504/3201 [13:38<1:04:03,  1.43s/batch]Batch 500/3201 Done, mean position loss: 23.147267491817473\n",
      "Batch 500/3201 Done, mean position loss: 23.014453649520874\n",
      "Training FF2:  15%|███▏                 | 493/3201 [13:40<1:22:41,  1.83s/batch]Batch 500/3201 Done, mean position loss: 23.209497468471525\n",
      "Training FF2:  16%|███▎                 | 505/3201 [13:41<1:15:06,  1.67s/batch]Batch 500/3201 Done, mean position loss: 22.891229124069213\n",
      "Training FF2:  16%|███▎                 | 502/3201 [13:41<1:14:49,  1.66s/batch]Batch 500/3201 Done, mean position loss: 22.57336899995804\n",
      "Training FF2:  16%|███▎                 | 509/3201 [13:44<1:16:11,  1.70s/batch]Batch 500/3201 Done, mean position loss: 22.658542556762697\n",
      "Training FF2:  16%|███▎                 | 506/3201 [13:43<1:14:08,  1.65s/batch]Batch 500/3201 Done, mean position loss: 22.96065551996231\n",
      "Training FF2:  16%|███▎                 | 507/3201 [13:45<1:16:40,  1.71s/batch]Batch 500/3201 Done, mean position loss: 22.78512377023697\n",
      "Training FF2:  16%|███▎                 | 507/3201 [13:45<1:03:16,  1.41s/batch]Batch 500/3201 Done, mean position loss: 22.954317953586582\n",
      "Training FF2:  16%|███▎                 | 509/3201 [13:46<1:04:31,  1.44s/batch]Batch 500/3201 Done, mean position loss: 22.777333631515503\n",
      "Training FF2:  16%|███▎                 | 513/3201 [13:46<1:13:14,  1.63s/batch]Batch 500/3201 Done, mean position loss: 22.244043879508972\n",
      "Training FF2:  16%|███▍                 | 516/3201 [13:52<1:10:07,  1.57s/batch]Batch 500/3201 Done, mean position loss: 23.32260528087616\n",
      "Training FF2:  16%|███▍                 | 515/3201 [13:59<1:18:55,  1.76s/batch]Batch 500/3201 Done, mean position loss: 23.46672093153\n",
      "Training FF2:  18%|███▊                 | 577/3201 [15:57<1:25:32,  1.96s/batch]Batch 600/3201 Done, mean position loss: 23.261570811271667\n",
      "Training FF2:  19%|████▎                  | 602/3201 [15:57<48:00,  1.11s/batch]Batch 600/3201 Done, mean position loss: 22.38079027891159\n",
      "Training FF2:  19%|███▉                 | 593/3201 [16:02<1:04:24,  1.48s/batch]Batch 600/3201 Done, mean position loss: 24.135629148483275\n",
      "Training FF2:  19%|███▉                 | 597/3201 [16:04<1:10:24,  1.62s/batch]Batch 600/3201 Done, mean position loss: 22.844185094833374\n",
      "Training FF2:  19%|███▉                 | 593/3201 [16:05<1:12:52,  1.68s/batch]Batch 600/3201 Done, mean position loss: 22.651911504268647\n",
      "Training FF2:  18%|███▊                 | 586/3201 [16:09<1:03:08,  1.45s/batch]Batch 600/3201 Done, mean position loss: 22.657101967334746\n",
      "Training FF2:  19%|███▉                 | 598/3201 [16:12<1:21:06,  1.87s/batch]Batch 600/3201 Done, mean position loss: 21.748490507602693\n",
      "Training FF2:  19%|███▉                 | 600/3201 [16:12<1:19:22,  1.83s/batch]Batch 600/3201 Done, mean position loss: 22.969735734462738\n",
      "Training FF2:  19%|███▉                 | 594/3201 [16:14<1:12:57,  1.68s/batch]Batch 600/3201 Done, mean position loss: 23.33102405309677\n",
      "Training FF2:  19%|███▉                 | 597/3201 [16:13<1:17:11,  1.78s/batch]Batch 600/3201 Done, mean position loss: 22.34555913209915\n",
      "Training FF2:  19%|███▉                 | 597/3201 [16:14<1:00:47,  1.40s/batch]Batch 600/3201 Done, mean position loss: 22.531324503421786\n",
      "Training FF2:  19%|███▉                 | 606/3201 [16:15<1:34:34,  2.19s/batch]Batch 600/3201 Done, mean position loss: 22.959329631328583\n",
      "Training FF2:  19%|███▉                 | 601/3201 [16:14<1:09:38,  1.61s/batch]Batch 600/3201 Done, mean position loss: 22.687738118171694\n",
      "Training FF2:  19%|███▉                 | 594/3201 [16:16<1:21:09,  1.87s/batch]Batch 600/3201 Done, mean position loss: 22.451552097797393\n",
      "Training FF2:  19%|███▉                 | 601/3201 [16:15<1:06:26,  1.53s/batch]Batch 600/3201 Done, mean position loss: 22.57654087305069\n",
      "Training FF2:  19%|███▉                 | 598/3201 [16:16<1:02:52,  1.45s/batch]Batch 600/3201 Done, mean position loss: 22.773570275306703\n",
      "Training FF2:  19%|███▉                 | 597/3201 [16:16<1:08:47,  1.59s/batch]Batch 600/3201 Done, mean position loss: 23.197964227199556\n",
      "Training FF2:  19%|███▉                 | 601/3201 [16:16<1:08:35,  1.58s/batch]Batch 600/3201 Done, mean position loss: 23.218597717285157\n",
      "Training FF2:  19%|███▉                 | 600/3201 [16:18<1:16:11,  1.76s/batch]Batch 600/3201 Done, mean position loss: 22.834601154327395\n",
      "Training FF2:  19%|███▉                 | 599/3201 [16:18<1:06:42,  1.54s/batch]Batch 600/3201 Done, mean position loss: 22.54334785223007\n",
      "Training FF2:  19%|███▉                 | 604/3201 [16:19<1:08:30,  1.58s/batch]Batch 600/3201 Done, mean position loss: 22.4999276638031\n",
      "Training FF2:  19%|███▉                 | 600/3201 [16:20<1:07:34,  1.56s/batch]Batch 600/3201 Done, mean position loss: 22.069845221042634\n",
      "Training FF2:  19%|████▎                  | 602/3201 [16:20<46:29,  1.07s/batch]Batch 600/3201 Done, mean position loss: 22.38982589006424\n",
      "Training FF2:  18%|███▉                 | 592/3201 [16:20<1:10:00,  1.61s/batch]Batch 600/3201 Done, mean position loss: 21.79201545715332\n",
      "Training FF2:  19%|███▉                 | 593/3201 [16:21<1:25:04,  1.96s/batch]Batch 600/3201 Done, mean position loss: 22.645052318573\n",
      "Training FF2:  19%|███▉                 | 606/3201 [16:21<1:17:49,  1.80s/batch]Batch 600/3201 Done, mean position loss: 22.473896386623384\n",
      "Training FF2:  19%|███▉                 | 603/3201 [16:21<1:25:17,  1.97s/batch]Batch 600/3201 Done, mean position loss: 22.47960286617279\n",
      "Training FF2:  19%|███▉                 | 601/3201 [16:21<1:15:28,  1.74s/batch]Batch 600/3201 Done, mean position loss: 23.02778932094574\n",
      "Training FF2:  19%|███▉                 | 602/3201 [16:22<1:04:12,  1.48s/batch]Batch 600/3201 Done, mean position loss: 22.871988298892973\n",
      "Training FF2:  19%|███▉                 | 606/3201 [16:23<1:01:00,  1.41s/batch]Batch 600/3201 Done, mean position loss: 22.721250226497652\n",
      "Training FF2:  19%|████▎                  | 606/3201 [16:25<50:38,  1.17s/batch]Batch 600/3201 Done, mean position loss: 22.501095757484435\n",
      "Training FF2:  19%|███▉                 | 608/3201 [16:26<1:08:00,  1.57s/batch]Batch 600/3201 Done, mean position loss: 22.860496108531954\n",
      "Training FF2:  19%|████                 | 610/3201 [16:27<1:00:00,  1.39s/batch]Batch 600/3201 Done, mean position loss: 23.5827676153183\n",
      "Training FF2:  19%|████                 | 620/3201 [16:31<1:05:08,  1.51s/batch]Batch 600/3201 Done, mean position loss: 22.44673931121826\n",
      "Training FF2:  19%|███▉                 | 608/3201 [16:30<1:14:58,  1.73s/batch]Batch 600/3201 Done, mean position loss: 22.850281093120575\n",
      "Training FF2:  19%|████▎                  | 608/3201 [16:31<57:43,  1.34s/batch]Batch 600/3201 Done, mean position loss: 21.937158074378964\n",
      "Training FF2:  19%|████▍                  | 616/3201 [16:34<59:13,  1.37s/batch]Batch 600/3201 Done, mean position loss: 22.541266121864318\n",
      "Training FF2:  19%|████                 | 619/3201 [16:34<1:20:31,  1.87s/batch]Batch 600/3201 Done, mean position loss: 22.88492688894272\n",
      "Training FF2:  19%|████                 | 616/3201 [16:42<1:18:37,  1.82s/batch]Batch 600/3201 Done, mean position loss: 23.000646154880524\n",
      "Training FF2:  19%|███▉                 | 606/3201 [16:42<1:10:30,  1.63s/batch]Batch 600/3201 Done, mean position loss: 22.560775291919708\n",
      "Training FF2:  21%|████▌                | 688/3201 [18:35<1:09:32,  1.66s/batch]Batch 700/3201 Done, mean position loss: 23.128761546611788\n",
      "Training FF2:  22%|████▉                  | 690/3201 [18:42<58:51,  1.41s/batch]Batch 700/3201 Done, mean position loss: 23.879959161281583\n",
      "Training FF2:  22%|█████                  | 706/3201 [18:49<55:10,  1.33s/batch]Batch 700/3201 Done, mean position loss: 22.661120948791503\n",
      "Training FF2:  21%|████▍                | 679/3201 [18:47<1:06:32,  1.58s/batch]Batch 700/3201 Done, mean position loss: 22.502718274593356\n",
      "Training FF2:  22%|████▌                | 689/3201 [18:50<1:16:49,  1.84s/batch]Batch 700/3201 Done, mean position loss: 22.646147308349608\n",
      "Training FF2:  22%|████▌                | 696/3201 [18:49<1:16:39,  1.84s/batch]Batch 700/3201 Done, mean position loss: 22.237420608997347\n",
      "Training FF2:  21%|████▌                | 687/3201 [18:51<1:30:47,  2.17s/batch]Batch 700/3201 Done, mean position loss: 22.43480223894119\n",
      "Training FF2:  21%|████▍                | 680/3201 [18:51<1:17:53,  1.85s/batch]Batch 700/3201 Done, mean position loss: 22.377087643146517\n",
      "Batch 700/3201 Done, mean position loss: 22.24380773544312\n",
      "Training FF2:  22%|████▌                | 690/3201 [18:52<1:19:48,  1.91s/batch]Batch 700/3201 Done, mean position loss: 21.66563687801361\n",
      "Training FF2:  22%|████▌                | 691/3201 [18:52<1:16:47,  1.84s/batch]Batch 700/3201 Done, mean position loss: 22.301249454021452\n",
      "Training FF2:  22%|█████                  | 710/3201 [18:54<49:05,  1.18s/batch]Batch 700/3201 Done, mean position loss: 22.34054857492447\n",
      "Training FF2:  22%|████▌                | 700/3201 [18:55<1:10:56,  1.70s/batch]Batch 700/3201 Done, mean position loss: 22.512115294933317\n",
      "Training FF2:  22%|████▋                | 706/3201 [18:58<1:11:00,  1.71s/batch]Batch 700/3201 Done, mean position loss: 21.685038044452668\n",
      "Training FF2:  22%|████▋                | 705/3201 [18:57<1:04:57,  1.56s/batch]Batch 700/3201 Done, mean position loss: 22.62734005212784\n",
      "Training FF2:  22%|████▌                | 691/3201 [18:58<1:14:03,  1.77s/batch]Batch 700/3201 Done, mean position loss: 22.586948654651643\n",
      "Training FF2:  22%|████▌                | 700/3201 [18:59<1:09:03,  1.66s/batch]Batch 700/3201 Done, mean position loss: 23.05282266616821\n",
      "Training FF2:  22%|████▋                | 706/3201 [18:58<1:01:54,  1.49s/batch]Batch 700/3201 Done, mean position loss: 22.24307993173599\n",
      "Training FF2:  22%|████▋                | 707/3201 [18:59<1:07:20,  1.62s/batch]Batch 700/3201 Done, mean position loss: 22.223693299293515\n",
      "Training FF2:  22%|████▋                | 708/3201 [19:00<1:18:38,  1.89s/batch]Batch 700/3201 Done, mean position loss: 22.21845253229141\n",
      "Training FF2:  22%|████▉                  | 691/3201 [19:00<47:07,  1.13s/batch]Batch 700/3201 Done, mean position loss: 22.481270115375516\n",
      "Training FF2:  22%|████▋                | 706/3201 [19:01<1:19:57,  1.92s/batch]Batch 700/3201 Done, mean position loss: 22.822227296829226\n",
      "Batch 700/3201 Done, mean position loss: 23.014718327522274\n",
      "Training FF2:  22%|████▌                | 699/3201 [19:01<1:10:07,  1.68s/batch]Batch 700/3201 Done, mean position loss: 22.53802030801773\n",
      "Training FF2:  22%|████▌                | 697/3201 [19:04<1:22:00,  1.97s/batch]Batch 700/3201 Done, mean position loss: 22.246010982990263\n",
      "Training FF2:  22%|████▋                | 706/3201 [19:05<1:13:06,  1.76s/batch]Batch 700/3201 Done, mean position loss: 21.605606336593628\n",
      "Training FF2:  22%|████▉                  | 693/3201 [19:06<57:11,  1.37s/batch]Batch 700/3201 Done, mean position loss: 23.028702852725985\n",
      "Training FF2:  22%|█████                  | 706/3201 [19:07<51:03,  1.23s/batch]Batch 700/3201 Done, mean position loss: 23.425686221122742\n",
      "Training FF2:  22%|████▋                | 710/3201 [19:08<1:01:17,  1.48s/batch]Batch 700/3201 Done, mean position loss: 22.38470369338989\n",
      "Training FF2:  22%|████▌                | 695/3201 [19:09<1:04:52,  1.55s/batch]Batch 700/3201 Done, mean position loss: 22.19010380268097\n",
      "Training FF2:  22%|████▋                | 707/3201 [19:09<1:18:45,  1.89s/batch]Batch 700/3201 Done, mean position loss: 22.325760979652404\n",
      "Training FF2:  22%|████▋                | 713/3201 [19:10<1:18:35,  1.90s/batch]Batch 700/3201 Done, mean position loss: 22.680557045936585\n",
      "Training FF2:  22%|████▌                | 694/3201 [19:13<1:23:49,  2.01s/batch]Batch 700/3201 Done, mean position loss: 21.788243803977963\n",
      "Training FF2:  22%|████▋                | 713/3201 [19:14<1:12:11,  1.74s/batch]Batch 700/3201 Done, mean position loss: 22.18549949169159\n",
      "Training FF2:  22%|████▋                | 714/3201 [19:15<1:18:20,  1.89s/batch]Batch 700/3201 Done, mean position loss: 22.284486548900606\n",
      "Training FF2:  22%|████▋                | 711/3201 [19:17<1:16:10,  1.84s/batch]Batch 700/3201 Done, mean position loss: 22.585089087486267\n",
      "Training FF2:  22%|████▋                | 714/3201 [19:21<1:25:15,  2.06s/batch]Batch 700/3201 Done, mean position loss: 22.652163672447205\n",
      "Training FF2:  22%|████▋                | 714/3201 [19:22<1:09:07,  1.67s/batch]Batch 700/3201 Done, mean position loss: 22.317140250205995\n",
      "Training FF2:  22%|████▋                | 705/3201 [19:23<1:06:48,  1.61s/batch]Batch 700/3201 Done, mean position loss: 22.52215787410736\n",
      "Training FF2:  23%|████▊                | 725/3201 [19:25<1:00:51,  1.47s/batch]Batch 700/3201 Done, mean position loss: 22.335054335594176\n",
      "Training FF2:  25%|█████▏               | 791/3201 [21:17<1:05:33,  1.63s/batch]Batch 800/3201 Done, mean position loss: 22.999033975601197\n",
      "Training FF2:  25%|█████▊                 | 805/3201 [21:19<45:04,  1.13s/batch]Batch 800/3201 Done, mean position loss: 23.675979840755463\n",
      "Training FF2:  25%|█████▏               | 797/3201 [21:25<1:03:49,  1.59s/batch]Batch 800/3201 Done, mean position loss: 22.104565677642825\n",
      "Training FF2:  25%|█████▋                 | 789/3201 [21:28<55:21,  1.38s/batch]Batch 800/3201 Done, mean position loss: 22.361333622932435\n",
      "Training FF2:  25%|█████▏               | 789/3201 [21:29<1:11:58,  1.79s/batch]Batch 800/3201 Done, mean position loss: 22.164497396945954\n",
      "Training FF2:  25%|█████▊                 | 805/3201 [21:32<54:36,  1.37s/batch]Batch 800/3201 Done, mean position loss: 22.44322547674179\n",
      "Training FF2:  25%|█████▏               | 799/3201 [21:31<1:01:03,  1.53s/batch]Batch 800/3201 Done, mean position loss: 22.47379186630249\n",
      "Training FF2:  25%|█████▏               | 797/3201 [21:32<1:03:36,  1.59s/batch]Batch 800/3201 Done, mean position loss: 22.399488646984103\n",
      "Training FF2:  25%|█████▎               | 812/3201 [21:32<1:08:47,  1.73s/batch]Batch 800/3201 Done, mean position loss: 21.568286821842193\n",
      "Training FF2:  25%|█████▊                 | 803/3201 [21:33<53:27,  1.34s/batch]Batch 800/3201 Done, mean position loss: 22.264064683914185\n",
      "Training FF2:  24%|█████▏               | 783/3201 [21:36<1:15:49,  1.88s/batch]Batch 800/3201 Done, mean position loss: 22.238315224647522\n",
      "Training FF2:  25%|█████▊                 | 804/3201 [21:35<55:59,  1.40s/batch]Batch 800/3201 Done, mean position loss: 22.055531728267667\n",
      "Training FF2:  24%|█████                | 781/3201 [21:35<1:14:36,  1.85s/batch]Batch 800/3201 Done, mean position loss: 22.11663352251053\n",
      "Training FF2:  25%|█████▎               | 807/3201 [21:37<1:07:49,  1.70s/batch]Batch 800/3201 Done, mean position loss: 22.147562761306762\n",
      "Training FF2:  25%|█████▋                 | 793/3201 [21:39<53:51,  1.34s/batch]Batch 800/3201 Done, mean position loss: 22.274368863105774\n",
      "Training FF2:  25%|█████▏               | 785/3201 [21:39<1:07:47,  1.68s/batch]Batch 800/3201 Done, mean position loss: 22.21383679151535\n",
      "Training FF2:  25%|█████▋                 | 796/3201 [21:38<56:10,  1.40s/batch]Batch 800/3201 Done, mean position loss: 22.004829030036927\n",
      "Training FF2:  25%|█████▊                 | 813/3201 [21:40<50:22,  1.27s/batch]Batch 800/3201 Done, mean position loss: 22.488253903388976\n",
      "Training FF2:  25%|█████▋                 | 795/3201 [21:42<50:59,  1.27s/batch]Batch 800/3201 Done, mean position loss: 22.822087960243227\n",
      "Training FF2:  25%|█████▋                 | 796/3201 [21:43<52:05,  1.30s/batch]Batch 800/3201 Done, mean position loss: 21.968417994976043\n",
      "Training FF2:  25%|█████▋                 | 797/3201 [21:44<51:26,  1.28s/batch]Batch 800/3201 Done, mean position loss: 22.27626934528351\n",
      "Training FF2:  25%|█████▏               | 789/3201 [21:45<1:03:21,  1.58s/batch]Batch 800/3201 Done, mean position loss: 22.86659913301468\n",
      "Training FF2:  25%|█████▏               | 789/3201 [21:44<1:01:26,  1.53s/batch]Batch 800/3201 Done, mean position loss: 23.14781487941742\n",
      "Training FF2:  25%|█████▊                 | 803/3201 [21:48<58:46,  1.47s/batch]Batch 800/3201 Done, mean position loss: 21.423876905441283\n",
      "Training FF2:  25%|█████▊                 | 809/3201 [21:48<55:19,  1.39s/batch]Batch 800/3201 Done, mean position loss: 22.08996964931488\n",
      "Training FF2:  25%|█████▏               | 789/3201 [21:48<1:04:26,  1.60s/batch]Batch 800/3201 Done, mean position loss: 22.682455792427064\n",
      "Training FF2:  25%|█████▎               | 813/3201 [21:50<1:11:57,  1.81s/batch]Batch 800/3201 Done, mean position loss: 22.53090558767319\n",
      "Training FF2:  25%|█████▊                 | 807/3201 [21:51<54:04,  1.36s/batch]Batch 800/3201 Done, mean position loss: 21.502217676639557\n",
      "Training FF2:  25%|█████▊                 | 810/3201 [21:52<51:47,  1.30s/batch]Batch 800/3201 Done, mean position loss: 22.1384673166275\n",
      "Training FF2:  25%|█████▎               | 815/3201 [21:53<1:09:52,  1.76s/batch]Batch 800/3201 Done, mean position loss: 21.9522052359581\n",
      "Training FF2:  26%|█████▎               | 818/3201 [21:56<1:02:54,  1.58s/batch]Batch 800/3201 Done, mean position loss: 22.9215074467659\n",
      "Training FF2:  26%|█████▎               | 818/3201 [21:59<1:19:47,  2.01s/batch]Batch 800/3201 Done, mean position loss: 22.329245443344117\n",
      "Training FF2:  26%|█████▎               | 817/3201 [21:59<1:18:15,  1.97s/batch]Batch 800/3201 Done, mean position loss: 21.710752127170565\n",
      "Training FF2:  25%|█████▎               | 814/3201 [22:03<1:03:51,  1.61s/batch]Batch 800/3201 Done, mean position loss: 22.19296267271042\n",
      "Training FF2:  26%|█████▍               | 820/3201 [22:02<1:08:22,  1.72s/batch]Batch 800/3201 Done, mean position loss: 22.357521603107454\n",
      "Training FF2:  26%|█████▍               | 821/3201 [22:06<1:06:12,  1.67s/batch]Batch 800/3201 Done, mean position loss: 22.118443903923033\n",
      "Training FF2:  26%|█████▎               | 819/3201 [22:07<1:08:38,  1.73s/batch]Batch 800/3201 Done, mean position loss: 21.96728398799896\n",
      "Training FF2:  26%|█████▍               | 825/3201 [22:13<1:12:17,  1.83s/batch]Batch 800/3201 Done, mean position loss: 22.205945045948027\n",
      "Training FF2:  26%|█████▉                 | 830/3201 [22:18<58:05,  1.47s/batch]Batch 800/3201 Done, mean position loss: 22.356597640514373\n",
      "Training FF2:  26%|█████▍               | 825/3201 [22:19<1:06:10,  1.67s/batch]Batch 800/3201 Done, mean position loss: 22.21479814529419\n",
      "Training FF2:  28%|██████▍                | 894/3201 [23:57<55:10,  1.44s/batch]Batch 900/3201 Done, mean position loss: 22.879671852588654\n",
      "Training FF2:  27%|██████▏                | 865/3201 [24:01<53:48,  1.38s/batch]Batch 900/3201 Done, mean position loss: 23.5447315454483\n",
      "Training FF2:  27%|██████▏                | 866/3201 [24:02<56:17,  1.45s/batch]Batch 900/3201 Done, mean position loss: 22.22130943775177\n",
      "Training FF2:  27%|██████▎                | 880/3201 [24:05<58:00,  1.50s/batch]Batch 900/3201 Done, mean position loss: 22.025120606422423\n",
      "Training FF2:  28%|█████▊               | 888/3201 [24:09<1:17:27,  2.01s/batch]Batch 900/3201 Done, mean position loss: 22.132044658660888\n",
      "Training FF2:  28%|█████▊               | 888/3201 [24:09<1:06:42,  1.73s/batch]Batch 900/3201 Done, mean position loss: 22.089821922779084\n",
      "Training FF2:  28%|█████▊               | 891/3201 [24:11<1:04:09,  1.67s/batch]Batch 900/3201 Done, mean position loss: 22.329443275928497\n",
      "Training FF2:  27%|█████▊               | 880/3201 [24:12<1:01:18,  1.58s/batch]Batch 900/3201 Done, mean position loss: 21.926284067630768\n",
      "Training FF2:  27%|█████▊               | 880/3201 [24:13<1:02:52,  1.63s/batch]Batch 900/3201 Done, mean position loss: 22.313234992027283\n",
      "Training FF2:  28%|█████▉               | 898/3201 [24:12<1:07:50,  1.77s/batch]Batch 900/3201 Done, mean position loss: 22.15447592258453\n",
      "Training FF2:  28%|██████▌                | 908/3201 [24:14<58:04,  1.52s/batch]Batch 900/3201 Done, mean position loss: 21.430125119686128\n",
      "Training FF2:  28%|█████▊               | 892/3201 [24:15<1:00:58,  1.58s/batch]Batch 900/3201 Done, mean position loss: 22.042825174331664\n",
      "Training FF2:  28%|██████▍                | 902/3201 [24:16<58:19,  1.52s/batch]Batch 900/3201 Done, mean position loss: 22.621181116104125\n",
      "Training FF2:  27%|█████▋               | 874/3201 [24:17<1:11:04,  1.83s/batch]Batch 900/3201 Done, mean position loss: 22.058724071979523\n",
      "Training FF2:  28%|█████▉               | 912/3201 [24:19<1:02:09,  1.63s/batch]Batch 900/3201 Done, mean position loss: 22.006176717281342\n",
      "Training FF2:  28%|██████▌                | 905/3201 [24:17<52:14,  1.37s/batch]Batch 900/3201 Done, mean position loss: 22.078319132328033\n",
      "Training FF2:  28%|█████▉               | 899/3201 [24:18<1:07:32,  1.76s/batch]Batch 900/3201 Done, mean position loss: 21.87209654092789\n",
      "Training FF2:  28%|██████▍                | 890/3201 [24:19<51:59,  1.35s/batch]Batch 900/3201 Done, mean position loss: 21.290789320468903\n",
      "Training FF2:  28%|██████▌                | 906/3201 [24:18<52:53,  1.38s/batch]Batch 900/3201 Done, mean position loss: 22.99899224281311\n",
      "Training FF2:  28%|██████▍                | 903/3201 [24:21<55:20,  1.44s/batch]Batch 900/3201 Done, mean position loss: 21.97983201742172\n",
      "Training FF2:  28%|█████▊               | 885/3201 [24:21<1:06:33,  1.72s/batch]Batch 900/3201 Done, mean position loss: 22.36480779647827\n",
      "Training FF2:  28%|█████▉               | 911/3201 [24:23<1:04:08,  1.68s/batch]Batch 900/3201 Done, mean position loss: 22.726470320224763\n",
      "Training FF2:  28%|██████▍                | 904/3201 [24:22<53:47,  1.41s/batch]Batch 900/3201 Done, mean position loss: 22.061902239322663\n",
      "Training FF2:  28%|██████▌                | 911/3201 [24:26<53:05,  1.39s/batch]Batch 900/3201 Done, mean position loss: 22.394468986988066\n",
      "Training FF2:  29%|██████▌                | 913/3201 [24:28<59:07,  1.55s/batch]Batch 900/3201 Done, mean position loss: 21.997936413288116\n",
      "Training FF2:  28%|█████▉               | 896/3201 [24:29<1:00:12,  1.57s/batch]Batch 900/3201 Done, mean position loss: 21.713519098758695\n",
      "Training FF2:  28%|██████▌                | 908/3201 [24:30<48:06,  1.26s/batch]Batch 900/3201 Done, mean position loss: 21.41123376369476\n",
      "Training FF2:  28%|██████▍                | 897/3201 [24:30<59:35,  1.55s/batch]Batch 900/3201 Done, mean position loss: 21.924940400123596\n",
      "Training FF2:  28%|██████▌                | 907/3201 [24:31<58:40,  1.53s/batch]Batch 900/3201 Done, mean position loss: 22.490301530361176\n",
      "Training FF2:  28%|█████▊               | 893/3201 [24:34<1:01:35,  1.60s/batch]Batch 900/3201 Done, mean position loss: 21.748486123085023\n",
      "Training FF2:  29%|██████               | 915/3201 [24:37<1:03:25,  1.66s/batch]Batch 900/3201 Done, mean position loss: 22.28110919713974\n",
      "Training FF2:  28%|██████▍                | 903/3201 [24:38<58:03,  1.52s/batch]Batch 900/3201 Done, mean position loss: 22.79367259979248\n",
      "Training FF2:  29%|██████▌                | 914/3201 [24:38<57:36,  1.51s/batch]Batch 900/3201 Done, mean position loss: 21.59903468847275\n",
      "Training FF2:  29%|██████▌                | 915/3201 [24:41<58:39,  1.54s/batch]Batch 900/3201 Done, mean position loss: 22.175778052806855\n",
      "Training FF2:  28%|█████▊               | 894/3201 [24:46<1:04:39,  1.68s/batch]Batch 900/3201 Done, mean position loss: 21.77254030227661\n",
      "Training FF2:  29%|██████               | 930/3201 [24:49<1:00:51,  1.61s/batch]Batch 900/3201 Done, mean position loss: 22.102931938171388\n",
      "Training FF2:  29%|██████▌                | 918/3201 [24:48<58:05,  1.53s/batch]Batch 900/3201 Done, mean position loss: 22.068013949394228\n",
      "Training FF2:  29%|██████               | 929/3201 [24:57<1:00:12,  1.59s/batch]Batch 900/3201 Done, mean position loss: 22.157324802875518\n",
      "Training FF2:  29%|██████               | 924/3201 [24:57<1:04:17,  1.69s/batch]Batch 900/3201 Done, mean position loss: 22.003348009586333\n",
      "Training FF2:  29%|██████▌                | 922/3201 [25:00<58:33,  1.54s/batch]Batch 900/3201 Done, mean position loss: 22.098286447525027\n",
      "Training FF2:  31%|███████                | 980/3201 [26:35<58:33,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 22.795267684459688\n",
      "Training FF2:  31%|███████                | 989/3201 [26:37<59:33,  1.62s/batch]Batch 1000/3201 Done, mean position loss: 22.10441563844681\n",
      "Training FF2:  31%|███████                | 979/3201 [26:40<59:35,  1.61s/batch]Batch 1000/3201 Done, mean position loss: 22.02663381576538\n",
      "Training FF2:  31%|██████▌              | 992/3201 [26:41<1:00:04,  1.63s/batch]Batch 1000/3201 Done, mean position loss: 23.40335904121399\n",
      "Training FF2:  31%|██████▉               | 1004/3201 [26:42<57:57,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 22.127081711292266\n",
      "Training FF2:  31%|███████▏               | 995/3201 [26:45<57:06,  1.55s/batch]Batch 1000/3201 Done, mean position loss: 21.962242801189422\n",
      "Training FF2:  30%|██████▉                | 967/3201 [26:45<55:54,  1.50s/batch]Batch 1000/3201 Done, mean position loss: 21.793947720527648\n",
      "Training FF2:  31%|███████                | 988/3201 [26:49<59:20,  1.61s/batch]Batch 1000/3201 Done, mean position loss: 22.17835282802582\n",
      "Training FF2:  31%|███████▏               | 996/3201 [26:50<57:07,  1.55s/batch]Batch 1000/3201 Done, mean position loss: 22.36115296602249\n",
      "Training FF2:  31%|██████▌              | 999/3201 [26:50<1:09:13,  1.89s/batch]Batch 1000/3201 Done, mean position loss: 21.39487898826599\n",
      "Training FF2:  31%|███████                | 986/3201 [26:51<55:44,  1.51s/batch]Batch 1000/3201 Done, mean position loss: 21.9079851603508\n",
      "Training FF2:  31%|██████▉               | 1008/3201 [26:54<55:43,  1.52s/batch]Batch 1000/3201 Done, mean position loss: 21.77159300327301\n",
      "Training FF2:  32%|██████▎             | 1010/3201 [26:55<1:01:40,  1.69s/batch]Batch 1000/3201 Done, mean position loss: 21.936445035934447\n",
      "Training FF2:  31%|██████▉               | 1003/3201 [26:54<58:42,  1.60s/batch]Batch 1000/3201 Done, mean position loss: 21.949469137191773\n",
      "Training FF2:  32%|██████▎             | 1011/3201 [26:54<1:08:00,  1.86s/batch]Batch 1000/3201 Done, mean position loss: 21.99223207950592\n",
      "Training FF2:  31%|███████▏               | 998/3201 [26:56<59:35,  1.62s/batch]Batch 1000/3201 Done, mean position loss: 22.144268014431\n",
      "Training FF2:  31%|███████                | 984/3201 [26:55<56:40,  1.53s/batch]Batch 1000/3201 Done, mean position loss: 21.228923499584198\n",
      "Training FF2:  31%|██████▌              | 995/3201 [26:58<1:05:58,  1.79s/batch]Batch 1000/3201 Done, mean position loss: 21.964126048088076\n",
      "Training FF2:  31%|██████▉               | 1004/3201 [26:58<54:34,  1.49s/batch]Batch 1000/3201 Done, mean position loss: 22.22496612548828\n",
      "Training FF2:  31%|██████▎             | 1005/3201 [26:59<1:01:34,  1.68s/batch]Batch 1000/3201 Done, mean position loss: 21.919871106147767\n",
      "Training FF2:  31%|██████▉               | 1002/3201 [27:00<54:33,  1.49s/batch]Batch 1000/3201 Done, mean position loss: 22.91054227590561\n",
      "Training FF2:  31%|██████▎             | 1005/3201 [27:01<1:02:55,  1.72s/batch]Batch 1000/3201 Done, mean position loss: 21.856029839515685\n",
      "Training FF2:  32%|██████▎             | 1015/3201 [27:03<1:02:42,  1.72s/batch]Batch 1000/3201 Done, mean position loss: 22.56707746267319\n",
      "Training FF2:  31%|██████▎             | 1004/3201 [27:03<1:03:12,  1.73s/batch]Batch 1000/3201 Done, mean position loss: 22.276544618606568\n",
      "Training FF2:  31%|██████▉               | 1007/3201 [27:04<51:37,  1.41s/batch]Batch 1000/3201 Done, mean position loss: 21.945125360488895\n",
      "Training FF2:  32%|██████▉               | 1012/3201 [27:07<48:32,  1.33s/batch]Batch 1000/3201 Done, mean position loss: 21.55449942111969\n",
      "Batch 1000/3201 Done, mean position loss: 21.586380116939544\n",
      "Batch 1000/3201 Done, mean position loss: 21.845136075019838\n",
      "Training FF2:  32%|██████▉               | 1011/3201 [27:10<57:40,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 22.37492642402649\n",
      "Training FF2:  32%|██████▉               | 1014/3201 [27:11<53:38,  1.47s/batch]Batch 1000/3201 Done, mean position loss: 21.326884150505066\n",
      "Training FF2:  31%|███████                | 986/3201 [27:15<55:31,  1.50s/batch]Batch 1000/3201 Done, mean position loss: 22.171744434833528\n",
      "Training FF2:  32%|██████▎             | 1010/3201 [27:19<1:03:53,  1.75s/batch]Batch 1000/3201 Done, mean position loss: 22.682299926280976\n",
      "Training FF2:  32%|███████               | 1026/3201 [27:20<53:19,  1.47s/batch]Batch 1000/3201 Done, mean position loss: 21.516318013668062\n",
      "Training FF2:  32%|██████▎             | 1015/3201 [27:21<1:04:25,  1.77s/batch]Batch 1000/3201 Done, mean position loss: 22.015672464370727\n",
      "Training FF2:  32%|██████▉               | 1011/3201 [27:22<57:14,  1.57s/batch]Batch 1000/3201 Done, mean position loss: 21.67577697277069\n",
      "Training FF2:  32%|██████▉               | 1010/3201 [27:25<57:18,  1.57s/batch]Batch 1000/3201 Done, mean position loss: 22.046281304359436\n",
      "Training FF2:  31%|██████▎             | 1006/3201 [27:28<1:08:03,  1.86s/batch]Batch 1000/3201 Done, mean position loss: 21.984943602085114\n",
      "Training FF2:  32%|██████▍             | 1025/3201 [27:33<1:03:57,  1.76s/batch]Batch 1000/3201 Done, mean position loss: 21.985069751739502\n",
      "Training FF2:  32%|███████               | 1031/3201 [27:38<51:56,  1.44s/batch]Batch 1000/3201 Done, mean position loss: 22.017923626899716\n",
      "Training FF2:  32%|██████▎             | 1010/3201 [27:38<1:00:25,  1.65s/batch]Batch 1000/3201 Done, mean position loss: 21.888449544906617\n",
      "Training FF2:  34%|███████▍              | 1082/3201 [29:16<56:33,  1.60s/batch]Batch 1100/3201 Done, mean position loss: 22.049257271289825\n",
      "Training FF2:  34%|██████▊             | 1081/3201 [29:21<1:04:10,  1.82s/batch]Batch 1100/3201 Done, mean position loss: 22.656265830993654\n",
      "Training FF2:  34%|███████▌              | 1094/3201 [29:22<54:10,  1.54s/batch]Batch 1100/3201 Done, mean position loss: 22.21229105710983\n",
      "Training FF2:  33%|██████▋             | 1070/3201 [29:22<1:03:02,  1.78s/batch]Batch 1100/3201 Done, mean position loss: 21.9977467417717\n",
      "Training FF2:  34%|███████▌              | 1093/3201 [29:23<57:13,  1.63s/batch]Batch 1100/3201 Done, mean position loss: 21.8856151843071\n",
      "Training FF2:  34%|███████▍              | 1082/3201 [29:26<54:30,  1.54s/batch]Batch 1100/3201 Done, mean position loss: 21.919940080642704\n",
      "Training FF2:  34%|███████▌              | 1099/3201 [29:27<54:11,  1.55s/batch]Batch 1100/3201 Done, mean position loss: 23.24735990524292\n",
      "Training FF2:  34%|██████▋             | 1077/3201 [29:27<1:02:02,  1.75s/batch]Batch 1100/3201 Done, mean position loss: 22.059188492298127\n",
      "Training FF2:  34%|███████▍              | 1090/3201 [29:28<55:18,  1.57s/batch]Batch 1100/3201 Done, mean position loss: 21.701362979412078\n",
      "Training FF2:  34%|███████▌              | 1104/3201 [29:30<46:56,  1.34s/batch]Batch 1100/3201 Done, mean position loss: 21.70492889404297\n",
      "Training FF2:  34%|███████▌              | 1093/3201 [29:33<54:14,  1.54s/batch]Batch 1100/3201 Done, mean position loss: 21.844758217334746\n",
      "Training FF2:  35%|███████▌              | 1106/3201 [29:36<51:41,  1.48s/batch]Batch 1100/3201 Done, mean position loss: 22.031033599376677\n",
      "Training FF2:  35%|███████▋              | 1110/3201 [29:35<52:44,  1.51s/batch]Batch 1100/3201 Done, mean position loss: 21.203178095817563\n",
      "Training FF2:  35%|███████▌              | 1107/3201 [29:37<49:40,  1.42s/batch]Batch 1100/3201 Done, mean position loss: 21.329763927459716\n",
      "Training FF2:  35%|███████▌              | 1109/3201 [29:40<50:09,  1.44s/batch]Batch 1100/3201 Done, mean position loss: 21.82380341529846\n",
      "Training FF2:  34%|███████▍              | 1091/3201 [29:39<54:53,  1.56s/batch]Batch 1100/3201 Done, mean position loss: 22.433302803039552\n",
      "Training FF2:  35%|██████▉             | 1111/3201 [29:40<1:15:09,  2.16s/batch]Batch 1100/3201 Done, mean position loss: 22.81740760087967\n",
      "Training FF2:  34%|██████▊             | 1092/3201 [29:42<1:03:27,  1.81s/batch]Batch 1100/3201 Done, mean position loss: 21.868679282665255\n",
      "Training FF2:  34%|███████▍              | 1076/3201 [29:41<52:42,  1.49s/batch]Batch 1100/3201 Done, mean position loss: 21.925452914237976\n",
      "Training FF2:  34%|██████▊             | 1096/3201 [29:43<1:00:00,  1.71s/batch]Batch 1100/3201 Done, mean position loss: 21.847855842113496\n",
      "Training FF2:  34%|███████▍              | 1077/3201 [29:43<54:34,  1.54s/batch]Batch 1100/3201 Done, mean position loss: 21.859522919654843\n",
      "Training FF2:  35%|███████▋              | 1114/3201 [29:47<51:40,  1.49s/batch]Batch 1100/3201 Done, mean position loss: 22.15623102426529\n",
      "Training FF2:  34%|██████▊             | 1084/3201 [29:46<1:02:51,  1.78s/batch]Batch 1100/3201 Done, mean position loss: 21.767288782596587\n",
      "Training FF2:  34%|███████▌              | 1100/3201 [29:49<52:34,  1.50s/batch]Batch 1100/3201 Done, mean position loss: 21.813591258525847\n",
      "Training FF2:  35%|███████▋              | 1115/3201 [29:51<44:49,  1.29s/batch]Batch 1100/3201 Done, mean position loss: 21.446875324249266\n",
      "Batch 1100/3201 Done, mean position loss: 21.917059519290923\n",
      "Training FF2:  35%|███████▌              | 1106/3201 [29:51<53:20,  1.53s/batch]Batch 1100/3201 Done, mean position loss: 22.167828190326688\n",
      "Training FF2:  34%|███████▍              | 1090/3201 [29:51<57:11,  1.63s/batch]Batch 1100/3201 Done, mean position loss: 22.25649801254272\n",
      "Training FF2:  34%|███████▌              | 1103/3201 [29:56<53:09,  1.52s/batch]Batch 1100/3201 Done, mean position loss: 21.504369041919706\n",
      "Training FF2:  35%|███████▌              | 1106/3201 [29:55<56:53,  1.63s/batch]Batch 1100/3201 Done, mean position loss: 21.267856724262238\n",
      "Training FF2:  34%|███████▌              | 1098/3201 [29:57<56:33,  1.61s/batch]Batch 1100/3201 Done, mean position loss: 22.60500418663025\n",
      "Training FF2:  35%|███████▋              | 1124/3201 [30:02<59:05,  1.71s/batch]Batch 1100/3201 Done, mean position loss: 22.06532999277115\n",
      "Batch 1100/3201 Done, mean position loss: 21.979235000610352\n",
      "Training FF2:  35%|███████▋              | 1117/3201 [30:04<48:57,  1.41s/batch]Batch 1100/3201 Done, mean position loss: 21.927993388175963\n",
      "Training FF2:  35%|███████▋              | 1126/3201 [30:09<57:20,  1.66s/batch]Batch 1100/3201 Done, mean position loss: 21.524960227012635\n",
      "Training FF2:  35%|███████▊              | 1129/3201 [30:10<56:58,  1.65s/batch]Batch 1100/3201 Done, mean position loss: 21.439572284221647\n",
      "Training FF2:  35%|███████▊              | 1135/3201 [30:16<59:01,  1.71s/batch]Batch 1100/3201 Done, mean position loss: 21.92358205795288\n",
      "Training FF2:  34%|██████▉             | 1101/3201 [30:16<1:03:30,  1.81s/batch]Batch 1100/3201 Done, mean position loss: 21.94115044116974\n",
      "Training FF2:  35%|███████             | 1123/3201 [30:19<1:01:41,  1.78s/batch]Batch 1100/3201 Done, mean position loss: 21.880447392463683\n",
      "Training FF2:  35%|██████▉             | 1113/3201 [30:25<1:04:39,  1.86s/batch]Batch 1100/3201 Done, mean position loss: 21.80798827648163\n",
      "Training FF2:  37%|████████▏             | 1192/3201 [32:01<50:50,  1.52s/batch]Batch 1200/3201 Done, mean position loss: 22.000455148220063\n",
      "Training FF2:  37%|████████              | 1176/3201 [32:02<50:35,  1.50s/batch]Batch 1200/3201 Done, mean position loss: 21.657052149772646\n",
      "Training FF2:  36%|███████▎            | 1164/3201 [32:03<1:06:44,  1.97s/batch]Batch 1200/3201 Done, mean position loss: 22.033162302970887\n",
      "Training FF2:  37%|████████▏             | 1196/3201 [32:05<56:28,  1.69s/batch]Batch 1200/3201 Done, mean position loss: 21.615553596019744\n",
      "Training FF2:  37%|████████▏             | 1191/3201 [32:06<47:29,  1.42s/batch]Batch 1200/3201 Done, mean position loss: 22.576115880012512\n",
      "Training FF2:  37%|████████▏             | 1196/3201 [32:09<45:59,  1.38s/batch]Batch 1200/3201 Done, mean position loss: 21.931572852134707\n",
      "Training FF2:  37%|███████▍            | 1197/3201 [32:10<1:01:27,  1.84s/batch]Batch 1200/3201 Done, mean position loss: 21.825524604320528\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:10<46:17,  1.39s/batch]Batch 1200/3201 Done, mean position loss: 21.964086401462552\n",
      "Training FF2:  38%|████████▎             | 1202/3201 [32:11<54:21,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.92732625722885\n",
      "Training FF2:  37%|████████▏             | 1196/3201 [32:13<58:28,  1.75s/batch]Batch 1200/3201 Done, mean position loss: 23.12124434709549\n",
      "Training FF2:  37%|████████▏             | 1199/3201 [32:14<58:16,  1.75s/batch]Batch 1200/3201 Done, mean position loss: 21.78791902780533\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:15<48:01,  1.44s/batch]Batch 1200/3201 Done, mean position loss: 21.166297435760498\n",
      "Training FF2:  37%|████████▏             | 1186/3201 [32:16<44:02,  1.31s/batch]Batch 1200/3201 Done, mean position loss: 22.311845293045046\n",
      "Training FF2:  37%|████████▏             | 1191/3201 [32:18<50:38,  1.51s/batch]Batch 1200/3201 Done, mean position loss: 22.724306511878968\n",
      "Training FF2:  37%|████████▏             | 1194/3201 [32:18<53:37,  1.60s/batch]Batch 1200/3201 Done, mean position loss: 21.722235715389253\n",
      "Training FF2:  37%|████████              | 1180/3201 [32:17<59:29,  1.77s/batch]Batch 1200/3201 Done, mean position loss: 21.77432478666306\n",
      "Training FF2:  37%|████████▏             | 1199/3201 [32:18<47:07,  1.41s/batch]Batch 1200/3201 Done, mean position loss: 21.747386472225188\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:21<45:39,  1.37s/batch]Batch 1200/3201 Done, mean position loss: 21.9107528591156\n",
      "Batch 1200/3201 Done, mean position loss: 21.78889569997787\n",
      "Training FF2:  37%|████████              | 1171/3201 [32:21<59:35,  1.76s/batch]Batch 1200/3201 Done, mean position loss: 21.303247146606445\n",
      "Training FF2:  38%|████████▎             | 1205/3201 [32:26<47:09,  1.42s/batch]Batch 1200/3201 Done, mean position loss: 21.874643671512604\n",
      "Training FF2:  38%|████████▎             | 1201/3201 [32:27<50:35,  1.52s/batch]Batch 1200/3201 Done, mean position loss: 21.81435647964478\n",
      "Training FF2:  37%|████████▏             | 1187/3201 [32:29<52:19,  1.56s/batch]Batch 1200/3201 Done, mean position loss: 22.175418703556062\n",
      "Training FF2:  38%|███████▌            | 1216/3201 [32:30<1:01:26,  1.86s/batch]Batch 1200/3201 Done, mean position loss: 21.650907623767853\n",
      "Training FF2:  37%|████████▏             | 1199/3201 [32:31<54:25,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.3547438788414\n",
      "Training FF2:  37%|████████▏             | 1198/3201 [32:35<48:51,  1.46s/batch]Batch 1200/3201 Done, mean position loss: 22.103038911819457\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:34<54:20,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.48141389608383\n",
      "Training FF2:  37%|████████▏             | 1189/3201 [32:37<56:28,  1.68s/batch]Batch 1200/3201 Done, mean position loss: 22.02807557344437\n",
      "Training FF2:  38%|███████▌            | 1201/3201 [32:39<1:02:20,  1.87s/batch]Batch 1200/3201 Done, mean position loss: 21.212815618515013\n",
      "Training FF2:  37%|████████▏             | 1186/3201 [32:39<52:02,  1.55s/batch]Batch 1200/3201 Done, mean position loss: 22.49042680978775\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:41<53:47,  1.61s/batch]Batch 1200/3201 Done, mean position loss: 21.74715172767639\n",
      "Training FF2:  38%|████████▎             | 1208/3201 [32:43<50:51,  1.53s/batch]Batch 1200/3201 Done, mean position loss: 21.875838627815245\n",
      "Training FF2:  37%|████████▏             | 1197/3201 [32:45<57:22,  1.72s/batch]Batch 1200/3201 Done, mean position loss: 21.967561643123624\n",
      "Training FF2:  38%|████████▍             | 1221/3201 [32:48<50:39,  1.54s/batch]Batch 1200/3201 Done, mean position loss: 21.904037196636203\n",
      "Training FF2:  38%|████████▎             | 1215/3201 [32:52<51:12,  1.55s/batch]Batch 1200/3201 Done, mean position loss: 21.490054991245273\n",
      "Training FF2:  38%|████████▍             | 1232/3201 [32:52<53:35,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.392352430820466\n",
      "Training FF2:  38%|████████▍             | 1225/3201 [32:57<52:17,  1.59s/batch]Batch 1200/3201 Done, mean position loss: 21.845772206783295\n",
      "Training FF2:  38%|████████▎             | 1208/3201 [33:03<52:50,  1.59s/batch]Batch 1200/3201 Done, mean position loss: 21.876692876815795\n",
      "Training FF2:  38%|███████▋            | 1229/3201 [33:05<1:02:29,  1.90s/batch]Batch 1200/3201 Done, mean position loss: 21.783843085765838\n",
      "Training FF2:  38%|████████▎             | 1218/3201 [33:11<51:15,  1.55s/batch]Batch 1200/3201 Done, mean position loss: 21.70664714097977\n",
      "Training FF2:  39%|████████▋             | 1259/3201 [34:39<56:35,  1.75s/batch]Batch 1300/3201 Done, mean position loss: 21.891463425159454\n",
      "Training FF2:  40%|████████▉             | 1292/3201 [34:43<48:41,  1.53s/batch]Batch 1300/3201 Done, mean position loss: 21.539607765674592\n",
      "Training FF2:  40%|████████▉             | 1294/3201 [34:47<45:14,  1.42s/batch]Batch 1300/3201 Done, mean position loss: 21.926377975940703\n",
      "Training FF2:  40%|████████▉             | 1292/3201 [34:46<55:53,  1.76s/batch]Batch 1300/3201 Done, mean position loss: 22.514873337745666\n",
      "Training FF2:  41%|████████▉             | 1298/3201 [34:47<50:56,  1.61s/batch]Batch 1300/3201 Done, mean position loss: 21.61305867910385\n",
      "Training FF2:  40%|████████▊             | 1283/3201 [34:52<47:55,  1.50s/batch]Batch 1300/3201 Done, mean position loss: 21.11343350172043\n",
      "Training FF2:  40%|████████            | 1286/3201 [34:52<1:00:03,  1.88s/batch]Batch 1300/3201 Done, mean position loss: 21.802236328124998\n",
      "Training FF2:  40%|████████▉             | 1295/3201 [34:52<52:19,  1.65s/batch]Batch 1300/3201 Done, mean position loss: 21.924949216842652\n",
      "Training FF2:  40%|████████            | 1289/3201 [34:54<1:04:56,  2.04s/batch]Batch 1300/3201 Done, mean position loss: 21.871049787998203\n",
      "Training FF2:  41%|████████            | 1300/3201 [34:54<1:00:06,  1.90s/batch]Batch 1300/3201 Done, mean position loss: 21.853853743076325\n",
      "Training FF2:  40%|████████▊             | 1289/3201 [34:55<54:35,  1.71s/batch]Batch 1300/3201 Done, mean position loss: 21.730962383747098\n",
      "Training FF2:  40%|███████▉            | 1276/3201 [34:57<1:03:51,  1.99s/batch]Batch 1300/3201 Done, mean position loss: 21.715666997432706\n",
      "Training FF2:  41%|████████▉             | 1302/3201 [34:58<57:13,  1.81s/batch]Batch 1300/3201 Done, mean position loss: 23.049941494464875\n",
      "Training FF2:  41%|████████▉             | 1298/3201 [34:59<56:20,  1.78s/batch]Batch 1300/3201 Done, mean position loss: 22.143027861118313\n",
      "Training FF2:  41%|████████▉             | 1306/3201 [34:59<45:40,  1.45s/batch]Batch 1300/3201 Done, mean position loss: 21.737453172206877\n",
      "Training FF2:  40%|████████▊             | 1288/3201 [35:01<50:21,  1.58s/batch]Batch 1300/3201 Done, mean position loss: 21.262199802398683\n",
      "Training FF2:  40%|████████▊             | 1288/3201 [35:00<57:31,  1.80s/batch]Batch 1300/3201 Done, mean position loss: 22.669021487236023\n",
      "Training FF2:  40%|████████▊             | 1289/3201 [35:04<56:36,  1.78s/batch]Batch 1300/3201 Done, mean position loss: 21.65506237745285\n",
      "Training FF2:  41%|████████▉             | 1307/3201 [35:04<51:52,  1.64s/batch]Batch 1300/3201 Done, mean position loss: 21.830661294460295\n",
      "Training FF2:  41%|████████▉             | 1297/3201 [35:04<47:11,  1.49s/batch]Batch 1300/3201 Done, mean position loss: 21.735209679603578\n",
      "Training FF2:  41%|█████████             | 1313/3201 [35:06<53:59,  1.72s/batch]Batch 1300/3201 Done, mean position loss: 21.75792001247406\n",
      "Training FF2:  41%|████████▉             | 1300/3201 [35:11<53:15,  1.68s/batch]Batch 1300/3201 Done, mean position loss: 21.25489722251892\n",
      "Training FF2:  40%|████████▊             | 1276/3201 [35:11<47:56,  1.49s/batch]Batch 1300/3201 Done, mean position loss: 21.82063107728958\n",
      "Training FF2:  40%|████████▉             | 1293/3201 [35:13<50:32,  1.59s/batch]Batch 1300/3201 Done, mean position loss: 22.144619235992433\n",
      "Training FF2:  41%|████████▉             | 1299/3201 [35:13<55:23,  1.75s/batch]Batch 1300/3201 Done, mean position loss: 21.584910516738894\n",
      "Training FF2:  40%|████████▊             | 1281/3201 [35:14<46:49,  1.46s/batch]Batch 1300/3201 Done, mean position loss: 21.489557802677155\n",
      "Training FF2:  40%|████████▊             | 1282/3201 [35:16<48:40,  1.52s/batch]Batch 1300/3201 Done, mean position loss: 21.179364457130433\n",
      "Training FF2:  40%|████████▉             | 1295/3201 [35:16<52:39,  1.66s/batch]Batch 1300/3201 Done, mean position loss: 22.0247225689888\n",
      "Training FF2:  41%|█████████             | 1323/3201 [35:23<54:30,  1.74s/batch]Batch 1300/3201 Done, mean position loss: 22.39405771970749\n",
      "Training FF2:  41%|████████▉             | 1305/3201 [35:23<51:42,  1.64s/batch]Batch 1300/3201 Done, mean position loss: 21.823462829589843\n",
      "Training FF2:  41%|█████████             | 1321/3201 [35:23<53:34,  1.71s/batch]Batch 1300/3201 Done, mean position loss: 21.713964059352875\n",
      "Training FF2:  40%|████████▉             | 1294/3201 [35:24<48:18,  1.52s/batch]Batch 1300/3201 Done, mean position loss: 21.924540765285492\n",
      "Training FF2:  41%|█████████             | 1310/3201 [35:27<48:28,  1.54s/batch]Batch 1300/3201 Done, mean position loss: 21.86225227832794\n",
      "Training FF2:  41%|█████████             | 1314/3201 [35:33<44:36,  1.42s/batch]Batch 1300/3201 Done, mean position loss: 21.44285386800766\n",
      "Training FF2:  41%|████████▉             | 1302/3201 [35:35<41:36,  1.31s/batch]Batch 1300/3201 Done, mean position loss: 21.880536375045775\n",
      "Training FF2:  41%|█████████             | 1326/3201 [35:40<51:53,  1.66s/batch]Batch 1300/3201 Done, mean position loss: 21.313102197647094\n",
      "Training FF2:  42%|█████████▏            | 1329/3201 [35:45<41:27,  1.33s/batch]Batch 1300/3201 Done, mean position loss: 21.80294214487076\n",
      "Training FF2:  41%|████████▉             | 1299/3201 [35:48<52:33,  1.66s/batch]Batch 1300/3201 Done, mean position loss: 21.806055722236636\n",
      "Training FF2:  42%|████████▎           | 1333/3201 [35:50<1:02:48,  2.02s/batch]Batch 1300/3201 Done, mean position loss: 21.70942341327667\n",
      "Training FF2:  41%|█████████             | 1324/3201 [35:51<51:32,  1.65s/batch]Batch 1300/3201 Done, mean position loss: 21.618583562374113\n",
      "Training FF2:  44%|█████████▌            | 1395/3201 [37:24<46:45,  1.55s/batch]Batch 1400/3201 Done, mean position loss: 21.778246095180513\n",
      "Training FF2:  44%|█████████▌            | 1396/3201 [37:25<45:44,  1.52s/batch]Batch 1400/3201 Done, mean position loss: 21.501263871192933\n",
      "Training FF2:  44%|█████████▌            | 1393/3201 [37:29<47:59,  1.59s/batch]Batch 1400/3201 Done, mean position loss: 21.58664141178131\n",
      "Training FF2:  44%|█████████▌            | 1397/3201 [37:31<48:42,  1.62s/batch]Batch 1400/3201 Done, mean position loss: 21.734957408905032\n",
      "Training FF2:  43%|█████████▍            | 1367/3201 [37:32<56:39,  1.85s/batch]Batch 1400/3201 Done, mean position loss: 21.862482025623322\n",
      "Training FF2:  43%|█████████▌            | 1391/3201 [37:33<56:41,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 21.06831853866577\n",
      "Training FF2:  43%|█████████▌            | 1387/3201 [37:34<44:12,  1.46s/batch]Batch 1400/3201 Done, mean position loss: 22.399869544506075\n",
      "Training FF2:  44%|█████████▌            | 1397/3201 [37:35<45:20,  1.51s/batch]Batch 1400/3201 Done, mean position loss: 22.930096657276156\n",
      "Training FF2:  43%|█████████▌            | 1392/3201 [37:36<48:26,  1.61s/batch]Batch 1400/3201 Done, mean position loss: 21.765918569564818\n",
      "Training FF2:  44%|█████████▋            | 1408/3201 [37:41<45:38,  1.53s/batch]Batch 1400/3201 Done, mean position loss: 21.7773121881485\n",
      "Training FF2:  44%|█████████▋            | 1413/3201 [37:41<33:49,  1.13s/batch]Batch 1400/3201 Done, mean position loss: 21.877960665225984\n",
      "Training FF2:  44%|█████████▋            | 1407/3201 [37:44<44:06,  1.48s/batch]Batch 1400/3201 Done, mean position loss: 21.628214578628537\n",
      "Training FF2:  43%|█████████▌            | 1390/3201 [37:42<48:23,  1.60s/batch]Batch 1400/3201 Done, mean position loss: 21.70198203086853\n",
      "Training FF2:  44%|█████████▋            | 1407/3201 [37:42<45:18,  1.52s/batch]Batch 1400/3201 Done, mean position loss: 21.246673202514646\n",
      "Training FF2:  43%|█████████▍            | 1380/3201 [37:44<53:06,  1.75s/batch]Batch 1400/3201 Done, mean position loss: 21.673702733516695\n",
      "Training FF2:  43%|█████████▍            | 1369/3201 [37:43<45:59,  1.51s/batch]Batch 1400/3201 Done, mean position loss: 21.59327561855316\n",
      "Training FF2:  44%|█████████▌            | 1399/3201 [37:44<46:39,  1.55s/batch]Batch 1400/3201 Done, mean position loss: 22.10887642145157\n",
      "Training FF2:  44%|█████████▌            | 1398/3201 [37:45<51:19,  1.71s/batch]Batch 1400/3201 Done, mean position loss: 21.707710926532748\n",
      "Training FF2:  44%|█████████▋            | 1403/3201 [37:48<48:43,  1.63s/batch]Batch 1400/3201 Done, mean position loss: 21.74090322732925\n",
      "Training FF2:  44%|█████████▋            | 1405/3201 [37:49<54:50,  1.83s/batch]Batch 1400/3201 Done, mean position loss: 21.455896928310395\n",
      "Training FF2:  44%|█████████▋            | 1404/3201 [37:50<52:34,  1.76s/batch]Batch 1400/3201 Done, mean position loss: 21.719784960746768\n",
      "Training FF2:  44%|█████████▋            | 1411/3201 [37:50<55:07,  1.85s/batch]Batch 1400/3201 Done, mean position loss: 22.586799919605255\n",
      "Training FF2:  44%|█████████▋            | 1405/3201 [37:51<50:45,  1.70s/batch]Batch 1400/3201 Done, mean position loss: 21.765516099929812\n",
      "Training FF2:  44%|█████████▋            | 1407/3201 [37:54<53:02,  1.77s/batch]Batch 1400/3201 Done, mean position loss: 21.476772933006288\n",
      "Training FF2:  44%|█████████▋            | 1403/3201 [37:55<58:20,  1.95s/batch]Batch 1400/3201 Done, mean position loss: 21.180553681850434\n",
      "Training FF2:  44%|█████████▋            | 1411/3201 [37:57<39:47,  1.33s/batch]Batch 1400/3201 Done, mean position loss: 22.098210701942442\n",
      "Training FF2:  44%|█████████▋            | 1409/3201 [37:58<47:10,  1.58s/batch]Batch 1400/3201 Done, mean position loss: 21.16095135450363\n",
      "Training FF2:  44%|█████████▋            | 1411/3201 [37:57<48:47,  1.64s/batch]Batch 1400/3201 Done, mean position loss: 21.922983129024505\n",
      "Training FF2:  43%|█████████▍            | 1381/3201 [38:00<42:11,  1.39s/batch]Batch 1400/3201 Done, mean position loss: 21.65649655580521\n",
      "Training FF2:  44%|█████████▋            | 1411/3201 [38:08<53:51,  1.81s/batch]Batch 1400/3201 Done, mean position loss: 21.72827919960022\n",
      "Training FF2:  45%|█████████▊            | 1429/3201 [38:08<47:22,  1.60s/batch]Batch 1400/3201 Done, mean position loss: 21.76502620458603\n",
      "Training FF2:  44%|█████████▋            | 1403/3201 [38:09<44:57,  1.50s/batch]Batch 1400/3201 Done, mean position loss: 22.298241093158722\n",
      "Training FF2:  44%|█████████▋            | 1412/3201 [38:13<57:57,  1.94s/batch]Batch 1400/3201 Done, mean position loss: 21.818352286815646\n",
      "Training FF2:  45%|█████████▊            | 1434/3201 [38:17<58:43,  1.99s/batch]Batch 1400/3201 Done, mean position loss: 21.83521186828613\n",
      "Training FF2:  44%|█████████▊            | 1423/3201 [38:17<56:51,  1.92s/batch]Batch 1400/3201 Done, mean position loss: 21.41517838716507\n",
      "Training FF2:  44%|█████████▊            | 1424/3201 [38:23<47:47,  1.61s/batch]Batch 1400/3201 Done, mean position loss: 21.234623615741725\n",
      "Training FF2:  44%|█████████▊            | 1423/3201 [38:32<55:04,  1.86s/batch]Batch 1400/3201 Done, mean position loss: 21.767159235477447\n",
      "Training FF2:  45%|█████████▉            | 1438/3201 [38:36<56:44,  1.93s/batch]Batch 1400/3201 Done, mean position loss: 21.522109537124635\n",
      "Training FF2:  45%|█████████▊            | 1436/3201 [38:39<51:53,  1.76s/batch]Batch 1400/3201 Done, mean position loss: 21.792770724296567\n",
      "Training FF2:  45%|█████████▊            | 1435/3201 [38:38<54:47,  1.86s/batch]Batch 1400/3201 Done, mean position loss: 21.636221992969514\n",
      "Training FF2:  46%|██████████            | 1469/3201 [40:05<47:36,  1.65s/batch]Batch 1500/3201 Done, mean position loss: 21.7431272983551\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [40:03<51:22,  1.80s/batch]Batch 1500/3201 Done, mean position loss: 21.55474326133728\n",
      "Training FF2:  47%|██████████▎           | 1497/3201 [40:11<46:49,  1.65s/batch]Batch 1500/3201 Done, mean position loss: 21.694079618453976\n",
      "Training FF2:  46%|██████████            | 1459/3201 [40:13<46:42,  1.61s/batch]Batch 1500/3201 Done, mean position loss: 21.421096079349518\n",
      "Training FF2:  47%|██████████▎           | 1501/3201 [40:14<51:19,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 21.667107446193697\n",
      "Training FF2:  47%|██████████▍           | 1511/3201 [40:16<37:36,  1.34s/batch]Batch 1500/3201 Done, mean position loss: 21.862072381973267\n",
      "Training FF2:  47%|██████████▎           | 1499/3201 [40:19<46:45,  1.65s/batch]Batch 1500/3201 Done, mean position loss: 21.649066405296324\n",
      "Training FF2:  46%|██████████▏           | 1486/3201 [40:19<47:01,  1.64s/batch]Batch 1500/3201 Done, mean position loss: 21.038417961597442\n",
      "Training FF2:  46%|██████████▏           | 1478/3201 [40:20<40:52,  1.42s/batch]Batch 1500/3201 Done, mean position loss: 22.849451138973237\n",
      "Training FF2:  46%|██████████▏           | 1484/3201 [40:20<45:38,  1.59s/batch]Batch 1500/3201 Done, mean position loss: 21.738285291194916\n",
      "Training FF2:  47%|██████████▎           | 1500/3201 [40:21<50:24,  1.78s/batch]Batch 1500/3201 Done, mean position loss: 21.684240550994872\n",
      "Training FF2:  46%|██████████            | 1464/3201 [40:22<43:22,  1.50s/batch]Batch 1500/3201 Done, mean position loss: 22.330371158123018\n",
      "Training FF2:  46%|██████████▏           | 1481/3201 [40:24<48:22,  1.69s/batch]Batch 1500/3201 Done, mean position loss: 21.208036215305327\n",
      "Training FF2:  47%|██████████▎           | 1503/3201 [40:25<41:53,  1.48s/batch]Batch 1500/3201 Done, mean position loss: 21.61067085981369\n",
      "Training FF2:  47%|██████████▍           | 1516/3201 [40:28<43:33,  1.55s/batch]Batch 1500/3201 Done, mean position loss: 21.673839428424834\n",
      "Training FF2:  47%|██████████▎           | 1505/3201 [40:27<48:02,  1.70s/batch]Batch 1500/3201 Done, mean position loss: 21.70686266899109\n",
      "Training FF2:  47%|██████████▎           | 1507/3201 [40:30<35:20,  1.25s/batch]Batch 1500/3201 Done, mean position loss: 21.58335693359375\n",
      "Training FF2:  47%|██████████▎           | 1497/3201 [40:31<55:31,  1.96s/batch]Batch 1500/3201 Done, mean position loss: 21.835835258960724\n",
      "Training FF2:  47%|██████████▏           | 1491/3201 [40:31<40:30,  1.42s/batch]Batch 1500/3201 Done, mean position loss: 21.1480370593071\n",
      "Training FF2:  47%|██████████▎           | 1509/3201 [40:33<49:59,  1.77s/batch]Batch 1500/3201 Done, mean position loss: 22.530751502513883\n",
      "Training FF2:  46%|██████████            | 1473/3201 [40:32<53:36,  1.86s/batch]Batch 1500/3201 Done, mean position loss: 21.630886714458466\n",
      "Training FF2:  47%|██████████▎           | 1497/3201 [40:33<33:11,  1.17s/batch]Batch 1500/3201 Done, mean position loss: 22.025572872161867\n",
      "Training FF2:  47%|██████████▎           | 1500/3201 [40:37<55:59,  1.98s/batch]Batch 1500/3201 Done, mean position loss: 21.459239406585695\n",
      "Training FF2:  47%|██████████▍           | 1514/3201 [40:39<45:45,  1.63s/batch]Batch 1500/3201 Done, mean position loss: 21.65706428527832\n",
      "Training FF2:  46%|██████████▏           | 1479/3201 [40:39<36:45,  1.28s/batch]Batch 1500/3201 Done, mean position loss: 21.665003859996794\n",
      "Training FF2:  46%|██████████▏           | 1476/3201 [40:41<50:59,  1.77s/batch]Batch 1500/3201 Done, mean position loss: 21.37690556049347\n",
      "Training FF2:  47%|██████████▍           | 1510/3201 [40:42<48:24,  1.72s/batch]Batch 1500/3201 Done, mean position loss: 21.72334132909775\n",
      "Training FF2:  47%|██████████▍           | 1513/3201 [40:45<29:33,  1.05s/batch]Batch 1500/3201 Done, mean position loss: 22.086031594276427\n",
      "Training FF2:  47%|██████████▎           | 1504/3201 [40:43<48:00,  1.70s/batch]Batch 1500/3201 Done, mean position loss: 21.115625581741334\n",
      "Training FF2:  46%|█████████▏          | 1477/3201 [40:44<1:02:11,  2.16s/batch]Batch 1500/3201 Done, mean position loss: 22.17250143766403\n",
      "Training FF2:  47%|██████████▎           | 1496/3201 [40:48<38:26,  1.35s/batch]Batch 1500/3201 Done, mean position loss: 21.668445448875424\n",
      "Training FF2:  47%|██████████▍           | 1518/3201 [40:47<39:57,  1.42s/batch]Batch 1500/3201 Done, mean position loss: 21.671994340419772\n",
      "Training FF2:  48%|██████████▍           | 1525/3201 [40:53<37:55,  1.36s/batch]Batch 1500/3201 Done, mean position loss: 21.70369273900986\n",
      "Training FF2:  48%|██████████▍           | 1522/3201 [41:00<35:11,  1.26s/batch]Batch 1500/3201 Done, mean position loss: 21.806338133811952\n",
      "Training FF2:  47%|██████████▎           | 1493/3201 [41:01<47:48,  1.68s/batch]Batch 1500/3201 Done, mean position loss: 21.325440366268158\n",
      "Training FF2:  48%|██████████▌           | 1530/3201 [41:13<49:21,  1.77s/batch]Batch 1500/3201 Done, mean position loss: 21.215476155281067\n",
      "Training FF2:  48%|██████████▍           | 1523/3201 [41:14<40:55,  1.46s/batch]Batch 1500/3201 Done, mean position loss: 21.711007897853854\n",
      "Training FF2:  48%|██████████▌           | 1541/3201 [41:16<33:24,  1.21s/batch]Batch 1500/3201 Done, mean position loss: 21.512905144691466\n",
      "Training FF2:  47%|██████████▎           | 1506/3201 [41:21<57:38,  2.04s/batch]Batch 1500/3201 Done, mean position loss: 21.733404791355134\n",
      "Training FF2:  48%|██████████▌           | 1530/3201 [41:22<54:06,  1.94s/batch]Batch 1500/3201 Done, mean position loss: 21.59804633617401\n",
      "Training FF2:  50%|██████████▉           | 1586/3201 [42:44<49:27,  1.84s/batch]Batch 1600/3201 Done, mean position loss: 21.540415546894074\n",
      "Training FF2:  50%|██████████▉           | 1598/3201 [42:45<36:06,  1.35s/batch]Batch 1600/3201 Done, mean position loss: 21.699625301361085\n",
      "Training FF2:  50%|██████████▉           | 1589/3201 [42:50<43:15,  1.61s/batch]Batch 1600/3201 Done, mean position loss: 21.649545164108275\n",
      "Training FF2:  50%|██████████▉           | 1589/3201 [42:55<40:48,  1.52s/batch]Batch 1600/3201 Done, mean position loss: 21.60678495168686\n",
      "Training FF2:  50%|██████████▉           | 1594/3201 [42:58<50:24,  1.88s/batch]Batch 1600/3201 Done, mean position loss: 21.802113871574402\n",
      "Training FF2:  50%|██████████▉           | 1597/3201 [43:02<37:52,  1.42s/batch]Batch 1600/3201 Done, mean position loss: 21.354835875034333\n",
      "Training FF2:  50%|██████████▉           | 1598/3201 [43:03<46:32,  1.74s/batch]Batch 1600/3201 Done, mean position loss: 22.256695351600648\n",
      "Training FF2:  50%|██████████▉           | 1599/3201 [43:05<38:07,  1.43s/batch]Batch 1600/3201 Done, mean position loss: 21.593803911209108\n",
      "Training FF2:  49%|██████████▊           | 1566/3201 [43:03<40:07,  1.47s/batch]Batch 1600/3201 Done, mean position loss: 21.017929773330685\n",
      "Training FF2:  50%|██████████▉           | 1596/3201 [43:08<53:44,  2.01s/batch]Batch 1600/3201 Done, mean position loss: 21.6235271859169\n",
      "Training FF2:  50%|███████████           | 1604/3201 [43:08<40:28,  1.52s/batch]Batch 1600/3201 Done, mean position loss: 21.775133686065672\n",
      "Training FF2:  50%|██████████▉           | 1598/3201 [43:09<38:59,  1.46s/batch]Batch 1600/3201 Done, mean position loss: 21.63002815246582\n",
      "Training FF2:  50%|██████████▉           | 1590/3201 [43:09<41:51,  1.56s/batch]Batch 1600/3201 Done, mean position loss: 21.712449350357055\n",
      "Training FF2:  50%|██████████▉           | 1599/3201 [43:10<43:12,  1.62s/batch]Batch 1600/3201 Done, mean position loss: 21.550823476314545\n",
      "Training FF2:  50%|██████████▉           | 1591/3201 [43:11<41:47,  1.56s/batch]Batch 1600/3201 Done, mean position loss: 21.136383581161496\n",
      "Training FF2:  50%|███████████           | 1608/3201 [43:12<43:32,  1.64s/batch]Batch 1600/3201 Done, mean position loss: 22.767446575164794\n",
      "Batch 1600/3201 Done, mean position loss: 21.67824008703232\n",
      "Training FF2:  50%|███████████           | 1608/3201 [43:13<38:34,  1.45s/batch]Batch 1600/3201 Done, mean position loss: 21.56335682630539\n",
      "Training FF2:  50%|██████████▉           | 1593/3201 [43:14<44:51,  1.67s/batch]Batch 1600/3201 Done, mean position loss: 21.101313729286197\n",
      "Training FF2:  50%|███████████           | 1605/3201 [43:15<40:55,  1.54s/batch]Batch 1600/3201 Done, mean position loss: 22.555376842021943\n",
      "Training FF2:  50%|██████████▉           | 1595/3201 [43:17<45:14,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.434301788806913\n",
      "Training FF2:  50%|███████████           | 1610/3201 [43:18<47:36,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.577085709571836\n",
      "Training FF2:  50%|██████████▉           | 1591/3201 [43:20<37:50,  1.41s/batch]Batch 1600/3201 Done, mean position loss: 21.684836838245392\n",
      "Training FF2:  50%|███████████           | 1608/3201 [43:21<48:58,  1.84s/batch]Batch 1600/3201 Done, mean position loss: 21.580183787345888\n",
      "Training FF2:  50%|███████████           | 1615/3201 [43:23<41:08,  1.56s/batch]Batch 1600/3201 Done, mean position loss: 21.97137385845184\n",
      "Training FF2:  50%|███████████           | 1606/3201 [43:25<40:33,  1.53s/batch]Batch 1600/3201 Done, mean position loss: 21.306637489795683\n",
      "Training FF2:  50%|███████████           | 1610/3201 [43:25<39:19,  1.48s/batch]Batch 1600/3201 Done, mean position loss: 21.6144141292572\n",
      "Training FF2:  50%|██████████▉           | 1592/3201 [43:28<51:45,  1.93s/batch]Batch 1600/3201 Done, mean position loss: 21.08432018995285\n",
      "Training FF2:  51%|███████████▏          | 1629/3201 [43:28<40:01,  1.53s/batch]Batch 1600/3201 Done, mean position loss: 21.580567994117736\n",
      "Training FF2:  51%|███████████▏          | 1620/3201 [43:31<44:20,  1.68s/batch]Batch 1600/3201 Done, mean position loss: 21.58754215717316\n",
      "Training FF2:  51%|███████████▏          | 1619/3201 [43:31<37:40,  1.43s/batch]Batch 1600/3201 Done, mean position loss: 22.051216955184938\n",
      "Training FF2:  50%|██████████▉           | 1586/3201 [43:32<40:04,  1.49s/batch]Batch 1600/3201 Done, mean position loss: 22.02795823097229\n",
      "Training FF2:  51%|███████████▏          | 1632/3201 [43:36<48:25,  1.85s/batch]Batch 1600/3201 Done, mean position loss: 21.62787301540375\n",
      "Training FF2:  50%|██████████▉           | 1589/3201 [43:41<44:22,  1.65s/batch]Batch 1600/3201 Done, mean position loss: 21.32167945623398\n",
      "Training FF2:  50%|██████████▉           | 1591/3201 [43:43<44:13,  1.65s/batch]Batch 1600/3201 Done, mean position loss: 21.755816090106965\n",
      "Training FF2:  51%|███████████▏          | 1625/3201 [43:57<37:07,  1.41s/batch]Batch 1600/3201 Done, mean position loss: 21.13442324638367\n",
      "Training FF2:  51%|███████████▏          | 1629/3201 [44:01<40:17,  1.54s/batch]Batch 1600/3201 Done, mean position loss: 21.676547043323517\n",
      "Training FF2:  51%|███████████▎          | 1640/3201 [44:01<43:08,  1.66s/batch]Batch 1600/3201 Done, mean position loss: 21.531873087882996\n",
      "Training FF2:  50%|██████████▉           | 1598/3201 [44:06<50:39,  1.90s/batch]Batch 1600/3201 Done, mean position loss: 21.64373034000397\n",
      "Training FF2:  52%|███████████▎          | 1655/3201 [44:11<47:16,  1.83s/batch]Batch 1600/3201 Done, mean position loss: 21.52779682636261\n",
      "Training FF2:  53%|███████████▌          | 1684/3201 [45:22<44:36,  1.76s/batch]Batch 1700/3201 Done, mean position loss: 21.614435958862305\n",
      "Training FF2:  53%|███████████▌          | 1690/3201 [45:28<45:05,  1.79s/batch]Batch 1700/3201 Done, mean position loss: 21.493084828853608\n",
      "Training FF2:  53%|███████████▌          | 1689/3201 [45:29<34:24,  1.37s/batch]Batch 1700/3201 Done, mean position loss: 21.601057832241057\n",
      "Training FF2:  52%|███████████▍          | 1660/3201 [45:36<40:51,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 21.57342336893082\n",
      "Training FF2:  53%|███████████▌          | 1686/3201 [45:39<46:05,  1.83s/batch]Batch 1700/3201 Done, mean position loss: 22.1979634809494\n",
      "Training FF2:  53%|███████████▋          | 1695/3201 [45:40<36:33,  1.46s/batch]Batch 1700/3201 Done, mean position loss: 21.7550799202919\n",
      "Training FF2:  53%|███████████▋          | 1698/3201 [45:40<43:42,  1.74s/batch]Batch 1700/3201 Done, mean position loss: 21.29662393569946\n",
      "Training FF2:  53%|███████████▌          | 1683/3201 [45:44<45:04,  1.78s/batch]Batch 1700/3201 Done, mean position loss: 21.723181424140932\n",
      "Training FF2:  53%|███████████▋          | 1695/3201 [45:46<38:27,  1.53s/batch]Batch 1700/3201 Done, mean position loss: 21.55607279777527\n",
      "Training FF2:  52%|███████████▍          | 1666/3201 [45:45<39:27,  1.54s/batch]Batch 1700/3201 Done, mean position loss: 20.9623574757576\n",
      "Training FF2:  53%|███████████▋          | 1706/3201 [45:47<38:53,  1.56s/batch]Batch 1700/3201 Done, mean position loss: 21.593290348052978\n",
      "Training FF2:  54%|███████████▊          | 1714/3201 [45:47<37:11,  1.50s/batch]Batch 1700/3201 Done, mean position loss: 21.499350619316097\n",
      "Training FF2:  53%|███████████▋          | 1703/3201 [45:48<39:15,  1.57s/batch]Batch 1700/3201 Done, mean position loss: 21.0972949552536\n",
      "Training FF2:  53%|███████████▊          | 1710/3201 [45:50<35:52,  1.44s/batch]Batch 1700/3201 Done, mean position loss: 21.537197999954223\n",
      "Training FF2:  52%|███████████▌          | 1679/3201 [45:50<43:44,  1.72s/batch]Batch 1700/3201 Done, mean position loss: 21.080852081775664\n",
      "Training FF2:  52%|███████████▍          | 1664/3201 [45:52<40:45,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 22.669886121749876\n",
      "Training FF2:  54%|███████████▊          | 1718/3201 [45:53<35:22,  1.43s/batch]Batch 1700/3201 Done, mean position loss: 21.599019391536714\n",
      "Training FF2:  53%|███████████▌          | 1689/3201 [45:55<41:33,  1.65s/batch]Batch 1700/3201 Done, mean position loss: 21.67111112833023\n",
      "Training FF2:  52%|███████████▍          | 1672/3201 [45:55<40:04,  1.57s/batch]Batch 1700/3201 Done, mean position loss: 22.512703647613527\n",
      "Training FF2:  53%|███████████▋          | 1706/3201 [45:58<40:46,  1.64s/batch]Batch 1700/3201 Done, mean position loss: 21.536836419105526\n",
      "Training FF2:  53%|███████████▋          | 1709/3201 [45:57<36:28,  1.47s/batch]Batch 1700/3201 Done, mean position loss: 21.641209397315983\n",
      "Training FF2:  54%|███████████▊          | 1722/3201 [45:59<37:47,  1.53s/batch]Batch 1700/3201 Done, mean position loss: 21.49641135454178\n",
      "Training FF2:  53%|███████████▋          | 1709/3201 [46:02<37:06,  1.49s/batch]Batch 1700/3201 Done, mean position loss: 21.061587080955505\n",
      "Training FF2:  52%|███████████▍          | 1670/3201 [46:03<44:16,  1.74s/batch]Batch 1700/3201 Done, mean position loss: 21.921971740722654\n",
      "Training FF2:  52%|███████████▍          | 1672/3201 [46:04<39:59,  1.57s/batch]Batch 1700/3201 Done, mean position loss: 21.41522411108017\n",
      "Training FF2:  54%|███████████▉          | 1729/3201 [46:04<37:43,  1.54s/batch]Batch 1700/3201 Done, mean position loss: 21.536456692218778\n",
      "Training FF2:  53%|███████████▋          | 1709/3201 [46:05<33:18,  1.34s/batch]Batch 1700/3201 Done, mean position loss: 21.575246496200563\n",
      "Training FF2:  54%|███████████▊          | 1719/3201 [46:06<37:13,  1.51s/batch]Batch 1700/3201 Done, mean position loss: 21.247397372722624\n",
      "Training FF2:  54%|███████████▊          | 1713/3201 [46:13<38:48,  1.56s/batch]Batch 1700/3201 Done, mean position loss: 21.536643075942994\n",
      "Training FF2:  54%|███████████▊          | 1718/3201 [46:11<36:41,  1.48s/batch]Batch 1700/3201 Done, mean position loss: 21.578863589763642\n",
      "Training FF2:  54%|███████████▊          | 1714/3201 [46:13<37:01,  1.49s/batch]Batch 1700/3201 Done, mean position loss: 21.904453845024108\n",
      "Training FF2:  53%|███████████▋          | 1695/3201 [46:15<37:50,  1.51s/batch]Batch 1700/3201 Done, mean position loss: 21.536708726882935\n",
      "Training FF2:  53%|███████████▌          | 1686/3201 [46:19<43:51,  1.74s/batch]Batch 1700/3201 Done, mean position loss: 22.03591836690903\n",
      "Training FF2:  53%|███████████▋          | 1707/3201 [46:23<39:51,  1.60s/batch]Batch 1700/3201 Done, mean position loss: 21.670241875648497\n",
      "Training FF2:  54%|███████████▊          | 1722/3201 [46:28<40:25,  1.64s/batch]Batch 1700/3201 Done, mean position loss: 21.276960692405698\n",
      "Training FF2:  55%|████████████          | 1746/3201 [46:38<40:25,  1.67s/batch]Batch 1700/3201 Done, mean position loss: 21.103650522232055\n",
      "Training FF2:  54%|███████████▊          | 1714/3201 [46:44<39:18,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 21.611265156269074\n",
      "Training FF2:  53%|███████████▋          | 1704/3201 [46:48<39:40,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 21.49700917959213\n",
      "Training FF2:  54%|███████████▉          | 1738/3201 [46:53<41:11,  1.69s/batch]Batch 1700/3201 Done, mean position loss: 21.619801306724547\n",
      "Training FF2:  54%|███████████▉          | 1729/3201 [46:55<42:55,  1.75s/batch]Batch 1700/3201 Done, mean position loss: 21.483674578666687\n",
      "Training FF2:  56%|████████████▎         | 1783/3201 [48:04<38:15,  1.62s/batch]Batch 1800/3201 Done, mean position loss: 21.542681586742404\n",
      "Training FF2:  55%|████████████▏         | 1776/3201 [48:06<39:23,  1.66s/batch]Batch 1800/3201 Done, mean position loss: 21.483496837615967\n",
      "Training FF2:  56%|████████████▎         | 1787/3201 [48:10<40:23,  1.71s/batch]Batch 1800/3201 Done, mean position loss: 21.56196281194687\n",
      "Training FF2:  55%|████████████▏         | 1767/3201 [48:18<40:25,  1.69s/batch]Batch 1800/3201 Done, mean position loss: 21.521827192306517\n",
      "Training FF2:  56%|████████████▎         | 1784/3201 [48:19<38:16,  1.62s/batch]Batch 1800/3201 Done, mean position loss: 21.26432014703751\n",
      "Training FF2:  56%|████████████▍         | 1807/3201 [48:20<45:14,  1.95s/batch]Batch 1800/3201 Done, mean position loss: 21.49610617160797\n",
      "Training FF2:  56%|████████████▍         | 1801/3201 [48:20<43:25,  1.86s/batch]Batch 1800/3201 Done, mean position loss: 21.693614087104798\n",
      "Training FF2:  55%|████████████          | 1760/3201 [48:24<47:37,  1.98s/batch]Batch 1800/3201 Done, mean position loss: 22.121090173721313\n",
      "Training FF2:  56%|████████████▎         | 1798/3201 [48:25<44:14,  1.89s/batch]Batch 1800/3201 Done, mean position loss: 20.938709893226623\n",
      "Training FF2:  56%|████████████▎         | 1790/3201 [48:28<36:29,  1.55s/batch]Batch 1800/3201 Done, mean position loss: 21.069109561443327\n",
      "Training FF2:  57%|████████████▍         | 1813/3201 [48:28<33:19,  1.44s/batch]Batch 1800/3201 Done, mean position loss: 21.68360091686249\n",
      "Training FF2:  56%|████████████▎         | 1799/3201 [48:31<38:49,  1.66s/batch]Batch 1800/3201 Done, mean position loss: 21.675432419776918\n",
      "Training FF2:  56%|████████████▎         | 1792/3201 [48:30<37:35,  1.60s/batch]Batch 1800/3201 Done, mean position loss: 21.600662360191343\n",
      "Training FF2:  55%|████████████          | 1759/3201 [48:32<43:06,  1.79s/batch]Batch 1800/3201 Done, mean position loss: 21.578509311676022\n",
      "Training FF2:  56%|████████████▎         | 1786/3201 [48:33<45:30,  1.93s/batch]Batch 1800/3201 Done, mean position loss: 21.464945986270905\n",
      "Training FF2:  56%|████████████▍         | 1807/3201 [48:34<34:32,  1.49s/batch]Batch 1800/3201 Done, mean position loss: 21.51658961057663\n",
      "Training FF2:  56%|████████████▎         | 1793/3201 [48:37<43:41,  1.86s/batch]Batch 1800/3201 Done, mean position loss: 22.5665784406662\n",
      "Training FF2:  56%|████████████▎         | 1797/3201 [48:38<41:37,  1.78s/batch]Batch 1800/3201 Done, mean position loss: 21.596470227241515\n",
      "Training FF2:  57%|████████████▍         | 1810/3201 [48:38<35:47,  1.54s/batch]Batch 1800/3201 Done, mean position loss: 21.02748994588852\n",
      "Training FF2:  56%|████████████▎         | 1798/3201 [48:43<42:09,  1.80s/batch]Batch 1800/3201 Done, mean position loss: 21.460713112354277\n",
      "Training FF2:  57%|████████████▍         | 1813/3201 [48:44<35:53,  1.55s/batch]Batch 1800/3201 Done, mean position loss: 21.46429036140442\n",
      "Training FF2:  56%|████████████▎         | 1800/3201 [48:45<43:26,  1.86s/batch]Batch 1800/3201 Done, mean position loss: 21.531683750152588\n",
      "Training FF2:  56%|████████████▎         | 1792/3201 [48:46<37:51,  1.61s/batch]Batch 1800/3201 Done, mean position loss: 21.46232355117798\n",
      "Training FF2:  56%|████████████▎         | 1789/3201 [48:47<38:09,  1.62s/batch]Batch 1800/3201 Done, mean position loss: 22.505140857696535\n",
      "Training FF2:  56%|████████████▎         | 1799/3201 [48:48<46:35,  1.99s/batch]Batch 1800/3201 Done, mean position loss: 21.194336309432984\n",
      "Training FF2:  57%|████████████▍         | 1811/3201 [48:52<38:17,  1.65s/batch]Batch 1800/3201 Done, mean position loss: 21.85955015897751\n",
      "Training FF2:  57%|████████████▍         | 1817/3201 [48:50<40:23,  1.75s/batch]Batch 1800/3201 Done, mean position loss: 21.03608371257782\n",
      "Training FF2:  56%|████████████▎         | 1786/3201 [48:52<40:26,  1.71s/batch]Batch 1800/3201 Done, mean position loss: 21.36855792760849\n",
      "Training FF2:  55%|████████████▏         | 1772/3201 [48:55<40:19,  1.69s/batch]Batch 1800/3201 Done, mean position loss: 21.452967519760133\n",
      "Training FF2:  56%|████████████▎         | 1796/3201 [48:58<39:54,  1.70s/batch]Batch 1800/3201 Done, mean position loss: 21.532700116634366\n",
      "Training FF2:  55%|████████████▏         | 1775/3201 [48:59<33:16,  1.40s/batch]Batch 1800/3201 Done, mean position loss: 21.490157527923586\n",
      "Training FF2:  56%|████████████▏         | 1781/3201 [49:01<40:39,  1.72s/batch]Batch 1800/3201 Done, mean position loss: 21.813438527584076\n",
      "Training FF2:  57%|████████████▌         | 1821/3201 [49:09<34:02,  1.48s/batch]Batch 1800/3201 Done, mean position loss: 21.956387305259703\n",
      "Training FF2:  57%|████████████▌         | 1820/3201 [49:15<43:31,  1.89s/batch]Batch 1800/3201 Done, mean position loss: 21.596894872188567\n",
      "Training FF2:  57%|████████████▌         | 1820/3201 [49:16<41:02,  1.78s/batch]Batch 1800/3201 Done, mean position loss: 21.242750129699708\n",
      "Training FF2:  57%|████████████▌         | 1835/3201 [49:33<32:34,  1.43s/batch]Batch 1800/3201 Done, mean position loss: 21.08537798881531\n",
      "Training FF2:  57%|████████████▋         | 1837/3201 [49:35<37:46,  1.66s/batch]Batch 1800/3201 Done, mean position loss: 21.59182254076004\n",
      "Training FF2:  57%|████████████▌         | 1833/3201 [49:35<38:07,  1.67s/batch]Batch 1800/3201 Done, mean position loss: 21.48543199300766\n",
      "Training FF2:  58%|████████████▋         | 1850/3201 [49:43<37:50,  1.68s/batch]Batch 1800/3201 Done, mean position loss: 21.582078306674955\n",
      "Training FF2:  58%|████████████▋         | 1841/3201 [49:44<33:18,  1.47s/batch]Batch 1800/3201 Done, mean position loss: 21.44892123937607\n",
      "Training FF2:  59%|████████████▉         | 1888/3201 [50:48<37:16,  1.70s/batch]Batch 1900/3201 Done, mean position loss: 21.502434461116792\n",
      "Training FF2:  59%|████████████▉         | 1886/3201 [50:50<33:44,  1.54s/batch]Batch 1900/3201 Done, mean position loss: 21.436320550441742\n",
      "Training FF2:  59%|████████████▉         | 1878/3201 [50:54<31:16,  1.42s/batch]Batch 1900/3201 Done, mean position loss: 21.50225283384323\n",
      "Training FF2:  59%|████████████▉         | 1883/3201 [51:00<31:44,  1.45s/batch]Batch 1900/3201 Done, mean position loss: 21.450346162319185\n",
      "Training FF2:  59%|████████████▉         | 1889/3201 [51:04<32:43,  1.50s/batch]Batch 1900/3201 Done, mean position loss: 21.651185073852538\n",
      "Training FF2:  59%|█████████████         | 1901/3201 [51:03<31:14,  1.44s/batch]Batch 1900/3201 Done, mean position loss: 21.478366448879243\n",
      "Training FF2:  59%|████████████▉         | 1884/3201 [51:05<39:28,  1.80s/batch]Batch 1900/3201 Done, mean position loss: 21.210988805294036\n",
      "Training FF2:  59%|█████████████         | 1900/3201 [51:08<34:32,  1.59s/batch]Batch 1900/3201 Done, mean position loss: 22.11890162706375\n",
      "Training FF2:  59%|█████████████         | 1895/3201 [51:10<35:39,  1.64s/batch]Batch 1900/3201 Done, mean position loss: 21.047064881324765\n",
      "Training FF2:  59%|████████████▉         | 1884/3201 [51:11<42:52,  1.95s/batch]Batch 1900/3201 Done, mean position loss: 20.913260757923126\n",
      "Training FF2:  59%|█████████████         | 1901/3201 [51:11<38:41,  1.79s/batch]Batch 1900/3201 Done, mean position loss: 21.586261296272276\n",
      "Training FF2:  58%|████████████▊         | 1862/3201 [51:12<35:41,  1.60s/batch]Batch 1900/3201 Done, mean position loss: 21.66332013130188\n",
      "Training FF2:  58%|████████████▊         | 1859/3201 [51:14<36:43,  1.64s/batch]Batch 1900/3201 Done, mean position loss: 21.587474923133847\n",
      "Training FF2:  59%|█████████████         | 1899/3201 [51:18<38:28,  1.77s/batch]Batch 1900/3201 Done, mean position loss: 21.488014047145843\n",
      "Batch 1900/3201 Done, mean position loss: 21.41330200433731\n",
      "Training FF2:  58%|████████████▊         | 1865/3201 [51:20<32:37,  1.47s/batch]Batch 1900/3201 Done, mean position loss: 21.02023371219635\n",
      "Training FF2:  59%|█████████████         | 1894/3201 [51:21<36:49,  1.69s/batch]Batch 1900/3201 Done, mean position loss: 21.633872294425963\n",
      "Training FF2:  59%|█████████████         | 1903/3201 [51:23<29:38,  1.37s/batch]Batch 1900/3201 Done, mean position loss: 21.481557240486147\n",
      "Training FF2:  58%|████████████▊         | 1867/3201 [51:23<33:01,  1.49s/batch]Batch 1900/3201 Done, mean position loss: 22.533508846759794\n",
      "Training FF2:  59%|█████████████         | 1896/3201 [51:23<33:35,  1.54s/batch]Batch 1900/3201 Done, mean position loss: 21.540856530666353\n",
      "Training FF2:  60%|█████████████▏        | 1923/3201 [51:30<41:03,  1.93s/batch]Batch 1900/3201 Done, mean position loss: 22.502473487854\n",
      "Training FF2:  60%|█████████████         | 1908/3201 [51:31<41:09,  1.91s/batch]Batch 1900/3201 Done, mean position loss: 21.403640525341032\n",
      "Training FF2:  59%|█████████████         | 1896/3201 [51:32<36:56,  1.70s/batch]Batch 1900/3201 Done, mean position loss: 21.441852169036864\n",
      "Training FF2:  60%|█████████████▏        | 1910/3201 [51:34<30:55,  1.44s/batch]Batch 1900/3201 Done, mean position loss: 21.424103286266327\n",
      "Training FF2:  59%|█████████████         | 1904/3201 [51:34<31:00,  1.43s/batch]Batch 1900/3201 Done, mean position loss: 21.146314010620117\n",
      "Training FF2:  60%|█████████████         | 1907/3201 [51:39<33:24,  1.55s/batch]Batch 1900/3201 Done, mean position loss: 21.360140690803526\n",
      "Training FF2:  60%|█████████████         | 1908/3201 [51:40<31:17,  1.45s/batch]Batch 1900/3201 Done, mean position loss: 21.81303947210312\n",
      "Training FF2:  60%|█████████████▏        | 1922/3201 [51:40<28:56,  1.36s/batch]Batch 1900/3201 Done, mean position loss: 21.479075684547425\n",
      "Batch 1900/3201 Done, mean position loss: 21.410825328826903\n",
      "Training FF2:  60%|█████████████         | 1909/3201 [51:42<33:43,  1.57s/batch]Batch 1900/3201 Done, mean position loss: 21.002730832099914\n",
      "Training FF2:  60%|█████████████▏        | 1914/3201 [51:47<38:27,  1.79s/batch]Batch 1900/3201 Done, mean position loss: 21.709216969013212\n",
      "Training FF2:  60%|█████████████▏        | 1910/3201 [51:49<34:32,  1.61s/batch]Batch 1900/3201 Done, mean position loss: 21.47433968067169\n",
      "Training FF2:  60%|█████████████▏        | 1920/3201 [51:52<42:59,  2.01s/batch]Batch 1900/3201 Done, mean position loss: 22.026653215885162\n",
      "Training FF2:  60%|█████████████▏        | 1918/3201 [51:56<37:32,  1.76s/batch]Batch 1900/3201 Done, mean position loss: 21.222425401210785\n",
      "Training FF2:  60%|█████████████▏        | 1913/3201 [52:00<33:57,  1.58s/batch]Batch 1900/3201 Done, mean position loss: 21.59671076774597\n",
      "Training FF2:  61%|█████████████▎        | 1944/3201 [52:16<32:28,  1.55s/batch]Batch 1900/3201 Done, mean position loss: 21.0713911819458\n",
      "Training FF2:  61%|█████████████▍        | 1959/3201 [52:22<32:17,  1.56s/batch]Batch 1900/3201 Done, mean position loss: 21.45208421945572\n",
      "Training FF2:  61%|█████████████▎        | 1945/3201 [52:23<33:05,  1.58s/batch]Batch 1900/3201 Done, mean position loss: 21.55273535490036\n",
      "Training FF2:  61%|█████████████▍        | 1947/3201 [52:26<32:23,  1.55s/batch]Batch 1900/3201 Done, mean position loss: 21.520949940681458\n",
      "Training FF2:  60%|█████████████▎        | 1929/3201 [52:28<33:59,  1.60s/batch]Batch 1900/3201 Done, mean position loss: 21.425370709896086\n",
      "Training FF2:  61%|█████████████▌        | 1966/3201 [53:30<36:19,  1.77s/batch]Batch 2000/3201 Done, mean position loss: 21.428157362937927\n",
      "Training FF2:  61%|█████████████▍        | 1960/3201 [53:33<31:07,  1.51s/batch]Batch 2000/3201 Done, mean position loss: 21.426821382045745\n",
      "Training FF2:  62%|█████████████▋        | 1986/3201 [53:39<29:12,  1.44s/batch]Batch 2000/3201 Done, mean position loss: 21.457309725284574\n",
      "Training FF2:  63%|█████████████▊        | 2001/3201 [53:37<34:45,  1.74s/batch]Batch 2000/3201 Done, mean position loss: 21.444148647785184\n",
      "Training FF2:  63%|█████████████▊        | 2008/3201 [53:43<30:45,  1.55s/batch]Batch 2000/3201 Done, mean position loss: 22.044935398101806\n",
      "Training FF2:  62%|█████████████▌        | 1975/3201 [53:45<30:54,  1.51s/batch]Batch 2000/3201 Done, mean position loss: 21.610593802928925\n",
      "Training FF2:  63%|█████████████▊        | 2011/3201 [53:49<36:00,  1.82s/batch]Batch 2000/3201 Done, mean position loss: 21.01736995458603\n",
      "Training FF2:  62%|█████████████▋        | 1998/3201 [53:50<35:24,  1.77s/batch]Batch 2000/3201 Done, mean position loss: 21.175520849227905\n",
      "Training FF2:  62%|█████████████▋        | 1984/3201 [53:51<36:04,  1.78s/batch]Batch 2000/3201 Done, mean position loss: 20.879884140491484\n",
      "Training FF2:  62%|█████████████▌        | 1981/3201 [53:54<31:22,  1.54s/batch]Batch 2000/3201 Done, mean position loss: 21.667155072689056\n",
      "Training FF2:  62%|█████████████▋        | 2000/3201 [53:54<37:03,  1.85s/batch]Batch 2000/3201 Done, mean position loss: 21.435183670520782\n",
      "Training FF2:  63%|█████████████▊        | 2009/3201 [53:55<35:40,  1.80s/batch]Batch 2000/3201 Done, mean position loss: 21.544710986614227\n",
      "Training FF2:  61%|█████████████▍        | 1963/3201 [53:58<32:13,  1.56s/batch]Batch 2000/3201 Done, mean position loss: 20.98577731847763\n",
      "Training FF2:  63%|█████████████▊        | 2012/3201 [53:59<38:42,  1.95s/batch]Batch 2000/3201 Done, mean position loss: 21.534817173480988\n",
      "Training FF2:  63%|█████████████▊        | 2008/3201 [54:02<33:04,  1.66s/batch]Batch 2000/3201 Done, mean position loss: 21.368439848423005\n",
      "Training FF2:  63%|█████████████▊        | 2003/3201 [54:02<29:43,  1.49s/batch]Batch 2000/3201 Done, mean position loss: 21.482215325832367\n",
      "Training FF2:  63%|█████████████▉        | 2020/3201 [54:02<32:34,  1.66s/batch]Batch 2000/3201 Done, mean position loss: 21.676035068035127\n",
      "Training FF2:  62%|█████████████▋        | 1997/3201 [54:03<34:30,  1.72s/batch]Batch 2000/3201 Done, mean position loss: 21.497463121414185\n",
      "Training FF2:  62%|█████████████▋        | 1999/3201 [54:06<30:58,  1.55s/batch]Batch 2000/3201 Done, mean position loss: 22.50236592531204\n",
      "Training FF2:  63%|█████████████▊        | 2013/3201 [54:09<26:04,  1.32s/batch]Batch 2000/3201 Done, mean position loss: 21.362348356246947\n",
      "Training FF2:  61%|█████████████▍        | 1961/3201 [54:10<32:00,  1.55s/batch]Batch 2000/3201 Done, mean position loss: 21.494846093654633\n",
      "Training FF2:  63%|█████████████▊        | 2003/3201 [54:13<32:19,  1.62s/batch]Batch 2000/3201 Done, mean position loss: 22.46094141483307\n",
      "Training FF2:  63%|█████████████▊        | 2011/3201 [54:16<27:08,  1.37s/batch]Batch 2000/3201 Done, mean position loss: 21.346010267734528\n",
      "Training FF2:  62%|█████████████▋        | 1994/3201 [54:19<28:57,  1.44s/batch]Batch 2000/3201 Done, mean position loss: 21.326265132427217\n",
      "Training FF2:  63%|█████████████▊        | 2016/3201 [54:21<35:38,  1.80s/batch]Batch 2000/3201 Done, mean position loss: 21.345463407039645\n",
      "Training FF2:  63%|█████████████▊        | 2008/3201 [54:19<31:06,  1.56s/batch]Batch 2000/3201 Done, mean position loss: 21.426400945186614\n",
      "Training FF2:  62%|█████████████▋        | 1993/3201 [54:26<31:42,  1.57s/batch]Batch 2000/3201 Done, mean position loss: 21.754965643882752\n",
      "Training FF2:  63%|█████████████▉        | 2029/3201 [54:26<30:00,  1.54s/batch]Batch 2000/3201 Done, mean position loss: 21.3970157122612\n",
      "Training FF2:  63%|█████████████▉        | 2029/3201 [54:28<32:23,  1.66s/batch]Batch 2000/3201 Done, mean position loss: 21.125465362071992\n",
      "Training FF2:  63%|█████████████▉        | 2027/3201 [54:30<37:27,  1.91s/batch]Batch 2000/3201 Done, mean position loss: 20.992467114925383\n",
      "Training FF2:  62%|█████████████▋        | 1991/3201 [54:31<36:20,  1.80s/batch]Batch 2000/3201 Done, mean position loss: 21.612055468559266\n",
      "Training FF2:  64%|██████████████        | 2037/3201 [54:34<27:05,  1.40s/batch]Batch 2000/3201 Done, mean position loss: 21.969892578125002\n",
      "Training FF2:  63%|█████████████▊        | 2001/3201 [54:34<32:53,  1.64s/batch]Batch 2000/3201 Done, mean position loss: 21.448004419803617\n",
      "Training FF2:  63%|█████████████▉        | 2019/3201 [54:40<30:07,  1.53s/batch]Batch 2000/3201 Done, mean position loss: 21.215072922706604\n",
      "Training FF2:  64%|██████████████        | 2046/3201 [54:47<31:07,  1.62s/batch]Batch 2000/3201 Done, mean position loss: 21.55467123031616\n",
      "Training FF2:  63%|█████████████▉        | 2029/3201 [55:03<36:18,  1.86s/batch]Batch 2000/3201 Done, mean position loss: 21.036809630393982\n",
      "Training FF2:  64%|██████████████        | 2044/3201 [55:05<32:48,  1.70s/batch]Batch 2000/3201 Done, mean position loss: 21.468539226055146\n",
      "Training FF2:  63%|█████████████▉        | 2024/3201 [55:09<30:45,  1.57s/batch]Batch 2000/3201 Done, mean position loss: 21.399151573181154\n",
      "Training FF2:  64%|██████████████        | 2039/3201 [55:13<33:28,  1.73s/batch]Batch 2000/3201 Done, mean position loss: 21.498139259815215\n",
      "Training FF2:  65%|██████████████▏       | 2067/3201 [55:19<35:11,  1.86s/batch]Batch 2000/3201 Done, mean position loss: 21.493728320598603\n",
      "Training FF2:  65%|██████████████▎       | 2080/3201 [56:10<33:20,  1.78s/batch]Batch 2100/3201 Done, mean position loss: 21.400452916622164\n",
      "Training FF2:  65%|██████████████▎       | 2079/3201 [56:15<26:30,  1.42s/batch]Batch 2100/3201 Done, mean position loss: 21.37308507680893\n",
      "Training FF2:  66%|██████████████▍       | 2098/3201 [56:20<28:27,  1.55s/batch]Batch 2100/3201 Done, mean position loss: 21.419908103942873\n",
      "Training FF2:  65%|██████████████▏       | 2067/3201 [56:26<33:05,  1.75s/batch]Batch 2100/3201 Done, mean position loss: 21.965081534385682\n",
      "Training FF2:  65%|██████████████▍       | 2092/3201 [56:27<34:31,  1.87s/batch]Batch 2100/3201 Done, mean position loss: 20.860193486213685\n",
      "Training FF2:  64%|██████████████        | 2048/3201 [56:29<32:31,  1.69s/batch]Batch 2100/3201 Done, mean position loss: 21.39650383234024\n",
      "Training FF2:  65%|██████████████▎       | 2077/3201 [56:30<30:26,  1.63s/batch]Batch 2100/3201 Done, mean position loss: 21.01544815301895\n",
      "Training FF2:  64%|██████████████        | 2051/3201 [56:32<26:00,  1.36s/batch]Batch 2100/3201 Done, mean position loss: 21.15289898633957\n",
      "Training FF2:  66%|██████████████▍       | 2106/3201 [56:33<28:51,  1.58s/batch]Batch 2100/3201 Done, mean position loss: 21.5766033244133\n",
      "Training FF2:  66%|██████████████▍       | 2106/3201 [56:38<34:49,  1.91s/batch]Batch 2100/3201 Done, mean position loss: 21.38011466741562\n",
      "Training FF2:  65%|██████████████▎       | 2089/3201 [56:38<28:17,  1.53s/batch]Batch 2100/3201 Done, mean position loss: 20.969915354251864\n",
      "Training FF2:  66%|██████████████▍       | 2109/3201 [56:39<28:08,  1.55s/batch]Batch 2100/3201 Done, mean position loss: 21.557324199676515\n",
      "Training FF2:  65%|██████████████▎       | 2084/3201 [56:40<28:44,  1.54s/batch]Batch 2100/3201 Done, mean position loss: 21.53287948846817\n",
      "Training FF2:  66%|██████████████▍       | 2108/3201 [56:41<33:30,  1.84s/batch]Batch 2100/3201 Done, mean position loss: 21.34723413705826\n",
      "Training FF2:  66%|██████████████▌       | 2118/3201 [56:43<27:36,  1.53s/batch]Batch 2100/3201 Done, mean position loss: 21.60942758798599\n",
      "Training FF2:  64%|██████████████        | 2055/3201 [56:45<30:40,  1.61s/batch]Batch 2100/3201 Done, mean position loss: 21.46699523687363\n",
      "Training FF2:  66%|██████████████▍       | 2107/3201 [56:48<27:58,  1.53s/batch]Batch 2100/3201 Done, mean position loss: 21.65444176673889\n",
      "Training FF2:  65%|██████████████▍       | 2092/3201 [56:47<32:29,  1.76s/batch]Batch 2100/3201 Done, mean position loss: 21.43357501268387\n",
      "Training FF2:  65%|██████████████▎       | 2087/3201 [56:53<34:30,  1.86s/batch]Batch 2100/3201 Done, mean position loss: 21.332668874263764\n",
      "Training FF2:  66%|██████████████▍       | 2105/3201 [56:53<33:31,  1.83s/batch]Batch 2100/3201 Done, mean position loss: 22.478993773460388\n",
      "Training FF2:  66%|██████████████▌       | 2115/3201 [56:56<27:26,  1.52s/batch]Batch 2100/3201 Done, mean position loss: 22.43491935253143\n",
      "Training FF2:  66%|██████████████▌       | 2113/3201 [56:56<24:27,  1.35s/batch]Batch 2100/3201 Done, mean position loss: 21.4736750292778\n",
      "Training FF2:  66%|██████████████▌       | 2116/3201 [56:58<30:02,  1.66s/batch]Batch 2100/3201 Done, mean position loss: 21.39659461259842\n",
      "Training FF2:  65%|██████████████▍       | 2094/3201 [57:03<30:56,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.326450798511505\n",
      "Training FF2:  66%|██████████████▌       | 2117/3201 [57:03<26:57,  1.49s/batch]Batch 2100/3201 Done, mean position loss: 21.315435004234317\n",
      "Batch 2100/3201 Done, mean position loss: 21.316526634693144\n",
      "Training FF2:  66%|██████████████▍       | 2097/3201 [57:08<29:43,  1.62s/batch]Batch 2100/3201 Done, mean position loss: 21.35501008272171\n",
      "Training FF2:  66%|██████████████▌       | 2117/3201 [57:09<28:51,  1.60s/batch]Batch 2100/3201 Done, mean position loss: 21.081299710273743\n",
      "Training FF2:  66%|██████████████▍       | 2097/3201 [57:13<28:59,  1.58s/batch]Batch 2100/3201 Done, mean position loss: 21.55568860769272\n",
      "Training FF2:  66%|██████████████▌       | 2113/3201 [57:15<30:33,  1.69s/batch]Batch 2100/3201 Done, mean position loss: 21.690050523281098\n",
      "Training FF2:  67%|██████████████▋       | 2131/3201 [57:16<28:24,  1.59s/batch]Batch 2100/3201 Done, mean position loss: 20.97261725664139\n",
      "Training FF2:  65%|██████████████▎       | 2079/3201 [57:20<31:21,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.376904151439668\n",
      "Training FF2:  67%|██████████████▋       | 2136/3201 [57:24<30:06,  1.70s/batch]Batch 2100/3201 Done, mean position loss: 21.927269394397737\n",
      "Training FF2:  66%|██████████████▍       | 2098/3201 [57:25<27:38,  1.50s/batch]Batch 2100/3201 Done, mean position loss: 21.18630707502365\n",
      "Training FF2:  67%|██████████████▋       | 2139/3201 [57:29<29:03,  1.64s/batch]Batch 2100/3201 Done, mean position loss: 21.471333470344547\n",
      "Training FF2:  67%|██████████████▋       | 2135/3201 [57:49<35:10,  1.98s/batch]Batch 2100/3201 Done, mean position loss: 21.029889957904818\n",
      "Training FF2:  66%|██████████████▌       | 2116/3201 [57:50<25:12,  1.39s/batch]Batch 2100/3201 Done, mean position loss: 21.4835880112648\n",
      "Training FF2:  67%|██████████████▊       | 2158/3201 [57:55<24:05,  1.39s/batch]Batch 2100/3201 Done, mean position loss: 21.347320015430448\n",
      "Training FF2:  67%|██████████████▋       | 2130/3201 [57:57<30:31,  1.71s/batch]Batch 2100/3201 Done, mean position loss: 21.407416462898254\n",
      "Training FF2:  66%|██████████████▌       | 2127/3201 [58:02<29:49,  1.67s/batch]Batch 2100/3201 Done, mean position loss: 21.451019995212555\n",
      "Training FF2:  68%|██████████████▉       | 2169/3201 [58:42<23:44,  1.38s/batch]Batch 2200/3201 Done, mean position loss: 21.324481673240662\n",
      "Training FF2:  68%|███████████████       | 2190/3201 [59:02<25:41,  1.52s/batch]Batch 2200/3201 Done, mean position loss: 21.377817764282227\n",
      "Training FF2:  68%|███████████████       | 2191/3201 [59:03<26:59,  1.60s/batch]Batch 2200/3201 Done, mean position loss: 21.322865977287293\n",
      "Training FF2:  69%|███████████████▏      | 2202/3201 [59:05<23:34,  1.42s/batch]Batch 2200/3201 Done, mean position loss: 20.853338184356687\n",
      "Training FF2:  69%|███████████████▏      | 2217/3201 [59:05<25:32,  1.56s/batch]Batch 2200/3201 Done, mean position loss: 20.98411921024322\n",
      "Training FF2:  69%|███████████████       | 2194/3201 [59:10<24:31,  1.46s/batch]Batch 2200/3201 Done, mean position loss: 21.96312711954117\n",
      "Training FF2:  68%|██████████████▉       | 2176/3201 [59:13<29:47,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.38110145330429\n",
      "Training FF2:  68%|██████████████▉       | 2168/3201 [59:14<28:38,  1.66s/batch]Batch 2200/3201 Done, mean position loss: 21.52670280456543\n",
      "Training FF2:  69%|███████████████       | 2198/3201 [59:16<32:05,  1.92s/batch]Batch 2200/3201 Done, mean position loss: 20.976321823596955\n",
      "Training FF2:  69%|███████████████▏      | 2204/3201 [59:18<24:45,  1.49s/batch]Batch 2200/3201 Done, mean position loss: 21.324174773693088\n",
      "Training FF2:  69%|███████████████▏      | 2203/3201 [59:19<27:02,  1.63s/batch]Batch 2200/3201 Done, mean position loss: 21.48223635673523\n",
      "Training FF2:  69%|███████████████       | 2200/3201 [59:19<29:42,  1.78s/batch]Batch 2200/3201 Done, mean position loss: 21.124130954742434\n",
      "Training FF2:  68%|██████████████▉       | 2178/3201 [59:20<30:48,  1.81s/batch]Batch 2200/3201 Done, mean position loss: 21.518215346336362\n",
      "Training FF2:  68%|██████████████▉       | 2170/3201 [59:21<27:47,  1.62s/batch]Batch 2200/3201 Done, mean position loss: 21.597075526714328\n",
      "Training FF2:  67%|██████████████▊       | 2160/3201 [59:25<31:56,  1.84s/batch]Batch 2200/3201 Done, mean position loss: 21.355628249645235\n",
      "Training FF2:  69%|███████████████       | 2197/3201 [59:29<29:45,  1.78s/batch]Batch 2200/3201 Done, mean position loss: 21.617874257564544\n",
      "Training FF2:  69%|███████████████▏      | 2213/3201 [59:32<23:57,  1.46s/batch]Batch 2200/3201 Done, mean position loss: 22.470708081722258\n",
      "Training FF2:  69%|███████████████▏      | 2213/3201 [59:36<27:59,  1.70s/batch]Batch 2200/3201 Done, mean position loss: 22.42543461561203\n",
      "Training FF2:  68%|██████████████▊       | 2162/3201 [59:37<29:16,  1.69s/batch]Batch 2200/3201 Done, mean position loss: 21.46782002687454\n",
      "Training FF2:  69%|███████████████       | 2200/3201 [59:37<29:39,  1.78s/batch]Batch 2200/3201 Done, mean position loss: 21.41696322917938\n",
      "Training FF2:  69%|███████████████▎      | 2221/3201 [59:38<26:50,  1.64s/batch]Batch 2200/3201 Done, mean position loss: 21.434983768463134\n",
      "Training FF2:  69%|███████████████▏      | 2204/3201 [59:40<26:21,  1.59s/batch]Batch 2200/3201 Done, mean position loss: 21.286008088588716\n",
      "Training FF2:  69%|███████████████       | 2200/3201 [59:40<28:39,  1.72s/batch]Batch 2200/3201 Done, mean position loss: 21.35729640960693\n",
      "Training FF2:  70%|███████████████▎      | 2225/3201 [59:41<28:08,  1.73s/batch]Batch 2200/3201 Done, mean position loss: 21.26173336267471\n",
      "Training FF2:  68%|██████████████▉       | 2167/3201 [59:44<26:31,  1.54s/batch]Batch 2200/3201 Done, mean position loss: 21.277238037586212\n",
      "Training FF2:  69%|███████████████▏      | 2209/3201 [59:52<24:54,  1.51s/batch]Batch 2200/3201 Done, mean position loss: 21.332435421943664\n",
      "Training FF2:  69%|███████████████▎      | 2222/3201 [59:53<28:04,  1.72s/batch]Batch 2200/3201 Done, mean position loss: 21.06508180856705\n",
      "Training FF2:  69%|███████████████▏      | 2204/3201 [59:56<26:50,  1.62s/batch]Batch 2200/3201 Done, mean position loss: 21.66355000257492\n",
      "Training FF2:  69%|███████████████▏      | 2213/3201 [59:57<26:26,  1.61s/batch]Batch 2200/3201 Done, mean position loss: 21.31743485212326\n",
      "Training FF2:  69%|███████████████▏      | 2217/3201 [59:59<27:03,  1.65s/batch]Batch 2200/3201 Done, mean position loss: 20.951692311763765\n",
      "Training FF2:  69%|█████████████▊      | 2214/3201 [1:00:00<23:51,  1.45s/batch]Batch 2200/3201 Done, mean position loss: 21.441961691379547\n",
      "Training FF2:  68%|█████████████▌      | 2177/3201 [1:00:01<26:57,  1.58s/batch]Batch 2200/3201 Done, mean position loss: 21.324134933948514\n",
      "Training FF2:  69%|█████████████▊      | 2204/3201 [1:00:05<23:13,  1.40s/batch]Batch 2200/3201 Done, mean position loss: 21.435528199672703\n",
      "Training FF2:  69%|█████████████▊      | 2209/3201 [1:00:07<29:33,  1.79s/batch]Batch 2200/3201 Done, mean position loss: 21.16375304222107\n",
      "Training FF2:  70%|█████████████▉      | 2238/3201 [1:00:11<27:50,  1.73s/batch]Batch 2200/3201 Done, mean position loss: 21.917211139202116\n",
      "Training FF2:  70%|█████████████▉      | 2231/3201 [1:00:29<26:47,  1.66s/batch]Batch 2200/3201 Done, mean position loss: 21.410735530853273\n",
      "Training FF2:  70%|█████████████▉      | 2238/3201 [1:00:34<19:58,  1.24s/batch]Batch 2200/3201 Done, mean position loss: 20.992387497425078\n",
      "Training FF2:  70%|█████████████▉      | 2229/3201 [1:00:41<24:56,  1.54s/batch]Batch 2200/3201 Done, mean position loss: 21.382852675914762\n",
      "Training FF2:  71%|██████████████▏     | 2263/3201 [1:00:43<27:11,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.425525639057163\n",
      "Training FF2:  70%|██████████████      | 2256/3201 [1:00:47<24:04,  1.53s/batch]Batch 2200/3201 Done, mean position loss: 21.349916601181032\n",
      "Training FF2:  71%|██████████████▏     | 2276/3201 [1:01:19<21:40,  1.41s/batch]Batch 2300/3201 Done, mean position loss: 21.319128110408784\n",
      "Training FF2:  71%|██████████████▏     | 2273/3201 [1:01:40<24:02,  1.55s/batch]Batch 2300/3201 Done, mean position loss: 21.356210625171663\n",
      "Training FF2:  70%|██████████████      | 2248/3201 [1:01:43<24:23,  1.54s/batch]Batch 2300/3201 Done, mean position loss: 21.304691398143767\n",
      "Training FF2:  71%|██████████████▏     | 2269/3201 [1:01:44<25:15,  1.63s/batch]Batch 2300/3201 Done, mean position loss: 20.955122740268706\n",
      "Training FF2:  72%|██████████████▎     | 2293/3201 [1:01:45<23:40,  1.56s/batch]Batch 2300/3201 Done, mean position loss: 20.837447504997254\n",
      "Training FF2:  71%|██████████████▎     | 2281/3201 [1:01:52<24:57,  1.63s/batch]Batch 2300/3201 Done, mean position loss: 21.898126718997954\n",
      "Training FF2:  71%|██████████████▏     | 2275/3201 [1:01:54<25:42,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 21.468562526702883\n",
      "Training FF2:  71%|██████████████▎     | 2285/3201 [1:01:56<19:08,  1.25s/batch]Batch 2300/3201 Done, mean position loss: 21.34973296880722\n",
      "Training FF2:  72%|██████████████▎     | 2293/3201 [1:01:55<22:53,  1.51s/batch]Batch 2300/3201 Done, mean position loss: 20.962918448448182\n",
      "Training FF2:  72%|██████████████▎     | 2298/3201 [1:01:58<25:33,  1.70s/batch]Batch 2300/3201 Done, mean position loss: 21.32468058586121\n",
      "Training FF2:  71%|██████████████▎     | 2284/3201 [1:01:58<30:02,  1.97s/batch]Batch 2300/3201 Done, mean position loss: 21.088698937892914\n",
      "Training FF2:  71%|██████████████▎     | 2285/3201 [1:02:00<27:54,  1.83s/batch]Batch 2300/3201 Done, mean position loss: 21.42830193758011\n",
      "Training FF2:  71%|██████████████▏     | 2271/3201 [1:02:03<23:46,  1.53s/batch]Batch 2300/3201 Done, mean position loss: 21.514563941955565\n",
      "Training FF2:  71%|██████████████▎     | 2287/3201 [1:02:02<23:46,  1.56s/batch]Batch 2300/3201 Done, mean position loss: 21.54588312625885\n",
      "Training FF2:  71%|██████████████▎     | 2282/3201 [1:02:03<28:55,  1.89s/batch]Batch 2300/3201 Done, mean position loss: 21.322212407588957\n",
      "Training FF2:  72%|██████████████▍     | 2317/3201 [1:02:08<21:37,  1.47s/batch]Batch 2300/3201 Done, mean position loss: 21.60100169181824\n",
      "Training FF2:  72%|██████████████▍     | 2315/3201 [1:02:13<22:51,  1.55s/batch]Batch 2300/3201 Done, mean position loss: 21.416404843330383\n",
      "Training FF2:  71%|██████████████▎     | 2285/3201 [1:02:14<24:37,  1.61s/batch]Batch 2300/3201 Done, mean position loss: 21.406287071704867\n",
      "Training FF2:  72%|██████████████▎     | 2290/3201 [1:02:16<25:42,  1.69s/batch]Batch 2300/3201 Done, mean position loss: 22.353363068103793\n",
      "Training FF2:  71%|██████████████▎     | 2287/3201 [1:02:19<22:43,  1.49s/batch]Batch 2300/3201 Done, mean position loss: 22.39780832529068\n",
      "Training FF2:  72%|██████████████▍     | 2310/3201 [1:02:21<24:49,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 21.39750871419907\n",
      "Training FF2:  72%|██████████████▍     | 2307/3201 [1:02:22<21:54,  1.47s/batch]Batch 2300/3201 Done, mean position loss: 21.22690541267395\n",
      "Training FF2:  73%|██████████████▌     | 2326/3201 [1:02:23<26:54,  1.85s/batch]Batch 2300/3201 Done, mean position loss: 21.24188629865646\n",
      "Training FF2:  73%|██████████████▌     | 2332/3201 [1:02:27<20:47,  1.44s/batch]Batch 2300/3201 Done, mean position loss: 21.337700600624082\n",
      "Training FF2:  72%|██████████████▎     | 2295/3201 [1:02:32<25:11,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 21.25328802108765\n",
      "Training FF2:  72%|██████████████▍     | 2305/3201 [1:02:33<22:28,  1.51s/batch]Batch 2300/3201 Done, mean position loss: 21.28721220254898\n",
      "Training FF2:  73%|██████████████▋     | 2350/3201 [1:02:34<22:12,  1.57s/batch]Batch 2300/3201 Done, mean position loss: 20.93257040500641\n",
      "Training FF2:  73%|██████████████▌     | 2326/3201 [1:02:36<20:49,  1.43s/batch]Batch 2300/3201 Done, mean position loss: 21.047493040561676\n",
      "Training FF2:  72%|██████████████▍     | 2310/3201 [1:02:37<25:56,  1.75s/batch]Batch 2300/3201 Done, mean position loss: 21.65269610643387\n",
      "Training FF2:  72%|██████████████▎     | 2300/3201 [1:02:40<27:33,  1.84s/batch]Batch 2300/3201 Done, mean position loss: 21.300245444774628\n",
      "Training FF2:  73%|██████████████▌     | 2333/3201 [1:02:41<20:02,  1.39s/batch]Batch 2300/3201 Done, mean position loss: 21.40057330131531\n",
      "Training FF2:  72%|██████████████▍     | 2307/3201 [1:02:42<26:25,  1.77s/batch]Batch 2300/3201 Done, mean position loss: 21.28057898044586\n",
      "Training FF2:  73%|██████████████▌     | 2331/3201 [1:02:46<23:53,  1.65s/batch]Batch 2300/3201 Done, mean position loss: 21.15800036430359\n",
      "Training FF2:  72%|██████████████▍     | 2316/3201 [1:02:49<22:27,  1.52s/batch]Batch 2300/3201 Done, mean position loss: 21.93298993110657\n",
      "Training FF2:  73%|██████████████▌     | 2334/3201 [1:02:50<24:46,  1.71s/batch]Batch 2300/3201 Done, mean position loss: 21.411798777580263\n",
      "Training FF2:  73%|██████████████▋     | 2348/3201 [1:03:08<23:16,  1.64s/batch]Batch 2300/3201 Done, mean position loss: 21.374567074775694\n",
      "Training FF2:  73%|██████████████▌     | 2322/3201 [1:03:17<26:53,  1.84s/batch]Batch 2300/3201 Done, mean position loss: 20.988830623626708\n",
      "Training FF2:  73%|██████████████▌     | 2324/3201 [1:03:20<24:33,  1.68s/batch]Batch 2300/3201 Done, mean position loss: 21.375766158103946\n",
      "Training FF2:  73%|██████████████▌     | 2325/3201 [1:03:22<27:46,  1.90s/batch]Batch 2300/3201 Done, mean position loss: 21.384802207946777\n",
      "Training FF2:  73%|██████████████▌     | 2330/3201 [1:03:34<22:16,  1.53s/batch]Batch 2300/3201 Done, mean position loss: 21.31713741540909\n",
      "Training FF2:  74%|██████████████▊     | 2375/3201 [1:03:56<22:29,  1.63s/batch]Batch 2400/3201 Done, mean position loss: 21.26330086946487\n",
      "Training FF2:  74%|██████████████▋     | 2358/3201 [1:04:22<22:09,  1.58s/batch]Batch 2400/3201 Done, mean position loss: 21.316522104740145\n",
      "Training FF2:  75%|██████████████▉     | 2394/3201 [1:04:24<22:30,  1.67s/batch]Batch 2400/3201 Done, mean position loss: 21.24917228460312\n",
      "Training FF2:  75%|███████████████     | 2403/3201 [1:04:25<19:43,  1.48s/batch]Batch 2400/3201 Done, mean position loss: 20.9305340051651\n",
      "Training FF2:  75%|██████████████▉     | 2398/3201 [1:04:35<22:57,  1.72s/batch]Batch 2400/3201 Done, mean position loss: 20.81046951532364\n",
      "Training FF2:  75%|██████████████▉     | 2395/3201 [1:04:38<23:21,  1.74s/batch]Batch 2400/3201 Done, mean position loss: 20.911617801189422\n",
      "Training FF2:  74%|██████████████▊     | 2367/3201 [1:04:39<26:43,  1.92s/batch]Batch 2400/3201 Done, mean position loss: 21.90374983549118\n",
      "Training FF2:  74%|██████████████▊     | 2380/3201 [1:04:40<24:03,  1.76s/batch]Batch 2400/3201 Done, mean position loss: 21.449277818202972\n",
      "Training FF2:  75%|██████████████▉     | 2398/3201 [1:04:41<22:48,  1.70s/batch]Batch 2400/3201 Done, mean position loss: 21.05455431461334\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:04:41<22:20,  1.67s/batch]Batch 2400/3201 Done, mean position loss: 21.39498135328293\n",
      "Training FF2:  75%|██████████████▉     | 2385/3201 [1:04:44<28:25,  2.09s/batch]Batch 2400/3201 Done, mean position loss: 21.276809444427492\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:04:45<25:51,  1.93s/batch]Batch 2400/3201 Done, mean position loss: 21.522600760459902\n",
      "Training FF2:  75%|██████████████▉     | 2390/3201 [1:04:46<23:35,  1.75s/batch]Batch 2400/3201 Done, mean position loss: 21.56809423685074\n",
      "Training FF2:  76%|███████████████▏    | 2431/3201 [1:04:46<19:25,  1.51s/batch]Batch 2400/3201 Done, mean position loss: 21.32100655078888\n",
      "Training FF2:  74%|██████████████▊     | 2373/3201 [1:04:48<24:30,  1.78s/batch]Batch 2400/3201 Done, mean position loss: 21.303841285705566\n",
      "Training FF2:  75%|███████████████     | 2401/3201 [1:04:49<24:03,  1.80s/batch]Batch 2400/3201 Done, mean position loss: 21.5019389629364\n",
      "Training FF2:  75%|██████████████▉     | 2387/3201 [1:04:53<19:26,  1.43s/batch]Batch 2400/3201 Done, mean position loss: 21.35264523267746\n",
      "Training FF2:  75%|███████████████     | 2409/3201 [1:04:58<20:46,  1.57s/batch]Batch 2400/3201 Done, mean position loss: 21.389717688560488\n",
      "Training FF2:  76%|███████████████▎    | 2441/3201 [1:05:02<16:15,  1.28s/batch]Batch 2400/3201 Done, mean position loss: 21.37751882791519\n",
      "Training FF2:  75%|██████████████▉     | 2394/3201 [1:05:03<21:26,  1.59s/batch]Batch 2400/3201 Done, mean position loss: 22.40845783472061\n",
      "Training FF2:  75%|███████████████     | 2409/3201 [1:05:11<22:50,  1.73s/batch]Batch 2400/3201 Done, mean position loss: 22.305610036849977\n",
      "Training FF2:  75%|██████████████▉     | 2391/3201 [1:05:11<24:00,  1.78s/batch]Batch 2400/3201 Done, mean position loss: 21.21650815248489\n",
      "Training FF2:  75%|██████████████▉     | 2388/3201 [1:05:11<21:58,  1.62s/batch]Batch 2400/3201 Done, mean position loss: 21.228502776622772\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:05:13<26:33,  1.99s/batch]Batch 2400/3201 Done, mean position loss: 21.233211092948913\n",
      "Training FF2:  75%|██████████████▉     | 2392/3201 [1:05:17<22:59,  1.71s/batch]Batch 2400/3201 Done, mean position loss: 21.32138741970062\n",
      "Training FF2:  76%|███████████████▏    | 2426/3201 [1:05:16<21:27,  1.66s/batch]Batch 2400/3201 Done, mean position loss: 21.269095396995546\n",
      "Training FF2:  74%|██████████████▊     | 2370/3201 [1:05:17<23:54,  1.73s/batch]Batch 2400/3201 Done, mean position loss: 21.017104721069337\n",
      "Training FF2:  76%|███████████████▏    | 2435/3201 [1:05:20<25:57,  2.03s/batch]Batch 2400/3201 Done, mean position loss: 20.919843339920043\n",
      "Training FF2:  75%|███████████████     | 2410/3201 [1:05:27<23:28,  1.78s/batch]Batch 2400/3201 Done, mean position loss: 21.322795143127443\n",
      "Training FF2:  76%|███████████████▏    | 2424/3201 [1:05:28<20:50,  1.61s/batch]Batch 2400/3201 Done, mean position loss: 21.271539974212647\n",
      "Training FF2:  75%|███████████████     | 2401/3201 [1:05:28<21:27,  1.61s/batch]Batch 2400/3201 Done, mean position loss: 21.602048170566558\n",
      "Training FF2:  76%|███████████████▏    | 2431/3201 [1:05:30<19:08,  1.49s/batch]Batch 2400/3201 Done, mean position loss: 21.135474264621735\n",
      "Training FF2:  74%|██████████████▊     | 2367/3201 [1:05:32<28:17,  2.04s/batch]Batch 2400/3201 Done, mean position loss: 21.349393253326415\n",
      "Training FF2:  76%|███████████████▏    | 2437/3201 [1:05:34<17:36,  1.38s/batch]Batch 2400/3201 Done, mean position loss: 21.943905923366547\n",
      "Training FF2:  76%|███████████████▏    | 2432/3201 [1:05:36<22:44,  1.77s/batch]Batch 2400/3201 Done, mean position loss: 21.421434469223023\n",
      "Training FF2:  77%|███████████████▎    | 2460/3201 [1:05:59<19:45,  1.60s/batch]Batch 2400/3201 Done, mean position loss: 21.357391817569734\n",
      "Training FF2:  75%|██████████████▉     | 2386/3201 [1:06:06<24:50,  1.83s/batch]Batch 2400/3201 Done, mean position loss: 20.950524463653565\n",
      "Training FF2:  76%|███████████████▏    | 2435/3201 [1:06:10<21:14,  1.66s/batch]Batch 2400/3201 Done, mean position loss: 21.342481877803802\n",
      "Training FF2:  77%|███████████████▎    | 2451/3201 [1:06:14<25:53,  2.07s/batch]Batch 2400/3201 Done, mean position loss: 21.33366831302643\n",
      "Training FF2:  76%|███████████████▏    | 2439/3201 [1:06:34<19:22,  1.53s/batch]Batch 2400/3201 Done, mean position loss: 21.26984752893448\n",
      "Training FF2:  77%|███████████████▎    | 2456/3201 [1:06:40<23:10,  1.87s/batch]Batch 2500/3201 Done, mean position loss: 21.23237168073654\n",
      "Training FF2:  77%|███████████████▍    | 2478/3201 [1:07:03<18:06,  1.50s/batch]Batch 2500/3201 Done, mean position loss: 21.230125925540925\n",
      "Training FF2:  78%|███████████████▌    | 2482/3201 [1:07:07<19:25,  1.62s/batch]Batch 2500/3201 Done, mean position loss: 21.296171970367432\n",
      "Training FF2:  77%|███████████████▎    | 2460/3201 [1:07:08<21:21,  1.73s/batch]Batch 2500/3201 Done, mean position loss: 20.88368476867676\n",
      "Training FF2:  77%|███████████████▍    | 2464/3201 [1:07:16<23:53,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.81890995979309\n",
      "Training FF2:  78%|███████████████▌    | 2495/3201 [1:07:19<19:50,  1.69s/batch]Batch 2500/3201 Done, mean position loss: 20.89361513853073\n",
      "Training FF2:  77%|███████████████▍    | 2470/3201 [1:07:20<22:15,  1.83s/batch]Batch 2500/3201 Done, mean position loss: 20.805999462604525\n",
      "Training FF2:  77%|███████████████▍    | 2473/3201 [1:07:21<19:22,  1.60s/batch]Batch 2500/3201 Done, mean position loss: 21.439119873046877\n",
      "Training FF2:  76%|███████████████▎    | 2448/3201 [1:07:24<23:49,  1.90s/batch]Batch 2500/3201 Done, mean position loss: 21.523566710948945\n",
      "Batch 2500/3201 Done, mean position loss: 21.26644391298294\n",
      "Training FF2:  76%|███████████████▎    | 2445/3201 [1:07:25<23:15,  1.85s/batch]Batch 2500/3201 Done, mean position loss: 21.042222719192505\n",
      "Training FF2:  78%|███████████████▋    | 2503/3201 [1:07:27<18:49,  1.62s/batch]Batch 2500/3201 Done, mean position loss: 21.278785991668702\n",
      "Training FF2:  78%|███████████████▌    | 2500/3201 [1:07:28<17:54,  1.53s/batch]Batch 2500/3201 Done, mean position loss: 21.618013980388643\n",
      "Training FF2:  78%|███████████████▋    | 2504/3201 [1:07:31<19:13,  1.66s/batch]Batch 2500/3201 Done, mean position loss: 21.477616252899168\n",
      "Training FF2:  76%|███████████████▎    | 2448/3201 [1:07:29<21:59,  1.75s/batch]Batch 2500/3201 Done, mean position loss: 21.378580698966978\n",
      "Training FF2:  78%|███████████████▋    | 2510/3201 [1:07:34<16:29,  1.43s/batch]Batch 2500/3201 Done, mean position loss: 21.255007770061493\n",
      "Training FF2:  78%|███████████████▌    | 2484/3201 [1:07:40<19:04,  1.60s/batch]Batch 2500/3201 Done, mean position loss: 21.33212714910507\n",
      "Training FF2:  78%|███████████████▌    | 2493/3201 [1:07:41<16:58,  1.44s/batch]Batch 2500/3201 Done, mean position loss: 21.38427759885788\n",
      "Training FF2:  79%|███████████████▋    | 2516/3201 [1:07:50<18:48,  1.65s/batch]Batch 2500/3201 Done, mean position loss: 22.375272364616393\n",
      "Training FF2:  79%|███████████████▊    | 2532/3201 [1:07:52<21:44,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.355959544181825\n",
      "Training FF2:  78%|███████████████▌    | 2500/3201 [1:07:57<19:42,  1.69s/batch]Batch 2500/3201 Done, mean position loss: 21.21048030614853\n",
      "Training FF2:  78%|███████████████▌    | 2486/3201 [1:07:57<22:03,  1.85s/batch]Batch 2500/3201 Done, mean position loss: 22.263181347846984\n",
      "Training FF2:  79%|███████████████▊    | 2521/3201 [1:07:57<16:46,  1.48s/batch]Batch 2500/3201 Done, mean position loss: 21.178333556652067\n",
      "Training FF2:  78%|███████████████▌    | 2490/3201 [1:08:02<18:19,  1.55s/batch]Batch 2500/3201 Done, mean position loss: 21.245302839279177\n",
      "Training FF2:  79%|███████████████▊    | 2527/3201 [1:08:02<18:31,  1.65s/batch]Batch 2500/3201 Done, mean position loss: 21.187191505432125\n",
      "Training FF2:  78%|███████████████▌    | 2489/3201 [1:08:03<22:10,  1.87s/batch]Batch 2500/3201 Done, mean position loss: 20.911591181755064\n",
      "Training FF2:  79%|███████████████▉    | 2542/3201 [1:08:07<15:55,  1.45s/batch]Batch 2500/3201 Done, mean position loss: 21.006694426536562\n",
      "Training FF2:  79%|███████████████▋    | 2517/3201 [1:08:07<19:15,  1.69s/batch]Batch 2500/3201 Done, mean position loss: 21.574065730571746\n",
      "Training FF2:  79%|███████████████▊    | 2525/3201 [1:08:08<19:26,  1.73s/batch]Batch 2500/3201 Done, mean position loss: 21.323131828308107\n",
      "Training FF2:  78%|███████████████▋    | 2506/3201 [1:08:08<18:01,  1.56s/batch]Batch 2500/3201 Done, mean position loss: 21.265900983810425\n",
      "Training FF2:  79%|███████████████▊    | 2533/3201 [1:08:16<17:35,  1.58s/batch]Batch 2500/3201 Done, mean position loss: 21.237478299140932\n",
      "Training FF2:  79%|███████████████▊    | 2525/3201 [1:08:20<17:11,  1.53s/batch]Batch 2500/3201 Done, mean position loss: 21.441017627716064\n",
      "Training FF2:  77%|███████████████▍    | 2477/3201 [1:08:19<21:03,  1.75s/batch]Batch 2500/3201 Done, mean position loss: 21.89733057260513\n",
      "Training FF2:  78%|███████████████▌    | 2482/3201 [1:08:20<19:34,  1.63s/batch]Batch 2500/3201 Done, mean position loss: 21.135560121536255\n",
      "Training FF2:  79%|███████████████▊    | 2532/3201 [1:08:23<19:42,  1.77s/batch]Batch 2500/3201 Done, mean position loss: 21.326691634655\n",
      "Training FF2:  80%|███████████████▉    | 2546/3201 [1:08:51<20:16,  1.86s/batch]Batch 2500/3201 Done, mean position loss: 21.32346062898636\n",
      "Training FF2:  78%|███████████████▋    | 2504/3201 [1:08:55<16:40,  1.44s/batch]Batch 2500/3201 Done, mean position loss: 20.938508381843565\n",
      "Training FF2:  79%|███████████████▊    | 2537/3201 [1:09:01<18:35,  1.68s/batch]Batch 2500/3201 Done, mean position loss: 21.27184416770935\n",
      "Training FF2:  80%|███████████████▉    | 2556/3201 [1:09:01<17:19,  1.61s/batch]Batch 2500/3201 Done, mean position loss: 21.303205099105835\n",
      "Training FF2:  80%|███████████████▉    | 2546/3201 [1:09:21<18:32,  1.70s/batch]Batch 2600/3201 Done, mean position loss: 21.1942310833931\n",
      "Training FF2:  80%|████████████████    | 2574/3201 [1:09:22<17:12,  1.65s/batch]Batch 2500/3201 Done, mean position loss: 21.226481487751006\n",
      "Training FF2:  80%|████████████████    | 2567/3201 [1:09:45<15:48,  1.50s/batch]Batch 2600/3201 Done, mean position loss: 21.210810537338254\n",
      "Training FF2:  82%|████████████████▎   | 2620/3201 [1:09:50<14:47,  1.53s/batch]Batch 2600/3201 Done, mean position loss: 20.899046671390536\n",
      "Training FF2:  81%|████████████████▏   | 2594/3201 [1:09:54<19:37,  1.94s/batch]Batch 2600/3201 Done, mean position loss: 21.275794031620027\n",
      "Training FF2:  81%|████████████████▏   | 2592/3201 [1:10:04<19:50,  1.95s/batch]Batch 2600/3201 Done, mean position loss: 20.79295706272125\n",
      "Training FF2:  80%|████████████████    | 2563/3201 [1:10:05<17:36,  1.66s/batch]Batch 2600/3201 Done, mean position loss: 20.87248532772064\n",
      "Training FF2:  80%|███████████████▉    | 2546/3201 [1:10:06<21:03,  1.93s/batch]Batch 2600/3201 Done, mean position loss: 21.34643347263336\n",
      "Training FF2:  81%|████████████████▏   | 2593/3201 [1:10:07<17:10,  1.69s/batch]Batch 2600/3201 Done, mean position loss: 21.788849482536314\n",
      "Training FF2:  81%|████████████████    | 2579/3201 [1:10:07<16:28,  1.59s/batch]Batch 2600/3201 Done, mean position loss: 21.230923464298247\n",
      "Training FF2:  81%|████████████████▏   | 2594/3201 [1:10:10<15:25,  1.52s/batch]Batch 2600/3201 Done, mean position loss: 21.53018836259842\n",
      "Training FF2:  80%|███████████████▉    | 2549/3201 [1:10:11<15:48,  1.45s/batch]Batch 2600/3201 Done, mean position loss: 21.574656369686128\n",
      "Training FF2:  81%|████████████████▏   | 2589/3201 [1:10:15<17:48,  1.75s/batch]Batch 2600/3201 Done, mean position loss: 21.413731458187105\n",
      "Training FF2:  80%|████████████████    | 2572/3201 [1:10:15<17:14,  1.65s/batch]Batch 2600/3201 Done, mean position loss: 21.477956147193908\n",
      "Training FF2:  81%|████████████████    | 2580/3201 [1:10:16<16:54,  1.63s/batch]Batch 2600/3201 Done, mean position loss: 21.254955821037292\n",
      "Training FF2:  81%|████████████████▎   | 2604/3201 [1:10:19<16:27,  1.65s/batch]Batch 2600/3201 Done, mean position loss: 20.997412843704225\n",
      "Training FF2:  82%|████████████████▎   | 2620/3201 [1:10:19<15:35,  1.61s/batch]Batch 2600/3201 Done, mean position loss: 21.193454618453977\n",
      "Training FF2:  81%|████████████████▎   | 2606/3201 [1:10:21<14:55,  1.51s/batch]Batch 2600/3201 Done, mean position loss: 21.370172905921937\n",
      "Training FF2:  81%|████████████████▏   | 2594/3201 [1:10:23<15:50,  1.57s/batch]Batch 2600/3201 Done, mean position loss: 21.332487494945525\n",
      "Training FF2:  82%|████████████████▎   | 2617/3201 [1:10:33<16:45,  1.72s/batch]Batch 2600/3201 Done, mean position loss: 21.339394056797026\n",
      "Training FF2:  82%|████████████████▍   | 2632/3201 [1:10:35<16:23,  1.73s/batch]Batch 2600/3201 Done, mean position loss: 22.373354427814483\n",
      "Training FF2:  81%|████████████████▏   | 2587/3201 [1:10:41<18:43,  1.83s/batch]Batch 2600/3201 Done, mean position loss: 22.18131690740585\n",
      "Training FF2:  81%|████████████████▎   | 2608/3201 [1:10:46<17:40,  1.79s/batch]Batch 2600/3201 Done, mean position loss: 21.216304383277894\n",
      "Training FF2:  81%|████████████████▏   | 2597/3201 [1:10:45<16:03,  1.59s/batch]Batch 2600/3201 Done, mean position loss: 21.16146926879883\n",
      "Training FF2:  81%|████████████████▎   | 2601/3201 [1:10:47<17:36,  1.76s/batch]Batch 2600/3201 Done, mean position loss: 21.151096487045287\n",
      "Training FF2:  81%|████████████████▏   | 2589/3201 [1:10:49<18:12,  1.78s/batch]Batch 2600/3201 Done, mean position loss: 21.152087969779966\n",
      "Training FF2:  81%|████████████████▏   | 2593/3201 [1:10:49<14:32,  1.43s/batch]Batch 2600/3201 Done, mean position loss: 21.544888415336608\n",
      "Training FF2:  82%|████████████████▎   | 2612/3201 [1:10:52<14:54,  1.52s/batch]Batch 2600/3201 Done, mean position loss: 20.89472859144211\n",
      "Training FF2:  81%|████████████████▎   | 2605/3201 [1:10:51<16:02,  1.61s/batch]Batch 2600/3201 Done, mean position loss: 21.237473046779634\n",
      "Training FF2:  82%|████████████████▍   | 2629/3201 [1:10:51<14:47,  1.55s/batch]Batch 2600/3201 Done, mean position loss: 21.315538082122806\n",
      "Training FF2:  81%|████████████████▏   | 2592/3201 [1:10:53<16:14,  1.60s/batch]Batch 2600/3201 Done, mean position loss: 20.978044428825378\n",
      "Training FF2:  81%|████████████████▏   | 2600/3201 [1:11:03<16:27,  1.64s/batch]Batch 2600/3201 Done, mean position loss: 21.136884310245513\n",
      "Training FF2:  82%|████████████████▎   | 2619/3201 [1:11:05<16:16,  1.68s/batch]Batch 2600/3201 Done, mean position loss: 21.38717659235001\n",
      "Training FF2:  80%|████████████████    | 2576/3201 [1:11:06<18:01,  1.73s/batch]Batch 2600/3201 Done, mean position loss: 21.901353640556337\n",
      "Training FF2:  82%|████████████████▍   | 2629/3201 [1:11:06<14:35,  1.53s/batch]Batch 2600/3201 Done, mean position loss: 21.216714379787447\n",
      "Training FF2:  82%|████████████████▎   | 2619/3201 [1:11:11<15:13,  1.57s/batch]Batch 2600/3201 Done, mean position loss: 21.286130390167234\n",
      "Training FF2:  83%|████████████████▌   | 2653/3201 [1:11:36<16:23,  1.80s/batch]Batch 2600/3201 Done, mean position loss: 20.896767537593842\n",
      "Training FF2:  83%|████████████████▌   | 2657/3201 [1:11:38<16:23,  1.81s/batch]Batch 2600/3201 Done, mean position loss: 21.323848984241486\n",
      "Training FF2:  81%|████████████████▏   | 2597/3201 [1:11:41<17:17,  1.72s/batch]Batch 2600/3201 Done, mean position loss: 21.28905873775482\n",
      "Training FF2:  83%|████████████████▌   | 2657/3201 [1:11:49<16:51,  1.86s/batch]Batch 2600/3201 Done, mean position loss: 21.24597368478775\n",
      "Training FF2:  82%|████████████████▎   | 2615/3201 [1:12:01<15:29,  1.59s/batch]Batch 2700/3201 Done, mean position loss: 21.197809286117554\n",
      "Training FF2:  82%|████████████████▎   | 2619/3201 [1:12:10<18:48,  1.94s/batch]Batch 2600/3201 Done, mean position loss: 21.19865761756897\n",
      "Training FF2:  83%|████████████████▋   | 2662/3201 [1:12:26<14:42,  1.64s/batch]Batch 2700/3201 Done, mean position loss: 21.155740027427672\n",
      "Training FF2:  83%|████████████████▋   | 2662/3201 [1:12:35<15:28,  1.72s/batch]Batch 2700/3201 Done, mean position loss: 20.840312857627872\n",
      "Training FF2:  84%|████████████████▊   | 2694/3201 [1:12:46<13:35,  1.61s/batch]Batch 2700/3201 Done, mean position loss: 21.25414888381958\n",
      "Training FF2:  84%|████████████████▊   | 2699/3201 [1:12:46<13:03,  1.56s/batch]Batch 2700/3201 Done, mean position loss: 20.769812057018278\n",
      "Training FF2:  85%|█████████████████   | 2732/3201 [1:12:51<12:15,  1.57s/batch]Batch 2700/3201 Done, mean position loss: 20.87193568229675\n",
      "Training FF2:  84%|████████████████▋   | 2680/3201 [1:12:53<15:18,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 21.212253999710082\n",
      "Training FF2:  83%|████████████████▌   | 2644/3201 [1:12:51<15:33,  1.68s/batch]Batch 2700/3201 Done, mean position loss: 21.30104262590408\n",
      "Training FF2:  84%|████████████████▊   | 2686/3201 [1:12:52<15:08,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 21.759653763771055\n",
      "Training FF2:  84%|████████████████▊   | 2700/3201 [1:12:56<14:21,  1.72s/batch]Batch 2700/3201 Done, mean position loss: 21.38523400783539\n",
      "Training FF2:  83%|████████████████▋   | 2668/3201 [1:12:57<15:11,  1.71s/batch]Batch 2700/3201 Done, mean position loss: 21.546003234386443\n",
      "Training FF2:  84%|████████████████▋   | 2680/3201 [1:12:57<12:41,  1.46s/batch]Batch 2700/3201 Done, mean position loss: 21.477098155021668\n",
      "Training FF2:  84%|████████████████▋   | 2678/3201 [1:12:58<13:53,  1.59s/batch]Batch 2700/3201 Done, mean position loss: 21.464301240444183\n",
      "Training FF2:  83%|████████████████▋   | 2670/3201 [1:12:58<14:34,  1.65s/batch]Batch 2700/3201 Done, mean position loss: 21.338093638420105\n",
      "Training FF2:  84%|████████████████▋   | 2675/3201 [1:13:01<18:11,  2.08s/batch]Batch 2700/3201 Done, mean position loss: 21.24111213207245\n",
      "Training FF2:  84%|████████████████▊   | 2694/3201 [1:13:01<16:04,  1.90s/batch]Batch 2700/3201 Done, mean position loss: 21.001407141685487\n",
      "Training FF2:  85%|████████████████▉   | 2711/3201 [1:13:06<12:28,  1.53s/batch]Batch 2700/3201 Done, mean position loss: 21.191009163856506\n",
      "Training FF2:  83%|████████████████▌   | 2656/3201 [1:13:07<14:56,  1.64s/batch]Batch 2700/3201 Done, mean position loss: 21.308649122714996\n",
      "Training FF2:  85%|████████████████▉   | 2711/3201 [1:13:12<12:31,  1.53s/batch]Batch 2700/3201 Done, mean position loss: 21.296104788780212\n",
      "Training FF2:  85%|████████████████▉   | 2713/3201 [1:13:18<13:38,  1.68s/batch]Batch 2700/3201 Done, mean position loss: 22.37702915906906\n",
      "Training FF2:  85%|█████████████████   | 2727/3201 [1:13:26<11:52,  1.50s/batch]Batch 2700/3201 Done, mean position loss: 22.14488335609436\n",
      "Training FF2:  85%|████████████████▉   | 2718/3201 [1:13:30<12:51,  1.60s/batch]Batch 2700/3201 Done, mean position loss: 21.13314688205719\n",
      "Training FF2:  86%|█████████████████   | 2737/3201 [1:13:32<12:00,  1.55s/batch]Batch 2700/3201 Done, mean position loss: 21.18870504617691\n",
      "Training FF2:  85%|█████████████████   | 2721/3201 [1:13:33<12:52,  1.61s/batch]Batch 2700/3201 Done, mean position loss: 21.51145891904831\n",
      "Training FF2:  84%|████████████████▊   | 2692/3201 [1:13:34<15:51,  1.87s/batch]Batch 2700/3201 Done, mean position loss: 20.882835993766783\n",
      "Training FF2:  84%|████████████████▊   | 2687/3201 [1:13:36<14:44,  1.72s/batch]Batch 2700/3201 Done, mean position loss: 21.18450400352478\n",
      "Training FF2:  84%|████████████████▊   | 2698/3201 [1:13:38<16:56,  2.02s/batch]Batch 2700/3201 Done, mean position loss: 21.126333436965943\n",
      "Training FF2:  85%|█████████████████   | 2734/3201 [1:13:39<13:44,  1.77s/batch]Batch 2700/3201 Done, mean position loss: 21.284000618457796\n",
      "Training FF2:  83%|████████████████▋   | 2667/3201 [1:13:40<19:11,  2.16s/batch]Batch 2700/3201 Done, mean position loss: 21.0817257976532\n",
      "Training FF2:  85%|████████████████▉   | 2711/3201 [1:13:44<12:19,  1.51s/batch]Batch 2700/3201 Done, mean position loss: 20.981299674510957\n",
      "Training FF2:  85%|████████████████▉   | 2717/3201 [1:13:44<13:49,  1.71s/batch]Batch 2700/3201 Done, mean position loss: 21.143093879222867\n",
      "Training FF2:  84%|████████████████▊   | 2681/3201 [1:13:46<11:46,  1.36s/batch]Batch 2700/3201 Done, mean position loss: 21.928199813365936\n",
      "Training FF2:  86%|█████████████████   | 2737/3201 [1:13:49<12:56,  1.67s/batch]Batch 2700/3201 Done, mean position loss: 21.346556096076966\n",
      "Training FF2:  86%|█████████████████▏  | 2741/3201 [1:13:50<08:57,  1.17s/batch]Batch 2700/3201 Done, mean position loss: 21.17693795442581\n",
      "Training FF2:  86%|█████████████████▏  | 2745/3201 [1:14:00<13:11,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 21.274032480716706\n",
      "Training FF2:  85%|█████████████████   | 2723/3201 [1:14:21<14:40,  1.84s/batch]Batch 2700/3201 Done, mean position loss: 21.265375378131868\n",
      "Training FF2:  85%|█████████████████   | 2727/3201 [1:14:22<13:43,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 20.88059248447418\n",
      "Training FF2:  85%|████████████████▉   | 2708/3201 [1:14:31<11:35,  1.41s/batch]Batch 2700/3201 Done, mean position loss: 21.2346755695343\n",
      "Training FF2:  85%|█████████████████   | 2735/3201 [1:14:37<11:57,  1.54s/batch]Batch 2800/3201 Done, mean position loss: 21.20204982280731\n",
      "Training FF2:  87%|█████████████████▎  | 2777/3201 [1:14:38<12:12,  1.73s/batch]Batch 2700/3201 Done, mean position loss: 21.28072921037674\n",
      "Training FF2:  86%|█████████████████▏  | 2744/3201 [1:14:57<12:52,  1.69s/batch]Batch 2700/3201 Done, mean position loss: 21.155936024188996\n",
      "Training FF2:  85%|█████████████████   | 2721/3201 [1:15:10<15:00,  1.88s/batch]Batch 2800/3201 Done, mean position loss: 21.13474219799042\n",
      "Training FF2:  87%|█████████████████▍  | 2794/3201 [1:15:15<10:06,  1.49s/batch]Batch 2800/3201 Done, mean position loss: 20.859267940521242\n",
      "Training FF2:  87%|█████████████████▍  | 2783/3201 [1:15:27<13:15,  1.90s/batch]Batch 2800/3201 Done, mean position loss: 21.286725974082948\n",
      "Training FF2:  87%|█████████████████▎  | 2770/3201 [1:15:28<12:45,  1.78s/batch]Batch 2800/3201 Done, mean position loss: 21.72283402442932\n",
      "Training FF2:  87%|█████████████████▎  | 2769/3201 [1:15:28<09:35,  1.33s/batch]Batch 2800/3201 Done, mean position loss: 20.74460096120834\n",
      "Training FF2:  87%|█████████████████▍  | 2786/3201 [1:15:31<11:21,  1.64s/batch]Batch 2800/3201 Done, mean position loss: 21.238070728778837\n",
      "Training FF2:  87%|█████████████████▎  | 2773/3201 [1:15:33<15:55,  2.23s/batch]Batch 2800/3201 Done, mean position loss: 20.8752414727211\n",
      "Training FF2:  88%|█████████████████▌  | 2804/3201 [1:15:34<11:40,  1.76s/batch]Batch 2800/3201 Done, mean position loss: 21.227232465744017\n",
      "Training FF2:  87%|█████████████████▎  | 2775/3201 [1:15:37<13:15,  1.87s/batch]Batch 2800/3201 Done, mean position loss: 21.457258646488192\n",
      "Training FF2:  87%|█████████████████▎  | 2777/3201 [1:15:42<13:46,  1.95s/batch]Batch 2800/3201 Done, mean position loss: 21.21036071538925\n",
      "Training FF2:  88%|█████████████████▌  | 2801/3201 [1:15:42<13:21,  2.00s/batch]Batch 2800/3201 Done, mean position loss: 21.33290318012238\n",
      "Training FF2:  88%|█████████████████▌  | 2803/3201 [1:15:44<10:42,  1.61s/batch]Batch 2800/3201 Done, mean position loss: 21.53812230348587\n",
      "Training FF2:  86%|█████████████████▏  | 2754/3201 [1:15:45<11:23,  1.53s/batch]Batch 2800/3201 Done, mean position loss: 21.462262477874756\n",
      "Training FF2:  88%|█████████████████▋  | 2821/3201 [1:15:46<09:02,  1.43s/batch]Batch 2800/3201 Done, mean position loss: 20.978196849823\n",
      "Training FF2:  88%|█████████████████▋  | 2823/3201 [1:15:48<11:14,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.33639183521271\n",
      "Training FF2:  87%|█████████████████▎  | 2769/3201 [1:15:52<12:34,  1.75s/batch]Batch 2800/3201 Done, mean position loss: 21.30764487028122\n",
      "Training FF2:  87%|█████████████████▍  | 2785/3201 [1:15:53<09:55,  1.43s/batch]Batch 2800/3201 Done, mean position loss: 21.29662296295166\n",
      "Training FF2:  88%|█████████████████▌  | 2812/3201 [1:15:56<11:20,  1.75s/batch]Batch 2800/3201 Done, mean position loss: 21.15860124826431\n",
      "Training FF2:  88%|█████████████████▌  | 2818/3201 [1:16:05<08:51,  1.39s/batch]Batch 2800/3201 Done, mean position loss: 22.31989451169968\n",
      "Training FF2:  87%|█████████████████▍  | 2789/3201 [1:16:10<12:46,  1.86s/batch]Batch 2800/3201 Done, mean position loss: 21.113248529434202\n",
      "Training FF2:  87%|█████████████████▍  | 2789/3201 [1:16:11<11:16,  1.64s/batch]Batch 2800/3201 Done, mean position loss: 21.184435102939606\n",
      "Training FF2:  89%|█████████████████▋  | 2839/3201 [1:16:17<09:39,  1.60s/batch]Batch 2800/3201 Done, mean position loss: 21.305164926052093\n",
      "Training FF2:  87%|█████████████████▍  | 2794/3201 [1:16:18<10:54,  1.61s/batch]Batch 2800/3201 Done, mean position loss: 22.088693017959596\n",
      "Training FF2:  88%|█████████████████▋  | 2831/3201 [1:16:20<10:34,  1.71s/batch]Batch 2800/3201 Done, mean position loss: 21.18155863761902\n",
      "Training FF2:  87%|█████████████████▍  | 2800/3201 [1:16:20<11:29,  1.72s/batch]Batch 2800/3201 Done, mean position loss: 21.48504247188568\n",
      "Training FF2:  88%|█████████████████▌  | 2802/3201 [1:16:23<12:02,  1.81s/batch]Batch 2800/3201 Done, mean position loss: 20.86042307853699\n",
      "Training FF2:  88%|█████████████████▋  | 2823/3201 [1:16:24<11:25,  1.81s/batch]Batch 2800/3201 Done, mean position loss: 21.906889584064484\n",
      "Training FF2:  88%|█████████████████▋  | 2825/3201 [1:16:24<08:46,  1.40s/batch]Batch 2800/3201 Done, mean position loss: 20.94270626306534\n",
      "Training FF2:  88%|█████████████████▌  | 2805/3201 [1:16:28<10:32,  1.60s/batch]Batch 2800/3201 Done, mean position loss: 21.10755901813507\n",
      "Training FF2:  88%|█████████████████▋  | 2830/3201 [1:16:30<09:48,  1.59s/batch]Batch 2800/3201 Done, mean position loss: 21.094321606159212\n",
      "Training FF2:  88%|█████████████████▌  | 2807/3201 [1:16:31<10:24,  1.59s/batch]Batch 2800/3201 Done, mean position loss: 21.320229063034056\n",
      "Training FF2:  87%|█████████████████▎  | 2776/3201 [1:16:30<12:41,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.12347324848175\n",
      "Training FF2:  88%|█████████████████▌  | 2807/3201 [1:16:40<11:05,  1.69s/batch]Batch 2800/3201 Done, mean position loss: 21.24240881204605\n",
      "Training FF2:  87%|█████████████████▍  | 2785/3201 [1:16:43<10:00,  1.44s/batch]Batch 2800/3201 Done, mean position loss: 21.181285996437072\n",
      "Training FF2:  90%|█████████████████▉  | 2867/3201 [1:17:00<08:54,  1.60s/batch]Batch 2800/3201 Done, mean position loss: 21.251272099018095\n",
      "Training FF2:  89%|█████████████████▋  | 2833/3201 [1:17:11<10:16,  1.68s/batch]Batch 2800/3201 Done, mean position loss: 20.849820623397825\n",
      "Training FF2:  88%|█████████████████▋  | 2831/3201 [1:17:11<10:20,  1.68s/batch]Batch 2800/3201 Done, mean position loss: 21.24047882795334\n",
      "Training FF2:  89%|█████████████████▉  | 2863/3201 [1:17:18<09:06,  1.62s/batch]Batch 2900/3201 Done, mean position loss: 21.137453804016115\n",
      "Training FF2:  90%|█████████████████▉  | 2869/3201 [1:17:26<08:33,  1.55s/batch]Batch 2800/3201 Done, mean position loss: 21.200638561248777\n",
      "Training FF2:  89%|█████████████████▊  | 2853/3201 [1:17:44<07:48,  1.35s/batch]Batch 2800/3201 Done, mean position loss: 21.108049223423002\n",
      "Training FF2:  89%|█████████████████▊  | 2860/3201 [1:17:54<09:51,  1.73s/batch]Batch 2900/3201 Done, mean position loss: 21.11815304040909\n",
      "Training FF2:  90%|██████████████████  | 2882/3201 [1:17:58<09:53,  1.86s/batch]Batch 2900/3201 Done, mean position loss: 20.830730912685393\n",
      "Training FF2:  90%|█████████████████▉  | 2872/3201 [1:18:08<10:24,  1.90s/batch]Batch 2900/3201 Done, mean position loss: 21.281145133972167\n",
      "Training FF2:  90%|██████████████████  | 2891/3201 [1:18:10<09:08,  1.77s/batch]Batch 2900/3201 Done, mean position loss: 21.209906661510466\n",
      "Training FF2:  90%|██████████████████  | 2885/3201 [1:18:12<09:29,  1.80s/batch]Batch 2900/3201 Done, mean position loss: 21.658670234680173\n",
      "Training FF2:  91%|██████████████████▏ | 2904/3201 [1:18:11<07:21,  1.49s/batch]Batch 2900/3201 Done, mean position loss: 20.73861717224121\n",
      "Training FF2:  89%|█████████████████▋  | 2839/3201 [1:18:16<11:19,  1.88s/batch]Batch 2900/3201 Done, mean position loss: 21.204847679138183\n",
      "Training FF2:  89%|█████████████████▉  | 2861/3201 [1:18:18<09:00,  1.59s/batch]Batch 2900/3201 Done, mean position loss: 20.85269697904587\n",
      "Training FF2:  90%|██████████████████  | 2892/3201 [1:18:24<10:05,  1.96s/batch]Batch 2900/3201 Done, mean position loss: 21.430344836711882\n",
      "Training FF2:  91%|██████████████████  | 2898/3201 [1:18:26<07:33,  1.50s/batch]Batch 2900/3201 Done, mean position loss: 21.300046646595\n",
      "Training FF2:  90%|█████████████████▉  | 2878/3201 [1:18:26<08:34,  1.59s/batch]Batch 2900/3201 Done, mean position loss: 21.455310401916506\n",
      "Training FF2:  90%|█████████████████▉  | 2875/3201 [1:18:30<08:35,  1.58s/batch]Batch 2900/3201 Done, mean position loss: 21.15971339941025\n",
      "Training FF2:  90%|█████████████████▉  | 2875/3201 [1:18:30<08:40,  1.60s/batch]Batch 2900/3201 Done, mean position loss: 21.303752472400664\n",
      "Training FF2:  92%|██████████████████▍ | 2949/3201 [1:18:31<06:40,  1.59s/batch]Batch 2900/3201 Done, mean position loss: 21.49666233778\n",
      "Training FF2:  91%|██████████████████▏ | 2915/3201 [1:18:32<08:24,  1.76s/batch]Batch 2900/3201 Done, mean position loss: 20.955925781726833\n",
      "Training FF2:  91%|██████████████████▏ | 2913/3201 [1:18:36<08:03,  1.68s/batch]Batch 2900/3201 Done, mean position loss: 21.293758080005645\n",
      "Training FF2:  90%|█████████████████▉  | 2874/3201 [1:18:39<08:11,  1.50s/batch]Batch 2900/3201 Done, mean position loss: 21.319840590953827\n",
      "Training FF2:  92%|██████████████████▍ | 2956/3201 [1:18:42<06:13,  1.53s/batch]Batch 2900/3201 Done, mean position loss: 21.122180418968203\n",
      "Training FF2:  89%|█████████████████▊  | 2858/3201 [1:18:48<09:25,  1.65s/batch]Batch 2900/3201 Done, mean position loss: 22.366806132793428\n",
      "Training FF2:  91%|██████████████████▏ | 2916/3201 [1:18:52<07:27,  1.57s/batch]Batch 2900/3201 Done, mean position loss: 21.149139914512634\n",
      "Training FF2:  90%|██████████████████  | 2889/3201 [1:18:53<08:27,  1.63s/batch]Batch 2900/3201 Done, mean position loss: 21.108056540489194\n",
      "Training FF2:  90%|██████████████████  | 2884/3201 [1:19:00<09:10,  1.74s/batch]Batch 2900/3201 Done, mean position loss: 21.45888679027557\n",
      "Training FF2:  91%|██████████████████  | 2899/3201 [1:19:00<06:49,  1.35s/batch]Batch 2900/3201 Done, mean position loss: 22.05725077390671\n",
      "Training FF2:  90%|██████████████████  | 2889/3201 [1:19:04<08:57,  1.72s/batch]Batch 2900/3201 Done, mean position loss: 21.10820603132248\n",
      "Training FF2:  91%|██████████████████  | 2900/3201 [1:19:04<08:03,  1.61s/batch]Batch 2900/3201 Done, mean position loss: 21.277478413581846\n",
      "Training FF2:  91%|██████████████████▏ | 2912/3201 [1:19:06<07:08,  1.48s/batch]Batch 2900/3201 Done, mean position loss: 21.893175547122954\n",
      "Training FF2:  92%|██████████████████▍ | 2943/3201 [1:19:08<07:40,  1.78s/batch]Batch 2900/3201 Done, mean position loss: 20.839673821926116\n",
      "Training FF2:  90%|██████████████████  | 2893/3201 [1:19:10<08:29,  1.65s/batch]Batch 2900/3201 Done, mean position loss: 20.94719115972519\n",
      "Training FF2:  92%|██████████████████▍ | 2941/3201 [1:19:12<07:24,  1.71s/batch]Batch 2900/3201 Done, mean position loss: 21.09044843196869\n",
      "Training FF2:  91%|██████████████████▏ | 2901/3201 [1:19:13<09:05,  1.82s/batch]Batch 2900/3201 Done, mean position loss: 21.317042870521547\n",
      "Training FF2:  91%|██████████████████▏ | 2917/3201 [1:19:15<07:35,  1.60s/batch]Batch 2900/3201 Done, mean position loss: 21.125851747989653\n",
      "Training FF2:  92%|██████████████████▎ | 2940/3201 [1:19:14<06:10,  1.42s/batch]Batch 2900/3201 Done, mean position loss: 21.076002068519593\n",
      "Training FF2:  92%|██████████████████▎ | 2937/3201 [1:19:25<07:42,  1.75s/batch]Batch 2900/3201 Done, mean position loss: 21.20325386047363\n",
      "Training FF2:  92%|██████████████████▎ | 2934/3201 [1:19:28<07:34,  1.70s/batch]Batch 2900/3201 Done, mean position loss: 21.116825354099273\n",
      "Training FF2:  91%|██████████████████▎ | 2923/3201 [1:19:44<06:37,  1.43s/batch]Batch 2900/3201 Done, mean position loss: 21.21243991613388\n",
      "Training FF2:  91%|██████████████████▎ | 2924/3201 [1:19:51<07:40,  1.66s/batch]Batch 3000/3201 Done, mean position loss: 21.105971944332122\n",
      "Training FF2:  92%|██████████████████▎ | 2929/3201 [1:19:59<07:16,  1.60s/batch]Batch 2900/3201 Done, mean position loss: 21.194187307357787\n",
      "Training FF2:  92%|██████████████████▍ | 2958/3201 [1:20:01<07:16,  1.79s/batch]Batch 2900/3201 Done, mean position loss: 20.84329922914505\n",
      "Training FF2:  92%|██████████████████▎ | 2938/3201 [1:20:06<06:32,  1.49s/batch]Batch 2900/3201 Done, mean position loss: 21.20893728017807\n",
      "Training FF2:  92%|██████████████████▍ | 2957/3201 [1:20:29<07:09,  1.76s/batch]Batch 2900/3201 Done, mean position loss: 21.094226002693176\n",
      "Training FF2:  94%|██████████████████▋ | 2998/3201 [1:20:36<05:13,  1.54s/batch]Batch 3000/3201 Done, mean position loss: 21.079244534969327\n",
      "Training FF2:  94%|██████████████████▋ | 2996/3201 [1:20:40<05:01,  1.47s/batch]Batch 3000/3201 Done, mean position loss: 20.83016687631607\n",
      "Training FF2:  93%|██████████████████▌ | 2964/3201 [1:20:49<06:36,  1.67s/batch]Batch 3000/3201 Done, mean position loss: 21.633621053695677\n",
      "Training FF2:  93%|██████████████████▌ | 2965/3201 [1:20:50<06:30,  1.65s/batch]Batch 3000/3201 Done, mean position loss: 21.197372527122496\n",
      "Training FF2:  93%|██████████████████▌ | 2970/3201 [1:20:54<06:27,  1.68s/batch]Batch 3000/3201 Done, mean position loss: 20.702856554985047\n",
      "Training FF2:  93%|██████████████████▌ | 2966/3201 [1:20:56<06:42,  1.71s/batch]Batch 3000/3201 Done, mean position loss: 21.243590624332427\n",
      "Training FF2:  94%|██████████████████▋ | 2996/3201 [1:21:01<05:22,  1.57s/batch]Batch 3000/3201 Done, mean position loss: 21.185483951568603\n",
      "Training FF2:  94%|██████████████████▊ | 3016/3201 [1:21:02<05:40,  1.84s/batch]Batch 3000/3201 Done, mean position loss: 20.85636805295944\n",
      "Training FF2:  93%|██████████████████▌ | 2973/3201 [1:21:03<05:42,  1.50s/batch]Batch 3000/3201 Done, mean position loss: 21.434008736610416\n",
      "Training FF2:  94%|██████████████████▊ | 3004/3201 [1:21:05<05:39,  1.72s/batch]Batch 3000/3201 Done, mean position loss: 21.30303722381592\n",
      "Training FF2:  93%|██████████████████▌ | 2963/3201 [1:21:08<05:43,  1.44s/batch]Batch 3000/3201 Done, mean position loss: 21.278441557884214\n",
      "Training FF2:  93%|██████████████████▌ | 2964/3201 [1:21:12<07:19,  1.85s/batch]Batch 3000/3201 Done, mean position loss: 21.402068095207216\n",
      "Training FF2:  93%|██████████████████▌ | 2976/3201 [1:21:14<07:16,  1.94s/batch]Batch 3000/3201 Done, mean position loss: 21.297300968170163\n",
      "Training FF2:  93%|██████████████████▋ | 2990/3201 [1:21:17<05:50,  1.66s/batch]Batch 3000/3201 Done, mean position loss: 21.129631528854368\n",
      "Training FF2:  93%|██████████████████▌ | 2969/3201 [1:21:17<05:49,  1.51s/batch]Batch 3000/3201 Done, mean position loss: 20.967194204330447\n",
      "Training FF2:  94%|██████████████████▊ | 3004/3201 [1:21:22<05:34,  1.70s/batch]Batch 3000/3201 Done, mean position loss: 21.49868937730789\n",
      "Training FF2:  92%|██████████████████▎ | 2935/3201 [1:21:23<07:24,  1.67s/batch]Batch 3000/3201 Done, mean position loss: 21.291399102210995\n",
      "Training FF2:  93%|██████████████████▋ | 2991/3201 [1:21:29<05:58,  1.71s/batch]Batch 3000/3201 Done, mean position loss: 21.127349696159364\n",
      "Training FF2:  94%|██████████████████▉ | 3021/3201 [1:21:31<06:16,  2.09s/batch]Batch 3000/3201 Done, mean position loss: 21.090334169864654\n",
      "Training FF2:  93%|██████████████████▋ | 2989/3201 [1:21:34<06:02,  1.71s/batch]Batch 3000/3201 Done, mean position loss: 21.103823404312134\n",
      "Training FF2:  94%|██████████████████▊ | 3009/3201 [1:21:37<05:32,  1.73s/batch]Batch 3000/3201 Done, mean position loss: 22.25625836610794\n",
      "Training FF2:  93%|██████████████████▋ | 2992/3201 [1:21:40<05:25,  1.56s/batch]Batch 3000/3201 Done, mean position loss: 21.998654317855834\n",
      "Training FF2:  94%|██████████████████▊ | 3008/3201 [1:21:45<05:22,  1.67s/batch]Batch 3000/3201 Done, mean position loss: 21.41436584472656\n",
      "Training FF2:  95%|██████████████████▉ | 3034/3201 [1:21:49<06:11,  2.23s/batch]Batch 3000/3201 Done, mean position loss: 21.09666105747223\n",
      "Training FF2:  94%|██████████████████▉ | 3022/3201 [1:21:51<05:27,  1.83s/batch]Batch 3000/3201 Done, mean position loss: 21.26096781730652\n",
      "Training FF2:  95%|██████████████████▉ | 3036/3201 [1:21:51<05:10,  1.88s/batch]Batch 3000/3201 Done, mean position loss: 20.832756752967832\n",
      "Training FF2:  94%|██████████████████▋ | 3000/3201 [1:21:54<05:45,  1.72s/batch]Batch 3000/3201 Done, mean position loss: 21.086753451824187\n",
      "Training FF2:  95%|██████████████████▉ | 3035/3201 [1:21:55<03:52,  1.40s/batch]Batch 3000/3201 Done, mean position loss: 20.91185186624527\n",
      "Training FF2:  94%|██████████████████▉ | 3022/3201 [1:21:56<04:42,  1.58s/batch]Batch 3000/3201 Done, mean position loss: 21.886985845565796\n",
      "Training FF2:  93%|██████████████████▋ | 2992/3201 [1:21:58<05:34,  1.60s/batch]Batch 3000/3201 Done, mean position loss: 21.281308038234712\n",
      "Training FF2:  95%|██████████████████▉ | 3034/3201 [1:21:58<05:09,  1.85s/batch]Batch 3000/3201 Done, mean position loss: 21.080460646152495\n",
      "Training FF2:  94%|██████████████████▊ | 3002/3201 [1:22:01<05:11,  1.57s/batch]Batch 3000/3201 Done, mean position loss: 21.144205935001374\n",
      "Training FF2:  94%|██████████████████▊ | 3010/3201 [1:22:13<04:42,  1.48s/batch]Batch 3000/3201 Done, mean position loss: 21.160404114723207\n",
      "Batch 3000/3201 Done, mean position loss: 21.10027149677277\n",
      "Training FF2:  95%|███████████████████ | 3046/3201 [1:22:33<04:15,  1.65s/batch]Batch 3000/3201 Done, mean position loss: 21.191660573482515\n",
      "Training FF2:  94%|██████████████████▋ | 2994/3201 [1:22:35<05:22,  1.56s/batch]Batch 3100/3201 Done, mean position loss: 21.097946972846984\n",
      "Training FF2:  94%|██████████████████▊ | 3019/3201 [1:22:46<05:11,  1.71s/batch]Batch 3000/3201 Done, mean position loss: 21.154804010391235\n",
      "Training FF2:  96%|███████████████████▏| 3063/3201 [1:22:47<03:25,  1.49s/batch]Batch 3000/3201 Done, mean position loss: 20.841615397930145\n",
      "Training FF2:  96%|███████████████████▏| 3074/3201 [1:22:52<03:59,  1.89s/batch]Batch 3000/3201 Done, mean position loss: 21.172036266326906\n",
      "Training FF2:  96%|███████████████████▏| 3074/3201 [1:23:14<04:09,  1.96s/batch]Batch 3000/3201 Done, mean position loss: 21.033930823802947\n",
      "Training FF2:  94%|██████████████████▉ | 3021/3201 [1:23:20<04:51,  1.62s/batch]Batch 3100/3201 Done, mean position loss: 21.07600310564041\n",
      "Training FF2:  95%|██████████████████▉ | 3029/3201 [1:23:21<04:34,  1.59s/batch]Batch 3100/3201 Done, mean position loss: 20.81079838037491\n",
      "Training FF2:  97%|███████████████████▎| 3091/3201 [1:23:37<03:20,  1.83s/batch]Batch 3100/3201 Done, mean position loss: 21.18893059015274\n",
      "Training FF2:  95%|██████████████████▉ | 3031/3201 [1:23:37<04:50,  1.71s/batch]Batch 3100/3201 Done, mean position loss: 21.617987134456634\n",
      "Training FF2:  96%|███████████████████▏| 3069/3201 [1:23:41<03:17,  1.50s/batch]Batch 3100/3201 Done, mean position loss: 20.701071095466613\n",
      "Training FF2:  96%|███████████████████▏| 3072/3201 [1:23:43<04:07,  1.92s/batch]Batch 3100/3201 Done, mean position loss: 21.25147228717804\n",
      "Training FF2:  97%|███████████████████▍| 3106/3201 [1:23:45<02:33,  1.61s/batch]Batch 3100/3201 Done, mean position loss: 20.85541004180908\n",
      "Training FF2:  97%|███████████████████▎| 3092/3201 [1:23:50<03:17,  1.81s/batch]Batch 3100/3201 Done, mean position loss: 21.162300186157225\n",
      "Training FF2:  98%|███████████████████▋| 3146/3201 [1:23:50<01:15,  1.37s/batch]Batch 3100/3201 Done, mean position loss: 21.39530701160431\n",
      "Training FF2:  95%|███████████████████ | 3041/3201 [1:23:52<04:23,  1.65s/batch]Batch 3100/3201 Done, mean position loss: 21.240353775024413\n",
      "Training FF2:  95%|███████████████████ | 3048/3201 [1:23:57<04:17,  1.69s/batch]Batch 3100/3201 Done, mean position loss: 21.25279768228531\n",
      "Training FF2:  96%|███████████████████▏| 3079/3201 [1:24:01<03:22,  1.66s/batch]Batch 3100/3201 Done, mean position loss: 21.27761468410492\n",
      "Training FF2:  96%|███████████████████▏| 3066/3201 [1:24:05<04:15,  1.90s/batch]Batch 3100/3201 Done, mean position loss: 21.104470455646513\n",
      "Training FF2:  96%|███████████████████▏| 3077/3201 [1:24:06<02:59,  1.45s/batch]Batch 3100/3201 Done, mean position loss: 21.3624787569046\n",
      "Training FF2:  97%|███████████████████▍| 3106/3201 [1:24:06<02:34,  1.62s/batch]Batch 3100/3201 Done, mean position loss: 20.948683784008026\n",
      "Training FF2:  96%|███████████████████▏| 3072/3201 [1:24:10<03:26,  1.60s/batch]Batch 3100/3201 Done, mean position loss: 21.28008448839188\n",
      "Training FF2:  96%|███████████████████▎| 3085/3201 [1:24:12<03:19,  1.72s/batch]Batch 3100/3201 Done, mean position loss: 21.479383280277254\n",
      "Training FF2:  98%|███████████████████▌| 3134/3201 [1:24:16<01:51,  1.67s/batch]Batch 3100/3201 Done, mean position loss: 21.124480512142185\n",
      "Training FF2:  97%|███████████████████▎| 3089/3201 [1:24:16<03:04,  1.65s/batch]Batch 3100/3201 Done, mean position loss: 21.106761169433593\n",
      "Training FF2:  98%|███████████████████▌| 3122/3201 [1:24:16<02:16,  1.72s/batch]Batch 3100/3201 Done, mean position loss: 21.09696429014206\n",
      "Training FF2:  97%|███████████████████▍| 3117/3201 [1:24:20<02:32,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 22.29270314216614\n",
      "Training FF2:  97%|███████████████████▍| 3109/3201 [1:24:28<02:21,  1.54s/batch]Batch 3100/3201 Done, mean position loss: 21.382952682971954\n",
      "Training FF2:  97%|███████████████████▍| 3116/3201 [1:24:31<02:16,  1.61s/batch]Batch 3100/3201 Done, mean position loss: 21.99279101371765\n",
      "Training FF2:  98%|███████████████████▌| 3130/3201 [1:24:35<02:08,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 21.24069258451462\n",
      "Training FF2:  98%|███████████████████▌| 3123/3201 [1:24:36<02:36,  2.00s/batch]Batch 3100/3201 Done, mean position loss: 20.828090147972105\n",
      "Training FF2:  97%|███████████████████▍| 3102/3201 [1:24:36<02:46,  1.68s/batch]Batch 3100/3201 Done, mean position loss: 21.12131309747696\n",
      "Training FF2:  97%|███████████████████▍| 3108/3201 [1:24:45<02:55,  1.89s/batch]Batch 3100/3201 Done, mean position loss: 21.04166609764099\n",
      "Training FF2:  97%|███████████████████▍| 3117/3201 [1:24:45<02:39,  1.89s/batch]Batch 3100/3201 Done, mean position loss: 21.84988780975342\n",
      "Training FF2:  96%|███████████████████▏| 3069/3201 [1:24:45<04:19,  1.97s/batch]Batch 3100/3201 Done, mean position loss: 21.06266894578934\n",
      "Training FF2:  97%|███████████████████▎| 3100/3201 [1:24:48<02:56,  1.74s/batch]Batch 3100/3201 Done, mean position loss: 20.90137298822403\n",
      "Training FF2:  99%|███████████████████▊| 3178/3201 [1:24:49<00:39,  1.71s/batch]Batch 3100/3201 Done, mean position loss: 21.254737455844875\n",
      "Training FF2:  98%|███████████████████▌| 3140/3201 [1:24:51<01:50,  1.80s/batch]Batch 3100/3201 Done, mean position loss: 21.128266441822053\n",
      "Training FF2:  98%|███████████████████▌| 3138/3201 [1:25:00<01:35,  1.52s/batch]Batch 3100/3201 Done, mean position loss: 21.080252695083615\n",
      "Training FF2:  97%|███████████████████▍| 3117/3201 [1:25:04<02:32,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 21.124509649276735\n",
      "Training FF2:  97%|███████████████████▍| 3114/3201 [1:25:27<02:40,  1.84s/batch]Batch 3100/3201 Done, mean position loss: 21.147781858444212\n",
      "Training FF2:  98%|███████████████████▋| 3145/3201 [1:25:31<01:36,  1.73s/batch]Batch 3200/3201 Done, mean position loss: 21.07162368774414\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:25:30<00:00,  1.60s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3161/3201 [1:25:38<01:08,  1.71s/batch]Batch 3100/3201 Done, mean position loss: 20.82100183010101\n",
      "Training FF2:  99%|███████████████████▊| 3164/3201 [1:25:41<01:00,  1.63s/batch]Batch 3100/3201 Done, mean position loss: 21.10988445520401\n",
      "Training FF2:  97%|███████████████████▍| 3105/3201 [1:25:42<02:37,  1.64s/batch]Batch 3100/3201 Done, mean position loss: 21.183173508644103\n",
      "Training FF2: 100%|███████████████████▉| 3190/3201 [1:26:02<00:16,  1.53s/batch]Batch 3200/3201 Done, mean position loss: 21.061430203914643\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:03<00:00,  1.61s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3179/3201 [1:26:06<00:35,  1.61s/batch]Batch 3100/3201 Done, mean position loss: 21.006305379867555\n",
      "Training FF2:  99%|███████████████████▋| 3158/3201 [1:26:12<01:12,  1.68s/batch]Batch 3200/3201 Done, mean position loss: 20.795694994926453\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:10<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3164/3201 [1:26:16<00:44,  1.21s/batch]Batch 3200/3201 Done, mean position loss: 21.178730485439303\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:16<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  98%|███████████████████▋| 3149/3201 [1:26:25<01:19,  1.53s/batch]Batch 3200/3201 Done, mean position loss: 21.58688919067383\n",
      "Batch 3200/3201 Done, mean position loss: 20.840263295173642\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:24<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:25<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3182/3201 [1:26:32<00:28,  1.50s/batch]Batch 3200/3201 Done, mean position loss: 20.68710057258606\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:30<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3182/3201 [1:26:32<00:29,  1.56s/batch]Batch 3200/3201 Done, mean position loss: 21.231362831592563\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:30<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3186/3201 [1:26:33<00:16,  1.07s/batch]Batch 3200/3201 Done, mean position loss: 21.179644496440886\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:33<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3166/3201 [1:26:37<00:51,  1.48s/batch]Batch 3200/3201 Done, mean position loss: 21.25130156993866\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:37<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3170/3201 [1:26:37<00:36,  1.18s/batch]Batch 3200/3201 Done, mean position loss: 21.386982643604277\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:36<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3158/3201 [1:26:38<01:04,  1.50s/batch]Batch 3200/3201 Done, mean position loss: 21.208377220630645\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:38<00:00,  1.62s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3183/3201 [1:26:44<00:19,  1.09s/batch]Batch 3200/3201 Done, mean position loss: 21.268761022090914\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:44<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  98%|███████████████████▋| 3147/3201 [1:26:46<00:54,  1.01s/batch]Batch 3200/3201 Done, mean position loss: 21.363617072105406\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:46<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3174/3201 [1:26:47<00:30,  1.12s/batch]Batch 3200/3201 Done, mean position loss: 20.919433681964875\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:49<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3182/3201 [1:26:51<00:22,  1.18s/batch]Batch 3200/3201 Done, mean position loss: 21.240885536670685\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:51<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3186/3201 [1:26:52<00:17,  1.16s/batch]Batch 3200/3201 Done, mean position loss: 21.08490150690079\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:51<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3192/3201 [1:26:53<00:12,  1.43s/batch]Batch 3200/3201 Done, mean position loss: 21.085597448349\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:52<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3194/3201 [1:26:53<00:07,  1.04s/batch]Batch 3200/3201 Done, mean position loss: 21.054699149131775\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:54<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3160/3201 [1:26:53<00:44,  1.09s/batch]Batch 3200/3201 Done, mean position loss: 21.499695713520047\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:53<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3154/3201 [1:26:53<00:47,  1.02s/batch]Batch 3200/3201 Done, mean position loss: 21.08673956155777\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:55<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3194/3201 [1:26:58<00:04,  1.55batch/s]Batch 3200/3201 Done, mean position loss: 21.333012216091156\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:26:59<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3188/3201 [1:26:59<00:07,  1.65batch/s]Batch 3200/3201 Done, mean position loss: 21.894504849910735\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:00<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  98%|███████████████████▋| 3150/3201 [1:27:00<00:19,  2.66batch/s]Batch 3200/3201 Done, mean position loss: 21.052769353389742\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:01<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:27:01<00:01,  1.40s/batch]Batch 3200/3201 Done, mean position loss: 22.320418341159822\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:02<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3162/3201 [1:27:02<00:28,  1.38batch/s]Batch 3200/3201 Done, mean position loss: 20.81891263961792\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:02<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3199/3201 [1:27:04<00:01,  1.57batch/s]Batch 3200/3201 Done, mean position loss: 21.056518144607544\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:03<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3198/3201 [1:27:05<00:01,  2.39batch/s]Batch 3200/3201 Done, mean position loss: 21.23260952711105\n",
      "Batch 3200/3201 Done, mean position loss: 21.21371693134308\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:05<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:05<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:27:06<00:00,  2.38batch/s]Batch 3200/3201 Done, mean position loss: 20.894082777500152\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:05<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3178/3201 [1:27:05<00:15,  1.50batch/s]Batch 3200/3201 Done, mean position loss: 21.026662430763245\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:06<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3194/3201 [1:27:06<00:02,  2.70batch/s]Batch 3200/3201 Done, mean position loss: 21.120511898994444\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:06<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3196/3201 [1:27:07<00:01,  3.05batch/s]Batch 3200/3201 Done, mean position loss: 21.80463231801987\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:06<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3182/3201 [1:27:07<00:07,  2.38batch/s]Batch 3200/3201 Done, mean position loss: 21.07363232374191\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:07<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3183/3201 [1:27:07<00:05,  3.40batch/s]Batch 3200/3201 Done, mean position loss: 21.083548612594605\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:08<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3188/3201 [1:27:11<00:03,  4.06batch/s]Batch 3200/3201 Done, mean position loss: 21.168971009254456\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:11<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3187/3201 [1:27:12<00:03,  4.15batch/s]Batch 3200/3201 Done, mean position loss: 21.0736537361145\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:12<00:00,  1.63s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3197/3201 [1:27:14<00:00,  4.33batch/s]Batch 3200/3201 Done, mean position loss: 21.155760850906375\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:14<00:00,  1.64s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3196/3201 [1:27:15<00:01,  4.45batch/s]Batch 3200/3201 Done, mean position loss: 20.970873172283174\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:15<00:00,  1.64s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:27:15<00:00,  4.97batch/s]Batch 3200/3201 Done, mean position loss: 20.801676630973816\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:27:16<00:00,  1.64s/batch]\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "!python ../model.py {0} {network_siz} {n_model} {0} {1} {'FF2'} {-8} {3201}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model00...\n",
      "model01...\n",
      "model02...\n",
      "model03...\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...\n",
      "model13...\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training growing_up:   0%|             | 98/20010 [05:44<15:27:59,  2.80s/batch]Batch 100/20010 Done, mean position loss: 64.45437704086304\n",
      "Training growing_up:   0%|             | 98/20010 [05:45<17:27:03,  3.16s/batch]Batch 100/20010 Done, mean position loss: 89.23802684068679\n",
      "Training growing_up:   0%|             | 99/20010 [05:46<14:33:34,  2.63s/batch]Batch 100/20010 Done, mean position loss: 65.32794777870177\n",
      "Training growing_up:   0%|             | 99/20010 [05:48<16:36:52,  3.00s/batch]Batch 100/20010 Done, mean position loss: 65.8837564277649\n",
      "Training growing_up:   0%|            | 100/20010 [05:49<15:38:56,  2.83s/batch]Batch 100/20010 Done, mean position loss: 99.42263910531997\n",
      "Training growing_up:   0%|            | 100/20010 [05:50<16:00:47,  2.90s/batch]Batch 100/20010 Done, mean position loss: 133.9230136346817\n",
      "Training growing_up:   0%|             | 99/20010 [05:52<16:59:37,  3.07s/batch]Batch 100/20010 Done, mean position loss: 66.06683263540268\n",
      "Training growing_up:   0%|             | 99/20010 [05:51<16:23:52,  2.96s/batch]Batch 100/20010 Done, mean position loss: 66.82869108438491\n",
      "Batch 100/20010 Done, mean position loss: 64.81650829792022\n",
      "Training growing_up:   0%|            | 100/20010 [05:52<16:58:24,  3.07s/batch]Batch 100/20010 Done, mean position loss: 64.13479721307755\n",
      "Training growing_up:   1%|            | 101/20010 [05:52<15:01:28,  2.72s/batch]Batch 100/20010 Done, mean position loss: 65.0387503361702\n",
      "Training growing_up:   1%|            | 104/20010 [05:53<15:26:20,  2.79s/batch]Batch 100/20010 Done, mean position loss: 64.37057027578354\n",
      "Batch 100/20010 Done, mean position loss: 65.50538749694825\n",
      "Training growing_up:   0%|             | 97/20010 [05:53<17:13:19,  3.11s/batch]Batch 100/20010 Done, mean position loss: 105.10988512992859\n",
      "Training growing_up:   1%|            | 102/20010 [05:54<15:43:28,  2.84s/batch]Batch 100/20010 Done, mean position loss: 74.72211581945419\n",
      "Training growing_up:   1%|            | 101/20010 [05:54<16:08:18,  2.92s/batch]Batch 100/20010 Done, mean position loss: 65.25192003965378\n",
      "Training growing_up:   0%|            | 100/20010 [05:54<19:00:24,  3.44s/batch]Batch 100/20010 Done, mean position loss: 63.878698039054875\n",
      "Training growing_up:   0%|            | 100/20010 [05:55<16:50:52,  3.05s/batch]Batch 100/20010 Done, mean position loss: 68.06409064769744\n",
      "Training growing_up:   1%|            | 101/20010 [05:54<15:57:36,  2.89s/batch]Batch 100/20010 Done, mean position loss: 66.6977992105484\n",
      "Training growing_up:   1%|            | 102/20010 [05:54<14:28:00,  2.62s/batch]Batch 100/20010 Done, mean position loss: 64.73652092933655\n",
      "Training growing_up:   0%|            | 100/20010 [05:55<15:37:00,  2.82s/batch]Batch 100/20010 Done, mean position loss: 65.78117540359497\n",
      "Training growing_up:   1%|            | 101/20010 [05:55<18:26:50,  3.34s/batch]Batch 100/20010 Done, mean position loss: 138.34293353319168\n",
      "Training growing_up:   0%|             | 98/20010 [05:56<16:34:30,  3.00s/batch]Batch 100/20010 Done, mean position loss: 64.42840322256089\n",
      "Training growing_up:   1%|            | 101/20010 [05:56<16:40:01,  3.01s/batch]Batch 100/20010 Done, mean position loss: 66.3819706916809\n",
      "Training growing_up:   0%|             | 99/20010 [05:57<17:00:24,  3.07s/batch]Batch 100/20010 Done, mean position loss: 66.54180044889449\n",
      "Training growing_up:   1%|            | 102/20010 [05:57<15:05:21,  2.73s/batch]Batch 100/20010 Done, mean position loss: 67.06070934295654\n",
      "Training growing_up:   1%|            | 103/20010 [05:57<15:18:01,  2.77s/batch]Batch 100/20010 Done, mean position loss: 65.75046641588212\n",
      "Training growing_up:   1%|            | 103/20010 [05:59<16:27:35,  2.98s/batch]Batch 100/20010 Done, mean position loss: 65.98912182807922\n",
      "Training growing_up:   1%|            | 103/20010 [05:59<16:22:43,  2.96s/batch]Batch 100/20010 Done, mean position loss: 64.50253838777542\n",
      "Training growing_up:   0%|            | 100/20010 [05:59<16:38:06,  3.01s/batch]Batch 100/20010 Done, mean position loss: 66.0126021361351\n",
      "Training growing_up:   0%|            | 100/20010 [06:00<17:40:26,  3.20s/batch]Batch 100/20010 Done, mean position loss: 95.08047663927078\n",
      "Batch 100/20010 Done, mean position loss: 66.14669072628021\n",
      "Training growing_up:   1%|            | 103/20010 [06:00<16:14:14,  2.94s/batch]Batch 100/20010 Done, mean position loss: 65.01101700544358\n",
      "Training growing_up:   0%|            | 100/20010 [06:01<15:58:31,  2.89s/batch]Batch 100/20010 Done, mean position loss: 67.7706129336357\n",
      "Training growing_up:   1%|            | 104/20010 [06:02<16:27:18,  2.98s/batch]Batch 100/20010 Done, mean position loss: 65.70073564052582\n",
      "Training growing_up:   1%|            | 101/20010 [06:02<16:23:39,  2.96s/batch]Batch 100/20010 Done, mean position loss: 65.39910264015198\n",
      "Training growing_up:   1%|            | 104/20010 [06:03<17:21:03,  3.14s/batch]Batch 100/20010 Done, mean position loss: 65.03139861106872\n",
      "Training growing_up:   1%|            | 105/20010 [06:04<16:28:28,  2.98s/batch]Batch 100/20010 Done, mean position loss: 64.52518371343612\n",
      "Training growing_up:   1%|            | 105/20010 [06:04<15:56:42,  2.88s/batch]Batch 100/20010 Done, mean position loss: 64.76491225719452\n",
      "Training growing_up:   1%|            | 109/20010 [06:07<15:14:21,  2.76s/batch]Batch 100/20010 Done, mean position loss: 64.71992970705033\n",
      "Training growing_up:   1%|            | 195/20010 [10:58<18:42:23,  3.40s/batch]Batch 200/20010 Done, mean position loss: 69.19539599895478\n",
      "Training growing_up:   1%|            | 197/20010 [10:58<18:50:09,  3.42s/batch]Batch 200/20010 Done, mean position loss: 62.801146252155306\n",
      "Training growing_up:   1%|            | 197/20010 [10:59<19:06:28,  3.47s/batch]Batch 200/20010 Done, mean position loss: 153.64841230392457\n",
      "Training growing_up:   1%|            | 193/20010 [11:00<17:30:48,  3.18s/batch]Batch 200/20010 Done, mean position loss: 62.53128578186036\n",
      "Training growing_up:   1%|            | 196/20010 [11:00<17:20:25,  3.15s/batch]Batch 200/20010 Done, mean position loss: 65.10638244628906\n",
      "Training growing_up:   1%|            | 198/20010 [11:01<19:21:06,  3.52s/batch]Batch 200/20010 Done, mean position loss: 89.2838954114914\n",
      "Training growing_up:   1%|            | 197/20010 [11:06<22:26:32,  4.08s/batch]Batch 200/20010 Done, mean position loss: 63.31340706110001\n",
      "Training growing_up:   1%|            | 203/20010 [11:06<19:24:15,  3.53s/batch]Batch 200/20010 Done, mean position loss: 60.09730960130692\n",
      "Training growing_up:   1%|            | 203/20010 [11:07<20:21:47,  3.70s/batch]Batch 200/20010 Done, mean position loss: 61.61964993953705\n",
      "Training growing_up:   1%|            | 196/20010 [11:08<21:04:54,  3.83s/batch]Batch 200/20010 Done, mean position loss: 63.55031249523163\n",
      "Training growing_up:   1%|            | 200/20010 [11:08<19:43:24,  3.58s/batch]Batch 200/20010 Done, mean position loss: 64.91820373535157\n",
      "Training growing_up:   1%|            | 198/20010 [11:09<21:18:11,  3.87s/batch]Batch 200/20010 Done, mean position loss: 108.95738385677336\n",
      "Training growing_up:   1%|            | 196/20010 [11:11<19:59:32,  3.63s/batch]Batch 200/20010 Done, mean position loss: 124.87792291402816\n",
      "Training growing_up:   1%|            | 202/20010 [11:11<19:16:40,  3.50s/batch]Batch 200/20010 Done, mean position loss: 98.26256782770157\n",
      "Training growing_up:   1%|            | 204/20010 [11:12<18:01:05,  3.28s/batch]Batch 200/20010 Done, mean position loss: 61.3036177611351\n",
      "Training growing_up:   1%|            | 197/20010 [11:12<19:34:33,  3.56s/batch]Batch 200/20010 Done, mean position loss: 64.21257870912552\n",
      "Batch 200/20010 Done, mean position loss: 63.406431355476386\n",
      "Training growing_up:   1%|            | 203/20010 [11:12<18:04:16,  3.28s/batch]Batch 200/20010 Done, mean position loss: 71.75658304691315\n",
      "Training growing_up:   1%|            | 199/20010 [11:12<17:57:50,  3.26s/batch]Batch 200/20010 Done, mean position loss: 68.37623182058334\n",
      "Training growing_up:   1%|            | 205/20010 [11:15<19:20:33,  3.52s/batch]Batch 200/20010 Done, mean position loss: 64.98553535461426\n",
      "Batch 200/20010 Done, mean position loss: 64.82412554264069\n",
      "Training growing_up:   1%|            | 203/20010 [11:18<16:39:39,  3.03s/batch]Batch 200/20010 Done, mean position loss: 63.68436138629914\n",
      "Training growing_up:   1%|            | 200/20010 [11:18<17:51:59,  3.25s/batch]Batch 200/20010 Done, mean position loss: 65.33298246145249\n",
      "Batch 200/20010 Done, mean position loss: 58.638433949947355\n",
      "Training growing_up:   1%|            | 201/20010 [11:18<17:43:39,  3.22s/batch]Batch 200/20010 Done, mean position loss: 66.69847612380981\n",
      "Training growing_up:   1%|            | 201/20010 [11:18<18:06:54,  3.29s/batch]Batch 200/20010 Done, mean position loss: 65.31849081993103\n",
      "Training growing_up:   1%|            | 205/20010 [11:19<16:33:05,  3.01s/batch]Batch 200/20010 Done, mean position loss: 65.04005583047866\n",
      "Training growing_up:   1%|            | 199/20010 [11:19<16:42:21,  3.04s/batch]Batch 200/20010 Done, mean position loss: 66.92917198181152\n",
      "Training growing_up:   1%|            | 204/20010 [11:20<15:15:55,  2.77s/batch]Batch 200/20010 Done, mean position loss: 62.637907433509824\n",
      "Training growing_up:   1%|            | 207/20010 [11:20<17:03:27,  3.10s/batch]Batch 200/20010 Done, mean position loss: 68.50813895463943\n",
      "Training growing_up:   1%|            | 200/20010 [11:22<15:50:22,  2.88s/batch]Batch 200/20010 Done, mean position loss: 83.60687862157822\n",
      "Training growing_up:   1%|            | 199/20010 [11:23<23:39:42,  4.30s/batch]Batch 200/20010 Done, mean position loss: 60.920458564758306\n",
      "Training growing_up:   1%|            | 208/20010 [11:23<17:46:34,  3.23s/batch]Batch 200/20010 Done, mean position loss: 91.25644251346588\n",
      "Training growing_up:   1%|            | 203/20010 [11:24<17:26:18,  3.17s/batch]Batch 200/20010 Done, mean position loss: 66.57321762561797\n",
      "Training growing_up:   1%|▏           | 209/20010 [11:25<16:16:32,  2.96s/batch]Batch 200/20010 Done, mean position loss: 65.00498536109924\n",
      "Training growing_up:   1%|            | 203/20010 [11:25<17:07:15,  3.11s/batch]Batch 200/20010 Done, mean position loss: 64.78128409862518\n",
      "Training growing_up:   1%|            | 206/20010 [11:27<16:57:57,  3.08s/batch]Batch 200/20010 Done, mean position loss: 71.41714637756347\n",
      "Training growing_up:   1%|            | 207/20010 [11:31<18:02:11,  3.28s/batch]Batch 200/20010 Done, mean position loss: 72.12768028020858\n",
      "Training growing_up:   1%|            | 204/20010 [11:32<19:30:21,  3.55s/batch]Batch 200/20010 Done, mean position loss: 67.90804708242416\n",
      "Training growing_up:   1%|            | 205/20010 [11:36<19:43:42,  3.59s/batch]Batch 200/20010 Done, mean position loss: 71.47337879657746\n",
      "Training growing_up:   1%|▏           | 292/20010 [15:38<13:07:43,  2.40s/batch]Batch 300/20010 Done, mean position loss: 61.28089426755905\n",
      "Training growing_up:   1%|▏           | 296/20010 [15:42<13:46:57,  2.52s/batch]Batch 300/20010 Done, mean position loss: 60.419556357860564\n",
      "Training growing_up:   1%|▏           | 297/20010 [15:43<13:52:49,  2.53s/batch]Batch 300/20010 Done, mean position loss: 59.552592735290524\n",
      "Training growing_up:   1%|▏           | 300/20010 [15:44<14:07:46,  2.58s/batch]Batch 300/20010 Done, mean position loss: 61.54812537431718\n",
      "Training growing_up:   1%|▏           | 299/20010 [15:46<17:21:43,  3.17s/batch]Batch 300/20010 Done, mean position loss: 60.06285072803498\n",
      "Training growing_up:   2%|▏           | 302/20010 [15:48<15:49:18,  2.89s/batch]Batch 300/20010 Done, mean position loss: 65.07362133979797\n",
      "Training growing_up:   1%|▏           | 294/20010 [15:49<15:34:29,  2.84s/batch]Batch 300/20010 Done, mean position loss: 55.95658771753311\n",
      "Training growing_up:   2%|▏           | 304/20010 [15:54<19:25:18,  3.55s/batch]Batch 300/20010 Done, mean position loss: 66.3090206003189\n",
      "Training growing_up:   1%|▏           | 300/20010 [15:54<18:49:32,  3.44s/batch]Batch 300/20010 Done, mean position loss: 58.614557125568396\n",
      "Training growing_up:   1%|▏           | 300/20010 [15:56<20:58:32,  3.83s/batch]Batch 300/20010 Done, mean position loss: 88.81927352905274\n",
      "Training growing_up:   1%|▏           | 292/20010 [15:57<20:48:05,  3.80s/batch]Batch 300/20010 Done, mean position loss: 63.521207249164576\n",
      "Training growing_up:   1%|▏           | 296/20010 [15:58<20:31:51,  3.75s/batch]Batch 300/20010 Done, mean position loss: 67.64066472053528\n",
      "Training growing_up:   2%|▏           | 301/20010 [15:57<19:25:19,  3.55s/batch]Batch 300/20010 Done, mean position loss: 61.37578076601028\n",
      "Training growing_up:   2%|▏           | 302/20010 [15:58<19:29:04,  3.56s/batch]Batch 300/20010 Done, mean position loss: 75.3894370317459\n",
      "Training growing_up:   1%|▏           | 299/20010 [15:58<19:33:48,  3.57s/batch]Batch 300/20010 Done, mean position loss: 58.54852415561676\n",
      "Training growing_up:   2%|▏           | 302/20010 [15:58<18:23:58,  3.36s/batch]Batch 300/20010 Done, mean position loss: 63.43678580522537\n",
      "Training growing_up:   1%|▏           | 297/20010 [15:58<20:56:55,  3.83s/batch]Batch 300/20010 Done, mean position loss: 59.617891407012934\n",
      "Training growing_up:   2%|▏           | 301/20010 [15:59<20:42:57,  3.78s/batch]Batch 300/20010 Done, mean position loss: 57.86986998319627\n",
      "Training growing_up:   2%|▏           | 305/20010 [16:03<18:56:11,  3.46s/batch]Batch 300/20010 Done, mean position loss: 56.189968454837796\n",
      "Training growing_up:   2%|▏           | 306/20010 [16:04<18:04:13,  3.30s/batch]Batch 300/20010 Done, mean position loss: 61.5308682179451\n",
      "Training growing_up:   2%|▏           | 308/20010 [16:06<18:43:17,  3.42s/batch]Batch 300/20010 Done, mean position loss: 69.83497729063035\n",
      "Training growing_up:   2%|▏           | 304/20010 [16:06<17:07:55,  3.13s/batch]Batch 300/20010 Done, mean position loss: 52.77757932662964\n",
      "Training growing_up:   1%|▏           | 299/20010 [16:07<17:57:28,  3.28s/batch]Batch 300/20010 Done, mean position loss: 73.34044196128845\n",
      "Training growing_up:   1%|▏           | 300/20010 [16:08<18:58:28,  3.47s/batch]Batch 300/20010 Done, mean position loss: 56.94594998836517\n",
      "Training growing_up:   2%|▏           | 305/20010 [16:08<19:01:18,  3.48s/batch]Batch 300/20010 Done, mean position loss: 59.549908487796785\n",
      "Training growing_up:   2%|▏           | 305/20010 [16:11<17:39:54,  3.23s/batch]Batch 300/20010 Done, mean position loss: 52.95751106739044\n",
      "Training growing_up:   1%|▏           | 299/20010 [16:11<19:03:32,  3.48s/batch]Batch 300/20010 Done, mean position loss: 56.577894904613494\n",
      "Training growing_up:   2%|▏           | 301/20010 [16:12<20:18:14,  3.71s/batch]Batch 300/20010 Done, mean position loss: 55.49945749521255\n",
      "Training growing_up:   2%|▏           | 301/20010 [16:12<18:22:18,  3.36s/batch]Batch 300/20010 Done, mean position loss: 85.51109996557236\n",
      "Training growing_up:   2%|▏           | 303/20010 [16:12<19:11:02,  3.50s/batch]Batch 300/20010 Done, mean position loss: 58.941645951271056\n",
      "Training growing_up:   1%|▏           | 297/20010 [16:13<16:42:07,  3.05s/batch]Batch 300/20010 Done, mean position loss: 59.68204941511154\n",
      "Training growing_up:   2%|▏           | 307/20010 [16:13<16:04:26,  2.94s/batch]Batch 300/20010 Done, mean position loss: 53.384162445068355\n",
      "Training growing_up:   2%|▏           | 302/20010 [16:15<16:20:47,  2.99s/batch]Batch 300/20010 Done, mean position loss: 68.98234956264496\n",
      "Training growing_up:   2%|▏           | 304/20010 [16:15<17:58:50,  3.28s/batch]Batch 300/20010 Done, mean position loss: 56.78875542402268\n",
      "Training growing_up:   2%|▏           | 311/20010 [16:17<18:02:15,  3.30s/batch]Batch 300/20010 Done, mean position loss: 53.05852259874344\n",
      "Training growing_up:   2%|▏           | 305/20010 [16:19<17:02:59,  3.11s/batch]Batch 300/20010 Done, mean position loss: 67.53808765053749\n",
      "Training growing_up:   2%|▏           | 306/20010 [16:20<15:01:37,  2.75s/batch]Batch 300/20010 Done, mean position loss: 54.4670766544342\n",
      "Training growing_up:   2%|▏           | 312/20010 [16:21<14:57:02,  2.73s/batch]Batch 300/20010 Done, mean position loss: 59.79867715358734\n",
      "Training growing_up:   2%|▏           | 313/20010 [16:23<14:34:56,  2.67s/batch]Batch 300/20010 Done, mean position loss: 67.45727517366409\n",
      "Training growing_up:   2%|▏           | 307/20010 [16:33<15:30:49,  2.83s/batch]Batch 300/20010 Done, mean position loss: 60.394278242588044\n",
      "Training growing_up:   2%|▏           | 390/20010 [20:26<16:22:07,  3.00s/batch]Batch 400/20010 Done, mean position loss: 68.59857671499252\n",
      "Training growing_up:   2%|▏           | 388/20010 [20:41<19:46:39,  3.63s/batch]Batch 400/20010 Done, mean position loss: 52.54323940515518\n",
      "Training growing_up:   2%|▏           | 391/20010 [20:42<16:15:58,  2.98s/batch]Batch 400/20010 Done, mean position loss: 51.943388328552246\n",
      "Training growing_up:   2%|▏           | 388/20010 [20:48<16:57:54,  3.11s/batch]Batch 400/20010 Done, mean position loss: 54.3913167643547\n",
      "Training growing_up:   2%|▏           | 395/20010 [20:49<19:18:15,  3.54s/batch]Batch 400/20010 Done, mean position loss: 52.96101045370102\n",
      "Training growing_up:   2%|▏           | 398/20010 [20:50<17:46:06,  3.26s/batch]Batch 400/20010 Done, mean position loss: 56.128093371391294\n",
      "Training growing_up:   2%|▏           | 393/20010 [20:50<17:18:36,  3.18s/batch]Batch 400/20010 Done, mean position loss: 56.82475118160247\n",
      "Training growing_up:   2%|▏           | 401/20010 [20:50<15:56:37,  2.93s/batch]Batch 400/20010 Done, mean position loss: 59.96387665987015\n",
      "Training growing_up:   2%|▏           | 391/20010 [20:50<17:36:43,  3.23s/batch]Batch 400/20010 Done, mean position loss: 59.202249577045436\n",
      "Training growing_up:   2%|▏           | 394/20010 [20:52<16:59:30,  3.12s/batch]Batch 400/20010 Done, mean position loss: 58.129538774490356\n",
      "Training growing_up:   2%|▏           | 398/20010 [20:54<18:03:05,  3.31s/batch]Batch 400/20010 Done, mean position loss: 55.01183589935303\n",
      "Training growing_up:   2%|▏           | 391/20010 [20:57<15:40:48,  2.88s/batch]Batch 400/20010 Done, mean position loss: 51.48793650865555\n",
      "Training growing_up:   2%|▏           | 398/20010 [20:57<16:20:55,  3.00s/batch]Batch 400/20010 Done, mean position loss: 57.718668763637545\n",
      "Training growing_up:   2%|▏           | 402/20010 [20:59<13:17:15,  2.44s/batch]Batch 400/20010 Done, mean position loss: 53.414956948757165\n",
      "Training growing_up:   2%|▏           | 400/20010 [21:00<15:07:07,  2.78s/batch]Batch 400/20010 Done, mean position loss: 91.14573693990708\n",
      "Training growing_up:   2%|▏           | 399/20010 [21:00<15:41:01,  2.88s/batch]Batch 400/20010 Done, mean position loss: 57.083607301712036\n",
      "Training growing_up:   2%|▏           | 395/20010 [21:02<16:25:57,  3.02s/batch]Batch 400/20010 Done, mean position loss: 55.22716275691986\n",
      "Training growing_up:   2%|▏           | 402/20010 [21:03<16:08:31,  2.96s/batch]Batch 400/20010 Done, mean position loss: 48.686281750202184\n",
      "Batch 400/20010 Done, mean position loss: 56.17935842752457\n",
      "Training growing_up:   2%|▏           | 404/20010 [21:05<15:19:58,  2.82s/batch]Batch 400/20010 Done, mean position loss: 57.265282526016236\n",
      "Training growing_up:   2%|▏           | 407/20010 [21:06<14:41:15,  2.70s/batch]Batch 400/20010 Done, mean position loss: 52.74315435647965\n",
      "Training growing_up:   2%|▏           | 410/20010 [21:08<16:26:49,  3.02s/batch]Batch 400/20010 Done, mean position loss: 66.51130933284759\n",
      "Batch 400/20010 Done, mean position loss: 55.630869483947755\n",
      "Training growing_up:   2%|▏           | 398/20010 [21:12<18:49:56,  3.46s/batch]Batch 400/20010 Done, mean position loss: 61.81408600091933\n",
      "Training growing_up:   2%|▏           | 401/20010 [21:12<17:08:54,  3.15s/batch]Batch 400/20010 Done, mean position loss: 59.19452723026275\n",
      "Training growing_up:   2%|▏           | 403/20010 [21:12<16:25:10,  3.01s/batch]Batch 400/20010 Done, mean position loss: 54.384087958335876\n",
      "Training growing_up:   2%|▏           | 404/20010 [21:12<19:41:29,  3.62s/batch]Batch 400/20010 Done, mean position loss: 55.7067076587677\n",
      "Training growing_up:   2%|▏           | 409/20010 [21:13<17:04:16,  3.14s/batch]Batch 400/20010 Done, mean position loss: 59.281500942707055\n",
      "Training growing_up:   2%|▏           | 404/20010 [21:14<19:20:06,  3.55s/batch]Batch 400/20010 Done, mean position loss: 57.624180846214294\n",
      "Training growing_up:   2%|▏           | 401/20010 [21:14<18:41:57,  3.43s/batch]Batch 400/20010 Done, mean position loss: 87.18923280715941\n",
      "Training growing_up:   2%|▏           | 404/20010 [21:15<16:22:32,  3.01s/batch]Batch 400/20010 Done, mean position loss: 60.17302861213684\n",
      "Training growing_up:   2%|▏           | 399/20010 [21:17<19:27:57,  3.57s/batch]Batch 400/20010 Done, mean position loss: 68.76255002498627\n",
      "Training growing_up:   2%|▏           | 400/20010 [21:21<19:41:50,  3.62s/batch]Batch 400/20010 Done, mean position loss: 59.96067421197891\n",
      "Training growing_up:   2%|▏           | 403/20010 [21:21<19:36:50,  3.60s/batch]Batch 400/20010 Done, mean position loss: 57.633889722824094\n",
      "Training growing_up:   2%|▏           | 410/20010 [21:23<18:49:59,  3.46s/batch]Batch 400/20010 Done, mean position loss: 51.7144603061676\n",
      "Training growing_up:   2%|▏           | 410/20010 [21:24<17:50:34,  3.28s/batch]Batch 400/20010 Done, mean position loss: 62.66113091468811\n",
      "Training growing_up:   2%|▏           | 404/20010 [21:24<16:56:24,  3.11s/batch]Batch 400/20010 Done, mean position loss: 55.75784477233887\n",
      "Training growing_up:   2%|▏           | 405/20010 [21:24<16:09:13,  2.97s/batch]Batch 400/20010 Done, mean position loss: 51.67048892259598\n",
      "Training growing_up:   2%|▎           | 422/20010 [21:31<17:58:41,  3.30s/batch]Batch 400/20010 Done, mean position loss: 72.46613064289093\n",
      "Training growing_up:   2%|▏           | 411/20010 [21:47<17:49:06,  3.27s/batch]Batch 400/20010 Done, mean position loss: 52.918744626045225\n",
      "Training growing_up:   2%|▎           | 489/20010 [25:40<14:47:09,  2.73s/batch]Batch 500/20010 Done, mean position loss: 55.77145782470703\n",
      "Training growing_up:   2%|▎           | 487/20010 [25:49<13:13:57,  2.44s/batch]Batch 500/20010 Done, mean position loss: 51.33493359088898\n",
      "Training growing_up:   2%|▎           | 493/20010 [25:52<14:58:58,  2.76s/batch]Batch 500/20010 Done, mean position loss: 54.26575375795365\n",
      "Training growing_up:   2%|▎           | 490/20010 [25:59<16:01:05,  2.95s/batch]Batch 500/20010 Done, mean position loss: 51.11307169675827\n",
      "Training growing_up:   3%|▎           | 505/20010 [26:00<14:26:41,  2.67s/batch]Batch 500/20010 Done, mean position loss: 51.81422430753708\n",
      "Training growing_up:   2%|▎           | 483/20010 [26:00<17:03:24,  3.14s/batch]Batch 500/20010 Done, mean position loss: 55.94795352220535\n",
      "Training growing_up:   2%|▎           | 491/20010 [26:01<15:57:31,  2.94s/batch]Batch 500/20010 Done, mean position loss: 65.40816387176514\n",
      "Training growing_up:   2%|▎           | 492/20010 [26:02<13:33:49,  2.50s/batch]Batch 500/20010 Done, mean position loss: 52.55174331903457\n",
      "Training growing_up:   2%|▎           | 496/20010 [26:04<16:26:57,  3.03s/batch]Batch 500/20010 Done, mean position loss: 103.87902100801467\n",
      "Training growing_up:   2%|▎           | 498/20010 [26:03<17:10:17,  3.17s/batch]Batch 500/20010 Done, mean position loss: 74.73761798143387\n",
      "Training growing_up:   2%|▎           | 495/20010 [26:12<15:50:27,  2.92s/batch]Batch 500/20010 Done, mean position loss: 50.83295331001282\n",
      "Training growing_up:   3%|▎           | 506/20010 [26:13<15:10:48,  2.80s/batch]Batch 500/20010 Done, mean position loss: 52.49794141292572\n",
      "Training growing_up:   2%|▎           | 496/20010 [26:13<14:41:02,  2.71s/batch]Batch 500/20010 Done, mean position loss: 52.2227926659584\n",
      "Training growing_up:   2%|▎           | 497/20010 [26:14<16:22:35,  3.02s/batch]Batch 500/20010 Done, mean position loss: 58.28322117090225\n",
      "Training growing_up:   3%|▎           | 501/20010 [26:14<15:43:04,  2.90s/batch]Batch 500/20010 Done, mean position loss: 60.363717224597934\n",
      "Training growing_up:   2%|▎           | 495/20010 [26:14<16:20:18,  3.01s/batch]Batch 500/20010 Done, mean position loss: 61.581671328544616\n",
      "Training growing_up:   3%|▎           | 508/20010 [26:18<15:31:33,  2.87s/batch]Batch 500/20010 Done, mean position loss: 64.55190050840378\n",
      "Training growing_up:   2%|▎           | 493/20010 [26:18<14:33:27,  2.69s/batch]Batch 500/20010 Done, mean position loss: 68.1991223692894\n",
      "Training growing_up:   3%|▎           | 503/20010 [26:20<15:19:59,  2.83s/batch]Batch 500/20010 Done, mean position loss: 57.65352526903152\n",
      "Training growing_up:   3%|▎           | 504/20010 [26:22<14:32:05,  2.68s/batch]Batch 500/20010 Done, mean position loss: 50.61057285547257\n",
      "Training growing_up:   3%|▎           | 510/20010 [26:24<16:07:24,  2.98s/batch]Batch 500/20010 Done, mean position loss: 49.492655515670776\n",
      "Training growing_up:   3%|▎           | 502/20010 [26:24<14:31:42,  2.68s/batch]Batch 500/20010 Done, mean position loss: 50.52126404285431\n",
      "Training growing_up:   3%|▎           | 509/20010 [26:26<14:48:58,  2.74s/batch]Batch 500/20010 Done, mean position loss: 54.94130269527435\n",
      "Training growing_up:   3%|▎           | 501/20010 [26:26<13:54:08,  2.57s/batch]Batch 500/20010 Done, mean position loss: 62.61021809101105\n",
      "Training growing_up:   3%|▎           | 510/20010 [26:28<14:37:07,  2.70s/batch]Batch 500/20010 Done, mean position loss: 56.335026257038116\n",
      "Training growing_up:   2%|▎           | 498/20010 [26:28<15:21:21,  2.83s/batch]Batch 500/20010 Done, mean position loss: 57.68839813232422\n",
      "Training growing_up:   2%|▎           | 500/20010 [26:28<15:44:13,  2.90s/batch]Batch 500/20010 Done, mean position loss: 51.45957192182541\n",
      "Training growing_up:   3%|▎           | 511/20010 [26:30<14:04:30,  2.60s/batch]Batch 500/20010 Done, mean position loss: 54.69927978038788\n",
      "Training growing_up:   3%|▎           | 501/20010 [26:30<16:09:05,  2.98s/batch]Batch 500/20010 Done, mean position loss: 55.77936393499374\n",
      "Training growing_up:   2%|▎           | 497/20010 [26:31<14:12:57,  2.62s/batch]Batch 500/20010 Done, mean position loss: 50.730497674942015\n",
      "Training growing_up:   3%|▎           | 508/20010 [26:32<15:16:06,  2.82s/batch]Batch 500/20010 Done, mean position loss: 50.688596796989444\n",
      "Training growing_up:   3%|▎           | 519/20010 [26:32<16:58:28,  3.14s/batch]Batch 500/20010 Done, mean position loss: 55.97557304859161\n",
      "Training growing_up:   3%|▎           | 508/20010 [26:36<15:11:42,  2.80s/batch]Batch 500/20010 Done, mean position loss: 51.37487680196762\n",
      "Training growing_up:   3%|▎           | 513/20010 [26:37<14:57:33,  2.76s/batch]Batch 500/20010 Done, mean position loss: 51.557280640602116\n",
      "Training growing_up:   3%|▎           | 516/20010 [26:38<13:17:41,  2.46s/batch]Batch 500/20010 Done, mean position loss: 51.068221418857576\n",
      "Training growing_up:   2%|▎           | 500/20010 [26:40<15:11:03,  2.80s/batch]Batch 500/20010 Done, mean position loss: 49.75275559425354\n",
      "Training growing_up:   3%|▎           | 515/20010 [26:41<14:00:41,  2.59s/batch]Batch 500/20010 Done, mean position loss: 51.41639560699463\n",
      "Training growing_up:   3%|▎           | 511/20010 [26:43<15:23:04,  2.84s/batch]Batch 500/20010 Done, mean position loss: 50.716209614276885\n",
      "Training growing_up:   3%|▎           | 508/20010 [26:49<16:37:24,  3.07s/batch]Batch 500/20010 Done, mean position loss: 58.50015825271606\n",
      "Training growing_up:   3%|▎           | 514/20010 [26:57<20:18:50,  3.75s/batch]Batch 500/20010 Done, mean position loss: 69.6591382074356\n",
      "Training growing_up:   3%|▎           | 576/20010 [30:34<13:34:45,  2.52s/batch]Batch 600/20010 Done, mean position loss: 69.9553956413269\n",
      "Training growing_up:   3%|▎           | 587/20010 [30:42<12:47:58,  2.37s/batch]Batch 600/20010 Done, mean position loss: 49.91646465301514\n",
      "Training growing_up:   3%|▎           | 583/20010 [30:44<13:26:04,  2.49s/batch]Batch 600/20010 Done, mean position loss: 49.22728569507599\n",
      "Training growing_up:   3%|▎           | 594/20010 [30:46<13:04:16,  2.42s/batch]Batch 600/20010 Done, mean position loss: 52.86004026889801\n",
      "Training growing_up:   3%|▎           | 599/20010 [30:47<15:19:21,  2.84s/batch]Batch 600/20010 Done, mean position loss: 52.36256424188613\n",
      "Training growing_up:   3%|▎           | 596/20010 [30:50<11:25:44,  2.12s/batch]Batch 600/20010 Done, mean position loss: 55.80274127483368\n",
      "Training growing_up:   3%|▎           | 596/20010 [30:50<10:56:04,  2.03s/batch]Batch 600/20010 Done, mean position loss: 53.31687798261642\n",
      "Training growing_up:   3%|▎           | 589/20010 [30:51<12:05:32,  2.24s/batch]Batch 600/20010 Done, mean position loss: 51.50463363885879\n",
      "Training growing_up:   3%|▎           | 590/20010 [30:52<13:42:47,  2.54s/batch]Batch 600/20010 Done, mean position loss: 51.52538632869721\n",
      "Training growing_up:   3%|▎           | 586/20010 [30:51<13:24:33,  2.49s/batch]Batch 600/20010 Done, mean position loss: 51.59752703428268\n",
      "Training growing_up:   3%|▎           | 598/20010 [31:02<17:01:17,  3.16s/batch]Batch 600/20010 Done, mean position loss: 47.029487705230714\n",
      "Training growing_up:   3%|▎           | 599/20010 [31:04<14:30:20,  2.69s/batch]Batch 600/20010 Done, mean position loss: 56.23504265069961\n",
      "Training growing_up:   3%|▎           | 596/20010 [31:05<15:54:57,  2.95s/batch]Batch 600/20010 Done, mean position loss: 71.15233119249343\n",
      "Training growing_up:   3%|▎           | 600/20010 [31:06<13:36:42,  2.52s/batch]Batch 600/20010 Done, mean position loss: 53.88873058319091\n",
      "Training growing_up:   3%|▎           | 607/20010 [31:07<13:25:29,  2.49s/batch]Batch 600/20010 Done, mean position loss: 47.04875100374222\n",
      "Training growing_up:   3%|▎           | 607/20010 [31:08<14:59:17,  2.78s/batch]Batch 600/20010 Done, mean position loss: 52.11968230724335\n",
      "Batch 600/20010 Done, mean position loss: 52.72797335147857\n",
      "Training growing_up:   3%|▎           | 597/20010 [31:08<15:39:33,  2.90s/batch]Batch 600/20010 Done, mean position loss: 48.2966780257225\n",
      "Training growing_up:   3%|▎           | 610/20010 [31:08<13:03:46,  2.42s/batch]Batch 600/20010 Done, mean position loss: 50.65860794782638\n",
      "Training growing_up:   3%|▎           | 596/20010 [31:10<14:22:38,  2.67s/batch]Batch 600/20010 Done, mean position loss: 48.875930025577546\n",
      "Training growing_up:   3%|▎           | 603/20010 [31:10<13:41:14,  2.54s/batch]Batch 600/20010 Done, mean position loss: 47.84766639947891\n",
      "Training growing_up:   3%|▎           | 599/20010 [31:13<13:05:11,  2.43s/batch]Batch 600/20010 Done, mean position loss: 48.649608583450316\n",
      "Training growing_up:   3%|▎           | 604/20010 [31:15<13:51:42,  2.57s/batch]Batch 600/20010 Done, mean position loss: 50.019970374107366\n",
      "Training growing_up:   3%|▎           | 607/20010 [31:17<13:28:30,  2.50s/batch]Batch 600/20010 Done, mean position loss: 50.656811292171476\n",
      "Training growing_up:   3%|▎           | 595/20010 [31:17<14:09:40,  2.63s/batch]Batch 600/20010 Done, mean position loss: 51.91970022678375Batch 600/20010 Done, mean position loss: 53.45473884105682\n",
      "Training growing_up:   3%|▎           | 601/20010 [31:17<13:28:11,  2.50s/batch]\n",
      "Training growing_up:   3%|▎           | 601/20010 [31:17<13:13:22,  2.45s/batch]Batch 600/20010 Done, mean position loss: 51.759644672870635\n",
      "Training growing_up:   3%|▎           | 603/20010 [31:22<11:55:29,  2.21s/batch]Batch 600/20010 Done, mean position loss: 49.46802644729615\n",
      "Training growing_up:   3%|▎           | 615/20010 [31:23<15:33:13,  2.89s/batch]Batch 600/20010 Done, mean position loss: 58.331485352516175\n",
      "Batch 600/20010 Done, mean position loss: 49.98941293001175\n",
      "Training growing_up:   3%|▎           | 604/20010 [31:24<13:03:32,  2.42s/batch]Batch 600/20010 Done, mean position loss: 48.562165787219996\n",
      "Training growing_up:   3%|▎           | 601/20010 [31:24<13:47:39,  2.56s/batch]Batch 600/20010 Done, mean position loss: 48.345355775356296\n",
      "Training growing_up:   3%|▎           | 617/20010 [31:25<14:11:20,  2.63s/batch]Batch 600/20010 Done, mean position loss: 50.650024912357324\n",
      "Training growing_up:   3%|▎           | 603/20010 [31:29<13:15:50,  2.46s/batch]Batch 600/20010 Done, mean position loss: 50.77679994821548\n",
      "Training growing_up:   3%|▎           | 596/20010 [31:31<14:25:26,  2.67s/batch]Batch 600/20010 Done, mean position loss: 49.795915324687954\n",
      "Training growing_up:   3%|▎           | 604/20010 [31:32<13:29:58,  2.50s/batch]Batch 600/20010 Done, mean position loss: 50.53907713890076\n",
      "Training growing_up:   3%|▎           | 606/20010 [31:36<12:41:59,  2.36s/batch]Batch 600/20010 Done, mean position loss: 50.514833395481105\n",
      "Training growing_up:   3%|▎           | 620/20010 [31:38<12:43:03,  2.36s/batch]Batch 600/20010 Done, mean position loss: 50.35125618457794\n",
      "Training growing_up:   3%|▎           | 607/20010 [31:40<13:28:25,  2.50s/batch]Batch 600/20010 Done, mean position loss: 53.19621602773667\n",
      "Training growing_up:   3%|▎           | 606/20010 [31:43<13:49:30,  2.56s/batch]Batch 600/20010 Done, mean position loss: 59.988248667716974\n",
      "Training growing_up:   3%|▍           | 688/20010 [35:03<15:08:27,  2.82s/batch]Batch 700/20010 Done, mean position loss: 56.264152541160584\n",
      "Training growing_up:   3%|▍           | 684/20010 [35:05<16:50:14,  3.14s/batch]Batch 700/20010 Done, mean position loss: 47.86859564781189\n",
      "Training growing_up:   3%|▍           | 699/20010 [35:11<13:49:09,  2.58s/batch]Batch 700/20010 Done, mean position loss: 55.7576500415802\n",
      "Training growing_up:   3%|▍           | 678/20010 [35:12<15:54:45,  2.96s/batch]Batch 700/20010 Done, mean position loss: 49.29331221580505\n",
      "Training growing_up:   3%|▍           | 683/20010 [35:15<16:21:41,  3.05s/batch]Batch 700/20010 Done, mean position loss: 48.78410999536514\n",
      "Training growing_up:   4%|▍           | 705/20010 [35:15<15:50:49,  2.96s/batch]Batch 700/20010 Done, mean position loss: 49.489206593036656\n",
      "Training growing_up:   3%|▍           | 689/20010 [35:16<14:46:03,  2.75s/batch]Batch 700/20010 Done, mean position loss: 43.378736748695374\n",
      "Training growing_up:   3%|▍           | 692/20010 [35:16<16:26:22,  3.06s/batch]Batch 700/20010 Done, mean position loss: 48.089114930629734\n",
      "Training growing_up:   3%|▍           | 686/20010 [35:19<15:55:12,  2.97s/batch]Batch 700/20010 Done, mean position loss: 46.006329295635226\n",
      "Batch 700/20010 Done, mean position loss: 51.606085867881774\n",
      "Training growing_up:   4%|▍           | 705/20010 [35:29<12:43:45,  2.37s/batch]Batch 700/20010 Done, mean position loss: 50.549674844741816\n",
      "Training growing_up:   3%|▍           | 697/20010 [35:30<15:19:42,  2.86s/batch]Batch 700/20010 Done, mean position loss: 51.057396681308745\n",
      "Training growing_up:   3%|▍           | 684/20010 [35:31<13:57:22,  2.60s/batch]Batch 700/20010 Done, mean position loss: 47.425040633678435\n",
      "Training growing_up:   3%|▍           | 700/20010 [35:32<16:12:45,  3.02s/batch]Batch 700/20010 Done, mean position loss: 45.05993232727051\n",
      "Training growing_up:   4%|▍           | 702/20010 [35:33<14:46:48,  2.76s/batch]Batch 700/20010 Done, mean position loss: 51.81349401950836\n",
      "Training growing_up:   4%|▍           | 708/20010 [35:35<13:53:36,  2.59s/batch]Batch 700/20010 Done, mean position loss: 47.23047253370285\n",
      "Training growing_up:   3%|▍           | 692/20010 [35:35<17:25:11,  3.25s/batch]Batch 700/20010 Done, mean position loss: 47.52858012676239\n",
      "Training growing_up:   3%|▍           | 697/20010 [35:38<15:49:44,  2.95s/batch]Batch 700/20010 Done, mean position loss: 45.79385473489761\n",
      "Training growing_up:   4%|▍           | 711/20010 [35:43<14:55:18,  2.78s/batch]Batch 700/20010 Done, mean position loss: 47.01435268878937\n",
      "Training growing_up:   3%|▍           | 697/20010 [35:44<17:24:19,  3.24s/batch]Batch 700/20010 Done, mean position loss: 51.01777688503265\n",
      "Training growing_up:   4%|▍           | 710/20010 [35:44<15:13:36,  2.84s/batch]Batch 700/20010 Done, mean position loss: 50.27689485788345\n",
      "Training growing_up:   3%|▍           | 697/20010 [35:45<15:38:19,  2.92s/batch]Batch 700/20010 Done, mean position loss: 48.523363456726074\n",
      "Training growing_up:   4%|▍           | 714/20010 [35:49<13:58:47,  2.61s/batch]Batch 700/20010 Done, mean position loss: 50.2056312251091\n",
      "Training growing_up:   4%|▍           | 709/20010 [35:53<17:11:11,  3.21s/batch]Batch 700/20010 Done, mean position loss: 49.039552090168\n",
      "Training growing_up:   4%|▍           | 704/20010 [35:53<15:10:04,  2.83s/batch]Batch 700/20010 Done, mean position loss: 56.07500592470169\n",
      "Training growing_up:   4%|▍           | 707/20010 [35:56<16:05:05,  3.00s/batch]Batch 700/20010 Done, mean position loss: 55.06871295928955\n",
      "Training growing_up:   4%|▍           | 716/20010 [35:56<14:24:29,  2.69s/batch]Batch 700/20010 Done, mean position loss: 47.009032802581785\n",
      "Batch 700/20010 Done, mean position loss: 46.522688033580785\n",
      "Batch 700/20010 Done, mean position loss: 53.78490432977676\n",
      "Training growing_up:   3%|▍           | 695/20010 [35:57<16:08:57,  3.01s/batch]Batch 700/20010 Done, mean position loss: 48.105959639549255\n",
      "Training growing_up:   4%|▍           | 705/20010 [35:56<15:59:55,  2.98s/batch]Batch 700/20010 Done, mean position loss: 64.30659188985825\n",
      "Training growing_up:   4%|▍           | 718/20010 [36:03<15:33:51,  2.90s/batch]Batch 700/20010 Done, mean position loss: 49.62234145879745\n",
      "Training growing_up:   4%|▍           | 704/20010 [36:04<15:09:13,  2.83s/batch]Batch 700/20010 Done, mean position loss: 46.42561946630478\n",
      "Training growing_up:   4%|▍           | 719/20010 [36:12<18:04:19,  3.37s/batch]Batch 700/20010 Done, mean position loss: 46.11065788984299\n",
      "Training growing_up:   4%|▍           | 706/20010 [36:14<18:46:09,  3.50s/batch]Batch 700/20010 Done, mean position loss: 63.02873915672301\n",
      "Training growing_up:   4%|▍           | 711/20010 [36:17<18:25:13,  3.44s/batch]Batch 700/20010 Done, mean position loss: 51.08496324062347\n",
      "Training growing_up:   4%|▍           | 715/20010 [36:18<17:16:12,  3.22s/batch]Batch 700/20010 Done, mean position loss: 53.588695294857025\n",
      "Training growing_up:   4%|▍           | 709/20010 [36:24<17:54:54,  3.34s/batch]Batch 700/20010 Done, mean position loss: 46.97037501573563\n",
      "Training growing_up:   4%|▍           | 718/20010 [36:25<18:00:36,  3.36s/batch]Batch 700/20010 Done, mean position loss: 62.707072496414185\n",
      "Training growing_up:   4%|▍           | 711/20010 [36:26<15:22:17,  2.87s/batch]Batch 700/20010 Done, mean position loss: 49.11888876676559\n",
      "Training growing_up:   4%|▍           | 782/20010 [40:02<15:43:21,  2.94s/batch]Batch 800/20010 Done, mean position loss: 76.93341520309448\n",
      "Training growing_up:   4%|▍           | 791/20010 [40:05<16:21:33,  3.06s/batch]Batch 800/20010 Done, mean position loss: 47.31596228837967\n",
      "Training growing_up:   4%|▍           | 787/20010 [40:08<15:35:49,  2.92s/batch]Batch 800/20010 Done, mean position loss: 48.160249614715575\n",
      "Training growing_up:   4%|▍           | 781/20010 [40:10<17:13:28,  3.22s/batch]Batch 800/20010 Done, mean position loss: 40.76809559106827\n",
      "Training growing_up:   4%|▍           | 804/20010 [40:14<15:15:41,  2.86s/batch]Batch 800/20010 Done, mean position loss: 47.17225404977798\n",
      "Training growing_up:   4%|▍           | 776/20010 [40:15<16:04:26,  3.01s/batch]Batch 800/20010 Done, mean position loss: 64.24279487133026\n",
      "Training growing_up:   4%|▍           | 781/20010 [40:17<16:15:20,  3.04s/batch]Batch 800/20010 Done, mean position loss: 51.2273760843277\n",
      "Training growing_up:   4%|▍           | 804/20010 [40:19<15:25:36,  2.89s/batch]Batch 800/20010 Done, mean position loss: 41.579418215751645\n",
      "Training growing_up:   4%|▍           | 806/20010 [40:20<16:02:04,  3.01s/batch]Batch 800/20010 Done, mean position loss: 50.939645261764525\n",
      "Training growing_up:   4%|▍           | 789/20010 [40:21<14:41:34,  2.75s/batch]Batch 800/20010 Done, mean position loss: 47.18328094482422\n",
      "Training growing_up:   4%|▍           | 806/20010 [40:27<15:00:43,  2.81s/batch]Batch 800/20010 Done, mean position loss: 47.48445646524429\n",
      "Training growing_up:   4%|▍           | 802/20010 [40:30<13:55:27,  2.61s/batch]Batch 800/20010 Done, mean position loss: 44.67456440448761\n",
      "Training growing_up:   4%|▍           | 787/20010 [40:34<15:32:14,  2.91s/batch]Batch 800/20010 Done, mean position loss: 44.45512639760971\n",
      "Training growing_up:   4%|▍           | 796/20010 [40:35<16:22:16,  3.07s/batch]Batch 800/20010 Done, mean position loss: 45.789829137325285\n",
      "Training growing_up:   4%|▍           | 808/20010 [40:36<15:28:36,  2.90s/batch]Batch 800/20010 Done, mean position loss: 45.83801514148712\n",
      "Training growing_up:   4%|▍           | 783/20010 [40:36<16:25:15,  3.07s/batch]Batch 800/20010 Done, mean position loss: 40.39882327318192\n",
      "Training growing_up:   4%|▍           | 798/20010 [40:36<17:05:57,  3.20s/batch]Batch 800/20010 Done, mean position loss: 48.9673077583313\n",
      "Training growing_up:   4%|▍           | 816/20010 [40:44<15:44:58,  2.95s/batch]Batch 800/20010 Done, mean position loss: 42.77241805791855\n",
      "Training growing_up:   4%|▍           | 799/20010 [40:46<16:18:55,  3.06s/batch]Batch 800/20010 Done, mean position loss: 49.7908896446228\n",
      "Training growing_up:   4%|▍           | 805/20010 [40:48<15:06:57,  2.83s/batch]Batch 800/20010 Done, mean position loss: 46.29881538152695\n",
      "Training growing_up:   4%|▍           | 800/20010 [40:52<14:28:23,  2.71s/batch]Batch 800/20010 Done, mean position loss: 44.75109008312225\n",
      "Training growing_up:   4%|▍           | 797/20010 [40:52<16:02:15,  3.01s/batch]Batch 800/20010 Done, mean position loss: 48.485490419864654\n",
      "Training growing_up:   4%|▍           | 801/20010 [40:52<16:05:31,  3.02s/batch]Batch 800/20010 Done, mean position loss: 47.67398385763168\n",
      "Training growing_up:   4%|▍           | 814/20010 [40:56<15:39:30,  2.94s/batch]Batch 800/20010 Done, mean position loss: 70.48673756599428\n",
      "Training growing_up:   4%|▍           | 800/20010 [40:56<17:38:17,  3.31s/batch]Batch 800/20010 Done, mean position loss: 46.17773412704467\n",
      "Training growing_up:   4%|▍           | 814/20010 [40:56<16:20:49,  3.07s/batch]Batch 800/20010 Done, mean position loss: 49.895624191761016\n",
      "Training growing_up:   4%|▍           | 809/20010 [40:58<14:06:49,  2.65s/batch]Batch 800/20010 Done, mean position loss: 44.86641725063324\n",
      "Training growing_up:   4%|▍           | 802/20010 [40:59<16:09:13,  3.03s/batch]Batch 800/20010 Done, mean position loss: 52.969368922710416\n",
      "Training growing_up:   4%|▍           | 812/20010 [41:01<14:35:03,  2.73s/batch]Batch 800/20010 Done, mean position loss: 57.32437172651291\n",
      "Training growing_up:   4%|▍           | 792/20010 [41:03<19:04:39,  3.57s/batch]Batch 800/20010 Done, mean position loss: 46.499775702953336\n",
      "Training growing_up:   4%|▍           | 811/20010 [41:03<14:30:36,  2.72s/batch]Batch 800/20010 Done, mean position loss: 43.709824748039246\n",
      "Training growing_up:   4%|▍           | 818/20010 [41:06<14:12:13,  2.66s/batch]Batch 800/20010 Done, mean position loss: 64.5382245516777\n",
      "Training growing_up:   4%|▍           | 808/20010 [41:11<14:51:05,  2.78s/batch]Batch 800/20010 Done, mean position loss: 44.368464734554294\n",
      "Training growing_up:   4%|▍           | 814/20010 [41:13<16:28:06,  3.09s/batch]Batch 800/20010 Done, mean position loss: 44.83232993602753\n",
      "Training growing_up:   4%|▍           | 798/20010 [41:20<15:22:13,  2.88s/batch]Batch 800/20010 Done, mean position loss: 51.30600098133087\n",
      "Training growing_up:   4%|▍           | 800/20010 [41:28<17:34:33,  3.29s/batch]Batch 800/20010 Done, mean position loss: 43.25132870197296\n",
      "Training growing_up:   4%|▍           | 812/20010 [41:29<16:03:26,  3.01s/batch]Batch 800/20010 Done, mean position loss: 52.67138435125351\n",
      "Training growing_up:   4%|▍           | 830/20010 [41:29<14:31:19,  2.73s/batch]Batch 800/20010 Done, mean position loss: 47.56213905096054\n",
      "Training growing_up:   4%|▍           | 810/20010 [41:30<16:16:32,  3.05s/batch]Batch 800/20010 Done, mean position loss: 47.78431194543839\n",
      "Training growing_up:   4%|▍           | 819/20010 [41:31<16:12:04,  3.04s/batch]Batch 800/20010 Done, mean position loss: 53.52823025465011\n",
      "Training growing_up:   4%|▌           | 883/20010 [44:47<15:14:42,  2.87s/batch]Batch 900/20010 Done, mean position loss: 45.950494592189784\n",
      "Training growing_up:   4%|▌           | 878/20010 [44:47<14:58:19,  2.82s/batch]Batch 900/20010 Done, mean position loss: 44.32847658395767\n",
      "Training growing_up:   4%|▌           | 889/20010 [44:51<13:41:08,  2.58s/batch]Batch 900/20010 Done, mean position loss: 48.66377047538757\n",
      "Training growing_up:   4%|▌           | 890/20010 [44:51<15:23:21,  2.90s/batch]Batch 900/20010 Done, mean position loss: 41.39119246482849\n",
      "Training growing_up:   5%|▌           | 903/20010 [44:52<13:28:52,  2.54s/batch]Batch 900/20010 Done, mean position loss: 44.102318050861356\n",
      "Training growing_up:   4%|▌           | 879/20010 [44:57<15:07:24,  2.85s/batch]Batch 900/20010 Done, mean position loss: 46.40411553621292\n",
      "Training growing_up:   4%|▌           | 898/20010 [44:58<15:46:47,  2.97s/batch]Batch 900/20010 Done, mean position loss: 44.372291381359105\n",
      "Training growing_up:   5%|▌           | 904/20010 [44:59<13:21:55,  2.52s/batch]Batch 900/20010 Done, mean position loss: 46.502851679325104\n",
      "Training growing_up:   4%|▌           | 886/20010 [45:03<11:50:17,  2.23s/batch]Batch 900/20010 Done, mean position loss: 59.56277585268021\n",
      "Training growing_up:   5%|▌           | 907/20010 [45:05<11:30:28,  2.17s/batch]Batch 900/20010 Done, mean position loss: 42.51601816415787\n",
      "Training growing_up:   4%|▌           | 897/20010 [45:10<15:01:23,  2.83s/batch]Batch 900/20010 Done, mean position loss: 48.504460070133206\n",
      "Training growing_up:   5%|▌           | 905/20010 [45:13<12:15:52,  2.31s/batch]Batch 900/20010 Done, mean position loss: 42.30576009988785\n",
      "Training growing_up:   5%|▌           | 909/20010 [45:13<15:13:59,  2.87s/batch]Batch 900/20010 Done, mean position loss: 38.65069524765015\n",
      "Training growing_up:   5%|▌           | 906/20010 [45:17<12:10:10,  2.29s/batch]Batch 900/20010 Done, mean position loss: 45.93673266649246\n",
      "Training growing_up:   5%|▌           | 907/20010 [45:20<14:12:13,  2.68s/batch]Batch 900/20010 Done, mean position loss: 47.10336499214172\n",
      "Training growing_up:   5%|▌           | 904/20010 [45:20<12:58:42,  2.45s/batch]Batch 900/20010 Done, mean position loss: 60.34292647123337\n",
      "Training growing_up:   4%|▌           | 886/20010 [45:21<15:09:24,  2.85s/batch]Batch 900/20010 Done, mean position loss: 43.76436580657959\n",
      "Training growing_up:   5%|▌           | 911/20010 [45:23<15:38:30,  2.95s/batch]Batch 900/20010 Done, mean position loss: 50.98628548622131\n",
      "Training growing_up:   4%|▌           | 892/20010 [45:23<12:48:15,  2.41s/batch]Batch 900/20010 Done, mean position loss: 41.586038041114804\n",
      "Training growing_up:   5%|▌           | 916/20010 [45:24<13:05:25,  2.47s/batch]Batch 900/20010 Done, mean position loss: 48.762036869525915\n",
      "Training growing_up:   5%|▌           | 909/20010 [45:30<13:00:43,  2.45s/batch]Batch 900/20010 Done, mean position loss: 40.91898057460785\n",
      "Training growing_up:   4%|▌           | 899/20010 [45:32<13:10:20,  2.48s/batch]Batch 900/20010 Done, mean position loss: 42.90737006425857\n",
      "Training growing_up:   5%|▌           | 902/20010 [45:33<12:26:10,  2.34s/batch]Batch 900/20010 Done, mean position loss: 45.99615540742874\n",
      "Training growing_up:   5%|▌           | 917/20010 [45:35<14:13:08,  2.68s/batch]Batch 900/20010 Done, mean position loss: 43.45738146066666\n",
      "Training growing_up:   5%|▌           | 910/20010 [45:35<14:48:36,  2.79s/batch]Batch 900/20010 Done, mean position loss: 41.63575617551804\n",
      "Training growing_up:   4%|▌           | 899/20010 [45:37<14:02:27,  2.64s/batch]Batch 900/20010 Done, mean position loss: 43.27904567718505\n",
      "Training growing_up:   4%|▌           | 896/20010 [45:38<13:37:02,  2.56s/batch]Batch 900/20010 Done, mean position loss: 44.33350140333176\n",
      "Training growing_up:   5%|▌           | 912/20010 [45:41<14:54:44,  2.81s/batch]Batch 900/20010 Done, mean position loss: 39.4341301202774\n",
      "Training growing_up:   4%|▌           | 889/20010 [45:42<13:54:14,  2.62s/batch]Batch 900/20010 Done, mean position loss: 46.07936993360519\n",
      "Training growing_up:   5%|▌           | 908/20010 [45:41<14:00:04,  2.64s/batch]Batch 900/20010 Done, mean position loss: 51.79886793136597\n",
      "Training growing_up:   5%|▌           | 908/20010 [45:41<13:04:33,  2.46s/batch]Batch 900/20010 Done, mean position loss: 49.84713819742203\n",
      "Training growing_up:   5%|▌           | 906/20010 [45:53<15:20:07,  2.89s/batch]Batch 900/20010 Done, mean position loss: 42.045339941978455\n",
      "Training growing_up:   4%|▌           | 897/20010 [45:54<12:48:19,  2.41s/batch]Batch 900/20010 Done, mean position loss: 48.24169872045516\n",
      "Training growing_up:   4%|▌           | 897/20010 [46:04<13:52:35,  2.61s/batch]Batch 900/20010 Done, mean position loss: 44.62252350568772\n",
      "Training growing_up:   4%|▌           | 900/20010 [46:05<15:30:55,  2.92s/batch]Batch 900/20010 Done, mean position loss: 42.13310896158218\n",
      "Training growing_up:   5%|▌           | 910/20010 [46:06<17:03:24,  3.21s/batch]Batch 900/20010 Done, mean position loss: 46.783660490512844\n",
      "Training growing_up:   5%|▌           | 913/20010 [46:07<16:30:40,  3.11s/batch]Batch 900/20010 Done, mean position loss: 47.90205274343491\n",
      "Training growing_up:   5%|▌           | 927/20010 [46:08<14:11:00,  2.68s/batch]Batch 900/20010 Done, mean position loss: 41.29449729681015\n",
      "Training growing_up:   5%|▌           | 930/20010 [46:12<16:34:04,  3.13s/batch]Batch 900/20010 Done, mean position loss: 44.7357449555397\n",
      "Training growing_up:   5%|▌           | 916/20010 [46:16<15:18:17,  2.89s/batch]Batch 900/20010 Done, mean position loss: 46.855948717594146\n",
      "Training growing_up:   5%|▌           | 997/20010 [49:11<13:45:38,  2.61s/batch]Batch 1000/20010 Done, mean position loss: 40.093359324932095\n",
      "Training growing_up:   5%|▌           | 987/20010 [49:17<14:36:56,  2.77s/batch]Batch 1000/20010 Done, mean position loss: 50.346410346031185\n",
      "Batch 1000/20010 Done, mean position loss: 42.50442608594895\n",
      "Training growing_up:   5%|▌           | 967/20010 [49:19<16:01:30,  3.03s/batch]Batch 1000/20010 Done, mean position loss: 55.21832901239395\n",
      "Training growing_up:   5%|▌          | 1003/20010 [49:21<13:20:36,  2.53s/batch]Batch 1000/20010 Done, mean position loss: 39.88027726888657\n",
      "Training growing_up:   5%|▌           | 996/20010 [49:21<14:57:29,  2.83s/batch]Batch 1000/20010 Done, mean position loss: 39.794999268054966\n",
      "Training growing_up:   5%|▌           | 986/20010 [49:22<15:51:53,  3.00s/batch]Batch 1000/20010 Done, mean position loss: 40.8102327632904\n",
      "Training growing_up:   5%|▌           | 976/20010 [49:25<15:02:00,  2.84s/batch]Batch 1000/20010 Done, mean position loss: 40.70539332866669\n",
      "Training growing_up:   5%|▌           | 973/20010 [49:28<17:03:26,  3.23s/batch]Batch 1000/20010 Done, mean position loss: 44.77284850597382\n",
      "Training growing_up:   5%|▌           | 991/20010 [49:37<14:00:01,  2.65s/batch]Batch 1000/20010 Done, mean position loss: 37.68829782962799\n",
      "Training growing_up:   5%|▌           | 991/20010 [49:43<14:49:57,  2.81s/batch]Batch 1000/20010 Done, mean position loss: 37.06560231924057\n",
      "Training growing_up:   5%|▌           | 987/20010 [49:44<16:12:47,  3.07s/batch]Batch 1000/20010 Done, mean position loss: 43.97633790254592\n",
      "Training growing_up:   5%|▌           | 979/20010 [49:44<14:12:11,  2.69s/batch]Batch 1000/20010 Done, mean position loss: 39.06327466487885\n",
      "Training growing_up:   5%|▌          | 1011/20010 [49:49<15:40:39,  2.97s/batch]Batch 1000/20010 Done, mean position loss: 35.39670508861542\n",
      "Training growing_up:   5%|▌           | 996/20010 [49:51<13:18:43,  2.52s/batch]Batch 1000/20010 Done, mean position loss: 73.06745056390763\n",
      "Training growing_up:   5%|▌           | 982/20010 [49:53<15:05:23,  2.85s/batch]Batch 1000/20010 Done, mean position loss: 42.469409649372096\n",
      "Training growing_up:   5%|▌           | 987/20010 [49:54<15:10:40,  2.87s/batch]Batch 1000/20010 Done, mean position loss: 35.92000730991363\n",
      "Training growing_up:   5%|▌           | 994/20010 [49:56<16:30:01,  3.12s/batch]Batch 1000/20010 Done, mean position loss: 39.860213549137114\n",
      "Training growing_up:   5%|▌           | 995/20010 [50:02<13:31:00,  2.56s/batch]Batch 1000/20010 Done, mean position loss: 38.56987240552902\n",
      "Training growing_up:   5%|▌           | 985/20010 [50:03<16:29:26,  3.12s/batch]Batch 1000/20010 Done, mean position loss: 37.059601657390594\n",
      "Training growing_up:   5%|▌          | 1016/20010 [50:05<17:19:51,  3.28s/batch]Batch 1000/20010 Done, mean position loss: 41.05802765130997\n",
      "Training growing_up:   5%|▌           | 997/20010 [50:06<14:58:16,  2.83s/batch]Batch 1000/20010 Done, mean position loss: 36.32061051368713\n",
      "Training growing_up:   5%|▌           | 987/20010 [50:08<16:01:43,  3.03s/batch]Batch 1000/20010 Done, mean position loss: 46.993880624771116\n",
      "Training growing_up:   5%|▌          | 1011/20010 [50:12<15:11:06,  2.88s/batch]Batch 1000/20010 Done, mean position loss: 40.27451141595841\n",
      "Training growing_up:   5%|▌          | 1021/20010 [50:15<17:00:53,  3.23s/batch]Batch 1000/20010 Done, mean position loss: 39.97501102924347\n",
      "Training growing_up:   5%|▌           | 993/20010 [50:15<15:36:44,  2.96s/batch]Batch 1000/20010 Done, mean position loss: 38.22693151712418\n",
      "Training growing_up:   5%|▌          | 1021/20010 [50:18<16:19:05,  3.09s/batch]Batch 1000/20010 Done, mean position loss: 38.07987150907516\n",
      "Training growing_up:   5%|▌           | 991/20010 [50:20<15:10:04,  2.87s/batch]Batch 1000/20010 Done, mean position loss: 48.8261159825325\n",
      "Batch 1000/20010 Done, mean position loss: 42.37176544904709\n",
      "Training growing_up:   5%|▌          | 1024/20010 [50:19<17:33:38,  3.33s/batch]Batch 1000/20010 Done, mean position loss: 42.364064435958866\n",
      "Training growing_up:   5%|▌          | 1003/20010 [50:25<15:35:25,  2.95s/batch]Batch 1000/20010 Done, mean position loss: 39.785675094127654\n",
      "Training growing_up:   5%|▌          | 1029/20010 [50:37<14:12:27,  2.69s/batch]Batch 1000/20010 Done, mean position loss: 39.693124549388884\n",
      "Training growing_up:   5%|▌          | 1016/20010 [50:38<15:50:20,  3.00s/batch]Batch 1000/20010 Done, mean position loss: 39.61236381530762\n",
      "Training growing_up:   5%|▌          | 1034/20010 [50:49<15:10:51,  2.88s/batch]Batch 1000/20010 Done, mean position loss: 45.69159207820892\n",
      "Training growing_up:   5%|▌          | 1014/20010 [50:49<14:59:40,  2.84s/batch]Batch 1000/20010 Done, mean position loss: 42.953599851131436\n",
      "Training growing_up:   5%|▌          | 1020/20010 [50:50<15:11:58,  2.88s/batch]Batch 1000/20010 Done, mean position loss: 42.274991669654845\n",
      "Training growing_up:   5%|▌          | 1002/20010 [50:51<15:01:12,  2.84s/batch]Batch 1000/20010 Done, mean position loss: 46.31868322134018\n",
      "Training growing_up:   5%|▌          | 1032/20010 [50:54<14:12:03,  2.69s/batch]Batch 1000/20010 Done, mean position loss: 42.81714018344879\n",
      "Training growing_up:   5%|▌          | 1000/20010 [50:58<15:55:12,  3.01s/batch]Batch 1000/20010 Done, mean position loss: 42.16049528121948\n",
      "Training growing_up:   5%|▌          | 1016/20010 [51:01<14:51:41,  2.82s/batch]Batch 1000/20010 Done, mean position loss: 43.120367102622986\n",
      "Training growing_up:   5%|▌          | 1064/20010 [54:00<15:16:46,  2.90s/batch]Batch 1100/20010 Done, mean position loss: 37.837072725296025\n",
      "Training growing_up:   5%|▌          | 1082/20010 [54:02<16:02:16,  3.05s/batch]Batch 1100/20010 Done, mean position loss: 45.47811793327332\n",
      "Training growing_up:   6%|▌          | 1102/20010 [54:03<13:45:03,  2.62s/batch]Batch 1100/20010 Done, mean position loss: 38.8525624346733\n",
      "Training growing_up:   5%|▌          | 1097/20010 [54:07<16:02:08,  3.05s/batch]Batch 1100/20010 Done, mean position loss: 44.28112407207489\n",
      "Training growing_up:   5%|▌          | 1066/20010 [54:10<15:45:24,  2.99s/batch]Batch 1100/20010 Done, mean position loss: 36.86751544952393\n",
      "Training growing_up:   5%|▌          | 1070/20010 [54:18<15:48:54,  3.01s/batch]Batch 1100/20010 Done, mean position loss: 38.631862120628355\n",
      "Training growing_up:   6%|▌          | 1101/20010 [54:19<15:31:19,  2.96s/batch]Batch 1100/20010 Done, mean position loss: 39.37702410697937\n",
      "Training growing_up:   6%|▌          | 1108/20010 [54:20<14:59:06,  2.85s/batch]Batch 1100/20010 Done, mean position loss: 40.06117135047913\n",
      "Training growing_up:   5%|▌          | 1081/20010 [54:21<17:51:25,  3.40s/batch]Batch 1100/20010 Done, mean position loss: 35.058187634944915\n",
      "Training growing_up:   5%|▌          | 1100/20010 [54:29<16:37:03,  3.16s/batch]Batch 1100/20010 Done, mean position loss: 35.21767386436463\n",
      "Training growing_up:   6%|▌          | 1105/20010 [54:30<14:54:12,  2.84s/batch]Batch 1100/20010 Done, mean position loss: 41.484990575313574\n",
      "Training growing_up:   5%|▌          | 1073/20010 [54:32<17:10:10,  3.26s/batch]Batch 1100/20010 Done, mean position loss: 39.46035031080246\n",
      "Training growing_up:   6%|▌          | 1114/20010 [54:36<14:23:21,  2.74s/batch]Batch 1100/20010 Done, mean position loss: 35.83891912221908\n",
      "Training growing_up:   5%|▌          | 1092/20010 [54:43<15:00:03,  2.85s/batch]Batch 1100/20010 Done, mean position loss: 67.14367403268814\n",
      "Training growing_up:   5%|▌          | 1079/20010 [54:49<15:49:16,  3.01s/batch]Batch 1100/20010 Done, mean position loss: 36.95673411369324\n",
      "Training growing_up:   6%|▌          | 1115/20010 [54:52<15:43:49,  3.00s/batch]Batch 1100/20010 Done, mean position loss: 40.06736493825912\n",
      "Training growing_up:   5%|▌          | 1090/20010 [54:53<13:47:42,  2.62s/batch]Batch 1100/20010 Done, mean position loss: 43.335442230701446\n",
      "Training growing_up:   6%|▌          | 1117/20010 [54:55<15:53:42,  3.03s/batch]Batch 1100/20010 Done, mean position loss: 36.94114631652832\n",
      "Training growing_up:   6%|▌          | 1113/20010 [54:57<16:53:12,  3.22s/batch]Batch 1100/20010 Done, mean position loss: 37.36695996999741\n",
      "Training growing_up:   6%|▌          | 1103/20010 [54:58<13:53:25,  2.64s/batch]Batch 1100/20010 Done, mean position loss: 40.81614159584045\n",
      "Training growing_up:   5%|▌          | 1098/20010 [55:04<15:57:23,  3.04s/batch]Batch 1100/20010 Done, mean position loss: 44.32275386810302\n",
      "Training growing_up:   6%|▌          | 1105/20010 [55:05<15:31:27,  2.96s/batch]Batch 1100/20010 Done, mean position loss: 37.652391393184665\n",
      "Training growing_up:   5%|▌          | 1099/20010 [55:08<16:31:58,  3.15s/batch]Batch 1100/20010 Done, mean position loss: 40.80805073022842\n",
      "Training growing_up:   6%|▌          | 1114/20010 [55:08<15:40:49,  2.99s/batch]Batch 1100/20010 Done, mean position loss: 39.78439921140671\n",
      "Training growing_up:   6%|▌          | 1118/20010 [55:10<14:45:10,  2.81s/batch]Batch 1100/20010 Done, mean position loss: 41.24394423246384\n",
      "Training growing_up:   6%|▌          | 1118/20010 [55:11<16:02:39,  3.06s/batch]Batch 1100/20010 Done, mean position loss: 37.37966212034225\n",
      "Training growing_up:   6%|▌          | 1115/20010 [55:12<15:19:45,  2.92s/batch]Batch 1100/20010 Done, mean position loss: 35.80754401683808\n",
      "Training growing_up:   5%|▌          | 1087/20010 [55:12<15:40:59,  2.98s/batch]Batch 1100/20010 Done, mean position loss: 36.698533337116245\n",
      "Training growing_up:   6%|▌          | 1119/20010 [55:15<16:30:49,  3.15s/batch]Batch 1100/20010 Done, mean position loss: 39.662873988151546\n",
      "Training growing_up:   6%|▌          | 1114/20010 [55:20<14:34:23,  2.78s/batch]Batch 1100/20010 Done, mean position loss: 36.93751472234726\n",
      "Training growing_up:   6%|▌          | 1126/20010 [55:25<15:06:44,  2.88s/batch]Batch 1100/20010 Done, mean position loss: 34.20670471668244\n",
      "Training growing_up:   5%|▌          | 1092/20010 [55:28<14:30:47,  2.76s/batch]Batch 1100/20010 Done, mean position loss: 37.49887341022492\n",
      "Training growing_up:   6%|▌          | 1123/20010 [55:35<14:26:45,  2.75s/batch]Batch 1100/20010 Done, mean position loss: 35.076675024032596\n",
      "Training growing_up:   6%|▌          | 1118/20010 [55:43<15:58:30,  3.04s/batch]Batch 1100/20010 Done, mean position loss: 40.32975743055344\n",
      "Training growing_up:   6%|▌          | 1135/20010 [55:50<14:38:43,  2.79s/batch]Batch 1100/20010 Done, mean position loss: 49.05244535923004\n",
      "Training growing_up:   6%|▌          | 1132/20010 [55:51<14:10:48,  2.70s/batch]Batch 1100/20010 Done, mean position loss: 38.9146897816658\n",
      "Training growing_up:   6%|▌          | 1127/20010 [55:52<16:05:07,  3.07s/batch]Batch 1100/20010 Done, mean position loss: 41.47838971376419\n",
      "Training growing_up:   6%|▌          | 1102/20010 [55:54<14:33:59,  2.77s/batch]Batch 1100/20010 Done, mean position loss: 39.50896037817001\n",
      "Training growing_up:   6%|▌          | 1120/20010 [55:54<16:30:18,  3.15s/batch]Batch 1100/20010 Done, mean position loss: 41.47555165052414\n",
      "Training growing_up:   6%|▌          | 1113/20010 [55:55<15:04:35,  2.87s/batch]Batch 1100/20010 Done, mean position loss: 36.5478131532669\n",
      "Training growing_up:   6%|▋          | 1198/20010 [58:45<14:29:12,  2.77s/batch]Batch 1200/20010 Done, mean position loss: 66.09059698343277\n",
      "Training growing_up:   6%|▋          | 1174/20010 [58:52<14:08:38,  2.70s/batch]Batch 1200/20010 Done, mean position loss: 34.14505714416504\n",
      "Training growing_up:   6%|▋          | 1174/20010 [58:53<16:21:29,  3.13s/batch]Batch 1200/20010 Done, mean position loss: 37.32525610208511\n",
      "Training growing_up:   6%|▋          | 1191/20010 [58:57<15:02:09,  2.88s/batch]Batch 1200/20010 Done, mean position loss: 41.10766110897064\n",
      "Training growing_up:   6%|▋          | 1183/20010 [59:04<13:20:01,  2.55s/batch]Batch 1200/20010 Done, mean position loss: 33.988857476711274\n",
      "Training growing_up:   6%|▋          | 1196/20010 [59:06<15:50:03,  3.03s/batch]Batch 1200/20010 Done, mean position loss: 37.91484919309616\n",
      "Training growing_up:   6%|▋          | 1177/20010 [59:08<15:22:23,  2.94s/batch]Batch 1200/20010 Done, mean position loss: 35.55399507761001\n",
      "Training growing_up:   6%|▋          | 1175/20010 [59:08<15:39:33,  2.99s/batch]Batch 1200/20010 Done, mean position loss: 38.36427170038223\n",
      "Training growing_up:   6%|▋          | 1172/20010 [59:09<14:35:40,  2.79s/batch]Batch 1200/20010 Done, mean position loss: 35.52254716634751\n",
      "Training growing_up:   6%|▋          | 1181/20010 [59:14<14:45:28,  2.82s/batch]Batch 1200/20010 Done, mean position loss: 34.20876780033112\n",
      "Training growing_up:   6%|▋          | 1179/20010 [59:17<12:15:14,  2.34s/batch]Batch 1200/20010 Done, mean position loss: 36.05528081178665\n",
      "Training growing_up:   6%|▋          | 1171/20010 [59:18<12:30:25,  2.39s/batch]Batch 1200/20010 Done, mean position loss: 36.87523774385453\n",
      "Training growing_up:   6%|▋          | 1216/20010 [59:25<11:54:00,  2.28s/batch]Batch 1200/20010 Done, mean position loss: 43.127919826507565\n",
      "Training growing_up:   6%|▋          | 1191/20010 [59:26<12:53:54,  2.47s/batch]Batch 1200/20010 Done, mean position loss: 34.9718229842186\n",
      "Training growing_up:   6%|▋          | 1177/20010 [59:33<13:42:29,  2.62s/batch]Batch 1200/20010 Done, mean position loss: 36.985432863235474\n",
      "Training growing_up:   6%|▋          | 1186/20010 [59:35<13:08:43,  2.51s/batch]Batch 1200/20010 Done, mean position loss: 35.36972708702087\n",
      "Training growing_up:   6%|▋          | 1177/20010 [59:36<14:40:08,  2.80s/batch]Batch 1200/20010 Done, mean position loss: 35.888931128978726\n",
      "Training growing_up:   6%|▋          | 1189/20010 [59:42<13:17:45,  2.54s/batch]Batch 1200/20010 Done, mean position loss: 37.30680510044098\n",
      "Training growing_up:   6%|▋          | 1219/20010 [59:42<13:43:53,  2.63s/batch]Batch 1200/20010 Done, mean position loss: 36.03249883890152\n",
      "Training growing_up:   6%|▋          | 1216/20010 [59:48<12:51:28,  2.46s/batch]Batch 1200/20010 Done, mean position loss: 37.04518358945847\n",
      "Training growing_up:   6%|▋          | 1196/20010 [59:50<12:34:28,  2.41s/batch]Batch 1200/20010 Done, mean position loss: 33.0042098736763\n",
      "Training growing_up:   6%|▋          | 1210/20010 [59:50<13:47:03,  2.64s/batch]Batch 1200/20010 Done, mean position loss: 38.825577785968775\n",
      "Training growing_up:   6%|▋          | 1189/20010 [59:53<13:22:08,  2.56s/batch]Batch 1200/20010 Done, mean position loss: 39.22444223165512\n",
      "Training growing_up:   6%|▋          | 1209/20010 [59:54<14:34:35,  2.79s/batch]Batch 1200/20010 Done, mean position loss: 37.8646775841713\n",
      "Training growing_up:   6%|▋          | 1185/20010 [59:55<13:02:48,  2.50s/batch]Batch 1200/20010 Done, mean position loss: 34.740982167720794\n",
      "Training growing_up:   6%|▋          | 1224/20010 [59:55<13:22:28,  2.56s/batch]Batch 1200/20010 Done, mean position loss: 38.288622560501096\n",
      "Training growing_up:   6%|▋          | 1217/20010 [59:56<14:41:02,  2.81s/batch]Batch 1200/20010 Done, mean position loss: 34.55914692878723\n",
      "Training growing_up:   6%|▋          | 1223/20010 [59:57<14:21:42,  2.75s/batch]Batch 1200/20010 Done, mean position loss: 35.64846757173538\n",
      "Training growing_up:   6%|▌        | 1206/20010 [1:00:03<13:11:44,  2.53s/batch]Batch 1200/20010 Done, mean position loss: 36.53229333877564\n",
      "Training growing_up:   6%|▌        | 1217/20010 [1:00:06<13:24:39,  2.57s/batch]Batch 1200/20010 Done, mean position loss: 36.37290115833282\n",
      "Training growing_up:   6%|▌        | 1230/20010 [1:00:10<14:21:56,  2.75s/batch]Batch 1200/20010 Done, mean position loss: 33.197870404720305\n",
      "Training growing_up:   6%|▌        | 1232/20010 [1:00:13<14:38:47,  2.81s/batch]Batch 1200/20010 Done, mean position loss: 33.76857649326325\n",
      "Training growing_up:   6%|▌        | 1211/20010 [1:00:16<14:53:53,  2.85s/batch]Batch 1200/20010 Done, mean position loss: 34.65496921777725\n",
      "Training growing_up:   6%|▌        | 1211/20010 [1:00:22<12:54:29,  2.47s/batch]Batch 1200/20010 Done, mean position loss: 36.12184543132782\n",
      "Training growing_up:   6%|▌        | 1207/20010 [1:00:27<11:56:59,  2.29s/batch]Batch 1200/20010 Done, mean position loss: 40.245130865573884\n",
      "Training growing_up:   6%|▌        | 1215/20010 [1:00:36<16:41:46,  3.20s/batch]Batch 1200/20010 Done, mean position loss: 35.886202824115756\n",
      "Training growing_up:   6%|▌        | 1220/20010 [1:00:38<12:33:26,  2.41s/batch]Batch 1200/20010 Done, mean position loss: 38.09887312412262\n",
      "Training growing_up:   6%|▌        | 1229/20010 [1:00:39<13:33:58,  2.60s/batch]Batch 1200/20010 Done, mean position loss: 41.544832136631015\n",
      "Training growing_up:   6%|▌        | 1239/20010 [1:00:39<13:17:16,  2.55s/batch]Batch 1200/20010 Done, mean position loss: 37.62965550899506\n",
      "Training growing_up:   6%|▌        | 1214/20010 [1:00:40<13:29:55,  2.59s/batch]Batch 1200/20010 Done, mean position loss: 35.301467907428744\n",
      "Training growing_up:   6%|▌        | 1279/20010 [1:03:06<16:22:09,  3.15s/batch]Batch 1300/20010 Done, mean position loss: 41.39044441699982\n",
      "Training growing_up:   6%|▌        | 1286/20010 [1:03:10<15:08:34,  2.91s/batch]Batch 1300/20010 Done, mean position loss: 34.532162137031555\n",
      "Training growing_up:   6%|▌        | 1283/20010 [1:03:15<14:44:02,  2.83s/batch]Batch 1300/20010 Done, mean position loss: 34.98918479442597\n",
      "Training growing_up:   6%|▌        | 1276/20010 [1:03:16<14:13:32,  2.73s/batch]Batch 1300/20010 Done, mean position loss: 40.47724843502045\n",
      "Training growing_up:   6%|▌        | 1260/20010 [1:03:21<16:05:15,  3.09s/batch]Batch 1300/20010 Done, mean position loss: 34.971468040943144\n",
      "Training growing_up:   6%|▌        | 1290/20010 [1:03:25<13:55:18,  2.68s/batch]Batch 1300/20010 Done, mean position loss: 35.424376883506774\n",
      "Training growing_up:   6%|▌        | 1280/20010 [1:03:28<14:49:38,  2.85s/batch]Batch 1300/20010 Done, mean position loss: 33.281878280639646\n",
      "Training growing_up:   7%|▌        | 1307/20010 [1:03:30<12:41:22,  2.44s/batch]Batch 1300/20010 Done, mean position loss: 35.822075252532954\n",
      "Training growing_up:   6%|▌        | 1268/20010 [1:03:36<16:01:38,  3.08s/batch]Batch 1300/20010 Done, mean position loss: 34.51207586288452\n",
      "Training growing_up:   6%|▌        | 1294/20010 [1:03:37<14:22:02,  2.76s/batch]Batch 1300/20010 Done, mean position loss: 37.8526882982254\n",
      "Training growing_up:   7%|▌        | 1310/20010 [1:03:39<15:07:09,  2.91s/batch]Batch 1300/20010 Done, mean position loss: 32.95255875110627\n",
      "Training growing_up:   6%|▌        | 1265/20010 [1:03:39<16:00:26,  3.07s/batch]Batch 1300/20010 Done, mean position loss: 35.045624589920045\n",
      "Training growing_up:   6%|▌        | 1282/20010 [1:03:49<15:26:29,  2.97s/batch]Batch 1300/20010 Done, mean position loss: 34.215432472229\n",
      "Training growing_up:   6%|▌        | 1296/20010 [1:03:52<14:50:37,  2.86s/batch]Batch 1300/20010 Done, mean position loss: 37.46098052978516\n",
      "Training growing_up:   7%|▌        | 1319/20010 [1:03:55<15:20:20,  2.95s/batch]Batch 1300/20010 Done, mean position loss: 32.958509736061096\n",
      "Training growing_up:   6%|▌        | 1283/20010 [1:04:02<14:48:40,  2.85s/batch]Batch 1300/20010 Done, mean position loss: 34.30515015363693\n",
      "Training growing_up:   6%|▌        | 1278/20010 [1:04:05<16:00:03,  3.08s/batch]Batch 1300/20010 Done, mean position loss: 34.55306781768799\n",
      "Training growing_up:   6%|▌        | 1288/20010 [1:04:06<14:54:05,  2.87s/batch]Batch 1300/20010 Done, mean position loss: 37.869334187507626\n",
      "Training growing_up:   6%|▌        | 1297/20010 [1:04:07<15:00:05,  2.89s/batch]Batch 1300/20010 Done, mean position loss: 34.428188543319706\n",
      "Training growing_up:   6%|▌        | 1290/20010 [1:04:08<14:25:28,  2.77s/batch]Batch 1300/20010 Done, mean position loss: 34.791461789608\n",
      "Training growing_up:   6%|▌        | 1278/20010 [1:04:15<13:50:15,  2.66s/batch]Batch 1300/20010 Done, mean position loss: 34.69811939477921\n",
      "Training growing_up:   6%|▌        | 1281/20010 [1:04:19<14:04:21,  2.70s/batch]Batch 1300/20010 Done, mean position loss: 35.2812758231163\n",
      "Training growing_up:   7%|▌        | 1301/20010 [1:04:19<14:52:39,  2.86s/batch]Batch 1300/20010 Done, mean position loss: 36.64248637199402\n",
      "Batch 1300/20010 Done, mean position loss: 37.337411990165705\n",
      "Training growing_up:   7%|▌        | 1310/20010 [1:04:20<14:49:55,  2.86s/batch]Batch 1300/20010 Done, mean position loss: 36.36352866888046\n",
      "Training growing_up:   7%|▌        | 1327/20010 [1:04:27<15:18:05,  2.95s/batch]Batch 1300/20010 Done, mean position loss: 37.42386156797409\n",
      "Training growing_up:   7%|▌        | 1305/20010 [1:04:29<13:53:46,  2.67s/batch]Batch 1300/20010 Done, mean position loss: 36.802340056896206\n",
      "Training growing_up:   6%|▌        | 1298/20010 [1:04:29<13:44:36,  2.64s/batch]Batch 1300/20010 Done, mean position loss: 34.993321573734285\n",
      "Training growing_up:   7%|▌        | 1305/20010 [1:04:32<15:34:42,  3.00s/batch]Batch 1300/20010 Done, mean position loss: 35.72384850502014\n",
      "Training growing_up:   7%|▌        | 1331/20010 [1:04:38<14:18:57,  2.76s/batch]Batch 1300/20010 Done, mean position loss: 34.01681714057922\n",
      "Training growing_up:   7%|▌        | 1330/20010 [1:04:42<14:57:47,  2.88s/batch]Batch 1300/20010 Done, mean position loss: 33.70650912523269\n",
      "Training growing_up:   7%|▌        | 1305/20010 [1:04:43<15:08:05,  2.91s/batch]Batch 1300/20010 Done, mean position loss: 35.68045022249222\n",
      "Training growing_up:   7%|▌        | 1328/20010 [1:04:53<12:49:52,  2.47s/batch]Batch 1300/20010 Done, mean position loss: 35.29595983743667\n",
      "Training growing_up:   7%|▌        | 1307/20010 [1:04:53<14:35:32,  2.81s/batch]Batch 1300/20010 Done, mean position loss: 35.58924604892731\n",
      "Training growing_up:   7%|▌        | 1323/20010 [1:05:03<13:20:37,  2.57s/batch]Batch 1300/20010 Done, mean position loss: 41.901793344020845\n",
      "Training growing_up:   7%|▌        | 1342/20010 [1:05:09<14:37:35,  2.82s/batch]Batch 1300/20010 Done, mean position loss: 34.20186696052551\n",
      "Training growing_up:   7%|▌        | 1316/20010 [1:05:10<13:02:27,  2.51s/batch]Batch 1300/20010 Done, mean position loss: 37.7073787522316\n",
      "Training growing_up:   7%|▌        | 1344/20010 [1:05:14<14:25:57,  2.78s/batch]Batch 1300/20010 Done, mean position loss: 34.547886967659\n",
      "Training growing_up:   7%|▌        | 1320/20010 [1:05:15<13:44:21,  2.65s/batch]Batch 1300/20010 Done, mean position loss: 34.767950098514554\n",
      "Training growing_up:   7%|▌        | 1302/20010 [1:05:18<13:04:28,  2.52s/batch]Batch 1300/20010 Done, mean position loss: 33.872051756381985\n",
      "Training growing_up:   7%|▋        | 1395/20010 [1:07:58<16:24:48,  3.17s/batch]Batch 1400/20010 Done, mean position loss: 35.99801717042923\n",
      "Training growing_up:   7%|▌        | 1374/20010 [1:07:58<15:05:33,  2.92s/batch]Batch 1400/20010 Done, mean position loss: 35.20605973958969\n",
      "Training growing_up:   7%|▋        | 1392/20010 [1:08:04<14:00:34,  2.71s/batch]Batch 1400/20010 Done, mean position loss: 33.867107882499695\n",
      "Training growing_up:   7%|▌        | 1379/20010 [1:08:14<15:05:17,  2.92s/batch]Batch 1400/20010 Done, mean position loss: 32.74641994714737\n",
      "Training growing_up:   7%|▌        | 1380/20010 [1:08:15<14:38:25,  2.83s/batch]Batch 1400/20010 Done, mean position loss: 35.69606131792069\n",
      "Training growing_up:   7%|▋        | 1407/20010 [1:08:14<15:04:18,  2.92s/batch]Batch 1400/20010 Done, mean position loss: 35.63455274820328\n",
      "Training growing_up:   7%|▋        | 1402/20010 [1:08:17<14:02:04,  2.72s/batch]Batch 1400/20010 Done, mean position loss: 34.35987297058106\n",
      "Training growing_up:   7%|▋        | 1406/20010 [1:08:19<15:35:42,  3.02s/batch]Batch 1400/20010 Done, mean position loss: 35.29068729877472\n",
      "Training growing_up:   7%|▋        | 1400/20010 [1:08:32<16:39:00,  3.22s/batch]Batch 1400/20010 Done, mean position loss: 34.39787506103516\n",
      "Training growing_up:   7%|▋        | 1393/20010 [1:08:34<14:57:35,  2.89s/batch]Batch 1400/20010 Done, mean position loss: 33.31781520366668\n",
      "Training growing_up:   7%|▋        | 1400/20010 [1:08:36<14:55:59,  2.89s/batch]Batch 1400/20010 Done, mean position loss: 34.94531900882721\n",
      "Training growing_up:   7%|▋        | 1413/20010 [1:08:39<14:41:01,  2.84s/batch]Batch 1400/20010 Done, mean position loss: 32.82675487756729\n",
      "Training growing_up:   7%|▋        | 1393/20010 [1:08:40<16:29:12,  3.19s/batch]Batch 1400/20010 Done, mean position loss: 34.62506622314453\n",
      "Training growing_up:   7%|▋        | 1396/20010 [1:08:49<15:49:12,  3.06s/batch]Batch 1400/20010 Done, mean position loss: 37.22661617517471\n",
      "Training growing_up:   7%|▌        | 1373/20010 [1:09:00<15:38:45,  3.02s/batch]Batch 1400/20010 Done, mean position loss: 34.814255237579346\n",
      "Training growing_up:   7%|▋        | 1417/20010 [1:09:04<14:47:43,  2.86s/batch]Batch 1400/20010 Done, mean position loss: 33.909081122875214\n",
      "Training growing_up:   7%|▌        | 1378/20010 [1:09:08<17:04:59,  3.30s/batch]Batch 1400/20010 Done, mean position loss: 34.1735927081108\n",
      "Training growing_up:   7%|▋        | 1423/20010 [1:09:09<16:27:41,  3.19s/batch]Batch 1400/20010 Done, mean position loss: 35.801603763103486\n",
      "Training growing_up:   7%|▋        | 1391/20010 [1:09:13<15:31:45,  3.00s/batch]Batch 1400/20010 Done, mean position loss: 37.806426155567166\n",
      "Training growing_up:   7%|▋        | 1403/20010 [1:09:16<18:15:11,  3.53s/batch]Batch 1400/20010 Done, mean position loss: 34.84452799081802\n",
      "Training growing_up:   7%|▋        | 1405/20010 [1:09:18<18:01:50,  3.49s/batch]Batch 1400/20010 Done, mean position loss: 35.197435569763186\n",
      "Training growing_up:   7%|▋        | 1401/20010 [1:09:19<17:38:53,  3.41s/batch]Batch 1400/20010 Done, mean position loss: 33.45694383382798\n",
      "Training growing_up:   7%|▋        | 1428/20010 [1:09:21<17:46:28,  3.44s/batch]Batch 1400/20010 Done, mean position loss: 33.03703173160553\n",
      "Training growing_up:   7%|▌        | 1388/20010 [1:09:22<17:26:33,  3.37s/batch]Batch 1400/20010 Done, mean position loss: 32.74472426652908\n",
      "Training growing_up:   7%|▋        | 1401/20010 [1:09:22<16:13:31,  3.14s/batch]Batch 1400/20010 Done, mean position loss: 35.91198816299439\n",
      "Training growing_up:   7%|▋        | 1397/20010 [1:09:23<18:10:51,  3.52s/batch]Batch 1400/20010 Done, mean position loss: 35.39115910053253\n",
      "Training growing_up:   7%|▋        | 1406/20010 [1:09:34<18:12:11,  3.52s/batch]Batch 1400/20010 Done, mean position loss: 35.54971948862075\n",
      "Training growing_up:   7%|▋        | 1421/20010 [1:09:36<15:30:06,  3.00s/batch]Batch 1400/20010 Done, mean position loss: 36.00937362909317\n",
      "Training growing_up:   7%|▋        | 1428/20010 [1:09:39<17:02:24,  3.30s/batch]Batch 1400/20010 Done, mean position loss: 36.16103901386261\n",
      "Training growing_up:   7%|▋        | 1431/20010 [1:09:48<16:37:46,  3.22s/batch]Batch 1400/20010 Done, mean position loss: 32.878474795818335\n",
      "Training growing_up:   7%|▋        | 1405/20010 [1:09:50<17:10:45,  3.32s/batch]Batch 1400/20010 Done, mean position loss: 33.75961775302887\n",
      "Training growing_up:   7%|▋        | 1426/20010 [1:09:58<19:20:42,  3.75s/batch]Batch 1400/20010 Done, mean position loss: 36.012067086696625\n",
      "Training growing_up:   7%|▋        | 1435/20010 [1:10:01<17:06:34,  3.32s/batch]Batch 1400/20010 Done, mean position loss: 34.29602680206298\n",
      "Training growing_up:   7%|▋        | 1392/20010 [1:10:06<17:59:11,  3.48s/batch]Batch 1400/20010 Done, mean position loss: 35.96034215927124\n",
      "Training growing_up:   7%|▋        | 1405/20010 [1:10:19<17:12:02,  3.33s/batch]Batch 1400/20010 Done, mean position loss: 37.46903334856034\n",
      "Training growing_up:   7%|▋        | 1409/20010 [1:10:27<18:11:05,  3.52s/batch]Batch 1400/20010 Done, mean position loss: 36.16605783224106\n",
      "Training growing_up:   7%|▋        | 1413/20010 [1:10:28<16:35:08,  3.21s/batch]Batch 1400/20010 Done, mean position loss: 36.98737184524536\n",
      "Training growing_up:   7%|▋        | 1434/20010 [1:10:30<17:28:15,  3.39s/batch]Batch 1400/20010 Done, mean position loss: 32.4549039888382\n",
      "Training growing_up:   7%|▋        | 1424/20010 [1:10:35<18:55:56,  3.67s/batch]Batch 1400/20010 Done, mean position loss: 34.154713792800905\n",
      "Training growing_up:   7%|▋        | 1423/20010 [1:10:35<16:52:20,  3.27s/batch]Batch 1400/20010 Done, mean position loss: 35.6756551527977\n",
      "Training growing_up:   7%|▋        | 1470/20010 [1:13:01<15:28:51,  3.01s/batch]Batch 1500/20010 Done, mean position loss: 34.441663734912865\n",
      "Training growing_up:   7%|▋        | 1485/20010 [1:13:05<14:45:07,  2.87s/batch]Batch 1500/20010 Done, mean position loss: 33.78417334079742\n",
      "Training growing_up:   7%|▋        | 1467/20010 [1:13:14<15:22:28,  2.98s/batch]Batch 1500/20010 Done, mean position loss: 33.39389398336411\n",
      "Training growing_up:   8%|▋        | 1507/20010 [1:13:21<13:40:22,  2.66s/batch]Batch 1500/20010 Done, mean position loss: 37.5276215338707\n",
      "Training growing_up:   7%|▋        | 1473/20010 [1:13:23<15:48:06,  3.07s/batch]Batch 1500/20010 Done, mean position loss: 35.897185986042025\n",
      "Training growing_up:   8%|▋        | 1503/20010 [1:13:28<13:47:05,  2.68s/batch]Batch 1500/20010 Done, mean position loss: 34.837342400550845\n",
      "Training growing_up:   7%|▋        | 1475/20010 [1:13:29<15:55:19,  3.09s/batch]Batch 1500/20010 Done, mean position loss: 33.9699997997284\n",
      "Training growing_up:   7%|▋        | 1457/20010 [1:13:31<16:31:31,  3.21s/batch]Batch 1500/20010 Done, mean position loss: 35.35791956186294\n",
      "Training growing_up:   8%|▋        | 1504/20010 [1:13:37<13:49:51,  2.69s/batch]Batch 1500/20010 Done, mean position loss: 34.524110150337215\n",
      "Training growing_up:   7%|▋        | 1471/20010 [1:13:44<15:00:18,  2.91s/batch]Batch 1500/20010 Done, mean position loss: 35.96044037580491\n",
      "Training growing_up:   7%|▋        | 1472/20010 [1:13:48<16:19:20,  3.17s/batch]Batch 1500/20010 Done, mean position loss: 32.25731927394867\n",
      "Training growing_up:   7%|▋        | 1491/20010 [1:13:49<14:53:00,  2.89s/batch]Batch 1500/20010 Done, mean position loss: 33.96688883781433\n",
      "Training growing_up:   7%|▋        | 1492/20010 [1:13:53<15:24:58,  3.00s/batch]Batch 1500/20010 Done, mean position loss: 33.02155374765396\n",
      "Training growing_up:   7%|▋        | 1478/20010 [1:14:05<15:13:00,  2.96s/batch]Batch 1500/20010 Done, mean position loss: 34.014375481605526\n",
      "Training growing_up:   7%|▋        | 1483/20010 [1:14:15<14:51:00,  2.89s/batch]Batch 1500/20010 Done, mean position loss: 33.600149207115166\n",
      "Training growing_up:   7%|▋        | 1490/20010 [1:14:16<13:31:54,  2.63s/batch]Batch 1500/20010 Done, mean position loss: 34.13727776765823\n",
      "Training growing_up:   8%|▋        | 1523/20010 [1:14:17<14:10:35,  2.76s/batch]Batch 1500/20010 Done, mean position loss: 33.403566234111786\n",
      "Training growing_up:   7%|▋        | 1496/20010 [1:14:18<14:10:49,  2.76s/batch]Batch 1500/20010 Done, mean position loss: 34.36903110742569\n",
      "Training growing_up:   8%|▋        | 1504/20010 [1:14:27<15:12:26,  2.96s/batch]Batch 1500/20010 Done, mean position loss: 33.03801284551621\n",
      "Training growing_up:   7%|▋        | 1479/20010 [1:14:30<14:40:49,  2.85s/batch]Batch 1500/20010 Done, mean position loss: 32.810549437999725\n",
      "Training growing_up:   7%|▋        | 1487/20010 [1:14:32<15:40:21,  3.05s/batch]Batch 1500/20010 Done, mean position loss: 32.766530492305755\n",
      "Training growing_up:   7%|▋        | 1494/20010 [1:14:35<16:15:03,  3.16s/batch]Batch 1500/20010 Done, mean position loss: 37.35300805330276\n",
      "Training growing_up:   7%|▋        | 1490/20010 [1:14:35<15:03:13,  2.93s/batch]Batch 1500/20010 Done, mean position loss: 34.27916745424271\n",
      "Training growing_up:   8%|▋        | 1517/20010 [1:14:37<16:50:25,  3.28s/batch]Batch 1500/20010 Done, mean position loss: 36.497446320056916\n",
      "Batch 1500/20010 Done, mean position loss: 35.613841924667355\n",
      "Training growing_up:   8%|▋        | 1512/20010 [1:14:38<16:33:03,  3.22s/batch]Batch 1500/20010 Done, mean position loss: 34.10161862134933\n",
      "Training growing_up:   8%|▋        | 1529/20010 [1:14:46<15:43:35,  3.06s/batch]Batch 1500/20010 Done, mean position loss: 31.56316487312317\n",
      "Training growing_up:   8%|▋        | 1506/20010 [1:14:51<14:33:18,  2.83s/batch]Batch 1500/20010 Done, mean position loss: 33.397859213352206\n",
      "Training growing_up:   8%|▋        | 1507/20010 [1:14:55<16:27:10,  3.20s/batch]Batch 1500/20010 Done, mean position loss: 33.32884397745133\n",
      "Training growing_up:   8%|▋        | 1524/20010 [1:14:59<17:11:21,  3.35s/batch]Batch 1500/20010 Done, mean position loss: 35.42275837659835\n",
      "Training growing_up:   8%|▋        | 1531/20010 [1:15:00<16:22:56,  3.19s/batch]Batch 1500/20010 Done, mean position loss: 33.28280830621719\n",
      "Training growing_up:   8%|▋        | 1523/20010 [1:15:10<14:55:40,  2.91s/batch]Batch 1500/20010 Done, mean position loss: 33.51840182065963\n",
      "Training growing_up:   8%|▋        | 1502/20010 [1:15:12<15:22:05,  2.99s/batch]Batch 1500/20010 Done, mean position loss: 32.26820735931396\n",
      "Training growing_up:   8%|▋        | 1536/20010 [1:15:15<16:07:50,  3.14s/batch]Batch 1500/20010 Done, mean position loss: 34.56370917320251\n",
      "Training growing_up:   7%|▋        | 1498/20010 [1:15:25<14:43:11,  2.86s/batch]Batch 1500/20010 Done, mean position loss: 34.50635457515716\n",
      "Training growing_up:   8%|▋        | 1552/20010 [1:15:34<15:05:36,  2.94s/batch]Batch 1500/20010 Done, mean position loss: 33.947546770572664\n",
      "Training growing_up:   7%|▋        | 1499/20010 [1:15:35<14:10:56,  2.76s/batch]Batch 1500/20010 Done, mean position loss: 32.79315182209015\n",
      "Training growing_up:   8%|▋        | 1522/20010 [1:15:40<16:50:38,  3.28s/batch]Batch 1500/20010 Done, mean position loss: 34.12558751583099\n",
      "Training growing_up:   8%|▋        | 1525/20010 [1:15:40<15:20:23,  2.99s/batch]Batch 1500/20010 Done, mean position loss: 35.11107161045074\n",
      "Training growing_up:   8%|▋        | 1523/20010 [1:15:43<14:01:20,  2.73s/batch]Batch 1500/20010 Done, mean position loss: 35.09408046960831\n",
      "Training growing_up:   8%|▋        | 1569/20010 [1:17:42<13:39:31,  2.67s/batch]Batch 1600/20010 Done, mean position loss: 32.48701951265335\n",
      "Training growing_up:   8%|▋        | 1583/20010 [1:17:42<11:58:22,  2.34s/batch]Batch 1600/20010 Done, mean position loss: 35.06538107633591\n",
      "Training growing_up:   8%|▋        | 1558/20010 [1:17:48<13:48:05,  2.69s/batch]Batch 1600/20010 Done, mean position loss: 33.19652876853943\n",
      "Training growing_up:   8%|▋        | 1586/20010 [1:17:55<12:49:09,  2.50s/batch]Batch 1600/20010 Done, mean position loss: 33.43121742725373\n",
      "Training growing_up:   8%|▋        | 1551/20010 [1:17:59<14:41:33,  2.87s/batch]Batch 1600/20010 Done, mean position loss: 37.49703895568848\n",
      "Training growing_up:   8%|▋        | 1581/20010 [1:18:04<12:56:48,  2.53s/batch]Batch 1600/20010 Done, mean position loss: 31.837244052886962\n",
      "Training growing_up:   8%|▋        | 1583/20010 [1:18:10<13:14:53,  2.59s/batch]Batch 1600/20010 Done, mean position loss: 33.11718137741089\n",
      "Training growing_up:   8%|▋        | 1594/20010 [1:18:11<14:35:10,  2.85s/batch]Batch 1600/20010 Done, mean position loss: 37.98202223539353\n",
      "Training growing_up:   8%|▋        | 1597/20010 [1:18:16<14:11:14,  2.77s/batch]Batch 1600/20010 Done, mean position loss: 34.06716184854508\n",
      "Training growing_up:   8%|▋        | 1577/20010 [1:18:17<13:08:44,  2.57s/batch]Batch 1600/20010 Done, mean position loss: 34.380800378322604\n",
      "Training growing_up:   8%|▋        | 1585/20010 [1:18:27<14:01:40,  2.74s/batch]Batch 1600/20010 Done, mean position loss: 31.274708375930786\n",
      "Training growing_up:   8%|▋        | 1596/20010 [1:18:32<14:09:41,  2.77s/batch]Batch 1600/20010 Done, mean position loss: 36.46788036346435\n",
      "Training growing_up:   8%|▋        | 1587/20010 [1:18:32<13:07:32,  2.56s/batch]Batch 1600/20010 Done, mean position loss: 33.127806563377376\n",
      "Training growing_up:   8%|▋        | 1622/20010 [1:18:37<15:32:16,  3.04s/batch]Batch 1600/20010 Done, mean position loss: 32.746157560348514\n",
      "Training growing_up:   8%|▋        | 1593/20010 [1:18:44<14:57:59,  2.93s/batch]Batch 1600/20010 Done, mean position loss: 34.57368827581406\n",
      "Training growing_up:   8%|▋        | 1579/20010 [1:18:48<14:59:58,  2.93s/batch]Batch 1600/20010 Done, mean position loss: 31.528318171501162\n",
      "Training growing_up:   8%|▋        | 1600/20010 [1:18:52<14:15:43,  2.79s/batch]Batch 1600/20010 Done, mean position loss: 32.195237519741056\n",
      "Training growing_up:   8%|▋        | 1618/20010 [1:18:56<14:19:28,  2.80s/batch]Batch 1600/20010 Done, mean position loss: 34.059583973884585\n",
      "Training growing_up:   8%|▋        | 1600/20010 [1:19:02<14:23:10,  2.81s/batch]Batch 1600/20010 Done, mean position loss: 34.072360711097716\n",
      "Training growing_up:   8%|▋        | 1593/20010 [1:19:06<15:41:32,  3.07s/batch]Batch 1600/20010 Done, mean position loss: 34.19305312395096\n",
      "Training growing_up:   8%|▋        | 1624/20010 [1:19:07<13:36:39,  2.67s/batch]Batch 1600/20010 Done, mean position loss: 33.73577163219451\n",
      "Training growing_up:   8%|▋        | 1621/20010 [1:19:08<15:10:38,  2.97s/batch]Batch 1600/20010 Done, mean position loss: 32.99798599243164\n",
      "Training growing_up:   8%|▋        | 1580/20010 [1:19:10<14:43:15,  2.88s/batch]Batch 1600/20010 Done, mean position loss: 33.87208216190338\n",
      "Training growing_up:   8%|▋        | 1615/20010 [1:19:11<14:32:51,  2.85s/batch]Batch 1600/20010 Done, mean position loss: 33.24999606132508\n",
      "Training growing_up:   8%|▋        | 1603/20010 [1:19:12<13:51:02,  2.71s/batch]Batch 1600/20010 Done, mean position loss: 31.665429260730743\n",
      "Training growing_up:   8%|▋        | 1604/20010 [1:19:14<14:58:59,  2.93s/batch]Batch 1600/20010 Done, mean position loss: 34.97010004281998\n",
      "Training growing_up:   8%|▋        | 1592/20010 [1:19:17<14:25:14,  2.82s/batch]Batch 1600/20010 Done, mean position loss: 31.634890165328976\n",
      "Training growing_up:   8%|▋        | 1599/20010 [1:19:22<13:55:03,  2.72s/batch]Batch 1600/20010 Done, mean position loss: 33.617590329647065\n",
      "Training growing_up:   8%|▋        | 1605/20010 [1:19:25<14:08:05,  2.76s/batch]Batch 1600/20010 Done, mean position loss: 31.684712555408478\n",
      "Training growing_up:   8%|▋        | 1602/20010 [1:19:27<13:48:56,  2.70s/batch]Batch 1600/20010 Done, mean position loss: 34.8681094455719\n",
      "Training growing_up:   8%|▋        | 1613/20010 [1:19:37<15:47:31,  3.09s/batch]Batch 1600/20010 Done, mean position loss: 31.96793848991394\n",
      "Training growing_up:   8%|▋        | 1646/20010 [1:19:43<13:52:20,  2.72s/batch]Batch 1600/20010 Done, mean position loss: 32.68951798200607\n",
      "Training growing_up:   8%|▋        | 1594/20010 [1:19:44<14:43:29,  2.88s/batch]Batch 1600/20010 Done, mean position loss: 34.28487800598145\n",
      "Training growing_up:   8%|▋        | 1617/20010 [1:19:52<14:20:54,  2.81s/batch]Batch 1600/20010 Done, mean position loss: 33.51801277637482\n",
      "Training growing_up:   8%|▋        | 1617/20010 [1:20:05<14:40:26,  2.87s/batch]Batch 1600/20010 Done, mean position loss: 33.37449505329132\n",
      "Training growing_up:   8%|▋        | 1639/20010 [1:20:06<14:51:02,  2.91s/batch]Batch 1600/20010 Done, mean position loss: 33.70886708259583\n",
      "Training growing_up:   8%|▋        | 1622/20010 [1:20:11<14:36:11,  2.86s/batch]Batch 1600/20010 Done, mean position loss: 32.36445630311966\n",
      "Training growing_up:   8%|▋        | 1658/20010 [1:20:16<14:44:25,  2.89s/batch]Batch 1600/20010 Done, mean position loss: 32.262280254364015\n",
      "Training growing_up:   8%|▋        | 1607/20010 [1:20:23<16:29:12,  3.23s/batch]Batch 1600/20010 Done, mean position loss: 34.42692527532577\n",
      "Training growing_up:   8%|▋        | 1604/20010 [1:20:24<14:26:16,  2.82s/batch]Batch 1600/20010 Done, mean position loss: 33.033023271560666\n",
      "Training growing_up:   8%|▊        | 1681/20010 [1:22:16<13:47:17,  2.71s/batch]Batch 1700/20010 Done, mean position loss: 31.53127436637878\n",
      "Training growing_up:   8%|▋        | 1666/20010 [1:22:16<14:14:18,  2.79s/batch]Batch 1700/20010 Done, mean position loss: 35.18587950468063\n",
      "Training growing_up:   8%|▋        | 1660/20010 [1:22:26<13:26:13,  2.64s/batch]Batch 1700/20010 Done, mean position loss: 33.58560425043106\n",
      "Training growing_up:   9%|▊        | 1708/20010 [1:22:35<13:46:08,  2.71s/batch]Batch 1700/20010 Done, mean position loss: 33.71964706659317\n",
      "Training growing_up:   8%|▊        | 1673/20010 [1:22:37<15:07:08,  2.97s/batch]Batch 1700/20010 Done, mean position loss: 34.647303647994995\n",
      "Training growing_up:   8%|▊        | 1684/20010 [1:22:44<15:26:24,  3.03s/batch]Batch 1700/20010 Done, mean position loss: 33.097971332073215\n",
      "Training growing_up:   8%|▊        | 1679/20010 [1:22:53<14:31:11,  2.85s/batch]Batch 1700/20010 Done, mean position loss: 32.128068377971644\n",
      "Training growing_up:   9%|▊        | 1710/20010 [1:23:01<15:09:13,  2.98s/batch]Batch 1700/20010 Done, mean position loss: 32.522049088478084\n",
      "Training growing_up:   9%|▊        | 1710/20010 [1:23:01<13:45:28,  2.71s/batch]Batch 1700/20010 Done, mean position loss: 31.24211191654205\n",
      "Training growing_up:   8%|▊        | 1695/20010 [1:23:02<13:01:24,  2.56s/batch]Batch 1700/20010 Done, mean position loss: 31.99960635662079\n",
      "Training growing_up:   9%|▊        | 1721/20010 [1:23:12<14:33:54,  2.87s/batch]Batch 1700/20010 Done, mean position loss: 31.73632758140564\n",
      "Training growing_up:   9%|▊        | 1701/20010 [1:23:13<15:14:46,  3.00s/batch]Batch 1700/20010 Done, mean position loss: 33.35722970724106\n",
      "Training growing_up:   8%|▊        | 1678/20010 [1:23:13<13:27:04,  2.64s/batch]Batch 1700/20010 Done, mean position loss: 34.50726506948471\n",
      "Training growing_up:   8%|▊        | 1680/20010 [1:23:19<14:30:15,  2.85s/batch]Batch 1700/20010 Done, mean position loss: 32.635132696628574\n",
      "Training growing_up:   9%|▊        | 1711/20010 [1:23:32<15:05:38,  2.97s/batch]Batch 1700/20010 Done, mean position loss: 33.771628515720366\n",
      "Training growing_up:   8%|▊        | 1670/20010 [1:23:32<15:30:29,  3.04s/batch]Batch 1700/20010 Done, mean position loss: 32.94913572072983\n",
      "Training growing_up:   8%|▊        | 1688/20010 [1:23:40<15:37:47,  3.07s/batch]Batch 1700/20010 Done, mean position loss: 31.565207488536835\n",
      "Training growing_up:   8%|▊        | 1688/20010 [1:23:42<14:27:46,  2.84s/batch]Batch 1700/20010 Done, mean position loss: 32.00683704614639\n",
      "Training growing_up:   9%|▊        | 1735/20010 [1:23:54<13:59:30,  2.76s/batch]Batch 1700/20010 Done, mean position loss: 34.60399209022522\n",
      "Training growing_up:   9%|▊        | 1719/20010 [1:23:55<14:27:42,  2.85s/batch]Batch 1700/20010 Done, mean position loss: 31.164297745227813\n",
      "Training growing_up:   8%|▊        | 1700/20010 [1:23:54<14:36:08,  2.87s/batch]Batch 1700/20010 Done, mean position loss: 32.5570436501503\n",
      "Training growing_up:   9%|▊        | 1719/20010 [1:23:57<15:43:34,  3.10s/batch]Batch 1700/20010 Done, mean position loss: 33.68535228729248\n",
      "Training growing_up:   8%|▊        | 1699/20010 [1:23:57<16:36:45,  3.27s/batch]Batch 1700/20010 Done, mean position loss: 31.678720905780793\n",
      "Batch 1700/20010 Done, mean position loss: 32.6184631228447\n",
      "Training growing_up:   9%|▊        | 1736/20010 [1:23:57<14:50:51,  2.93s/batch]Batch 1700/20010 Done, mean position loss: 33.78678584337234\n",
      "Training growing_up:   8%|▊        | 1674/20010 [1:23:59<16:12:52,  3.18s/batch]Batch 1700/20010 Done, mean position loss: 33.00249379873276\n",
      "Training growing_up:   9%|▊        | 1703/20010 [1:24:03<14:41:48,  2.89s/batch]Batch 1700/20010 Done, mean position loss: 32.80247020244599\n",
      "Training growing_up:   9%|▊        | 1723/20010 [1:24:10<16:09:27,  3.18s/batch]Batch 1700/20010 Done, mean position loss: 31.498142008781432\n",
      "Training growing_up:   9%|▊        | 1743/20010 [1:24:18<17:37:42,  3.47s/batch]Batch 1700/20010 Done, mean position loss: 32.593955504894254\n",
      "Training growing_up:   8%|▊        | 1688/20010 [1:24:19<14:35:32,  2.87s/batch]Batch 1700/20010 Done, mean position loss: 33.58008204460144\n",
      "Training growing_up:   9%|▊        | 1710/20010 [1:24:20<14:04:01,  2.77s/batch]Batch 1700/20010 Done, mean position loss: 31.7815730547905\n",
      "Training growing_up:   8%|▊        | 1698/20010 [1:24:30<16:52:33,  3.32s/batch]Batch 1700/20010 Done, mean position loss: 31.729725306034087\n",
      "Training growing_up:   9%|▊        | 1704/20010 [1:24:39<16:05:22,  3.16s/batch]Batch 1700/20010 Done, mean position loss: 33.03400252103806\n",
      "Training growing_up:   9%|▊        | 1718/20010 [1:24:47<15:12:54,  2.99s/batch]Batch 1700/20010 Done, mean position loss: 33.478350493907925\n",
      "Training growing_up:   8%|▊        | 1693/20010 [1:24:57<16:46:49,  3.30s/batch]Batch 1700/20010 Done, mean position loss: 32.176124765872956\n",
      "Training growing_up:   9%|▊        | 1732/20010 [1:25:04<14:48:46,  2.92s/batch]Batch 1700/20010 Done, mean position loss: 33.36886685371399\n",
      "Training growing_up:   9%|▊        | 1716/20010 [1:25:06<16:22:47,  3.22s/batch]Batch 1700/20010 Done, mean position loss: 32.392923746109005\n",
      "Training growing_up:   8%|▊        | 1698/20010 [1:25:12<14:37:21,  2.87s/batch]Batch 1700/20010 Done, mean position loss: 31.42383342504501\n",
      "Training growing_up:   9%|▊        | 1737/20010 [1:25:21<16:16:03,  3.20s/batch]Batch 1700/20010 Done, mean position loss: 31.377626326084133\n",
      "Training growing_up:   9%|▊        | 1706/20010 [1:25:28<16:21:09,  3.22s/batch]Batch 1700/20010 Done, mean position loss: 32.64083577394486\n",
      "Training growing_up:   9%|▊        | 1764/20010 [1:27:04<15:34:01,  3.07s/batch]Batch 1800/20010 Done, mean position loss: 32.1805588388443\n",
      "Training growing_up:   9%|▊        | 1742/20010 [1:27:06<15:48:35,  3.12s/batch]Batch 1800/20010 Done, mean position loss: 30.79476933717728\n",
      "Training growing_up:   9%|▊        | 1759/20010 [1:27:30<15:09:54,  2.99s/batch]Batch 1800/20010 Done, mean position loss: 32.446129693984986\n",
      "Training growing_up:   9%|▊        | 1777/20010 [1:27:30<13:29:42,  2.66s/batch]Batch 1800/20010 Done, mean position loss: 33.22132267951966\n",
      "Training growing_up:   9%|▊        | 1769/20010 [1:27:32<15:11:13,  3.00s/batch]Batch 1800/20010 Done, mean position loss: 33.24263489484787\n",
      "Training growing_up:   9%|▊        | 1777/20010 [1:27:42<15:23:15,  3.04s/batch]Batch 1800/20010 Done, mean position loss: 31.39190672159195\n",
      "Training growing_up:   9%|▊        | 1757/20010 [1:27:50<14:02:13,  2.77s/batch]Batch 1800/20010 Done, mean position loss: 31.139657063484194\n",
      "Training growing_up:   9%|▊        | 1756/20010 [1:27:52<16:20:43,  3.22s/batch]Batch 1800/20010 Done, mean position loss: 30.384820961952208\n",
      "Training growing_up:   9%|▊        | 1777/20010 [1:27:53<15:50:44,  3.13s/batch]Batch 1800/20010 Done, mean position loss: 30.979307560920716\n",
      "Training growing_up:   9%|▊        | 1753/20010 [1:27:57<16:16:13,  3.21s/batch]Batch 1800/20010 Done, mean position loss: 31.60011766433716\n",
      "Training growing_up:   9%|▊        | 1802/20010 [1:28:00<13:10:55,  2.61s/batch]Batch 1800/20010 Done, mean position loss: 32.18094585418701\n",
      "Training growing_up:   9%|▊        | 1777/20010 [1:28:02<15:08:44,  2.99s/batch]Batch 1800/20010 Done, mean position loss: 32.33026976108551\n",
      "Training growing_up:   9%|▊        | 1785/20010 [1:28:03<14:39:09,  2.89s/batch]Batch 1800/20010 Done, mean position loss: 30.992732799053194\n",
      "Training growing_up:   9%|▊        | 1767/20010 [1:28:14<14:11:09,  2.80s/batch]Batch 1800/20010 Done, mean position loss: 32.06852560758591\n",
      "Training growing_up:   9%|▊        | 1809/20010 [1:28:25<13:57:21,  2.76s/batch]Batch 1800/20010 Done, mean position loss: 31.191587600708004\n",
      "Training growing_up:   9%|▊        | 1779/20010 [1:28:33<15:50:25,  3.13s/batch]Batch 1800/20010 Done, mean position loss: 31.250018041133885\n",
      "Training growing_up:   9%|▊        | 1808/20010 [1:28:34<14:48:45,  2.93s/batch]Batch 1800/20010 Done, mean position loss: 31.40418193101883\n",
      "Training growing_up:   9%|▊        | 1797/20010 [1:28:38<14:07:43,  2.79s/batch]Batch 1800/20010 Done, mean position loss: 31.172704143524168\n",
      "Training growing_up:   9%|▊        | 1781/20010 [1:28:42<17:18:16,  3.42s/batch]Batch 1800/20010 Done, mean position loss: 30.674260568618777\n",
      "Training growing_up:   9%|▊        | 1783/20010 [1:28:47<15:34:11,  3.08s/batch]Batch 1800/20010 Done, mean position loss: 32.48398566246033\n",
      "Training growing_up:   9%|▊        | 1779/20010 [1:28:50<15:00:12,  2.96s/batch]Batch 1800/20010 Done, mean position loss: 34.19574208498001\n",
      "Training growing_up:   9%|▊        | 1801/20010 [1:28:49<14:24:38,  2.85s/batch]Batch 1800/20010 Done, mean position loss: 32.529094257354735\n",
      "Training growing_up:   9%|▊        | 1821/20010 [1:28:51<14:27:40,  2.86s/batch]Batch 1800/20010 Done, mean position loss: 32.325091602802274\n",
      "Training growing_up:   9%|▊        | 1810/20010 [1:28:52<13:49:32,  2.73s/batch]Batch 1800/20010 Done, mean position loss: 30.157002696990965\n",
      "Training growing_up:   9%|▊        | 1820/20010 [1:28:52<14:52:54,  2.95s/batch]Batch 1800/20010 Done, mean position loss: 31.6055594420433\n",
      "Training growing_up:   9%|▊        | 1801/20010 [1:28:52<14:22:00,  2.84s/batch]Batch 1800/20010 Done, mean position loss: 33.492537693977354\n",
      "Training growing_up:   9%|▊        | 1779/20010 [1:29:03<16:16:04,  3.21s/batch]Batch 1800/20010 Done, mean position loss: 31.58206086397171\n",
      "Training growing_up:   9%|▊        | 1806/20010 [1:29:06<15:14:29,  3.01s/batch]Batch 1800/20010 Done, mean position loss: 32.75653425216675\n",
      "Training growing_up:   9%|▊        | 1802/20010 [1:29:08<14:33:24,  2.88s/batch]Batch 1800/20010 Done, mean position loss: 30.904525804519654\n",
      "Training growing_up:   9%|▊        | 1812/20010 [1:29:14<15:09:53,  3.00s/batch]Batch 1800/20010 Done, mean position loss: 30.577398555278776\n",
      "Training growing_up:   9%|▊        | 1816/20010 [1:29:15<14:08:52,  2.80s/batch]Batch 1800/20010 Done, mean position loss: 31.828501226902006\n",
      "Training growing_up:   9%|▊        | 1807/20010 [1:29:33<15:34:13,  3.08s/batch]Batch 1800/20010 Done, mean position loss: 32.47225761890411\n",
      "Training growing_up:   9%|▊        | 1835/20010 [1:29:40<18:24:58,  3.65s/batch]Batch 1800/20010 Done, mean position loss: 32.16550886631012\n",
      "Training growing_up:   9%|▊        | 1801/20010 [1:29:40<16:20:00,  3.23s/batch]Batch 1800/20010 Done, mean position loss: 32.81423758506775\n",
      "Training growing_up:   9%|▊        | 1854/20010 [1:29:59<14:29:16,  2.87s/batch]Batch 1800/20010 Done, mean position loss: 32.63622253417968\n",
      "Training growing_up:   9%|▊        | 1842/20010 [1:30:02<14:12:46,  2.82s/batch]Batch 1800/20010 Done, mean position loss: 31.706618561744687\n",
      "Training growing_up:   9%|▊        | 1823/20010 [1:30:06<15:12:03,  3.01s/batch]Batch 1800/20010 Done, mean position loss: 32.35923294067383\n",
      "Training growing_up:   9%|▊        | 1852/20010 [1:30:11<13:14:29,  2.63s/batch]Batch 1800/20010 Done, mean position loss: 31.51166056394577\n",
      "Training growing_up:   9%|▊        | 1856/20010 [1:30:24<15:25:08,  3.06s/batch]Batch 1800/20010 Done, mean position loss: 30.28415433883667\n",
      "Training growing_up:   9%|▊        | 1837/20010 [1:30:26<14:38:11,  2.90s/batch]Batch 1800/20010 Done, mean position loss: 32.29875694274902\n",
      "Training growing_up:   9%|▊        | 1884/20010 [1:31:53<12:42:40,  2.52s/batch]Batch 1900/20010 Done, mean position loss: 33.90224807500839\n",
      "Training growing_up:   9%|▊        | 1842/20010 [1:32:00<15:02:58,  2.98s/batch]Batch 1900/20010 Done, mean position loss: 32.31199423789978\n",
      "Training growing_up:   9%|▊        | 1891/20010 [1:32:12<12:51:04,  2.55s/batch]Batch 1900/20010 Done, mean position loss: 31.238253262043\n",
      "Training growing_up:  10%|▊        | 1906/20010 [1:32:13<13:31:21,  2.69s/batch]Batch 1900/20010 Done, mean position loss: 30.965527205467225\n",
      "Training growing_up:   9%|▊        | 1877/20010 [1:32:22<13:04:15,  2.60s/batch]Batch 1900/20010 Done, mean position loss: 30.676065230369566\n",
      "Training growing_up:   9%|▊        | 1869/20010 [1:32:27<12:53:24,  2.56s/batch]Batch 1900/20010 Done, mean position loss: 31.10094957113266\n",
      "Training growing_up:   9%|▊        | 1884/20010 [1:32:37<10:55:02,  2.17s/batch]Batch 1900/20010 Done, mean position loss: 31.15130952119827\n",
      "Training growing_up:   9%|▊        | 1882/20010 [1:32:41<14:51:07,  2.95s/batch]Batch 1900/20010 Done, mean position loss: 32.559894440174105\n",
      "Training growing_up:   9%|▊        | 1883/20010 [1:32:43<12:54:46,  2.56s/batch]Batch 1900/20010 Done, mean position loss: 31.04669068336487\n",
      "Training growing_up:  10%|▊        | 1921/20010 [1:32:42<12:03:41,  2.40s/batch]Batch 1900/20010 Done, mean position loss: 33.86789683818817\n",
      "Training growing_up:   9%|▊        | 1860/20010 [1:32:46<15:57:57,  3.17s/batch]Batch 1900/20010 Done, mean position loss: 32.953652043342586\n",
      "Training growing_up:   9%|▊        | 1861/20010 [1:32:48<14:42:55,  2.92s/batch]Batch 1900/20010 Done, mean position loss: 30.59027495384216\n",
      "Training growing_up:   9%|▊        | 1885/20010 [1:32:49<13:44:01,  2.73s/batch]Batch 1900/20010 Done, mean position loss: 32.706802551746364\n",
      "Training growing_up:   9%|▊        | 1893/20010 [1:32:50<15:09:57,  3.01s/batch]Batch 1900/20010 Done, mean position loss: 31.79129273414612\n",
      "Training growing_up:  10%|▊        | 1904/20010 [1:32:54<13:34:36,  2.70s/batch]Batch 1900/20010 Done, mean position loss: 30.704217731952667\n",
      "Training growing_up:  10%|▊        | 1920/20010 [1:33:09<11:28:08,  2.28s/batch]Batch 1900/20010 Done, mean position loss: 31.124732148647308\n",
      "Training growing_up:   9%|▊        | 1876/20010 [1:33:10<13:35:08,  2.70s/batch]Batch 1900/20010 Done, mean position loss: 30.69412373542786\n",
      "Training growing_up:  10%|▊        | 1913/20010 [1:33:20<14:09:39,  2.82s/batch]Batch 1900/20010 Done, mean position loss: 31.29369278907776\n",
      "Training growing_up:   9%|▊        | 1873/20010 [1:33:20<13:46:29,  2.73s/batch]Batch 1900/20010 Done, mean position loss: 30.828805418014525\n",
      "Training growing_up:   9%|▊        | 1889/20010 [1:33:24<15:33:36,  3.09s/batch]Batch 1900/20010 Done, mean position loss: 31.262163026332853\n",
      "Training growing_up:  10%|▊        | 1927/20010 [1:33:29<14:42:17,  2.93s/batch]Batch 1900/20010 Done, mean position loss: 31.645981500148775\n",
      "Training growing_up:   9%|▊        | 1900/20010 [1:33:31<13:04:46,  2.60s/batch]Batch 1900/20010 Done, mean position loss: 31.308821823596954\n",
      "Training growing_up:  10%|▊        | 1901/20010 [1:33:31<10:38:15,  2.11s/batch]Batch 1900/20010 Done, mean position loss: 31.786650564670563\n",
      "Training growing_up:  10%|▊        | 1906/20010 [1:33:31<12:09:54,  2.42s/batch]Batch 1900/20010 Done, mean position loss: 35.7267396903038\n",
      "Training growing_up:  10%|▊        | 1907/20010 [1:33:33<12:14:38,  2.43s/batch]Batch 1900/20010 Done, mean position loss: 31.010146889686585\n",
      "Training growing_up:  10%|▊        | 1903/20010 [1:33:34<12:06:17,  2.41s/batch]Batch 1900/20010 Done, mean position loss: 31.371693139076232\n",
      "Training growing_up:   9%|▊        | 1897/20010 [1:33:37<13:12:56,  2.63s/batch]Batch 1900/20010 Done, mean position loss: 30.991185913085936\n",
      "Training growing_up:   9%|▊        | 1887/20010 [1:33:41<13:30:09,  2.68s/batch]Batch 1900/20010 Done, mean position loss: 32.424232089519506\n",
      "Training growing_up:  10%|▊        | 1906/20010 [1:33:49<13:05:06,  2.60s/batch]Batch 1900/20010 Done, mean position loss: 30.68537875890732\n",
      "Training growing_up:  10%|▊        | 1931/20010 [1:33:55<11:06:15,  2.21s/batch]Batch 1900/20010 Done, mean position loss: 30.733827443122863\n",
      "Training growing_up:  10%|▊        | 1927/20010 [1:33:55<13:28:31,  2.68s/batch]Batch 1900/20010 Done, mean position loss: 31.944579477310185\n",
      "Training growing_up:   9%|▊        | 1881/20010 [1:34:00<12:42:08,  2.52s/batch]Batch 1900/20010 Done, mean position loss: 34.17656225681305\n",
      "Training growing_up:  10%|▊        | 1916/20010 [1:34:15<14:47:52,  2.94s/batch]Batch 1900/20010 Done, mean position loss: 32.70675331115723\n",
      "Training growing_up:   9%|▊        | 1895/20010 [1:34:18<12:41:30,  2.52s/batch]Batch 1900/20010 Done, mean position loss: 30.511464335918426\n",
      "Training growing_up:   9%|▊        | 1900/20010 [1:34:31<13:45:45,  2.74s/batch]Batch 1900/20010 Done, mean position loss: 32.20120944261551\n",
      "Training growing_up:  10%|▊        | 1929/20010 [1:34:33<14:20:55,  2.86s/batch]Batch 1900/20010 Done, mean position loss: 32.08970478773117\n",
      "Training growing_up:  10%|▊        | 1922/20010 [1:34:35<14:05:29,  2.80s/batch]Batch 1900/20010 Done, mean position loss: 31.195004394054415\n",
      "Training growing_up:  10%|▊        | 1941/20010 [1:34:55<15:05:49,  3.01s/batch]Batch 1900/20010 Done, mean position loss: 31.32456251859665\n",
      "Training growing_up:  10%|▊        | 1924/20010 [1:34:56<14:06:29,  2.81s/batch]Batch 1900/20010 Done, mean position loss: 31.294781408309934\n",
      "Training growing_up:  10%|▉        | 1951/20010 [1:35:00<13:38:05,  2.72s/batch]Batch 1900/20010 Done, mean position loss: 32.622395203113555\n",
      "Training growing_up:  10%|▉        | 1963/20010 [1:36:20<14:00:26,  2.79s/batch]Batch 2000/20010 Done, mean position loss: 32.424247810840605\n",
      "Training growing_up:  10%|▊        | 1932/20010 [1:36:25<14:01:47,  2.79s/batch]Batch 2000/20010 Done, mean position loss: 33.54337729215622\n",
      "Training growing_up:  10%|▉        | 1951/20010 [1:36:41<15:10:24,  3.02s/batch]Batch 2000/20010 Done, mean position loss: 31.27058552026749\n",
      "Training growing_up:  10%|▉        | 1946/20010 [1:36:46<15:32:08,  3.10s/batch]Batch 2000/20010 Done, mean position loss: 31.352474615573882\n",
      "Training growing_up:  10%|▉        | 1970/20010 [1:36:55<16:00:09,  3.19s/batch]Batch 2000/20010 Done, mean position loss: 32.448531799316406\n",
      "Training growing_up:  10%|▉        | 1956/20010 [1:36:54<13:44:18,  2.74s/batch]Batch 2000/20010 Done, mean position loss: 29.325764374732973\n",
      "Training growing_up:  10%|▉        | 1972/20010 [1:37:11<12:58:31,  2.59s/batch]Batch 2000/20010 Done, mean position loss: 31.464274382591245\n",
      "Training growing_up:  10%|▉        | 1997/20010 [1:37:11<15:51:47,  3.17s/batch]Batch 2000/20010 Done, mean position loss: 30.89112483739853\n",
      "Training growing_up:  10%|▉        | 1982/20010 [1:37:18<14:31:29,  2.90s/batch]Batch 2000/20010 Done, mean position loss: 31.455990467071533\n",
      "Training growing_up:  10%|▉        | 1980/20010 [1:37:23<14:08:31,  2.82s/batch]Batch 2000/20010 Done, mean position loss: 31.849838132858277\n",
      "Training growing_up:  10%|▉        | 2003/20010 [1:37:23<13:37:57,  2.73s/batch]Batch 2000/20010 Done, mean position loss: 31.411751582622525\n",
      "Training growing_up:  10%|▉        | 1981/20010 [1:37:23<14:53:56,  2.98s/batch]Batch 2000/20010 Done, mean position loss: 30.07261388540268\n",
      "Training growing_up:  10%|▉        | 1994/20010 [1:37:25<13:49:36,  2.76s/batch]Batch 2000/20010 Done, mean position loss: 31.632847340106963\n",
      "Training growing_up:  10%|▉        | 1992/20010 [1:37:27<14:50:13,  2.96s/batch]Batch 2000/20010 Done, mean position loss: 31.782265298366546\n",
      "Training growing_up:  10%|▉        | 1991/20010 [1:37:32<13:10:38,  2.63s/batch]Batch 2000/20010 Done, mean position loss: 30.706020720005036\n",
      "Training growing_up:  10%|▉        | 1957/20010 [1:37:44<14:17:37,  2.85s/batch]Batch 2000/20010 Done, mean position loss: 33.456759157180784\n",
      "Training growing_up:  10%|▉        | 1976/20010 [1:37:52<13:16:34,  2.65s/batch]Batch 2000/20010 Done, mean position loss: 32.33488928794861\n",
      "Training growing_up:  10%|▉        | 1997/20010 [1:38:02<14:24:15,  2.88s/batch]Batch 2000/20010 Done, mean position loss: 31.671937658786774\n",
      "Batch 2000/20010 Done, mean position loss: 30.825859982967376\n",
      "Training growing_up:  10%|▉        | 2009/20010 [1:38:08<15:54:00,  3.18s/batch]Batch 2000/20010 Done, mean position loss: 31.007654452323912\n",
      "Training growing_up:  10%|▉        | 2018/20010 [1:38:08<16:09:49,  3.23s/batch]Batch 2000/20010 Done, mean position loss: 31.25646502494812\n",
      "Training growing_up:  10%|▉        | 2029/20010 [1:38:14<13:26:20,  2.69s/batch]Batch 2000/20010 Done, mean position loss: 33.23224026679992\n",
      "Training growing_up:  10%|▉        | 2000/20010 [1:38:14<13:50:53,  2.77s/batch]Batch 2000/20010 Done, mean position loss: 35.11901540994644\n",
      "Training growing_up:  10%|▉        | 2018/20010 [1:38:15<13:26:54,  2.69s/batch]Batch 2000/20010 Done, mean position loss: 31.22147107601166\n",
      "Training growing_up:  10%|▉        | 2004/20010 [1:38:16<13:58:45,  2.79s/batch]Batch 2000/20010 Done, mean position loss: 31.729642078876495\n",
      "Training growing_up:  10%|▉        | 1979/20010 [1:38:21<14:22:42,  2.87s/batch]Batch 2000/20010 Done, mean position loss: 32.837127528190614\n",
      "Training growing_up:  10%|▉        | 1972/20010 [1:38:23<15:55:56,  3.18s/batch]Batch 2000/20010 Done, mean position loss: 31.478134007453917\n",
      "Training growing_up:  10%|▉        | 2005/20010 [1:38:25<14:25:16,  2.88s/batch]Batch 2000/20010 Done, mean position loss: 33.809379484653476\n",
      "Training growing_up:  10%|▉        | 2034/20010 [1:38:29<15:26:55,  3.09s/batch]Batch 2000/20010 Done, mean position loss: 29.669772968292236\n",
      "Training growing_up:  10%|▉        | 1984/20010 [1:38:37<16:02:01,  3.20s/batch]Batch 2000/20010 Done, mean position loss: 31.24850400209427\n",
      "Training growing_up:  10%|▉        | 2037/20010 [1:38:39<15:47:26,  3.16s/batch]Batch 2000/20010 Done, mean position loss: 30.138927845954896\n",
      "Training growing_up:  10%|▉        | 2021/20010 [1:38:47<12:49:49,  2.57s/batch]Batch 2000/20010 Done, mean position loss: 33.227821810245516\n",
      "Training growing_up:  10%|▉        | 2048/20010 [1:39:07<13:27:52,  2.70s/batch]Batch 2000/20010 Done, mean position loss: 31.614124233722684\n",
      "Training growing_up:  10%|▉        | 1993/20010 [1:39:08<14:11:16,  2.83s/batch]Batch 2000/20010 Done, mean position loss: 33.21508040904999\n",
      "Training growing_up:  10%|▉        | 2007/20010 [1:39:23<13:04:30,  2.61s/batch]Batch 2000/20010 Done, mean position loss: 30.8742285656929\n",
      "Training growing_up:  10%|▉        | 2066/20010 [1:39:25<15:15:18,  3.06s/batch]Batch 2000/20010 Done, mean position loss: 30.843781385421757\n",
      "Training growing_up:  10%|▉        | 2025/20010 [1:39:31<16:18:16,  3.26s/batch]Batch 2000/20010 Done, mean position loss: 32.638872458934785\n",
      "Training growing_up:  10%|▉        | 1998/20010 [1:39:49<16:45:27,  3.35s/batch]Batch 2000/20010 Done, mean position loss: 30.50685682535171\n",
      "Training growing_up:  10%|▉        | 2057/20010 [1:39:54<14:49:33,  2.97s/batch]Batch 2000/20010 Done, mean position loss: 32.07377067565918\n",
      "Training growing_up:  10%|▉        | 2053/20010 [1:39:56<16:34:27,  3.32s/batch]Batch 2000/20010 Done, mean position loss: 35.729188313484194\n",
      "Training growing_up:  10%|▉        | 2028/20010 [1:41:08<13:20:22,  2.67s/batch]Batch 2100/20010 Done, mean position loss: 30.434129600524905\n",
      "Training growing_up:  10%|▉        | 2029/20010 [1:41:12<14:57:29,  2.99s/batch]Batch 2100/20010 Done, mean position loss: 30.931322288513186\n",
      "Training growing_up:  10%|▉        | 2067/20010 [1:41:30<13:53:32,  2.79s/batch]Batch 2100/20010 Done, mean position loss: 31.1504931139946\n",
      "Training growing_up:  10%|▉        | 2067/20010 [1:41:34<15:37:24,  3.13s/batch]Batch 2100/20010 Done, mean position loss: 31.466190581321715\n",
      "Training growing_up:  10%|▉        | 2090/20010 [1:41:43<15:03:47,  3.03s/batch]Batch 2100/20010 Done, mean position loss: 31.60930181980133\n",
      "Training growing_up:  10%|▉        | 2042/20010 [1:41:49<14:25:10,  2.89s/batch]Batch 2100/20010 Done, mean position loss: 30.919840915203096\n",
      "Training growing_up:  10%|▉        | 2080/20010 [1:42:02<15:53:30,  3.19s/batch]Batch 2100/20010 Done, mean position loss: 30.113488290309903\n",
      "Training growing_up:  11%|▉        | 2121/20010 [1:42:05<13:49:41,  2.78s/batch]Batch 2100/20010 Done, mean position loss: 29.312653136253356\n",
      "Training growing_up:  10%|▉        | 2076/20010 [1:42:09<13:17:46,  2.67s/batch]Batch 2100/20010 Done, mean position loss: 31.899725618362428\n",
      "Training growing_up:  11%|▉        | 2114/20010 [1:42:10<12:24:32,  2.50s/batch]Batch 2100/20010 Done, mean position loss: 30.04917515039444\n",
      "Training growing_up:  10%|▉        | 2054/20010 [1:42:10<13:52:52,  2.78s/batch]Batch 2100/20010 Done, mean position loss: 31.755296123027804\n",
      "Training growing_up:  10%|▉        | 2060/20010 [1:42:16<15:59:53,  3.21s/batch]Batch 2100/20010 Done, mean position loss: 31.106708941459658\n",
      "Training growing_up:  10%|▉        | 2075/20010 [1:42:18<14:06:32,  2.83s/batch]Batch 2100/20010 Done, mean position loss: 31.550549051761628\n",
      "Training growing_up:  10%|▉        | 2085/20010 [1:42:21<16:00:38,  3.22s/batch]Batch 2100/20010 Done, mean position loss: 30.877682139873507\n",
      "Training growing_up:  10%|▉        | 2060/20010 [1:42:27<13:25:36,  2.69s/batch]Batch 2100/20010 Done, mean position loss: 31.654663169384\n",
      "Training growing_up:  10%|▉        | 2064/20010 [1:42:37<15:20:34,  3.08s/batch]Batch 2100/20010 Done, mean position loss: 30.762568335533146\n",
      "Training growing_up:  10%|▉        | 2089/20010 [1:42:39<15:37:52,  3.14s/batch]Batch 2100/20010 Done, mean position loss: 30.55075616121292\n",
      "Training growing_up:  11%|▉        | 2122/20010 [1:42:50<15:46:47,  3.18s/batch]Batch 2100/20010 Done, mean position loss: 31.05764492034912\n",
      "Training growing_up:  10%|▉        | 2083/20010 [1:42:53<13:52:41,  2.79s/batch]Batch 2100/20010 Done, mean position loss: 29.481636159420013\n",
      "Training growing_up:  11%|▉        | 2113/20010 [1:43:02<15:52:53,  3.19s/batch]Batch 2100/20010 Done, mean position loss: 30.78447808265686\n",
      "Training growing_up:  10%|▉        | 2099/20010 [1:43:02<14:19:11,  2.88s/batch]Batch 2100/20010 Done, mean position loss: 30.190163218975066\n",
      "Training growing_up:  10%|▉        | 2101/20010 [1:43:03<14:37:08,  2.94s/batch]Batch 2100/20010 Done, mean position loss: 30.693200414180758\n",
      "Training growing_up:  10%|▉        | 2065/20010 [1:43:10<13:56:48,  2.80s/batch]Batch 2100/20010 Done, mean position loss: 34.160782754421234\n",
      "Training growing_up:  10%|▉        | 2093/20010 [1:43:13<14:35:37,  2.93s/batch]Batch 2100/20010 Done, mean position loss: 31.528921484947205\n",
      "Training growing_up:  11%|▉        | 2105/20010 [1:43:14<15:06:29,  3.04s/batch]Batch 2100/20010 Done, mean position loss: 31.878170633316046\n",
      "Batch 2100/20010 Done, mean position loss: 31.00054316997528\n",
      "Training growing_up:  11%|▉        | 2136/20010 [1:43:17<16:50:23,  3.39s/batch]Batch 2100/20010 Done, mean position loss: 30.19265253305435\n",
      "Training growing_up:  10%|▉        | 2092/20010 [1:43:19<14:46:47,  2.97s/batch]Batch 2100/20010 Done, mean position loss: 32.6045503282547\n",
      "Training growing_up:  11%|▉        | 2123/20010 [1:43:24<14:42:28,  2.96s/batch]Batch 2100/20010 Done, mean position loss: 30.82994616985321\n",
      "Training growing_up:  10%|▉        | 2091/20010 [1:43:37<15:10:24,  3.05s/batch]Batch 2100/20010 Done, mean position loss: 29.87312096595764\n",
      "Training growing_up:  10%|▉        | 2101/20010 [1:43:37<16:30:27,  3.32s/batch]Batch 2100/20010 Done, mean position loss: 32.66042557477952\n",
      "Training growing_up:  11%|▉        | 2141/20010 [1:43:45<13:37:46,  2.75s/batch]Batch 2100/20010 Done, mean position loss: 33.26172961950302\n",
      "Training growing_up:  11%|▉        | 2136/20010 [1:44:04<13:47:03,  2.78s/batch]Batch 2100/20010 Done, mean position loss: 29.894787604808805\n",
      "Training growing_up:  11%|▉        | 2122/20010 [1:44:04<12:42:15,  2.56s/batch]Batch 2100/20010 Done, mean position loss: 30.610194482803344\n",
      "Training growing_up:  11%|▉        | 2128/20010 [1:44:21<12:49:35,  2.58s/batch]Batch 2100/20010 Done, mean position loss: 32.26815186262131\n",
      "Training growing_up:  11%|▉        | 2134/20010 [1:44:26<13:26:50,  2.71s/batch]Batch 2100/20010 Done, mean position loss: 29.655100963115693\n",
      "Training growing_up:  11%|▉        | 2135/20010 [1:44:31<14:50:15,  2.99s/batch]Batch 2100/20010 Done, mean position loss: 31.101079378128052\n",
      "Training growing_up:  11%|▉        | 2149/20010 [1:44:53<15:55:33,  3.21s/batch]Batch 2100/20010 Done, mean position loss: 31.28108142852783\n",
      "Training growing_up:  11%|▉        | 2104/20010 [1:45:01<13:59:04,  2.81s/batch]Batch 2100/20010 Done, mean position loss: 32.31140995979309\n",
      "Training growing_up:  11%|▉        | 2114/20010 [1:45:01<13:44:17,  2.76s/batch]Batch 2100/20010 Done, mean position loss: 32.032377972602845\n",
      "Training growing_up:  11%|▉        | 2155/20010 [1:46:11<21:57:43,  4.43s/batch]Batch 2200/20010 Done, mean position loss: 31.01805708885193\n",
      "Training growing_up:  11%|▉        | 2166/20010 [1:46:20<20:24:39,  4.12s/batch]Batch 2200/20010 Done, mean position loss: 30.508132479190827\n",
      "Training growing_up:  11%|▉        | 2182/20010 [1:46:37<17:22:52,  3.51s/batch]Batch 2200/20010 Done, mean position loss: 30.86872350931168\n",
      "Training growing_up:  11%|▉        | 2151/20010 [1:46:52<15:38:13,  3.15s/batch]Batch 2200/20010 Done, mean position loss: 30.79631329774857\n",
      "Training growing_up:  11%|▉        | 2143/20010 [1:46:53<17:12:19,  3.47s/batch]Batch 2200/20010 Done, mean position loss: 30.642633872032164\n",
      "Training growing_up:  11%|▉        | 2194/20010 [1:46:59<15:53:38,  3.21s/batch]Batch 2200/20010 Done, mean position loss: 31.125321671962737\n",
      "Training growing_up:  11%|▉        | 2166/20010 [1:47:10<17:20:33,  3.50s/batch]Batch 2200/20010 Done, mean position loss: 30.558813955783844\n",
      "Training growing_up:  11%|▉        | 2213/20010 [1:47:16<16:09:42,  3.27s/batch]Batch 2200/20010 Done, mean position loss: 29.83024093866348\n",
      "Training growing_up:  11%|▉        | 2193/20010 [1:47:21<16:34:38,  3.35s/batch]Batch 2200/20010 Done, mean position loss: 30.964533655643464\n",
      "Training growing_up:  11%|▉        | 2187/20010 [1:47:26<17:11:35,  3.47s/batch]Batch 2200/20010 Done, mean position loss: 30.174053783416745\n",
      "Training growing_up:  11%|▉        | 2142/20010 [1:47:28<16:59:49,  3.42s/batch]Batch 2200/20010 Done, mean position loss: 29.94605591058731\n",
      "Training growing_up:  11%|▉        | 2155/20010 [1:47:35<20:57:38,  4.23s/batch]Batch 2200/20010 Done, mean position loss: 30.623314661979677\n",
      "Training growing_up:  11%|▉        | 2182/20010 [1:47:40<18:35:10,  3.75s/batch]Batch 2200/20010 Done, mean position loss: 31.8647301530838\n",
      "Training growing_up:  11%|▉        | 2184/20010 [1:47:47<14:48:11,  2.99s/batch]Batch 2200/20010 Done, mean position loss: 31.05610817670822\n",
      "Training growing_up:  11%|▉        | 2217/20010 [1:47:50<16:47:48,  3.40s/batch]Batch 2200/20010 Done, mean position loss: 29.691126229763032\n",
      "Training growing_up:  11%|▉        | 2191/20010 [1:47:51<14:26:42,  2.92s/batch]Batch 2200/20010 Done, mean position loss: 30.430656514167786\n",
      "Training growing_up:  11%|▉        | 2162/20010 [1:48:01<14:16:17,  2.88s/batch]Batch 2200/20010 Done, mean position loss: 29.91462317943573\n",
      "Training growing_up:  11%|▉        | 2198/20010 [1:48:04<14:02:22,  2.84s/batch]Batch 2200/20010 Done, mean position loss: 30.5573016500473\n",
      "Training growing_up:  11%|▉        | 2166/20010 [1:48:12<13:43:43,  2.77s/batch]Batch 2200/20010 Done, mean position loss: 30.13487407207489\n",
      "Training growing_up:  11%|▉        | 2209/20010 [1:48:14<13:33:11,  2.74s/batch]Batch 2200/20010 Done, mean position loss: 30.61284277677536\n",
      "Training growing_up:  11%|▉        | 2212/20010 [1:48:19<12:47:32,  2.59s/batch]Batch 2200/20010 Done, mean position loss: 31.2300514626503\n",
      "Training growing_up:  11%|▉        | 2205/20010 [1:48:21<13:37:51,  2.76s/batch]Batch 2200/20010 Done, mean position loss: 28.85526345729828\n",
      "Training growing_up:  11%|▉        | 2196/20010 [1:48:23<13:27:52,  2.72s/batch]Batch 2200/20010 Done, mean position loss: 30.569005877971648\n",
      "Training growing_up:  11%|▉        | 2202/20010 [1:48:24<11:50:47,  2.39s/batch]Batch 2200/20010 Done, mean position loss: 32.245144667625425\n",
      "Training growing_up:  11%|█        | 2232/20010 [1:48:24<12:57:39,  2.62s/batch]Batch 2200/20010 Done, mean position loss: 29.851787235736847\n",
      "Training growing_up:  11%|▉        | 2193/20010 [1:48:31<13:29:36,  2.73s/batch]Batch 2200/20010 Done, mean position loss: 30.98328268766403\n",
      "Training growing_up:  11%|█        | 2235/20010 [1:48:34<17:39:26,  3.58s/batch]Batch 2200/20010 Done, mean position loss: 30.336288049221036\n",
      "Training growing_up:  11%|▉        | 2206/20010 [1:48:40<17:11:40,  3.48s/batch]Batch 2200/20010 Done, mean position loss: 30.70276087999344\n",
      "Training growing_up:  11%|█        | 2230/20010 [1:48:42<16:41:31,  3.38s/batch]Batch 2200/20010 Done, mean position loss: 29.571717286109923\n",
      "Training growing_up:  11%|█        | 2229/20010 [1:49:05<18:25:53,  3.73s/batch]Batch 2200/20010 Done, mean position loss: 31.31560555934906\n",
      "Training growing_up:  11%|▉        | 2210/20010 [1:49:14<15:48:55,  3.20s/batch]Batch 2200/20010 Done, mean position loss: 30.218307955265047\n",
      "Training growing_up:  11%|█        | 2230/20010 [1:49:20<19:16:25,  3.90s/batch]Batch 2200/20010 Done, mean position loss: 30.619108440876005\n",
      "Training growing_up:  11%|▉        | 2218/20010 [1:49:41<20:31:10,  4.15s/batch]Batch 2200/20010 Done, mean position loss: 30.86845153093338\n",
      "Training growing_up:  11%|█        | 2237/20010 [1:49:48<18:52:36,  3.82s/batch]Batch 2200/20010 Done, mean position loss: 29.794676692485808\n",
      "Training growing_up:  11%|▉        | 2200/20010 [1:50:07<17:30:09,  3.54s/batch]Batch 2200/20010 Done, mean position loss: 29.182192623615265\n",
      "Training growing_up:  11%|█        | 2233/20010 [1:50:10<14:39:58,  2.97s/batch]Batch 2200/20010 Done, mean position loss: 31.233188586235045\n",
      "Training growing_up:  11%|▉        | 2190/20010 [1:50:17<24:41:52,  4.99s/batch]Batch 2200/20010 Done, mean position loss: 31.439107978343966\n",
      "Training growing_up:  11%|█        | 2237/20010 [1:50:35<18:10:35,  3.68s/batch]Batch 2200/20010 Done, mean position loss: 30.564798169136047\n",
      "Training growing_up:  11%|█        | 2242/20010 [1:50:58<21:27:20,  4.35s/batch]Batch 2200/20010 Done, mean position loss: 32.511075389385226\n",
      "Training growing_up:  11%|█        | 2259/20010 [1:51:04<17:37:54,  3.58s/batch]Batch 2200/20010 Done, mean position loss: 32.96639592409134\n",
      "Training growing_up:  11%|█        | 2261/20010 [1:51:52<13:48:24,  2.80s/batch]Batch 2300/20010 Done, mean position loss: 31.200521173477174\n",
      "Training growing_up:  11%|█        | 2229/20010 [1:51:54<14:54:30,  3.02s/batch]Batch 2300/20010 Done, mean position loss: 30.59633070707321\n",
      "Training growing_up:  11%|█        | 2234/20010 [1:52:09<15:10:21,  3.07s/batch]Batch 2300/20010 Done, mean position loss: 31.329511337280273\n",
      "Training growing_up:  11%|█        | 2294/20010 [1:52:19<11:57:18,  2.43s/batch]Batch 2300/20010 Done, mean position loss: 30.227470800876617\n",
      "Training growing_up:  11%|█        | 2250/20010 [1:52:26<15:18:34,  3.10s/batch]Batch 2300/20010 Done, mean position loss: 31.640798063278197\n",
      "Training growing_up:  11%|█        | 2278/20010 [1:52:27<13:50:44,  2.81s/batch]Batch 2300/20010 Done, mean position loss: 30.44256051540375\n",
      "Training growing_up:  11%|█        | 2240/20010 [1:52:39<14:01:35,  2.84s/batch]Batch 2300/20010 Done, mean position loss: 29.876041350364687\n",
      "Training growing_up:  11%|█        | 2245/20010 [1:52:42<15:46:35,  3.20s/batch]Batch 2300/20010 Done, mean position loss: 29.959498867988586\n",
      "Training growing_up:  11%|█        | 2246/20010 [1:52:45<15:45:21,  3.19s/batch]Batch 2300/20010 Done, mean position loss: 29.68262058734894\n",
      "Training growing_up:  11%|█        | 2267/20010 [1:52:51<14:06:12,  2.86s/batch]Batch 2300/20010 Done, mean position loss: 32.8770510673523\n",
      "Training growing_up:  12%|█        | 2314/20010 [1:52:54<13:11:16,  2.68s/batch]Batch 2300/20010 Done, mean position loss: 31.118180668354036\n",
      "Training growing_up:  11%|█        | 2287/20010 [1:53:04<14:14:05,  2.89s/batch]Batch 2300/20010 Done, mean position loss: 30.087051935195923\n",
      "Training growing_up:  11%|█        | 2297/20010 [1:53:05<13:45:00,  2.79s/batch]Batch 2300/20010 Done, mean position loss: 30.945353033542634\n",
      "Training growing_up:  12%|█        | 2323/20010 [1:53:11<13:38:32,  2.78s/batch]Batch 2300/20010 Done, mean position loss: 30.14823781490326\n",
      "Training growing_up:  12%|█        | 2318/20010 [1:53:15<12:44:03,  2.59s/batch]Batch 2300/20010 Done, mean position loss: 29.360664675235746\n",
      "Training growing_up:  11%|█        | 2265/20010 [1:53:15<14:48:16,  3.00s/batch]Batch 2300/20010 Done, mean position loss: 30.70334498643875\n",
      "Training growing_up:  12%|█        | 2308/20010 [1:53:34<13:51:53,  2.82s/batch]Batch 2300/20010 Done, mean position loss: 29.862261044979096\n",
      "Training growing_up:  12%|█        | 2328/20010 [1:53:35<14:08:37,  2.88s/batch]Batch 2300/20010 Done, mean position loss: 30.57638818025589\n",
      "Training growing_up:  11%|█        | 2296/20010 [1:53:36<15:16:30,  3.10s/batch]Batch 2300/20010 Done, mean position loss: 30.620096974372863\n",
      "Training growing_up:  11%|█        | 2276/20010 [1:53:44<15:20:10,  3.11s/batch]Batch 2300/20010 Done, mean position loss: 30.7612664103508\n",
      "Training growing_up:  12%|█        | 2330/20010 [1:53:44<12:57:31,  2.64s/batch]Batch 2300/20010 Done, mean position loss: 31.271434285640716\n",
      "Training growing_up:  11%|█        | 2289/20010 [1:53:49<13:20:50,  2.71s/batch]Batch 2300/20010 Done, mean position loss: 29.240836622714998\n",
      "Training growing_up:  11%|█        | 2265/20010 [1:53:51<14:09:24,  2.87s/batch]Batch 2300/20010 Done, mean position loss: 32.04494107484818\n",
      "Training growing_up:  12%|█        | 2307/20010 [1:53:53<15:09:43,  3.08s/batch]Batch 2300/20010 Done, mean position loss: 30.311265497207643\n",
      "Training growing_up:  12%|█        | 2303/20010 [1:53:56<15:48:23,  3.21s/batch]Batch 2300/20010 Done, mean position loss: 28.17077656030655\n",
      "Training growing_up:  12%|█        | 2320/20010 [1:54:05<20:26:42,  4.16s/batch]Batch 2300/20010 Done, mean position loss: 31.712487959861754\n",
      "Training growing_up:  12%|█        | 2349/20010 [1:54:13<19:42:04,  4.02s/batch]Batch 2300/20010 Done, mean position loss: 30.672795729637144\n",
      "Training growing_up:  12%|█        | 2306/20010 [1:54:13<20:10:47,  4.10s/batch]Batch 2300/20010 Done, mean position loss: 32.28812533378601\n",
      "Training growing_up:  12%|█        | 2348/20010 [1:54:15<19:08:58,  3.90s/batch]Batch 2300/20010 Done, mean position loss: 30.557204236984255\n",
      "Training growing_up:  12%|█        | 2308/20010 [1:54:37<18:07:18,  3.69s/batch]Batch 2300/20010 Done, mean position loss: 31.236916704177858\n",
      "Training growing_up:  12%|█        | 2327/20010 [1:54:47<20:34:19,  4.19s/batch]Batch 2300/20010 Done, mean position loss: 30.351153650283813\n",
      "Training growing_up:  12%|█        | 2315/20010 [1:55:00<20:27:42,  4.16s/batch]Batch 2300/20010 Done, mean position loss: 30.404008827209474\n",
      "Training growing_up:  12%|█        | 2337/20010 [1:55:25<20:22:42,  4.15s/batch]Batch 2300/20010 Done, mean position loss: 29.966832954883575\n",
      "Training growing_up:  12%|█        | 2341/20010 [1:55:31<17:55:18,  3.65s/batch]Batch 2300/20010 Done, mean position loss: 31.130991408824922\n",
      "Training growing_up:  12%|█        | 2326/20010 [1:55:52<17:08:10,  3.49s/batch]Batch 2300/20010 Done, mean position loss: 32.055456659793855\n",
      "Training growing_up:  12%|█        | 2319/20010 [1:55:57<19:28:56,  3.96s/batch]Batch 2300/20010 Done, mean position loss: 31.928634796142575\n",
      "Training growing_up:  12%|█        | 2368/20010 [1:55:59<19:42:57,  4.02s/batch]Batch 2300/20010 Done, mean position loss: 30.302082622051238\n",
      "Training growing_up:  12%|█        | 2352/20010 [1:56:18<14:38:16,  2.98s/batch]Batch 2300/20010 Done, mean position loss: 31.43806258201599\n",
      "Training growing_up:  12%|█        | 2312/20010 [1:56:33<16:02:33,  3.26s/batch]Batch 2300/20010 Done, mean position loss: 32.205358541011805\n",
      "Training growing_up:  12%|█        | 2343/20010 [1:56:39<14:24:01,  2.93s/batch]Batch 2300/20010 Done, mean position loss: 31.883136076927187\n",
      "Training growing_up:  12%|█        | 2392/20010 [1:57:19<15:33:20,  3.18s/batch]Batch 2400/20010 Done, mean position loss: 30.04623574256897\n",
      "Training growing_up:  12%|█        | 2327/20010 [1:57:24<17:01:59,  3.47s/batch]Batch 2400/20010 Done, mean position loss: 31.84302921772003\n",
      "Training growing_up:  12%|█        | 2372/20010 [1:57:45<15:22:10,  3.14s/batch]Batch 2400/20010 Done, mean position loss: 30.4671146941185\n",
      "Training growing_up:  12%|█        | 2410/20010 [1:57:48<16:13:22,  3.32s/batch]Batch 2400/20010 Done, mean position loss: 29.705686292648316\n",
      "Batch 2400/20010 Done, mean position loss: 30.966591079235076\n",
      "Training growing_up:  12%|█        | 2369/20010 [1:57:57<15:19:19,  3.13s/batch]Batch 2400/20010 Done, mean position loss: 31.05146305799484\n",
      "Training growing_up:  12%|█        | 2352/20010 [1:58:11<14:24:45,  2.94s/batch]Batch 2400/20010 Done, mean position loss: 28.823039891719816\n",
      "Training growing_up:  12%|█        | 2361/20010 [1:58:19<14:36:22,  2.98s/batch]Batch 2400/20010 Done, mean position loss: 29.418095586299895\n",
      "Training growing_up:  12%|█        | 2347/20010 [1:58:22<15:55:31,  3.25s/batch]Batch 2400/20010 Done, mean position loss: 30.948710100650786\n",
      "Training growing_up:  12%|█        | 2378/20010 [1:58:23<14:36:33,  2.98s/batch]Batch 2400/20010 Done, mean position loss: 29.82617987155914\n",
      "Training growing_up:  12%|█        | 2349/20010 [1:58:28<15:42:34,  3.20s/batch]Batch 2400/20010 Done, mean position loss: 29.874551799297333\n",
      "Training growing_up:  12%|█        | 2387/20010 [1:58:38<15:51:50,  3.24s/batch]Batch 2400/20010 Done, mean position loss: 32.58426889181137\n",
      "Training growing_up:  12%|█        | 2426/20010 [1:58:40<14:54:01,  3.05s/batch]Batch 2400/20010 Done, mean position loss: 31.431758885383605\n",
      "Training growing_up:  12%|█        | 2429/20010 [1:58:49<13:59:46,  2.87s/batch]Batch 2400/20010 Done, mean position loss: 29.27822555065155\n",
      "Training growing_up:  12%|█        | 2421/20010 [1:58:49<16:03:30,  3.29s/batch]Batch 2400/20010 Done, mean position loss: 30.378023879528044\n",
      "Batch 2400/20010 Done, mean position loss: 31.988823964595795\n",
      "Training growing_up:  12%|█        | 2429/20010 [1:59:11<13:49:22,  2.83s/batch]Batch 2400/20010 Done, mean position loss: 31.69917488336563\n",
      "Training growing_up:  12%|█        | 2362/20010 [1:59:12<14:23:26,  2.94s/batch]Batch 2400/20010 Done, mean position loss: 29.5271457695961\n",
      "Training growing_up:  12%|█        | 2409/20010 [1:59:13<15:34:04,  3.18s/batch]Batch 2400/20010 Done, mean position loss: 29.360679800510407\n",
      "Training growing_up:  12%|█        | 2428/20010 [1:59:16<14:09:17,  2.90s/batch]Batch 2400/20010 Done, mean position loss: 32.739793176651006\n",
      "Training growing_up:  12%|█        | 2421/20010 [1:59:21<15:03:12,  3.08s/batch]Batch 2400/20010 Done, mean position loss: 30.278325550556183\n",
      "Training growing_up:  12%|█        | 2405/20010 [1:59:22<13:48:02,  2.82s/batch]Batch 2400/20010 Done, mean position loss: 29.769728870391845\n",
      "Training growing_up:  12%|█        | 2397/20010 [1:59:32<13:32:56,  2.77s/batch]Batch 2400/20010 Done, mean position loss: 28.610700521469116\n",
      "Training growing_up:  12%|█        | 2430/20010 [1:59:32<12:29:49,  2.56s/batch]Batch 2400/20010 Done, mean position loss: 29.97562718868256\n",
      "Training growing_up:  12%|█        | 2418/20010 [1:59:34<14:19:19,  2.93s/batch]Batch 2400/20010 Done, mean position loss: 31.68835065603256\n",
      "Training growing_up:  12%|█        | 2389/20010 [1:59:37<13:29:51,  2.76s/batch]Batch 2400/20010 Done, mean position loss: 30.184454538822173\n",
      "Training growing_up:  12%|█        | 2368/20010 [1:59:44<13:29:18,  2.75s/batch]Batch 2400/20010 Done, mean position loss: 30.56151879310608\n",
      "Training growing_up:  12%|█        | 2373/20010 [1:59:45<14:48:58,  3.02s/batch]Batch 2400/20010 Done, mean position loss: 31.734292125701906\n",
      "Training growing_up:  12%|█        | 2425/20010 [1:59:49<14:38:34,  3.00s/batch]Batch 2400/20010 Done, mean position loss: 31.374042053222656\n",
      "Training growing_up:  12%|█        | 2403/20010 [1:59:55<14:27:06,  2.95s/batch]Batch 2400/20010 Done, mean position loss: 30.749716796875\n",
      "Training growing_up:  12%|█        | 2460/20010 [2:00:08<13:51:54,  2.84s/batch]Batch 2400/20010 Done, mean position loss: 29.06361783742905\n",
      "Training growing_up:  12%|█        | 2430/20010 [2:00:14<15:03:47,  3.08s/batch]Batch 2400/20010 Done, mean position loss: 30.775725874900818\n",
      "Training growing_up:  12%|█        | 2448/20010 [2:00:39<14:02:54,  2.88s/batch]Batch 2400/20010 Done, mean position loss: 31.398373763561246\n",
      "Training growing_up:  12%|█        | 2430/20010 [2:00:42<15:36:01,  3.19s/batch]Batch 2400/20010 Done, mean position loss: 30.524583013057708\n",
      "Training growing_up:  12%|█        | 2426/20010 [2:01:04<16:06:09,  3.30s/batch]Batch 2400/20010 Done, mean position loss: 31.78087593317032\n",
      "Training growing_up:  12%|█        | 2388/20010 [2:01:04<13:03:05,  2.67s/batch]Batch 2400/20010 Done, mean position loss: 29.10155854701996\n",
      "Training growing_up:  12%|█        | 2449/20010 [2:01:06<14:08:26,  2.90s/batch]Batch 2400/20010 Done, mean position loss: 30.812241749763487\n",
      "Training growing_up:  12%|█        | 2447/20010 [2:01:22<13:24:58,  2.75s/batch]Batch 2400/20010 Done, mean position loss: 31.378482174873355\n",
      "Training growing_up:  12%|█        | 2398/20010 [2:01:36<15:23:20,  3.15s/batch]Batch 2400/20010 Done, mean position loss: 32.160914156436924\n",
      "Training growing_up:  12%|█        | 2446/20010 [2:01:46<11:53:28,  2.44s/batch]Batch 2400/20010 Done, mean position loss: 31.2241792678833\n",
      "Training growing_up:  12%|█        | 2457/20010 [2:02:08<14:09:24,  2.90s/batch]Batch 2500/20010 Done, mean position loss: 31.597283446788786\n",
      "Training growing_up:  12%|█        | 2490/20010 [2:02:19<15:10:03,  3.12s/batch]Batch 2500/20010 Done, mean position loss: 31.566486325263977\n",
      "Training growing_up:  12%|█        | 2487/20010 [2:02:39<14:24:16,  2.96s/batch]Batch 2500/20010 Done, mean position loss: 28.974765672683716\n",
      "Training growing_up:  12%|█        | 2432/20010 [2:02:40<13:31:45,  2.77s/batch]Batch 2500/20010 Done, mean position loss: 29.55896453380585\n",
      "Training growing_up:  12%|█        | 2458/20010 [2:02:46<14:33:22,  2.99s/batch]Batch 2500/20010 Done, mean position loss: 38.0578778386116\n",
      "Training growing_up:  12%|█        | 2488/20010 [2:02:53<13:55:35,  2.86s/batch]Batch 2500/20010 Done, mean position loss: 30.457687215805052\n",
      "Training growing_up:  12%|█        | 2465/20010 [2:03:07<15:44:01,  3.23s/batch]Batch 2500/20010 Done, mean position loss: 28.725602166652678\n",
      "Training growing_up:  12%|█        | 2482/20010 [2:03:12<12:50:55,  2.64s/batch]Batch 2500/20010 Done, mean position loss: 30.548493967056274\n",
      "Training growing_up:  12%|█        | 2453/20010 [2:03:20<15:19:17,  3.14s/batch]Batch 2500/20010 Done, mean position loss: 29.295775523185732\n",
      "Training growing_up:  12%|█        | 2501/20010 [2:03:21<13:52:18,  2.85s/batch]Batch 2500/20010 Done, mean position loss: 29.63099742412567\n",
      "Training growing_up:  12%|█        | 2471/20010 [2:03:26<15:09:40,  3.11s/batch]Batch 2500/20010 Done, mean position loss: 30.121672334671022\n",
      "Training growing_up:  13%|█▏       | 2514/20010 [2:03:30<14:08:10,  2.91s/batch]Batch 2500/20010 Done, mean position loss: 31.900927319526673\n",
      "Training growing_up:  12%|█        | 2484/20010 [2:03:39<11:28:48,  2.36s/batch]Batch 2500/20010 Done, mean position loss: 28.902578554153443\n",
      "Training growing_up:  13%|█▏       | 2509/20010 [2:03:41<12:04:04,  2.48s/batch]Batch 2500/20010 Done, mean position loss: 31.04550048351288\n",
      "Training growing_up:  12%|█        | 2484/20010 [2:03:41<14:56:33,  3.07s/batch]Batch 2500/20010 Done, mean position loss: 30.693870589733123\n",
      "Training growing_up:  13%|█▏       | 2510/20010 [2:03:43<11:51:39,  2.44s/batch]Batch 2500/20010 Done, mean position loss: 29.749801487922667\n",
      "Training growing_up:  13%|█▏       | 2527/20010 [2:03:53<11:31:49,  2.37s/batch]Batch 2500/20010 Done, mean position loss: 29.427821621894836\n",
      "Training growing_up:  13%|█▏       | 2510/20010 [2:04:04<14:41:41,  3.02s/batch]Batch 2500/20010 Done, mean position loss: 29.51427935600281\n",
      "Training growing_up:  13%|█▏       | 2518/20010 [2:04:06<15:54:38,  3.27s/batch]Batch 2500/20010 Done, mean position loss: 29.233601500988005\n",
      "Training growing_up:  12%|█        | 2470/20010 [2:04:06<14:47:12,  3.03s/batch]Batch 2500/20010 Done, mean position loss: 28.26914143562317\n",
      "Training growing_up:  13%|█▏       | 2535/20010 [2:04:09<12:33:27,  2.59s/batch]Batch 2500/20010 Done, mean position loss: 32.967841711044315\n",
      "Training growing_up:  13%|█▏       | 2504/20010 [2:04:12<12:08:23,  2.50s/batch]Batch 2500/20010 Done, mean position loss: 29.715122916698455\n",
      "Training growing_up:  13%|█▏       | 2514/20010 [2:04:17<15:01:28,  3.09s/batch]Batch 2500/20010 Done, mean position loss: 30.332677490711212\n",
      "Training growing_up:  13%|█▏       | 2506/20010 [2:04:19<13:27:11,  2.77s/batch]Batch 2500/20010 Done, mean position loss: 30.944636907577515\n",
      "Training growing_up:  13%|█▏       | 2518/20010 [2:04:28<13:26:26,  2.77s/batch]Batch 2500/20010 Done, mean position loss: 30.850681955814363\n",
      "Training growing_up:  12%|█        | 2495/20010 [2:04:31<11:02:10,  2.27s/batch]Batch 2500/20010 Done, mean position loss: 30.537753877639773\n",
      "Training growing_up:  13%|█▏       | 2542/20010 [2:04:38<13:09:36,  2.71s/batch]Batch 2500/20010 Done, mean position loss: 29.884850738048556\n",
      "Training growing_up:  13%|█▏       | 2533/20010 [2:04:45<12:46:18,  2.63s/batch]Batch 2500/20010 Done, mean position loss: 29.039209997653963\n",
      "Training growing_up:  13%|█▏       | 2509/20010 [2:04:47<12:33:51,  2.58s/batch]Batch 2500/20010 Done, mean position loss: 29.24737781524658\n",
      "Training growing_up:  13%|█▏       | 2519/20010 [2:04:48<11:15:27,  2.32s/batch]Batch 2500/20010 Done, mean position loss: 30.012498593330385\n",
      "Training growing_up:  13%|█▏       | 2508/20010 [2:04:55<12:20:43,  2.54s/batch]Batch 2500/20010 Done, mean position loss: 29.523217418193816\n",
      "Training growing_up:  13%|█▏       | 2505/20010 [2:04:56<13:51:59,  2.85s/batch]Batch 2500/20010 Done, mean position loss: 30.656465408802035\n",
      "Training growing_up:  13%|█▏       | 2558/20010 [2:05:20<12:24:25,  2.56s/batch]Batch 2500/20010 Done, mean position loss: 30.423681850433347\n",
      "Training growing_up:  13%|█▏       | 2517/20010 [2:05:26<11:16:11,  2.32s/batch]Batch 2500/20010 Done, mean position loss: 28.242435166835783\n",
      "Training growing_up:  13%|█▏       | 2521/20010 [2:05:47<14:16:05,  2.94s/batch]Batch 2500/20010 Done, mean position loss: 29.078507242202758\n",
      "Training growing_up:  13%|█▏       | 2522/20010 [2:05:48<12:17:37,  2.53s/batch]Batch 2500/20010 Done, mean position loss: 29.99864671468735\n",
      "Training growing_up:  13%|█▏       | 2570/20010 [2:05:49<12:04:09,  2.49s/batch]Batch 2500/20010 Done, mean position loss: 30.764704425334934\n",
      "Training growing_up:  13%|█▏       | 2541/20010 [2:06:04<11:33:20,  2.38s/batch]Batch 2500/20010 Done, mean position loss: 30.04154030799866\n",
      "Training growing_up:  13%|█▏       | 2510/20010 [2:06:11<11:54:24,  2.45s/batch]Batch 2500/20010 Done, mean position loss: 31.121503508090974\n",
      "Training growing_up:  13%|█▏       | 2570/20010 [2:06:28<14:53:00,  3.07s/batch]Batch 2500/20010 Done, mean position loss: 29.54735136270523\n",
      "Training growing_up:  13%|█▏       | 2566/20010 [2:06:37<11:57:07,  2.47s/batch]Batch 2600/20010 Done, mean position loss: 31.208031868934633\n",
      "Training growing_up:  13%|█▏       | 2519/20010 [2:06:55<14:47:19,  3.04s/batch]Batch 2600/20010 Done, mean position loss: 30.387790966033936\n",
      "Training growing_up:  13%|█▏       | 2581/20010 [2:07:12<13:40:08,  2.82s/batch]Batch 2600/20010 Done, mean position loss: 28.659514265060427\n",
      "Training growing_up:  13%|█▏       | 2586/20010 [2:07:14<13:52:15,  2.87s/batch]Batch 2600/20010 Done, mean position loss: 29.472149889469147\n",
      "Training growing_up:  13%|█▏       | 2586/20010 [2:07:19<13:31:05,  2.79s/batch]Batch 2600/20010 Done, mean position loss: 32.37711572885513\n",
      "Training growing_up:  13%|█▏       | 2584/20010 [2:07:26<13:18:42,  2.75s/batch]Batch 2600/20010 Done, mean position loss: 29.602392468452454\n",
      "Training growing_up:  13%|█▏       | 2588/20010 [2:07:37<14:17:44,  2.95s/batch]Batch 2600/20010 Done, mean position loss: 28.499529013633726\n",
      "Training growing_up:  13%|█▏       | 2590/20010 [2:07:43<13:51:03,  2.86s/batch]Batch 2600/20010 Done, mean position loss: 30.60449683427811\n",
      "Training growing_up:  13%|█▏       | 2593/20010 [2:07:46<12:29:39,  2.58s/batch]Batch 2600/20010 Done, mean position loss: 30.2702916431427\n",
      "Training growing_up:  13%|█▏       | 2594/20010 [2:07:51<14:43:01,  3.04s/batch]Batch 2600/20010 Done, mean position loss: 29.236008989810944\n",
      "Training growing_up:  13%|█▏       | 2545/20010 [2:07:55<15:16:47,  3.15s/batch]Batch 2600/20010 Done, mean position loss: 28.273858861923216\n",
      "Training growing_up:  13%|█▏       | 2542/20010 [2:08:02<14:08:49,  2.92s/batch]Batch 2600/20010 Done, mean position loss: 31.493962478637698\n",
      "Training growing_up:  13%|█▏       | 2577/20010 [2:08:11<13:42:48,  2.83s/batch]Batch 2600/20010 Done, mean position loss: 29.14667623758316\n",
      "Training growing_up:  13%|█▏       | 2540/20010 [2:08:12<15:57:33,  3.29s/batch]Batch 2600/20010 Done, mean position loss: 30.54561867713928\n",
      "Training growing_up:  13%|█▏       | 2564/20010 [2:08:13<13:40:26,  2.82s/batch]Batch 2600/20010 Done, mean position loss: 29.514450824260713\n",
      "Training growing_up:  13%|█▏       | 2637/20010 [2:08:14<14:08:34,  2.93s/batch]Batch 2600/20010 Done, mean position loss: 29.379370548725127\n",
      "Training growing_up:  13%|█▏       | 2591/20010 [2:08:20<13:37:57,  2.82s/batch]Batch 2600/20010 Done, mean position loss: 30.22931763648987\n",
      "Training growing_up:  13%|█▏       | 2559/20010 [2:08:37<15:08:04,  3.12s/batch]Batch 2600/20010 Done, mean position loss: 28.937628791332244\n",
      "Training growing_up:  13%|█▏       | 2579/20010 [2:08:38<15:04:11,  3.11s/batch]Batch 2600/20010 Done, mean position loss: 29.374554417133332\n",
      "Training growing_up:  13%|█▏       | 2626/20010 [2:08:44<11:18:04,  2.34s/batch]Batch 2600/20010 Done, mean position loss: 29.97142208099365\n",
      "Training growing_up:  13%|█▏       | 2634/20010 [2:08:48<14:21:19,  2.97s/batch]Batch 2600/20010 Done, mean position loss: 29.460815510749818\n",
      "Batch 2600/20010 Done, mean position loss: 29.026158401966093\n",
      "Training growing_up:  13%|█▏       | 2591/20010 [2:08:59<14:15:53,  2.95s/batch]Batch 2600/20010 Done, mean position loss: 28.075620255470273\n",
      "Batch 2600/20010 Done, mean position loss: 29.614626965522767\n",
      "Training growing_up:  13%|█▏       | 2603/20010 [2:09:04<13:51:36,  2.87s/batch]Batch 2600/20010 Done, mean position loss: 31.14601249933243\n",
      "Training growing_up:  13%|█▏       | 2569/20010 [2:09:11<16:31:13,  3.41s/batch]Batch 2600/20010 Done, mean position loss: 30.52056636333466\n",
      "Training growing_up:  13%|█▏       | 2609/20010 [2:09:24<15:18:02,  3.17s/batch]Batch 2600/20010 Done, mean position loss: 29.441572260856628\n",
      "Training growing_up:  13%|█▏       | 2635/20010 [2:09:28<13:20:03,  2.76s/batch]Batch 2600/20010 Done, mean position loss: 30.003830194473267\n",
      "Training growing_up:  13%|█▏       | 2638/20010 [2:09:28<14:15:06,  2.95s/batch]Batch 2600/20010 Done, mean position loss: 31.333371891975403\n",
      "Training growing_up:  13%|█▏       | 2615/20010 [2:09:29<14:37:56,  3.03s/batch]Batch 2600/20010 Done, mean position loss: 31.419424130916596\n",
      "Training growing_up:  13%|█▏       | 2616/20010 [2:09:32<13:58:52,  2.89s/batch]Batch 2600/20010 Done, mean position loss: 31.456808886528016\n",
      "Training growing_up:  13%|█▏       | 2614/20010 [2:09:45<15:49:09,  3.27s/batch]Batch 2600/20010 Done, mean position loss: 29.487043101787567\n",
      "Training growing_up:  13%|█▏       | 2661/20010 [2:10:02<12:29:49,  2.59s/batch]Batch 2600/20010 Done, mean position loss: 30.01288586139679\n",
      "Training growing_up:  13%|█▏       | 2587/20010 [2:10:16<14:38:45,  3.03s/batch]Batch 2600/20010 Done, mean position loss: 28.198395688533783\n",
      "Training growing_up:  13%|█▏       | 2652/20010 [2:10:37<14:10:48,  2.94s/batch]Batch 2600/20010 Done, mean position loss: 32.718788726329805\n",
      "Training growing_up:  13%|█▏       | 2661/20010 [2:10:41<15:03:38,  3.13s/batch]Batch 2600/20010 Done, mean position loss: 28.390200622081757\n",
      "Training growing_up:  13%|█▏       | 2612/20010 [2:10:49<15:47:15,  3.27s/batch]Batch 2600/20010 Done, mean position loss: 29.58009269475937\n",
      "Training growing_up:  13%|█▏       | 2641/20010 [2:10:57<14:14:52,  2.95s/batch]Batch 2600/20010 Done, mean position loss: 29.637165472507476\n",
      "Training growing_up:  13%|█▏       | 2638/20010 [2:11:15<14:59:35,  3.11s/batch]Batch 2600/20010 Done, mean position loss: 29.94354275226593\n",
      "Training growing_up:  13%|█▏       | 2662/20010 [2:11:19<14:35:04,  3.03s/batch]Batch 2700/20010 Done, mean position loss: 29.389182283878323\n",
      "Training growing_up:  13%|█▏       | 2635/20010 [2:11:25<13:51:42,  2.87s/batch]Batch 2600/20010 Done, mean position loss: 31.173974454402924\n",
      "Training growing_up:  13%|█▏       | 2634/20010 [2:11:35<14:09:53,  2.93s/batch]Batch 2700/20010 Done, mean position loss: 29.51035770177841\n",
      "Training growing_up:  13%|█▏       | 2615/20010 [2:11:58<14:32:01,  3.01s/batch]Batch 2700/20010 Done, mean position loss: 30.0929221200943\n",
      "Training growing_up:  14%|█▏       | 2702/20010 [2:12:00<11:51:06,  2.47s/batch]Batch 2700/20010 Done, mean position loss: 30.776827194690703\n",
      "Training growing_up:  13%|█▏       | 2655/20010 [2:12:09<13:50:01,  2.87s/batch]Batch 2700/20010 Done, mean position loss: 30.359775171279907\n",
      "Training growing_up:  13%|█▏       | 2683/20010 [2:12:15<14:21:01,  2.98s/batch]Batch 2700/20010 Done, mean position loss: 29.96765543460846\n",
      "Training growing_up:  13%|█▏       | 2675/20010 [2:12:19<14:54:36,  3.10s/batch]Batch 2700/20010 Done, mean position loss: 31.041638753414155\n",
      "Training growing_up:  13%|█▏       | 2668/20010 [2:12:30<14:55:40,  3.10s/batch]Batch 2700/20010 Done, mean position loss: 28.395559937953948\n",
      "Training growing_up:  14%|█▏       | 2705/20010 [2:12:40<12:22:29,  2.57s/batch]Batch 2700/20010 Done, mean position loss: 30.445680997371674\n",
      "Training growing_up:  14%|█▏       | 2715/20010 [2:12:41<16:19:02,  3.40s/batch]Batch 2700/20010 Done, mean position loss: 28.4389075756073\n",
      "Training growing_up:  13%|█▏       | 2690/20010 [2:12:43<15:19:47,  3.19s/batch]Batch 2700/20010 Done, mean position loss: 30.521422212123873\n",
      "Training growing_up:  13%|█▏       | 2662/20010 [2:12:54<13:39:50,  2.84s/batch]Batch 2700/20010 Done, mean position loss: 30.370172660350796\n",
      "Training growing_up:  14%|█▏       | 2736/20010 [2:13:01<12:44:36,  2.66s/batch]Batch 2700/20010 Done, mean position loss: 29.76143963813782\n",
      "Training growing_up:  13%|█▏       | 2673/20010 [2:13:03<13:54:54,  2.89s/batch]Batch 2700/20010 Done, mean position loss: 29.542044806480405\n",
      "Training growing_up:  13%|█▏       | 2657/20010 [2:13:07<16:32:26,  3.43s/batch]Batch 2700/20010 Done, mean position loss: 29.436784796714782\n",
      "Training growing_up:  14%|█▏       | 2725/20010 [2:13:10<14:43:58,  3.07s/batch]Batch 2700/20010 Done, mean position loss: 29.635009984970093\n",
      "Training growing_up:  13%|█▏       | 2654/20010 [2:13:15<13:28:07,  2.79s/batch]Batch 2700/20010 Done, mean position loss: 30.289580461978915\n",
      "Training growing_up:  14%|█▏       | 2712/20010 [2:13:33<13:21:07,  2.78s/batch]Batch 2700/20010 Done, mean position loss: 30.342647509574892\n",
      "Training growing_up:  14%|█▏       | 2710/20010 [2:13:37<15:52:31,  3.30s/batch]Batch 2700/20010 Done, mean position loss: 29.028366088867188\n",
      "Training growing_up:  13%|█▏       | 2664/20010 [2:13:46<14:53:57,  3.09s/batch]Batch 2700/20010 Done, mean position loss: 30.291728065013885\n",
      "Training growing_up:  14%|█▏       | 2752/20010 [2:13:47<14:11:43,  2.96s/batch]Batch 2700/20010 Done, mean position loss: 28.89470616579056\n",
      "Training growing_up:  14%|█▏       | 2737/20010 [2:13:48<15:20:48,  3.20s/batch]Batch 2700/20010 Done, mean position loss: 28.787788901329037\n",
      "Training growing_up:  13%|█▏       | 2700/20010 [2:14:05<15:41:20,  3.26s/batch]Batch 2700/20010 Done, mean position loss: 30.274877779483795\n",
      "Training growing_up:  14%|█▏       | 2723/20010 [2:14:06<14:01:44,  2.92s/batch]Batch 2700/20010 Done, mean position loss: 29.136624019145966\n",
      "Training growing_up:  13%|█▏       | 2700/20010 [2:14:08<13:55:31,  2.90s/batch]Batch 2700/20010 Done, mean position loss: 31.560356881618496\n",
      "Training growing_up:  13%|█▏       | 2695/20010 [2:14:11<13:38:28,  2.84s/batch]Batch 2700/20010 Done, mean position loss: 29.411945548057552\n",
      "Training growing_up:  13%|█▏       | 2699/20010 [2:14:23<13:32:42,  2.82s/batch]Batch 2700/20010 Done, mean position loss: 29.616615085601808\n",
      "Training growing_up:  13%|█▏       | 2660/20010 [2:14:24<12:33:26,  2.61s/batch]Batch 2700/20010 Done, mean position loss: 30.993485827445983\n",
      "Training growing_up:  14%|█▏       | 2706/20010 [2:14:25<14:10:34,  2.95s/batch]Batch 2700/20010 Done, mean position loss: 29.978516631126404\n",
      "Training growing_up:  14%|█▏       | 2709/20010 [2:14:29<13:37:49,  2.84s/batch]Batch 2700/20010 Done, mean position loss: 29.794214096069336\n",
      "Training growing_up:  13%|█▏       | 2694/20010 [2:14:37<13:47:17,  2.87s/batch]Batch 2700/20010 Done, mean position loss: 30.168819134235385\n",
      "Training growing_up:  14%|█▏       | 2774/20010 [2:14:48<13:51:51,  2.90s/batch]Batch 2700/20010 Done, mean position loss: 27.929244301319123\n",
      "Training growing_up:  14%|█▏       | 2751/20010 [2:14:57<14:24:27,  3.01s/batch]Batch 2700/20010 Done, mean position loss: 29.114350254535672\n",
      "Training growing_up:  14%|█▏       | 2755/20010 [2:15:16<14:49:58,  3.09s/batch]Batch 2700/20010 Done, mean position loss: 29.917224938869477\n",
      "Training growing_up:  13%|█▏       | 2692/20010 [2:15:38<14:48:37,  3.08s/batch]Batch 2700/20010 Done, mean position loss: 30.216728143692016\n",
      "Training growing_up:  14%|█▎       | 2791/20010 [2:15:38<14:13:08,  2.97s/batch]Batch 2700/20010 Done, mean position loss: 29.113745725154878\n",
      "Training growing_up:  14%|█▏       | 2704/20010 [2:15:48<14:35:10,  3.03s/batch]Batch 2700/20010 Done, mean position loss: 29.706698923110963\n",
      "Training growing_up:  14%|█▏       | 2777/20010 [2:16:05<14:29:24,  3.03s/batch]Batch 2700/20010 Done, mean position loss: 29.38886472940445\n",
      "Training growing_up:  14%|█▏       | 2735/20010 [2:16:07<13:24:30,  2.79s/batch]Batch 2800/20010 Done, mean position loss: 29.676671578884125\n",
      "Training growing_up:  14%|█▏       | 2755/20010 [2:16:15<14:37:35,  3.05s/batch]Batch 2700/20010 Done, mean position loss: 31.49845330715179\n",
      "Training growing_up:  14%|█▏       | 2758/20010 [2:16:23<13:42:55,  2.86s/batch]Batch 2800/20010 Done, mean position loss: 29.514185037612915\n",
      "Training growing_up:  14%|█▏       | 2759/20010 [2:16:26<14:30:01,  3.03s/batch]Batch 2700/20010 Done, mean position loss: 29.781478254795076\n",
      "Training growing_up:  14%|█▎       | 2795/20010 [2:16:51<13:26:04,  2.81s/batch]Batch 2800/20010 Done, mean position loss: 28.149697115421297\n",
      "Training growing_up:  14%|█▎       | 2801/20010 [2:16:51<14:40:36,  3.07s/batch]Batch 2800/20010 Done, mean position loss: 31.932614216804506\n",
      "Training growing_up:  14%|█▏       | 2771/20010 [2:16:58<13:11:58,  2.76s/batch]Batch 2800/20010 Done, mean position loss: 30.632056453227996\n",
      "Training growing_up:  14%|█▎       | 2799/20010 [2:17:07<13:44:40,  2.87s/batch]Batch 2800/20010 Done, mean position loss: 28.802931041717528\n",
      "Training growing_up:  14%|█▎       | 2809/20010 [2:17:13<13:53:18,  2.91s/batch]Batch 2800/20010 Done, mean position loss: 28.35651192188263\n",
      "Training growing_up:  14%|█▏       | 2724/20010 [2:17:23<13:02:30,  2.72s/batch]Batch 2800/20010 Done, mean position loss: 28.562225837707523\n",
      "Training growing_up:  14%|█▏       | 2772/20010 [2:17:34<15:31:08,  3.24s/batch]Batch 2800/20010 Done, mean position loss: 29.780558900833128\n",
      "Training growing_up:  14%|█▏       | 2747/20010 [2:17:38<14:52:11,  3.10s/batch]Batch 2800/20010 Done, mean position loss: 30.308801448345186\n",
      "Training growing_up:  14%|█▎       | 2792/20010 [2:17:39<14:54:35,  3.12s/batch]Batch 2800/20010 Done, mean position loss: 29.00548084259033\n",
      "Training growing_up:  14%|█▏       | 2769/20010 [2:17:49<12:58:44,  2.71s/batch]Batch 2800/20010 Done, mean position loss: 29.920845296382904\n",
      "Training growing_up:  14%|█▎       | 2794/20010 [2:17:51<12:40:43,  2.65s/batch]Batch 2800/20010 Done, mean position loss: 30.40421145439148\n",
      "Training growing_up:  14%|█▎       | 2833/20010 [2:17:54<13:09:07,  2.76s/batch]Batch 2800/20010 Done, mean position loss: 29.514553148746494\n",
      "Training growing_up:  14%|█▎       | 2823/20010 [2:17:58<14:20:49,  3.01s/batch]Batch 2800/20010 Done, mean position loss: 29.44597456455231\n",
      "Training growing_up:  14%|█▏       | 2741/20010 [2:18:06<15:26:22,  3.22s/batch]Batch 2800/20010 Done, mean position loss: 33.612252480983734\n",
      "Training growing_up:  14%|█▎       | 2803/20010 [2:18:12<15:01:17,  3.14s/batch]Batch 2800/20010 Done, mean position loss: 33.39920277357101\n",
      "Training growing_up:  14%|█▎       | 2798/20010 [2:18:26<12:47:19,  2.67s/batch]Batch 2800/20010 Done, mean position loss: 30.394596421718596\n",
      "Training growing_up:  14%|█▎       | 2785/20010 [2:18:31<13:50:05,  2.89s/batch]Batch 2800/20010 Done, mean position loss: 28.881846718788147\n",
      "Training growing_up:  14%|█▎       | 2800/20010 [2:18:32<13:53:12,  2.90s/batch]Batch 2800/20010 Done, mean position loss: 30.05627333641052\n",
      "Training growing_up:  14%|█▎       | 2787/20010 [2:18:37<13:45:47,  2.88s/batch]Batch 2800/20010 Done, mean position loss: 29.128110458850863\n",
      "Training growing_up:  14%|█▎       | 2838/20010 [2:18:37<13:33:05,  2.84s/batch]Batch 2800/20010 Done, mean position loss: 28.031962060928343\n",
      "Training growing_up:  14%|█▏       | 2772/20010 [2:18:53<14:39:09,  3.06s/batch]Batch 2800/20010 Done, mean position loss: 30.303954474925995\n",
      "Training growing_up:  14%|█▎       | 2855/20010 [2:19:01<14:26:20,  3.03s/batch]Batch 2800/20010 Done, mean position loss: 28.62302145242691\n",
      "Training growing_up:  14%|█▎       | 2845/20010 [2:19:03<13:00:41,  2.73s/batch]Batch 2800/20010 Done, mean position loss: 30.27614156007767\n",
      "Training growing_up:  14%|█▎       | 2805/20010 [2:19:05<14:22:54,  3.01s/batch]Batch 2800/20010 Done, mean position loss: 28.64137749671936\n",
      "Training growing_up:  14%|█▏       | 2773/20010 [2:19:11<12:39:24,  2.64s/batch]Batch 2800/20010 Done, mean position loss: 29.48974870681763\n",
      "Training growing_up:  14%|█▎       | 2815/20010 [2:19:17<14:16:26,  2.99s/batch]Batch 2800/20010 Done, mean position loss: 29.884323279857636\n",
      "Training growing_up:  14%|█▏       | 2776/20010 [2:19:19<12:32:01,  2.62s/batch]Batch 2800/20010 Done, mean position loss: 29.666363356113433\n",
      "Training growing_up:  14%|█▏       | 2767/20010 [2:19:21<14:44:56,  3.08s/batch]Batch 2800/20010 Done, mean position loss: 29.97823433637619\n",
      "Training growing_up:  14%|█▏       | 2768/20010 [2:19:28<11:52:32,  2.48s/batch]Batch 2800/20010 Done, mean position loss: 29.049191370010377\n",
      "Training growing_up:  14%|█▎       | 2808/20010 [2:19:39<14:30:13,  3.04s/batch]Batch 2800/20010 Done, mean position loss: 29.32887404203415\n",
      "Training growing_up:  14%|█▎       | 2876/20010 [2:19:43<12:38:49,  2.66s/batch]Batch 2800/20010 Done, mean position loss: 29.583289959430694\n",
      "Training growing_up:  14%|█▎       | 2876/20010 [2:20:19<13:41:47,  2.88s/batch]Batch 2800/20010 Done, mean position loss: 29.41857197999954\n",
      "Training growing_up:  14%|█▎       | 2886/20010 [2:20:26<12:18:00,  2.59s/batch]Batch 2800/20010 Done, mean position loss: 28.65013091802597\n",
      "Training growing_up:  14%|█▎       | 2879/20010 [2:20:28<13:38:53,  2.87s/batch]Batch 2800/20010 Done, mean position loss: 31.62401638507843\n",
      "Training growing_up:  14%|█▎       | 2797/20010 [2:20:41<12:17:15,  2.57s/batch]Batch 2800/20010 Done, mean position loss: 30.196308641433713\n",
      "Training growing_up:  14%|█▎       | 2838/20010 [2:20:47<13:30:07,  2.83s/batch]Batch 2900/20010 Done, mean position loss: 29.349729914665225\n",
      "Training growing_up:  14%|█▎       | 2868/20010 [2:20:51<11:40:09,  2.45s/batch]Batch 2800/20010 Done, mean position loss: 30.09500039815903\n",
      "Training growing_up:  14%|█▎       | 2842/20010 [2:20:57<12:54:29,  2.71s/batch]Batch 2800/20010 Done, mean position loss: 31.14514600038528\n",
      "Training growing_up:  14%|█▎       | 2877/20010 [2:21:05<11:07:33,  2.34s/batch]Batch 2900/20010 Done, mean position loss: 30.276599225997924\n",
      "Training growing_up:  14%|█▎       | 2813/20010 [2:21:12<13:06:46,  2.75s/batch]Batch 2800/20010 Done, mean position loss: 30.43846147298813\n",
      "Training growing_up:  14%|█▎       | 2875/20010 [2:21:25<11:01:13,  2.32s/batch]Batch 2900/20010 Done, mean position loss: 28.12134903907776\n",
      "Batch 2900/20010 Done, mean position loss: 30.14513594150543\n",
      "Training growing_up:  15%|█▎       | 2903/20010 [2:21:29<10:41:00,  2.25s/batch]Batch 2900/20010 Done, mean position loss: 30.537296698093414\n",
      "Training growing_up:  14%|█▎       | 2846/20010 [2:21:42<12:48:24,  2.69s/batch]Batch 2900/20010 Done, mean position loss: 29.022082827091218\n",
      "Training growing_up:  15%|█▎       | 2916/20010 [2:21:45<12:32:41,  2.64s/batch]Batch 2900/20010 Done, mean position loss: 30.039276249408722\n",
      "Training growing_up:  15%|█▎       | 2912/20010 [2:21:53<15:08:56,  3.19s/batch]Batch 2900/20010 Done, mean position loss: 29.685085818767547\n",
      "Training growing_up:  15%|█▎       | 2917/20010 [2:22:07<13:26:38,  2.83s/batch]Batch 2900/20010 Done, mean position loss: 30.13427860498428\n",
      "Training growing_up:  14%|█▎       | 2870/20010 [2:22:12<13:02:21,  2.74s/batch]Batch 2900/20010 Done, mean position loss: 29.02308403491974\n",
      "Training growing_up:  14%|█▎       | 2898/20010 [2:22:14<14:07:30,  2.97s/batch]Batch 2900/20010 Done, mean position loss: 28.80928787946701\n",
      "Training growing_up:  14%|█▎       | 2893/20010 [2:22:22<13:51:26,  2.91s/batch]Batch 2900/20010 Done, mean position loss: 30.289924790859224\n",
      "Training growing_up:  14%|█▎       | 2861/20010 [2:22:29<15:00:04,  3.15s/batch]Batch 2900/20010 Done, mean position loss: 29.01704684019089\n",
      "Training growing_up:  14%|█▎       | 2835/20010 [2:22:34<14:14:28,  2.99s/batch]Batch 2900/20010 Done, mean position loss: 29.1108696103096\n",
      "Training growing_up:  15%|█▎       | 2909/20010 [2:22:38<14:47:59,  3.12s/batch]Batch 2900/20010 Done, mean position loss: 29.36236559391022\n",
      "Training growing_up:  14%|█▎       | 2848/20010 [2:22:40<14:26:18,  3.03s/batch]Batch 2900/20010 Done, mean position loss: 28.25987403869629\n",
      "Training growing_up:  14%|█▎       | 2874/20010 [2:22:45<14:40:58,  3.08s/batch]Batch 2900/20010 Done, mean position loss: 29.024283039569852\n",
      "Training growing_up:  15%|█▎       | 2911/20010 [2:23:06<14:09:11,  2.98s/batch]Batch 2900/20010 Done, mean position loss: 29.339708220958713\n",
      "Training growing_up:  15%|█▎       | 2944/20010 [2:23:07<13:50:40,  2.92s/batch]Batch 2900/20010 Done, mean position loss: 29.95358760356903\n",
      "Training growing_up:  14%|█▎       | 2848/20010 [2:23:08<15:20:34,  3.22s/batch]Batch 2900/20010 Done, mean position loss: 28.227891061306003\n",
      "Training growing_up:  14%|█▎       | 2887/20010 [2:23:11<13:04:38,  2.75s/batch]Batch 2900/20010 Done, mean position loss: 30.15416181087494\n",
      "Training growing_up:  15%|█▎       | 2905/20010 [2:23:16<12:35:29,  2.65s/batch]Batch 2900/20010 Done, mean position loss: 30.502184438705445\n",
      "Training growing_up:  15%|█▎       | 2959/20010 [2:23:29<12:44:55,  2.69s/batch]Batch 2900/20010 Done, mean position loss: 28.894054713249204\n",
      "Training growing_up:  15%|█▎       | 2923/20010 [2:23:43<14:36:29,  3.08s/batch]Batch 2900/20010 Done, mean position loss: 30.00824820280075\n",
      "Training growing_up:  15%|█▎       | 2906/20010 [2:23:44<13:16:06,  2.79s/batch]Batch 2900/20010 Done, mean position loss: 29.408080303668974\n",
      "Training growing_up:  14%|█▎       | 2898/20010 [2:23:44<14:26:30,  3.04s/batch]Batch 2900/20010 Done, mean position loss: 28.898645324707033\n",
      "Training growing_up:  15%|█▎       | 2912/20010 [2:23:46<12:41:28,  2.67s/batch]Batch 2900/20010 Done, mean position loss: 29.76258300542831\n",
      "Training growing_up:  15%|█▎       | 2929/20010 [2:23:52<13:05:22,  2.76s/batch]Batch 2900/20010 Done, mean position loss: 29.898735704421995\n",
      "Training growing_up:  15%|█▎       | 2906/20010 [2:23:56<13:17:26,  2.80s/batch]Batch 2900/20010 Done, mean position loss: 30.783499848842624\n",
      "Training growing_up:  14%|█▎       | 2897/20010 [2:24:04<14:20:24,  3.02s/batch]Batch 2900/20010 Done, mean position loss: 29.5555122590065\n",
      "Training growing_up:  14%|█▎       | 2878/20010 [2:24:06<13:59:42,  2.94s/batch]Batch 2900/20010 Done, mean position loss: 29.406211833953858\n",
      "Training growing_up:  15%|█▎       | 2946/20010 [2:24:16<12:35:17,  2.66s/batch]Batch 2900/20010 Done, mean position loss: 28.56060257434845\n",
      "Training growing_up:  15%|█▎       | 2964/20010 [2:24:24<13:37:22,  2.88s/batch]Batch 2900/20010 Done, mean position loss: 29.572687942981723\n",
      "Training growing_up:  15%|█▎       | 2921/20010 [2:25:06<15:24:31,  3.25s/batch]Batch 2900/20010 Done, mean position loss: 27.322017424106598\n",
      "Training growing_up:  15%|█▎       | 2925/20010 [2:25:13<13:31:44,  2.85s/batch]Batch 2900/20010 Done, mean position loss: 28.121186487674713\n",
      "Training growing_up:  15%|█▎       | 2904/20010 [2:25:15<13:52:50,  2.92s/batch]Batch 2900/20010 Done, mean position loss: 30.26577157497406\n",
      "Training growing_up:  15%|█▎       | 2948/20010 [2:25:26<14:21:11,  3.03s/batch]Batch 3000/20010 Done, mean position loss: 29.348932976722715\n",
      "Training growing_up:  15%|█▎       | 2938/20010 [2:25:38<13:16:37,  2.80s/batch]Batch 2900/20010 Done, mean position loss: 29.714783351421357\n",
      "Training growing_up:  15%|█▎       | 2966/20010 [2:25:48<14:41:33,  3.10s/batch]Batch 2900/20010 Done, mean position loss: 29.677227547168734\n",
      "Training growing_up:  15%|█▎       | 2905/20010 [2:25:48<13:40:24,  2.88s/batch]Batch 2900/20010 Done, mean position loss: 28.11987976551056\n",
      "Training growing_up:  15%|█▎       | 2946/20010 [2:25:51<13:36:28,  2.87s/batch]Batch 3000/20010 Done, mean position loss: 31.353696730136875\n",
      "Training growing_up:  15%|█▎       | 2954/20010 [2:26:01<13:41:50,  2.89s/batch]Batch 2900/20010 Done, mean position loss: 35.18438898324966\n",
      "Training growing_up:  15%|█▎       | 2948/20010 [2:26:08<15:35:24,  3.29s/batch]Batch 3000/20010 Done, mean position loss: 28.50716761350632\n",
      "Training growing_up:  15%|█▎       | 2962/20010 [2:26:11<11:45:01,  2.48s/batch]Batch 3000/20010 Done, mean position loss: 28.147659783363345\n",
      "Training growing_up:  15%|█▎       | 2967/20010 [2:26:15<13:18:46,  2.81s/batch]Batch 3000/20010 Done, mean position loss: 31.128380224704742\n",
      "Training growing_up:  15%|█▎       | 2973/20010 [2:26:30<14:33:15,  3.08s/batch]Batch 3000/20010 Done, mean position loss: 29.312166149616242\n",
      "Training growing_up:  15%|█▎       | 3001/20010 [2:26:31<13:24:42,  2.84s/batch]Batch 3000/20010 Done, mean position loss: 28.87963514328003\n",
      "Training growing_up:  15%|█▎       | 2959/20010 [2:26:39<11:42:06,  2.47s/batch]Batch 3000/20010 Done, mean position loss: 29.655941631793972\n",
      "Training growing_up:  15%|█▎       | 3010/20010 [2:26:57<14:03:01,  2.98s/batch]Batch 3000/20010 Done, mean position loss: 29.324706449508668\n",
      "Training growing_up:  15%|█▎       | 2954/20010 [2:26:59<15:25:55,  3.26s/batch]Batch 3000/20010 Done, mean position loss: 28.192949287891388\n",
      "Training growing_up:  15%|█▎       | 3013/20010 [2:27:07<14:18:50,  3.03s/batch]Batch 3000/20010 Done, mean position loss: 28.38226408958435\n",
      "Training growing_up:  15%|█▎       | 2967/20010 [2:27:10<16:12:16,  3.42s/batch]Batch 3000/20010 Done, mean position loss: 29.804297664165496\n",
      "Training growing_up:  15%|█▎       | 2989/20010 [2:27:22<12:58:38,  2.74s/batch]Batch 3000/20010 Done, mean position loss: 31.313011300563815\n",
      "Batch 3000/20010 Done, mean position loss: 28.12831152677536\n",
      "Training growing_up:  15%|█▎       | 2986/20010 [2:27:37<13:54:22,  2.94s/batch]Batch 3000/20010 Done, mean position loss: 28.37113494634628\n",
      "Training growing_up:  15%|█▎       | 2949/20010 [2:27:38<13:33:06,  2.86s/batch]Batch 3000/20010 Done, mean position loss: 29.379590594768523\n",
      "Training growing_up:  15%|█▎       | 3007/20010 [2:27:40<12:27:55,  2.64s/batch]Batch 3000/20010 Done, mean position loss: 29.62213357448578\n",
      "Training growing_up:  15%|█▎       | 3052/20010 [2:27:57<14:53:11,  3.16s/batch]Batch 3000/20010 Done, mean position loss: 29.24103885650635\n",
      "Training growing_up:  15%|█▎       | 3012/20010 [2:27:59<16:01:17,  3.39s/batch]Batch 3000/20010 Done, mean position loss: 29.56642399549484\n",
      "Training growing_up:  15%|█▎       | 2939/20010 [2:27:59<14:49:58,  3.13s/batch]Batch 3000/20010 Done, mean position loss: 29.025643849372862\n",
      "Training growing_up:  15%|█▎       | 3054/20010 [2:28:03<15:12:35,  3.23s/batch]Batch 3000/20010 Done, mean position loss: 29.081880905628203\n",
      "Training growing_up:  15%|█▎       | 2992/20010 [2:28:08<13:45:52,  2.91s/batch]Batch 3000/20010 Done, mean position loss: 28.358782262802123\n",
      "Training growing_up:  15%|█▎       | 2965/20010 [2:28:21<13:58:28,  2.95s/batch]Batch 3000/20010 Done, mean position loss: 29.351527931690217\n",
      "Training growing_up:  15%|█▎       | 3029/20010 [2:28:30<12:36:13,  2.67s/batch]Batch 3000/20010 Done, mean position loss: 29.9852455163002\n",
      "Training growing_up:  15%|█▎       | 2999/20010 [2:28:32<14:08:01,  2.99s/batch]Batch 3000/20010 Done, mean position loss: 28.60195508956909\n",
      "Training growing_up:  15%|█▎       | 3033/20010 [2:28:37<12:35:23,  2.67s/batch]Batch 3000/20010 Done, mean position loss: 29.187726118564605\n",
      "Training growing_up:  15%|█▎       | 3035/20010 [2:28:44<13:28:47,  2.86s/batch]Batch 3000/20010 Done, mean position loss: 28.807655854225157\n",
      "Training growing_up:  15%|█▎       | 3037/20010 [2:28:49<13:21:15,  2.83s/batch]Batch 3000/20010 Done, mean position loss: 29.216938593387603\n",
      "Training growing_up:  15%|█▎       | 3039/20010 [2:28:54<12:49:52,  2.72s/batch]Batch 3000/20010 Done, mean position loss: 29.89230787038803\n",
      "Training growing_up:  15%|█▎       | 3020/20010 [2:29:02<13:59:07,  2.96s/batch]Batch 3000/20010 Done, mean position loss: 29.672185463905336\n",
      "Training growing_up:  15%|█▎       | 2971/20010 [2:29:11<14:50:41,  3.14s/batch]Batch 3000/20010 Done, mean position loss: 29.05998149633408\n",
      "Training growing_up:  15%|█▍       | 3063/20010 [2:29:19<14:08:41,  3.00s/batch]Batch 3000/20010 Done, mean position loss: 30.505926117897033\n",
      "Batch 3000/20010 Done, mean position loss: 28.760283644199372\n",
      "Training growing_up:  15%|█▎       | 3047/20010 [2:30:12<15:08:27,  3.21s/batch]Batch 3000/20010 Done, mean position loss: 27.82206954240799\n",
      "Training growing_up:  15%|█▎       | 2983/20010 [2:30:13<14:38:49,  3.10s/batch]Batch 3000/20010 Done, mean position loss: 29.960979125499726\n",
      "Training growing_up:  15%|█▍       | 3079/20010 [2:30:13<15:16:50,  3.25s/batch]Batch 3000/20010 Done, mean position loss: 28.283076639175416\n",
      "Training growing_up:  15%|█▎       | 3004/20010 [2:30:21<15:15:44,  3.23s/batch]Batch 3100/20010 Done, mean position loss: 28.558776226043697\n",
      "Training growing_up:  15%|█▍       | 3078/20010 [2:30:45<12:34:10,  2.67s/batch]Batch 3000/20010 Done, mean position loss: 29.2594944691658\n",
      "Training growing_up:  15%|█▍       | 3068/20010 [2:30:51<14:17:00,  3.04s/batch]Batch 3100/20010 Done, mean position loss: 30.995298676490783\n",
      "Training growing_up:  15%|█▎       | 3051/20010 [2:30:52<13:20:16,  2.83s/batch]Batch 3000/20010 Done, mean position loss: 28.649925897121427\n",
      "Training growing_up:  15%|█▍       | 3092/20010 [2:30:58<12:48:43,  2.73s/batch]Batch 3000/20010 Done, mean position loss: 29.407054004669188\n",
      "Training growing_up:  15%|█▍       | 3069/20010 [2:31:07<13:34:46,  2.89s/batch]Batch 3000/20010 Done, mean position loss: 29.923409845829013\n",
      "Training growing_up:  15%|█▎       | 3037/20010 [2:31:11<16:25:44,  3.48s/batch]Batch 3100/20010 Done, mean position loss: 29.533098075389862\n",
      "Training growing_up:  15%|█▍       | 3077/20010 [2:31:18<13:10:35,  2.80s/batch]Batch 3100/20010 Done, mean position loss: 27.611461102962494\n",
      "Training growing_up:  15%|█▎       | 3057/20010 [2:31:20<12:56:42,  2.75s/batch]Batch 3100/20010 Done, mean position loss: 29.218090801239015\n",
      "Training growing_up:  16%|█▍       | 3123/20010 [2:31:26<13:59:05,  2.98s/batch]Batch 3100/20010 Done, mean position loss: 29.074764988422395\n",
      "Training growing_up:  15%|█▍       | 3082/20010 [2:31:32<14:54:57,  3.17s/batch]Batch 3100/20010 Done, mean position loss: 28.9570823097229\n",
      "Training growing_up:  15%|█▎       | 3047/20010 [2:31:36<13:34:06,  2.88s/batch]Batch 3100/20010 Done, mean position loss: 28.69942450046539\n",
      "Training growing_up:  15%|█▍       | 3089/20010 [2:31:56<13:50:52,  2.95s/batch]Batch 3100/20010 Done, mean position loss: 28.838773920536042\n",
      "Training growing_up:  15%|█▍       | 3072/20010 [2:32:00<13:47:51,  2.93s/batch]Batch 3100/20010 Done, mean position loss: 29.40735358476639\n",
      "Training growing_up:  16%|█▍       | 3116/20010 [2:32:04<15:22:39,  3.28s/batch]Batch 3100/20010 Done, mean position loss: 29.015322914123537\n",
      "Training growing_up:  15%|█▍       | 3058/20010 [2:32:07<13:51:33,  2.94s/batch]Batch 3100/20010 Done, mean position loss: 28.666074531078337\n",
      "Training growing_up:  15%|█▍       | 3091/20010 [2:32:27<13:34:35,  2.89s/batch]Batch 3100/20010 Done, mean position loss: 31.08645045042038\n",
      "Training growing_up:  15%|█▍       | 3092/20010 [2:32:30<15:43:02,  3.34s/batch]Batch 3100/20010 Done, mean position loss: 29.309021944999692\n",
      "Training growing_up:  15%|█▎       | 3046/20010 [2:32:32<14:43:04,  3.12s/batch]Batch 3100/20010 Done, mean position loss: 27.904877660274508\n",
      "Training growing_up:  15%|█▍       | 3094/20010 [2:32:33<12:49:19,  2.73s/batch]Batch 3100/20010 Done, mean position loss: 28.655191783905032\n",
      "Training growing_up:  15%|█▍       | 3087/20010 [2:32:45<12:38:47,  2.69s/batch]Batch 3100/20010 Done, mean position loss: 29.737087712287902\n",
      "Training growing_up:  16%|█▍       | 3128/20010 [2:32:52<12:53:20,  2.75s/batch]Batch 3100/20010 Done, mean position loss: 29.076806943416592\n",
      "Training growing_up:  15%|█▎       | 3053/20010 [2:32:53<14:45:12,  3.13s/batch]Batch 3100/20010 Done, mean position loss: 29.43825295448303\n",
      "Training growing_up:  15%|█▎       | 3057/20010 [2:32:55<13:54:13,  2.95s/batch]Batch 3100/20010 Done, mean position loss: 28.85865518569946\n",
      "Training growing_up:  15%|█▍       | 3089/20010 [2:32:57<13:00:01,  2.77s/batch]Batch 3100/20010 Done, mean position loss: 30.701183207035065\n",
      "Training growing_up:  16%|█▍       | 3119/20010 [2:32:58<15:39:52,  3.34s/batch]Batch 3100/20010 Done, mean position loss: 27.632301943302156\n",
      "Training growing_up:  16%|█▍       | 3130/20010 [2:33:26<13:49:06,  2.95s/batch]Batch 3100/20010 Done, mean position loss: 29.271086444854735\n",
      "Training growing_up:  16%|█▍       | 3145/20010 [2:33:26<14:41:00,  3.13s/batch]Batch 3100/20010 Done, mean position loss: 28.853201503753663\n",
      "Training growing_up:  15%|█▍       | 3066/20010 [2:33:33<14:19:19,  3.04s/batch]Batch 3100/20010 Done, mean position loss: 30.770413508415224\n",
      "Training growing_up:  16%|█▍       | 3148/20010 [2:33:38<13:17:18,  2.84s/batch]Batch 3100/20010 Done, mean position loss: 28.786119434833527\n",
      "Training growing_up:  15%|█▍       | 3095/20010 [2:33:39<14:53:41,  3.17s/batch]Batch 3100/20010 Done, mean position loss: 28.81143091917038\n",
      "Training growing_up:  16%|█▍       | 3159/20010 [2:33:40<15:07:42,  3.23s/batch]Batch 3100/20010 Done, mean position loss: 28.96245908975601\n",
      "Training growing_up:  15%|█▍       | 3074/20010 [2:33:55<13:29:51,  2.87s/batch]Batch 3100/20010 Done, mean position loss: 29.89913468360901\n",
      "Training growing_up:  16%|█▍       | 3149/20010 [2:34:05<13:49:59,  2.95s/batch]Batch 3100/20010 Done, mean position loss: 28.305926575660706\n",
      "Training growing_up:  15%|█▍       | 3069/20010 [2:34:11<14:29:42,  3.08s/batch]Batch 3100/20010 Done, mean position loss: 28.636528446674344\n",
      "Training growing_up:  15%|█▍       | 3067/20010 [2:34:19<14:30:20,  3.08s/batch]Batch 3100/20010 Done, mean position loss: 29.61463818311691\n",
      "Training growing_up:  16%|█▍       | 3173/20010 [2:34:22<13:04:43,  2.80s/batch]Batch 3100/20010 Done, mean position loss: 28.374548568725587\n",
      "Training growing_up:  16%|█▍       | 3181/20010 [2:35:09<11:48:32,  2.53s/batch]Batch 3100/20010 Done, mean position loss: 32.6244729089737\n",
      "Training growing_up:  16%|█▍       | 3153/20010 [2:35:13<12:54:25,  2.76s/batch]Batch 3200/20010 Done, mean position loss: 28.80654770374298\n",
      "Training growing_up:  16%|█▍       | 3148/20010 [2:35:13<11:11:00,  2.39s/batch]Batch 3100/20010 Done, mean position loss: 28.561082401275634\n",
      "Training growing_up:  16%|█▍       | 3150/20010 [2:35:14<11:04:17,  2.36s/batch]Batch 3100/20010 Done, mean position loss: 27.738568599224088\n",
      "Training growing_up:  16%|█▍       | 3145/20010 [2:35:35<11:47:37,  2.52s/batch]Batch 3100/20010 Done, mean position loss: 29.280369794368745\n",
      "Training growing_up:  16%|█▍       | 3166/20010 [2:35:41<13:40:50,  2.92s/batch]Batch 3200/20010 Done, mean position loss: 28.83888077735901\n",
      "Training growing_up:  16%|█▍       | 3213/20010 [2:35:47<14:46:21,  3.17s/batch]Batch 3100/20010 Done, mean position loss: 28.564839479923247\n",
      "Training growing_up:  16%|█▍       | 3132/20010 [2:35:51<15:09:10,  3.23s/batch]Batch 3100/20010 Done, mean position loss: 30.28862962245941\n",
      "Training growing_up:  16%|█▍       | 3207/20010 [2:35:57<13:25:34,  2.88s/batch]Batch 3100/20010 Done, mean position loss: 30.178186163902286\n",
      "Training growing_up:  16%|█▍       | 3186/20010 [2:36:04<13:16:35,  2.84s/batch]Batch 3200/20010 Done, mean position loss: 29.127933835983278\n",
      "Training growing_up:  16%|█▍       | 3219/20010 [2:36:04<13:38:00,  2.92s/batch]Batch 3200/20010 Done, mean position loss: 29.002494189739224\n",
      "Training growing_up:  16%|█▍       | 3143/20010 [2:36:08<12:49:03,  2.74s/batch]Batch 3200/20010 Done, mean position loss: 29.72678722143173\n",
      "Training growing_up:  16%|█▍       | 3159/20010 [2:36:16<13:00:28,  2.78s/batch]Batch 3200/20010 Done, mean position loss: 28.646829149723054\n",
      "Training growing_up:  16%|█▍       | 3214/20010 [2:36:16<12:56:50,  2.78s/batch]Batch 3200/20010 Done, mean position loss: 28.517202849388124\n",
      "Training growing_up:  16%|█▍       | 3151/20010 [2:36:32<14:41:35,  3.14s/batch]Batch 3200/20010 Done, mean position loss: 29.325309803485872\n",
      "Training growing_up:  16%|█▍       | 3223/20010 [2:36:42<13:16:16,  2.85s/batch]Batch 3200/20010 Done, mean position loss: 28.37340701341629\n",
      "Training growing_up:  16%|█▍       | 3166/20010 [2:36:49<13:38:41,  2.92s/batch]Batch 3200/20010 Done, mean position loss: 27.800626122951506\n",
      "Training growing_up:  16%|█▍       | 3184/20010 [2:36:59<13:33:00,  2.90s/batch]Batch 3200/20010 Done, mean position loss: 27.89381949424744\n",
      "Training growing_up:  16%|█▍       | 3223/20010 [2:37:03<12:52:47,  2.76s/batch]Batch 3200/20010 Done, mean position loss: 29.80828663825989\n",
      "Training growing_up:  16%|█▍       | 3179/20010 [2:37:19<14:01:07,  3.00s/batch]Batch 3200/20010 Done, mean position loss: 29.016285557746887\n",
      "Training growing_up:  16%|█▍       | 3194/20010 [2:37:21<14:48:22,  3.17s/batch]Batch 3200/20010 Done, mean position loss: 31.34867124557495\n",
      "Training growing_up:  16%|█▍       | 3226/20010 [2:37:23<11:51:05,  2.54s/batch]Batch 3200/20010 Done, mean position loss: 28.01847661972046\n",
      "Training growing_up:  16%|█▍       | 3231/20010 [2:37:28<13:43:39,  2.95s/batch]Batch 3200/20010 Done, mean position loss: 27.97339729309082\n",
      "Training growing_up:  16%|█▍       | 3184/20010 [2:37:42<14:19:48,  3.07s/batch]Batch 3200/20010 Done, mean position loss: 28.266275155544278\n",
      "Training growing_up:  16%|█▍       | 3140/20010 [2:37:44<14:50:35,  3.17s/batch]Batch 3200/20010 Done, mean position loss: 27.634411816596987\n",
      "Training growing_up:  16%|█▍       | 3173/20010 [2:37:46<13:07:12,  2.81s/batch]Batch 3200/20010 Done, mean position loss: 31.006081895828245\n",
      "Training growing_up:  16%|█▍       | 3255/20010 [2:37:47<14:22:49,  3.09s/batch]Batch 3200/20010 Done, mean position loss: 28.533629412651063\n",
      "Training growing_up:  16%|█▍       | 3211/20010 [2:37:49<13:22:52,  2.87s/batch]Batch 3200/20010 Done, mean position loss: 29.058983693122865\n",
      "Training growing_up:  16%|█▍       | 3241/20010 [2:37:56<13:00:10,  2.79s/batch]Batch 3200/20010 Done, mean position loss: 28.31118326663971\n",
      "Training growing_up:  16%|█▍       | 3216/20010 [2:38:11<13:26:28,  2.88s/batch]Batch 3200/20010 Done, mean position loss: 29.10163220405579\n",
      "Training growing_up:  16%|█▍       | 3245/20010 [2:38:16<14:34:15,  3.13s/batch]Batch 3200/20010 Done, mean position loss: 28.145484962463378\n",
      "Training growing_up:  16%|█▍       | 3189/20010 [2:38:22<13:34:36,  2.91s/batch]Batch 3200/20010 Done, mean position loss: 29.264202756881712\n",
      "Training growing_up:  16%|█▍       | 3251/20010 [2:38:24<12:49:39,  2.76s/batch]Batch 3200/20010 Done, mean position loss: 29.858358726501464\n",
      "Training growing_up:  16%|█▍       | 3204/20010 [2:38:25<13:58:43,  2.99s/batch]Batch 3200/20010 Done, mean position loss: 29.070800576210022\n",
      "Training growing_up:  16%|█▍       | 3238/20010 [2:38:29<12:41:27,  2.72s/batch]Batch 3200/20010 Done, mean position loss: 28.65160855770111\n",
      "Training growing_up:  16%|█▍       | 3198/20010 [2:38:45<14:17:10,  3.06s/batch]Batch 3200/20010 Done, mean position loss: 31.315167016983033\n",
      "Training growing_up:  16%|█▍       | 3204/20010 [2:38:53<14:11:11,  3.04s/batch]Batch 3200/20010 Done, mean position loss: 28.162661306858062\n",
      "Training growing_up:  16%|█▍       | 3260/20010 [2:38:56<12:25:23,  2.67s/batch]Batch 3200/20010 Done, mean position loss: 29.47992568016052\n",
      "Training growing_up:  16%|█▍       | 3248/20010 [2:39:08<14:45:19,  3.17s/batch]Batch 3200/20010 Done, mean position loss: 29.371088407039643\n",
      "Training growing_up:  16%|█▍       | 3220/20010 [2:39:18<13:08:56,  2.82s/batch]Batch 3200/20010 Done, mean position loss: 27.755991294384003\n",
      "Training growing_up:  16%|█▍       | 3252/20010 [2:39:56<12:09:07,  2.61s/batch]Batch 3200/20010 Done, mean position loss: 30.73529226779938\n",
      "Training growing_up:  16%|█▍       | 3227/20010 [2:39:57<11:45:22,  2.52s/batch]Batch 3300/20010 Done, mean position loss: 28.313261311054227\n",
      "Training growing_up:  16%|█▍       | 3273/20010 [2:40:06<13:37:50,  2.93s/batch]Batch 3200/20010 Done, mean position loss: 28.12605251312256\n",
      "Training growing_up:  16%|█▍       | 3289/20010 [2:40:14<13:40:00,  2.94s/batch]Batch 3200/20010 Done, mean position loss: 28.454212985038758\n",
      "Training growing_up:  16%|█▍       | 3208/20010 [2:40:26<12:47:37,  2.74s/batch]Batch 3300/20010 Done, mean position loss: 27.651100406646727\n",
      "Training growing_up:  16%|█▍       | 3195/20010 [2:40:29<14:01:44,  3.00s/batch]Batch 3200/20010 Done, mean position loss: 31.860888390541074\n",
      "Training growing_up:  16%|█▍       | 3263/20010 [2:40:43<12:18:49,  2.65s/batch]Batch 3200/20010 Done, mean position loss: 28.162422585487366\n",
      "Training growing_up:  16%|█▍       | 3218/20010 [2:40:45<13:39:36,  2.93s/batch]Batch 3200/20010 Done, mean position loss: 30.324323766231537\n",
      "Training growing_up:  16%|█▍       | 3264/20010 [2:40:46<12:09:21,  2.61s/batch]Batch 3200/20010 Done, mean position loss: 29.075024819374082\n",
      "Training growing_up:  16%|█▍       | 3297/20010 [2:40:48<14:27:56,  3.12s/batch]Batch 3300/20010 Done, mean position loss: 28.752465963363647\n",
      "Training growing_up:  16%|█▍       | 3202/20010 [2:40:49<14:06:39,  3.02s/batch]Batch 3300/20010 Done, mean position loss: 28.264157869815826\n",
      "Training growing_up:  16%|█▍       | 3216/20010 [2:40:52<15:10:22,  3.25s/batch]Batch 3300/20010 Done, mean position loss: 29.319479508399965\n",
      "Training growing_up:  16%|█▍       | 3269/20010 [2:41:00<12:57:16,  2.79s/batch]Batch 3300/20010 Done, mean position loss: 28.68109385251999\n",
      "Training growing_up:  16%|█▍       | 3207/20010 [2:41:03<13:19:27,  2.85s/batch]Batch 3300/20010 Done, mean position loss: 26.98353514671326\n",
      "Training growing_up:  16%|█▍       | 3277/20010 [2:41:21<13:07:19,  2.82s/batch]Batch 3300/20010 Done, mean position loss: 28.392201614379882\n",
      "Training growing_up:  16%|█▍       | 3279/20010 [2:41:30<13:06:32,  2.82s/batch]Batch 3300/20010 Done, mean position loss: 28.19056457519531\n",
      "Training growing_up:  16%|█▍       | 3291/20010 [2:41:38<13:26:54,  2.90s/batch]Batch 3300/20010 Done, mean position loss: 28.187444381713867\n",
      "Training growing_up:  16%|█▍       | 3285/20010 [2:41:48<13:53:47,  2.99s/batch]Batch 3300/20010 Done, mean position loss: 27.767518446445465\n",
      "Training growing_up:  16%|█▍       | 3234/20010 [2:41:52<13:52:54,  2.98s/batch]Batch 3300/20010 Done, mean position loss: 27.355755414962772\n",
      "Training growing_up:  16%|█▍       | 3288/20010 [2:42:06<11:52:13,  2.56s/batch]Batch 3300/20010 Done, mean position loss: 29.207093431949616\n",
      "Training growing_up:  16%|█▍       | 3239/20010 [2:42:08<14:39:19,  3.15s/batch]Batch 3300/20010 Done, mean position loss: 28.122171618938445\n",
      "Training growing_up:  17%|█▍       | 3303/20010 [2:42:13<14:19:42,  3.09s/batch]Batch 3300/20010 Done, mean position loss: 28.85555554151535\n",
      "Training growing_up:  17%|█▍       | 3308/20010 [2:42:27<13:18:59,  2.87s/batch]Batch 3300/20010 Done, mean position loss: 28.38071293592453\n",
      "Training growing_up:  17%|█▌       | 3342/20010 [2:42:32<14:04:03,  3.04s/batch]Batch 3300/20010 Done, mean position loss: 28.166025164127348\n",
      "Training growing_up:  17%|█▍       | 3315/20010 [2:42:34<13:28:09,  2.90s/batch]Batch 3300/20010 Done, mean position loss: 27.519339323043823\n",
      "Training growing_up:  16%|█▍       | 3289/20010 [2:42:37<12:23:53,  2.67s/batch]Batch 3300/20010 Done, mean position loss: 30.544848363399502\n",
      "Training growing_up:  16%|█▍       | 3301/20010 [2:42:37<15:20:07,  3.30s/batch]Batch 3300/20010 Done, mean position loss: 28.854692082405087\n",
      "Training growing_up:  16%|█▍       | 3269/20010 [2:42:38<15:24:26,  3.31s/batch]Batch 3300/20010 Done, mean position loss: 28.5764058804512\n",
      "Training growing_up:  17%|█▍       | 3322/20010 [2:42:48<13:14:55,  2.86s/batch]Batch 3300/20010 Done, mean position loss: 28.126227061748505\n",
      "Training growing_up:  17%|█▍       | 3311/20010 [2:43:05<14:10:46,  3.06s/batch]Batch 3300/20010 Done, mean position loss: 28.48575609922409\n",
      "Training growing_up:  17%|█▍       | 3313/20010 [2:43:13<14:44:24,  3.18s/batch]Batch 3300/20010 Done, mean position loss: 28.57905709028244\n",
      "Training growing_up:  17%|█▌       | 3348/20010 [2:43:14<15:37:10,  3.37s/batch]Batch 3300/20010 Done, mean position loss: 29.767718400955197\n",
      "Training growing_up:  16%|█▍       | 3252/20010 [2:43:20<13:22:22,  2.87s/batch]Batch 3300/20010 Done, mean position loss: 29.262151358127593\n",
      "Training growing_up:  16%|█▍       | 3253/20010 [2:43:24<14:37:26,  3.14s/batch]Batch 3300/20010 Done, mean position loss: 29.571179187297822\n",
      "Training growing_up:  17%|█▍       | 3303/20010 [2:43:29<13:12:53,  2.85s/batch]Batch 3300/20010 Done, mean position loss: 27.549497656822204\n",
      "Training growing_up:  16%|█▍       | 3267/20010 [2:43:43<15:11:41,  3.27s/batch]Batch 3300/20010 Done, mean position loss: 29.57253595352173\n",
      "Training growing_up:  17%|█▍       | 3316/20010 [2:43:53<13:57:40,  3.01s/batch]Batch 3300/20010 Done, mean position loss: 28.400113639831545\n",
      "Training growing_up:  17%|█▍       | 3312/20010 [2:44:00<12:36:25,  2.72s/batch]Batch 3300/20010 Done, mean position loss: 29.295623569488527\n",
      "Training growing_up:  16%|█▍       | 3268/20010 [2:44:09<13:36:39,  2.93s/batch]Batch 3300/20010 Done, mean position loss: 29.050333795547488\n",
      "Training growing_up:  17%|█▌       | 3377/20010 [2:44:16<12:34:39,  2.72s/batch]Batch 3300/20010 Done, mean position loss: 28.28512062072754\n",
      "Training growing_up:  16%|█▍       | 3284/20010 [2:44:57<15:11:43,  3.27s/batch]Batch 3300/20010 Done, mean position loss: 30.674113495349886\n",
      "Training growing_up:  16%|█▍       | 3284/20010 [2:45:04<14:18:37,  3.08s/batch]Batch 3400/20010 Done, mean position loss: 28.46742155790329\n",
      "Training growing_up:  17%|█▌       | 3404/20010 [2:45:13<13:45:26,  2.98s/batch]Batch 3300/20010 Done, mean position loss: 28.15972883939743\n",
      "Training growing_up:  17%|█▌       | 3399/20010 [2:45:20<12:16:53,  2.66s/batch]Batch 3300/20010 Done, mean position loss: 27.615866985321045\n",
      "Training growing_up:  17%|█▌       | 3375/20010 [2:45:26<12:54:25,  2.79s/batch]Batch 3400/20010 Done, mean position loss: 28.940674543380737\n",
      "Training growing_up:  17%|█▌       | 3376/20010 [2:45:29<12:56:33,  2.80s/batch]Batch 3300/20010 Done, mean position loss: 28.912271580696107\n",
      "Training growing_up:  17%|█▌       | 3351/20010 [2:45:43<13:03:59,  2.82s/batch]Batch 3400/20010 Done, mean position loss: 28.097262952327725\n",
      "Training growing_up:  17%|█▌       | 3376/20010 [2:45:47<13:42:34,  2.97s/batch]Batch 3300/20010 Done, mean position loss: 28.55942367553711\n",
      "Training growing_up:  17%|█▌       | 3399/20010 [2:45:47<12:46:13,  2.77s/batch]Batch 3400/20010 Done, mean position loss: 29.002321851253512\n",
      "Training growing_up:  17%|█▌       | 3400/20010 [2:45:50<12:45:37,  2.77s/batch]Batch 3400/20010 Done, mean position loss: 28.658462381362916\n",
      "Training growing_up:  17%|█▍       | 3303/20010 [2:45:53<13:37:35,  2.94s/batch]Batch 3300/20010 Done, mean position loss: 28.664900398254396\n",
      "Batch 3400/20010 Done, mean position loss: 28.425654740333556\n",
      "Training growing_up:  17%|█▌       | 3402/20010 [2:45:55<12:11:21,  2.64s/batch]Batch 3300/20010 Done, mean position loss: 27.731878838539124\n",
      "Training growing_up:  17%|█▍       | 3305/20010 [2:45:58<13:16:43,  2.86s/batch]Batch 3400/20010 Done, mean position loss: 28.62999910593033\n",
      "Training growing_up:  17%|█▌       | 3397/20010 [2:46:13<15:00:34,  3.25s/batch]Batch 3400/20010 Done, mean position loss: 28.041492426395415\n",
      "Training growing_up:  17%|█▌       | 3414/20010 [2:46:24<14:03:59,  3.05s/batch]Batch 3400/20010 Done, mean position loss: 29.476577639579773\n",
      "Training growing_up:  17%|█▌       | 3415/20010 [2:46:38<13:40:36,  2.97s/batch]Batch 3400/20010 Done, mean position loss: 27.625962138175964\n",
      "Training growing_up:  17%|█▌       | 3408/20010 [2:46:42<11:45:02,  2.55s/batch]Batch 3400/20010 Done, mean position loss: 27.14506440401077\n",
      "Training growing_up:  17%|█▌       | 3404/20010 [2:46:51<12:50:52,  2.79s/batch]Batch 3400/20010 Done, mean position loss: 28.173042852878574\n",
      "Training growing_up:  17%|█▌       | 3385/20010 [2:47:01<14:30:45,  3.14s/batch]Batch 3400/20010 Done, mean position loss: 28.792172250747683\n",
      "Training growing_up:  17%|█▌       | 3390/20010 [2:47:04<12:48:33,  2.77s/batch]Batch 3400/20010 Done, mean position loss: 30.75667499065399\n",
      "Training growing_up:  17%|█▍       | 3326/20010 [2:47:10<13:00:51,  2.81s/batch]Batch 3400/20010 Done, mean position loss: 29.979296495914458\n",
      "Training growing_up:  17%|█▌       | 3397/20010 [2:47:26<12:11:55,  2.64s/batch]Batch 3400/20010 Done, mean position loss: 27.69543566465378\n",
      "Training growing_up:  17%|█▌       | 3401/20010 [2:47:26<12:35:49,  2.73s/batch]Batch 3400/20010 Done, mean position loss: 28.380352787971496\n",
      "Training growing_up:  17%|█▌       | 3341/20010 [2:47:29<12:41:46,  2.74s/batch]Batch 3400/20010 Done, mean position loss: 28.399178128242493\n",
      "Training growing_up:  17%|█▌       | 3401/20010 [2:47:30<15:20:54,  3.33s/batch]Batch 3400/20010 Done, mean position loss: 29.306469407081607\n",
      "Training growing_up:  17%|█▌       | 3427/20010 [2:47:35<11:55:57,  2.59s/batch]Batch 3400/20010 Done, mean position loss: 28.884439952373505\n",
      "Training growing_up:  17%|█▌       | 3335/20010 [2:47:40<14:12:20,  3.07s/batch]Batch 3400/20010 Done, mean position loss: 30.242602715492247\n",
      "Training growing_up:  17%|█▌       | 3406/20010 [2:47:50<13:31:43,  2.93s/batch]Batch 3400/20010 Done, mean position loss: 27.94389415025711\n",
      "Training growing_up:  17%|█▌       | 3444/20010 [2:47:58<12:10:04,  2.64s/batch]Batch 3400/20010 Done, mean position loss: 27.945444560050966\n",
      "Training growing_up:  17%|█▌       | 3438/20010 [2:48:09<13:35:06,  2.95s/batch]Batch 3400/20010 Done, mean position loss: 28.059636714458463\n",
      "Training growing_up:  17%|█▌       | 3384/20010 [2:48:13<15:22:02,  3.33s/batch]Batch 3400/20010 Done, mean position loss: 28.641418728828434\n",
      "Training growing_up:  17%|█▌       | 3424/20010 [2:48:16<14:53:51,  3.23s/batch]Batch 3400/20010 Done, mean position loss: 28.79437946796417\n",
      "Training growing_up:  17%|█▌       | 3442/20010 [2:48:17<13:58:24,  3.04s/batch]Batch 3400/20010 Done, mean position loss: 28.514787056446075\n",
      "Training growing_up:  17%|█▌       | 3453/20010 [2:48:29<13:41:28,  2.98s/batch]Batch 3400/20010 Done, mean position loss: 27.738258821964266\n",
      "Training growing_up:  17%|█▌       | 3416/20010 [2:48:44<14:36:51,  3.17s/batch]Batch 3400/20010 Done, mean position loss: 29.951722748279572\n",
      "Training growing_up:  17%|█▌       | 3414/20010 [2:48:55<13:46:14,  2.99s/batch]Batch 3400/20010 Done, mean position loss: 28.407318563461303\n",
      "Training growing_up:  17%|█▌       | 3398/20010 [2:49:02<12:25:11,  2.69s/batch]Batch 3400/20010 Done, mean position loss: 29.88413198709488\n",
      "Training growing_up:  17%|█▌       | 3421/20010 [2:49:07<11:36:27,  2.52s/batch]Batch 3400/20010 Done, mean position loss: 27.754535672664645\n",
      "Training growing_up:  17%|█▌       | 3453/20010 [2:49:10<10:33:29,  2.30s/batch]Batch 3400/20010 Done, mean position loss: 27.976541495323183\n",
      "Training growing_up:  17%|█▌       | 3460/20010 [2:49:49<11:43:31,  2.55s/batch]Batch 3500/20010 Done, mean position loss: 27.931004598140717\n",
      "Training growing_up:  17%|█▌       | 3451/20010 [2:49:51<12:03:43,  2.62s/batch]Batch 3400/20010 Done, mean position loss: 31.855866446495057\n",
      "Training growing_up:  17%|█▌       | 3462/20010 [2:49:58<14:31:02,  3.16s/batch]Batch 3400/20010 Done, mean position loss: 27.207188189029694\n",
      "Training growing_up:  17%|█▌       | 3457/20010 [2:50:07<12:04:58,  2.63s/batch]Batch 3400/20010 Done, mean position loss: 28.112026398181914\n",
      "Training growing_up:  17%|█▌       | 3391/20010 [2:50:14<12:47:15,  2.77s/batch]Batch 3500/20010 Done, mean position loss: 29.44938098192215\n",
      "Training growing_up:  17%|█▌       | 3498/20010 [2:50:24<14:12:54,  3.10s/batch]Batch 3400/20010 Done, mean position loss: 28.711912417411803\n",
      "Training growing_up:  17%|█▌       | 3500/20010 [2:50:32<13:11:18,  2.88s/batch]Batch 3500/20010 Done, mean position loss: 27.51700656890869\n",
      "Training growing_up:  18%|█▌       | 3508/20010 [2:50:34<13:05:18,  2.86s/batch]Batch 3500/20010 Done, mean position loss: 34.505871827602384\n",
      "Training growing_up:  17%|█▌       | 3433/20010 [2:50:35<13:18:45,  2.89s/batch]Batch 3500/20010 Done, mean position loss: 29.436038608551026\n",
      "Training growing_up:  17%|█▌       | 3468/20010 [2:50:39<12:21:19,  2.69s/batch]Batch 3500/20010 Done, mean position loss: 27.12713100194931\n",
      "Training growing_up:  17%|█▌       | 3479/20010 [2:50:39<11:39:39,  2.54s/batch]Batch 3500/20010 Done, mean position loss: 27.20984673500061\n",
      "Training growing_up:  17%|█▌       | 3436/20010 [2:50:45<11:37:50,  2.53s/batch]Batch 3400/20010 Done, mean position loss: 27.874046750068665\n",
      "Training growing_up:  17%|█▌       | 3485/20010 [2:50:49<12:49:26,  2.79s/batch]Batch 3400/20010 Done, mean position loss: 28.19914622545242\n",
      "Training growing_up:  17%|█▌       | 3476/20010 [2:50:57<12:33:13,  2.73s/batch]Batch 3500/20010 Done, mean position loss: 27.17410141468048\n",
      "Training growing_up:  18%|█▌       | 3517/20010 [2:50:59<13:23:24,  2.92s/batch]Batch 3400/20010 Done, mean position loss: 27.796438829898833\n",
      "Training growing_up:  17%|█▌       | 3415/20010 [2:51:03<13:37:06,  2.95s/batch]Batch 3500/20010 Done, mean position loss: 28.289339921474458\n",
      "Training growing_up:  18%|█▌       | 3533/20010 [2:51:20<12:43:41,  2.78s/batch]Batch 3500/20010 Done, mean position loss: 29.17568431854248\n",
      "Training growing_up:  17%|█▌       | 3479/20010 [2:51:26<12:48:05,  2.79s/batch]Batch 3500/20010 Done, mean position loss: 27.062687158584595\n",
      "Training growing_up:  18%|█▌       | 3522/20010 [2:51:34<12:28:17,  2.72s/batch]Batch 3500/20010 Done, mean position loss: 30.130209417343142\n",
      "Training growing_up:  17%|█▌       | 3474/20010 [2:51:41<13:27:10,  2.93s/batch]Batch 3500/20010 Done, mean position loss: 30.271268789768218\n",
      "Training growing_up:  17%|█▌       | 3422/20010 [2:51:51<14:23:52,  3.12s/batch]Batch 3500/20010 Done, mean position loss: 28.71209824323654\n",
      "Training growing_up:  17%|█▌       | 3501/20010 [2:51:51<12:56:43,  2.82s/batch]Batch 3500/20010 Done, mean position loss: 28.304816377162936\n",
      "Training growing_up:  17%|█▌       | 3496/20010 [2:52:06<12:35:07,  2.74s/batch]Batch 3500/20010 Done, mean position loss: 28.231392285823823\n",
      "Training growing_up:  18%|█▌       | 3518/20010 [2:52:07<11:53:11,  2.59s/batch]Batch 3500/20010 Done, mean position loss: 27.175772275924686\n",
      "Training growing_up:  17%|█▌       | 3439/20010 [2:52:12<12:15:25,  2.66s/batch]Batch 3500/20010 Done, mean position loss: 28.76533582687378\n",
      "Training growing_up:  18%|█▌       | 3504/20010 [2:52:14<12:38:05,  2.76s/batch]Batch 3500/20010 Done, mean position loss: 28.01254419326782\n",
      "Training growing_up:  18%|█▌       | 3554/20010 [2:52:20<13:38:25,  2.98s/batch]Batch 3500/20010 Done, mean position loss: 28.236947934627533\n",
      "Training growing_up:  18%|█▌       | 3542/20010 [2:52:28<12:44:30,  2.79s/batch]Batch 3500/20010 Done, mean position loss: 29.009921739101408\n",
      "Training growing_up:  18%|█▌       | 3525/20010 [2:52:33<13:12:55,  2.89s/batch]Batch 3500/20010 Done, mean position loss: 29.1062171626091\n",
      "Training growing_up:  17%|█▌       | 3474/20010 [2:52:36<14:25:44,  3.14s/batch]Batch 3500/20010 Done, mean position loss: 27.872888877391816\n",
      "Training growing_up:  18%|█▌       | 3547/20010 [2:52:47<11:43:13,  2.56s/batch]Batch 3500/20010 Done, mean position loss: 28.015062396526336\n",
      "Training growing_up:  18%|█▌       | 3552/20010 [2:52:56<13:07:29,  2.87s/batch]Batch 3500/20010 Done, mean position loss: 28.587692165374754\n",
      "Training growing_up:  17%|█▌       | 3460/20010 [2:52:56<12:13:05,  2.66s/batch]Batch 3500/20010 Done, mean position loss: 28.310819458961486\n",
      "Training growing_up:  18%|█▌       | 3530/20010 [2:53:05<13:15:43,  2.90s/batch]Batch 3500/20010 Done, mean position loss: 28.237910232543946\n",
      "Training growing_up:  18%|█▌       | 3520/20010 [2:53:09<13:34:05,  2.96s/batch]Batch 3500/20010 Done, mean position loss: 26.871794052124024\n",
      "Training growing_up:  18%|█▌       | 3511/20010 [2:53:24<12:01:53,  2.63s/batch]Batch 3500/20010 Done, mean position loss: 29.246319162845612\n",
      "Training growing_up:  18%|█▌       | 3512/20010 [2:53:34<12:11:51,  2.66s/batch]Batch 3500/20010 Done, mean position loss: 28.662995550632477\n",
      "Training growing_up:  18%|█▌       | 3552/20010 [2:53:53<13:50:43,  3.03s/batch]Batch 3500/20010 Done, mean position loss: 27.658303306102752\n",
      "Training growing_up:  18%|█▌       | 3525/20010 [2:53:53<13:01:46,  2.85s/batch]Batch 3500/20010 Done, mean position loss: 28.510753324031832\n",
      "Training growing_up:  18%|█▌       | 3572/20010 [2:53:54<12:14:28,  2.68s/batch]Batch 3500/20010 Done, mean position loss: 28.082634065151215\n",
      "Training growing_up:  18%|█▌       | 3551/20010 [2:54:36<13:27:58,  2.95s/batch]Batch 3600/20010 Done, mean position loss: 28.206847088336943\n",
      "Training growing_up:  18%|█▌       | 3545/20010 [2:54:41<12:41:24,  2.77s/batch]Batch 3500/20010 Done, mean position loss: 29.10573473215103\n",
      "Training growing_up:  18%|█▌       | 3592/20010 [2:54:52<13:12:25,  2.90s/batch]Batch 3600/20010 Done, mean position loss: 28.290334734916687\n",
      "Batch 3500/20010 Done, mean position loss: 27.64726449728012\n",
      "Training growing_up:  18%|█▌       | 3522/20010 [2:54:55<12:14:35,  2.67s/batch]Batch 3500/20010 Done, mean position loss: 26.965438628196715\n",
      "Training growing_up:  18%|█▌       | 3610/20010 [2:55:18<13:07:06,  2.88s/batch]Batch 3500/20010 Done, mean position loss: 28.274292623996736\n",
      "Training growing_up:  18%|█▌       | 3502/20010 [2:55:20<12:51:22,  2.80s/batch]Batch 3600/20010 Done, mean position loss: 27.143681313991546\n",
      "Training growing_up:  17%|█▌       | 3489/20010 [2:55:22<14:02:52,  3.06s/batch]Batch 3600/20010 Done, mean position loss: 31.151885192394253\n",
      "Training growing_up:  18%|█▌       | 3550/20010 [2:55:26<13:02:55,  2.85s/batch]Batch 3600/20010 Done, mean position loss: 26.9385954117775\n",
      "Training growing_up:  18%|█▌       | 3516/20010 [2:55:28<13:43:20,  3.00s/batch]Batch 3600/20010 Done, mean position loss: 29.887825016975405\n",
      "Training growing_up:  18%|█▌       | 3581/20010 [2:55:34<13:54:23,  3.05s/batch]Batch 3600/20010 Done, mean position loss: 28.369272427558897\n",
      "Training growing_up:  18%|█▌       | 3574/20010 [2:55:44<16:00:23,  3.51s/batch]Batch 3500/20010 Done, mean position loss: 29.052354290485383\n",
      "Training growing_up:  17%|█▌       | 3496/20010 [2:55:45<14:41:37,  3.20s/batch]Batch 3500/20010 Done, mean position loss: 27.789893143177032\n",
      "Training growing_up:  18%|█▌       | 3541/20010 [2:55:50<12:52:25,  2.81s/batch]Batch 3600/20010 Done, mean position loss: 28.563796348571778\n",
      "Training growing_up:  18%|█▌       | 3512/20010 [2:55:51<13:23:06,  2.92s/batch]Batch 3600/20010 Done, mean position loss: 27.161742920875547\n",
      "Training growing_up:  18%|█▌       | 3591/20010 [2:56:01<13:17:08,  2.91s/batch]Batch 3500/20010 Done, mean position loss: 27.343265411853793\n",
      "Training growing_up:  18%|█▌       | 3574/20010 [2:56:09<15:16:44,  3.35s/batch]Batch 3600/20010 Done, mean position loss: 28.1481872010231\n",
      "Training growing_up:  18%|█▌       | 3584/20010 [2:56:15<12:48:55,  2.81s/batch]Batch 3600/20010 Done, mean position loss: 27.67007780790329\n",
      "Training growing_up:  18%|█▋       | 3624/20010 [2:56:32<14:02:42,  3.09s/batch]Batch 3600/20010 Done, mean position loss: 28.89386969566345\n",
      "Training growing_up:  18%|█▌       | 3572/20010 [2:56:35<14:16:25,  3.13s/batch]Batch 3600/20010 Done, mean position loss: 28.86873517513275\n",
      "Training growing_up:  18%|█▌       | 3574/20010 [2:56:41<13:55:59,  3.05s/batch]Batch 3600/20010 Done, mean position loss: 29.527118883132935\n",
      "Training growing_up:  18%|█▌       | 3592/20010 [2:56:42<14:12:23,  3.12s/batch]Batch 3600/20010 Done, mean position loss: 28.428073873519896\n",
      "Training growing_up:  18%|█▌       | 3527/20010 [2:57:06<14:51:06,  3.24s/batch]Batch 3600/20010 Done, mean position loss: 27.160348160266878\n",
      "Training growing_up:  18%|█▌       | 3610/20010 [2:57:08<13:49:36,  3.04s/batch]Batch 3600/20010 Done, mean position loss: 27.34574518442154\n",
      "Training growing_up:  18%|█▌       | 3577/20010 [2:57:08<13:19:31,  2.92s/batch]Batch 3600/20010 Done, mean position loss: 28.14398017168045\n",
      "Training growing_up:  18%|█▋       | 3615/20010 [2:57:11<12:40:54,  2.78s/batch]Batch 3600/20010 Done, mean position loss: 27.978115644454956\n",
      "Training growing_up:  18%|█▌       | 3605/20010 [2:57:19<13:12:35,  2.90s/batch]Batch 3600/20010 Done, mean position loss: 27.75298308134079\n",
      "Training growing_up:  18%|█▋       | 3626/20010 [2:57:28<13:50:09,  3.04s/batch]Batch 3600/20010 Done, mean position loss: 27.60830796480179\n",
      "Training growing_up:  18%|█▋       | 3634/20010 [2:57:31<12:42:58,  2.80s/batch]Batch 3600/20010 Done, mean position loss: 29.170918149948122\n",
      "Training growing_up:  18%|█▋       | 3645/20010 [2:57:32<13:00:30,  2.86s/batch]Batch 3600/20010 Done, mean position loss: 28.654855115413664\n",
      "Training growing_up:  18%|█▌       | 3582/20010 [2:57:40<14:18:05,  3.13s/batch]Batch 3600/20010 Done, mean position loss: 29.052894802093505\n",
      "Training growing_up:  18%|█▌       | 3611/20010 [2:57:46<12:29:31,  2.74s/batch]Batch 3600/20010 Done, mean position loss: 29.134053270816803\n",
      "Training growing_up:  18%|█▌       | 3542/20010 [2:57:51<14:19:48,  3.13s/batch]Batch 3600/20010 Done, mean position loss: 28.10209569454193\n",
      "Training growing_up:  18%|█▌       | 3589/20010 [2:58:02<14:00:58,  3.07s/batch]Batch 3600/20010 Done, mean position loss: 29.819607980251313\n",
      "Training growing_up:  18%|█▋       | 3615/20010 [2:58:13<13:11:04,  2.90s/batch]Batch 3600/20010 Done, mean position loss: 27.70458412885666\n",
      "Training growing_up:  18%|█▌       | 3590/20010 [2:58:22<13:10:41,  2.89s/batch]Batch 3600/20010 Done, mean position loss: 29.243468747138976\n",
      "Training growing_up:  18%|█▋       | 3668/20010 [2:58:38<13:38:43,  3.01s/batch]Batch 3600/20010 Done, mean position loss: 27.54725654125214\n",
      "Training growing_up:  18%|█▌       | 3563/20010 [2:58:55<14:11:15,  3.11s/batch]Batch 3600/20010 Done, mean position loss: 28.756036710739135\n",
      "Training growing_up:  18%|█▋       | 3638/20010 [2:58:55<13:48:46,  3.04s/batch]Batch 3600/20010 Done, mean position loss: 27.668242263793946\n",
      "Training growing_up:  18%|█▋       | 3651/20010 [2:59:05<13:39:27,  3.01s/batch]Batch 3600/20010 Done, mean position loss: 28.21127368927002\n",
      "Training growing_up:  18%|█▋       | 3637/20010 [2:59:21<14:34:01,  3.20s/batch]Batch 3700/20010 Done, mean position loss: 28.092613334655763\n",
      "Training growing_up:  18%|█▋       | 3646/20010 [2:59:49<13:36:21,  2.99s/batch]Batch 3700/20010 Done, mean position loss: 28.011793355941773\n",
      "Training growing_up:  18%|█▋       | 3616/20010 [2:59:49<13:47:30,  3.03s/batch]Batch 3600/20010 Done, mean position loss: 29.713466312885284\n",
      "Training growing_up:  18%|█▋       | 3632/20010 [2:59:58<13:44:49,  3.02s/batch]Batch 3600/20010 Done, mean position loss: 26.99411165237427\n",
      "Training growing_up:  18%|█▋       | 3620/20010 [3:00:00<12:41:10,  2.79s/batch]Batch 3600/20010 Done, mean position loss: 25.924888195991517\n",
      "Training growing_up:  18%|█▋       | 3699/20010 [3:00:15<13:22:29,  2.95s/batch]Batch 3700/20010 Done, mean position loss: 27.178837115764615\n",
      "Training growing_up:  18%|█▋       | 3646/20010 [3:00:18<12:13:26,  2.69s/batch]Batch 3700/20010 Done, mean position loss: 27.10176933526993\n",
      "Training growing_up:  18%|█▌       | 3609/20010 [3:00:21<13:24:25,  2.94s/batch]Batch 3700/20010 Done, mean position loss: 28.337313921451567\n",
      "Training growing_up:  19%|█▋       | 3703/20010 [3:00:26<12:26:18,  2.75s/batch]Batch 3700/20010 Done, mean position loss: 29.896827621459963\n",
      "Training growing_up:  18%|█▌       | 3589/20010 [3:00:29<14:27:46,  3.17s/batch]Batch 3600/20010 Done, mean position loss: 28.293748171329497\n",
      "Training growing_up:  18%|█▋       | 3671/20010 [3:00:34<14:33:03,  3.21s/batch]Batch 3700/20010 Done, mean position loss: 28.70731148481369\n",
      "Training growing_up:  19%|█▋       | 3711/20010 [3:00:50<14:17:41,  3.16s/batch]Batch 3700/20010 Done, mean position loss: 27.641063191890716\n",
      "Training growing_up:  18%|█▌       | 3609/20010 [3:00:51<12:31:25,  2.75s/batch]Batch 3700/20010 Done, mean position loss: 28.065178768634798\n",
      "Training growing_up:  18%|█▋       | 3652/20010 [3:00:53<12:31:04,  2.75s/batch]Batch 3600/20010 Done, mean position loss: 28.959499530792236\n",
      "Training growing_up:  18%|█▋       | 3646/20010 [3:00:55<12:42:02,  2.79s/batch]Batch 3600/20010 Done, mean position loss: 28.056478798389435\n",
      "Training growing_up:  18%|█▌       | 3606/20010 [3:01:09<14:19:06,  3.14s/batch]Batch 3600/20010 Done, mean position loss: 28.880108857154845\n",
      "Training growing_up:  18%|█▋       | 3683/20010 [3:01:11<14:16:29,  3.15s/batch]Batch 3700/20010 Done, mean position loss: 27.47050375699997\n",
      "Training growing_up:  18%|█▋       | 3686/20010 [3:01:19<13:43:19,  3.03s/batch]Batch 3700/20010 Done, mean position loss: 26.572392518520356\n",
      "Training growing_up:  18%|█▌       | 3610/20010 [3:01:22<13:16:41,  2.91s/batch]Batch 3700/20010 Done, mean position loss: 28.45016649246216\n",
      "Training growing_up:  18%|█▋       | 3676/20010 [3:01:22<13:50:12,  3.05s/batch]Batch 3700/20010 Done, mean position loss: 30.48232698917389\n",
      "Training growing_up:  18%|█▋       | 3690/20010 [3:01:33<12:49:02,  2.83s/batch]Batch 3700/20010 Done, mean position loss: 28.21502231121063\n",
      "Training growing_up:  18%|█▋       | 3656/20010 [3:01:40<13:23:24,  2.95s/batch]Batch 3700/20010 Done, mean position loss: 28.492651343345642\n",
      "Training growing_up:  19%|█▋       | 3715/20010 [3:02:04<13:49:46,  3.06s/batch]Batch 3700/20010 Done, mean position loss: 26.924547593593598\n",
      "Training growing_up:  18%|█▋       | 3668/20010 [3:02:04<14:06:50,  3.11s/batch]Batch 3700/20010 Done, mean position loss: 27.57858859539032\n",
      "Training growing_up:  18%|█▋       | 3694/20010 [3:02:05<13:16:35,  2.93s/batch]Batch 3700/20010 Done, mean position loss: 27.783681912422182\n",
      "Training growing_up:  18%|█▋       | 3692/20010 [3:02:06<13:48:59,  3.05s/batch]Batch 3700/20010 Done, mean position loss: 29.01115399360657\n",
      "Training growing_up:  18%|█▋       | 3691/20010 [3:02:17<13:09:21,  2.90s/batch]Batch 3700/20010 Done, mean position loss: 27.707295429706573\n",
      "Training growing_up:  19%|█▋       | 3708/20010 [3:02:26<13:28:03,  2.97s/batch]Batch 3700/20010 Done, mean position loss: 27.99846399784088\n",
      "Training growing_up:  18%|█▋       | 3697/20010 [3:02:32<15:05:59,  3.33s/batch]Batch 3700/20010 Done, mean position loss: 27.955265939235687\n",
      "Training growing_up:  19%|█▋       | 3725/20010 [3:02:33<13:52:39,  3.07s/batch]Batch 3700/20010 Done, mean position loss: 27.798887910842897\n",
      "Training growing_up:  19%|█▋       | 3744/20010 [3:02:37<11:36:01,  2.57s/batch]Batch 3700/20010 Done, mean position loss: 28.757412860393526\n",
      "Training growing_up:  18%|█▋       | 3681/20010 [3:02:45<13:04:43,  2.88s/batch]Batch 3700/20010 Done, mean position loss: 28.203722088336946\n",
      "Training growing_up:  19%|█▋       | 3753/20010 [3:02:48<12:48:57,  2.84s/batch]Batch 3700/20010 Done, mean position loss: 27.69868000745773\n",
      "Training growing_up:  19%|█▋       | 3720/20010 [3:03:03<14:42:56,  3.25s/batch]Batch 3700/20010 Done, mean position loss: 28.27857812166214\n",
      "Training growing_up:  19%|█▋       | 3715/20010 [3:03:16<13:05:10,  2.89s/batch]Batch 3700/20010 Done, mean position loss: 27.085967068672183\n",
      "Training growing_up:  18%|█▋       | 3669/20010 [3:03:22<13:06:21,  2.89s/batch]Batch 3700/20010 Done, mean position loss: 29.57150190114975\n",
      "Training growing_up:  18%|█▋       | 3698/20010 [3:03:41<11:01:08,  2.43s/batch]Batch 3700/20010 Done, mean position loss: 28.829575469493864\n",
      "Training growing_up:  19%|█▋       | 3761/20010 [3:03:47<14:26:51,  3.20s/batch]Batch 3700/20010 Done, mean position loss: 28.36198827981949\n",
      "Training growing_up:  19%|█▋       | 3730/20010 [3:03:51<12:20:59,  2.73s/batch]Batch 3700/20010 Done, mean position loss: 27.826973133087158\n",
      "Training growing_up:  19%|█▋       | 3775/20010 [3:03:53<11:49:02,  2.62s/batch]Batch 3700/20010 Done, mean position loss: 28.130722606182097\n",
      "Training growing_up:  18%|█▋       | 3659/20010 [3:04:00<15:33:55,  3.43s/batch]Batch 3800/20010 Done, mean position loss: 27.6888290810585\n",
      "Training growing_up:  19%|█▋       | 3790/20010 [3:04:34<12:39:51,  2.81s/batch]Batch 3700/20010 Done, mean position loss: 29.676867611408234\n",
      "Training growing_up:  19%|█▋       | 3815/20010 [3:04:35<12:22:45,  2.75s/batch]Batch 3800/20010 Done, mean position loss: 27.93098536014557\n",
      "Training growing_up:  19%|█▋       | 3794/20010 [3:04:50<13:09:35,  2.92s/batch]Batch 3700/20010 Done, mean position loss: 26.75219404935837\n",
      "Training growing_up:  19%|█▋       | 3748/20010 [3:04:51<11:10:55,  2.48s/batch]Batch 3700/20010 Done, mean position loss: 28.06600121736526\n",
      "Training growing_up:  19%|█▋       | 3781/20010 [3:05:03<13:19:36,  2.96s/batch]Batch 3800/20010 Done, mean position loss: 27.849880039691925\n",
      "Training growing_up:  19%|█▋       | 3755/20010 [3:05:04<13:57:18,  3.09s/batch]Batch 3800/20010 Done, mean position loss: 28.26652706384659\n",
      "Training growing_up:  19%|█▋       | 3766/20010 [3:05:08<13:52:46,  3.08s/batch]Batch 3800/20010 Done, mean position loss: 26.648682594299316\n",
      "Training growing_up:  19%|█▋       | 3768/20010 [3:05:11<12:43:14,  2.82s/batch]Batch 3800/20010 Done, mean position loss: 28.113156623840332\n",
      "Training growing_up:  19%|█▋       | 3803/20010 [3:05:16<12:30:05,  2.78s/batch]Batch 3700/20010 Done, mean position loss: 27.764601440429686\n",
      "Training growing_up:  19%|█▋       | 3716/20010 [3:05:19<13:18:15,  2.94s/batch]Batch 3800/20010 Done, mean position loss: 27.791191103458402\n",
      "Training growing_up:  19%|█▋       | 3791/20010 [3:05:33<13:34:27,  3.01s/batch]Batch 3800/20010 Done, mean position loss: 27.430101161003112\n",
      "Training growing_up:  19%|█▋       | 3786/20010 [3:05:43<13:31:09,  3.00s/batch]Batch 3800/20010 Done, mean position loss: 27.631923518180848\n",
      "Training growing_up:  19%|█▋       | 3796/20010 [3:05:44<12:47:02,  2.84s/batch]Batch 3700/20010 Done, mean position loss: 28.236334283351898\n",
      "Training growing_up:  19%|█▋       | 3779/20010 [3:05:53<13:07:44,  2.91s/batch]Batch 3700/20010 Done, mean position loss: 27.628073337078092\n",
      "Training growing_up:  19%|█▋       | 3777/20010 [3:05:56<12:18:44,  2.73s/batch]Batch 3700/20010 Done, mean position loss: 28.426747097969052\n",
      "Training growing_up:  19%|█▋       | 3800/20010 [3:05:58<12:31:01,  2.78s/batch]Batch 3800/20010 Done, mean position loss: 26.969325621128085\n",
      "Training growing_up:  19%|█▋       | 3818/20010 [3:06:01<14:22:00,  3.19s/batch]Batch 3800/20010 Done, mean position loss: 29.38041344165802\n",
      "Training growing_up:  19%|█▋       | 3747/20010 [3:06:05<14:57:25,  3.31s/batch]Batch 3800/20010 Done, mean position loss: 27.948348057270053\n",
      "Training growing_up:  19%|█▋       | 3824/20010 [3:06:09<13:04:37,  2.91s/batch]Batch 3800/20010 Done, mean position loss: 28.294252665042876\n",
      "Training growing_up:  19%|█▋       | 3779/20010 [3:06:25<14:23:14,  3.19s/batch]Batch 3800/20010 Done, mean position loss: 27.481076154708862\n",
      "Training growing_up:  19%|█▋       | 3795/20010 [3:06:28<13:28:38,  2.99s/batch]Batch 3800/20010 Done, mean position loss: 28.814751641750334\n",
      "Training growing_up:  19%|█▋       | 3745/20010 [3:06:42<11:28:02,  2.54s/batch]Batch 3800/20010 Done, mean position loss: 29.328102984428405\n",
      "Training growing_up:  19%|█▋       | 3814/20010 [3:06:45<12:18:36,  2.74s/batch]Batch 3800/20010 Done, mean position loss: 27.640121603012084\n",
      "Training growing_up:  19%|█▋       | 3762/20010 [3:06:46<13:30:29,  2.99s/batch]Batch 3800/20010 Done, mean position loss: 28.57128361463547\n",
      "Training growing_up:  19%|█▋       | 3772/20010 [3:06:46<13:12:44,  2.93s/batch]Batch 3800/20010 Done, mean position loss: 27.058530700206756\n",
      "Training growing_up:  19%|█▋       | 3820/20010 [3:06:56<12:58:12,  2.88s/batch]Batch 3800/20010 Done, mean position loss: 28.559819836616516\n",
      "Training growing_up:  19%|█▋       | 3747/20010 [3:07:10<14:21:07,  3.18s/batch]Batch 3800/20010 Done, mean position loss: 27.792848117351532\n",
      "Training growing_up:  19%|█▋       | 3858/20010 [3:07:20<12:08:25,  2.71s/batch]Batch 3800/20010 Done, mean position loss: 27.336070587635042\n",
      "Training growing_up:  19%|█▋       | 3802/20010 [3:07:22<12:24:21,  2.76s/batch]Batch 3800/20010 Done, mean position loss: 28.016825647354125\n",
      "Training growing_up:  19%|█▋       | 3731/20010 [3:07:25<13:18:10,  2.94s/batch]Batch 3800/20010 Done, mean position loss: 27.674061596393585\n",
      "Training growing_up:  19%|█▋       | 3815/20010 [3:07:26<12:58:51,  2.89s/batch]Batch 3800/20010 Done, mean position loss: 27.97927208185196\n",
      "Training growing_up:  19%|█▋       | 3817/20010 [3:07:30<11:38:21,  2.59s/batch]Batch 3800/20010 Done, mean position loss: 27.795203104019166\n",
      "Training growing_up:  19%|█▋       | 3813/20010 [3:07:54<12:44:05,  2.83s/batch]Batch 3800/20010 Done, mean position loss: 27.464929223060608\n",
      "Training growing_up:  19%|█▋       | 3830/20010 [3:08:04<13:00:18,  2.89s/batch]Batch 3800/20010 Done, mean position loss: 26.662027328014375\n",
      "Training growing_up:  19%|█▋       | 3746/20010 [3:08:10<13:25:15,  2.97s/batch]Batch 3800/20010 Done, mean position loss: 29.728869967460632\n",
      "Training growing_up:  19%|█▋       | 3798/20010 [3:08:32<12:02:10,  2.67s/batch]Batch 3800/20010 Done, mean position loss: 28.07034915924072\n",
      "Training growing_up:  19%|█▋       | 3810/20010 [3:08:37<13:27:40,  2.99s/batch]Batch 3800/20010 Done, mean position loss: 28.144873225688933\n",
      "Training growing_up:  19%|█▋       | 3758/20010 [3:08:40<11:58:31,  2.65s/batch]Batch 3900/20010 Done, mean position loss: 26.74515440940857\n",
      "Training growing_up:  19%|█▋       | 3833/20010 [3:08:41<13:34:24,  3.02s/batch]Batch 3800/20010 Done, mean position loss: 27.42466955423355\n",
      "Training growing_up:  19%|█▋       | 3780/20010 [3:08:44<13:01:10,  2.89s/batch]Batch 3800/20010 Done, mean position loss: 28.133145518302918\n",
      "Training growing_up:  19%|█▋       | 3842/20010 [3:09:20<12:53:09,  2.87s/batch]Batch 3900/20010 Done, mean position loss: 28.75157872915268\n",
      "Training growing_up:  19%|█▋       | 3844/20010 [3:09:31<12:35:39,  2.80s/batch]Batch 3800/20010 Done, mean position loss: 30.433248016834256\n",
      "Training growing_up:  19%|█▋       | 3888/20010 [3:09:44<13:35:32,  3.04s/batch]Batch 3900/20010 Done, mean position loss: 26.998181138038635\n",
      "Training growing_up:  19%|█▋       | 3885/20010 [3:09:49<14:38:51,  3.27s/batch]Batch 3800/20010 Done, mean position loss: 27.163550436496735\n",
      "Training growing_up:  19%|█▋       | 3850/20010 [3:09:51<14:52:30,  3.31s/batch]Batch 3800/20010 Done, mean position loss: 26.04631916761398\n",
      "Training growing_up:  19%|█▋       | 3871/20010 [3:09:53<15:20:20,  3.42s/batch]Batch 3900/20010 Done, mean position loss: 26.788716971874237\n",
      "Training growing_up:  19%|█▋       | 3873/20010 [3:10:04<12:03:53,  2.69s/batch]Batch 3900/20010 Done, mean position loss: 29.094282240867614\n",
      "Training growing_up:  19%|█▋       | 3787/20010 [3:10:08<14:36:16,  3.24s/batch]Batch 3900/20010 Done, mean position loss: 28.574504816532137\n",
      "Training growing_up:  19%|█▋       | 3875/20010 [3:10:09<11:41:15,  2.61s/batch]Batch 3900/20010 Done, mean position loss: 26.056502301692962\n",
      "Training growing_up:  20%|█▊       | 3912/20010 [3:10:16<11:21:57,  2.54s/batch]Batch 3800/20010 Done, mean position loss: 27.44725250482559\n",
      "Training growing_up:  19%|█▊       | 3891/20010 [3:10:27<14:51:42,  3.32s/batch]Batch 3900/20010 Done, mean position loss: 28.36228657722473\n",
      "Training growing_up:  19%|█▋       | 3849/20010 [3:10:38<12:37:50,  2.81s/batch]Batch 3900/20010 Done, mean position loss: 27.22173449754715\n",
      "Training growing_up:  19%|█▋       | 3798/20010 [3:10:42<13:33:34,  3.01s/batch]Batch 3800/20010 Done, mean position loss: 27.379134759902954\n",
      "Training growing_up:  20%|█▊       | 3916/20010 [3:10:50<12:20:39,  2.76s/batch]Batch 3900/20010 Done, mean position loss: 27.825111463069916\n",
      "Training growing_up:  20%|█▊       | 3914/20010 [3:10:50<15:07:58,  3.38s/batch]Batch 3900/20010 Done, mean position loss: 27.625099194049838\n",
      "Training growing_up:  19%|█▋       | 3860/20010 [3:10:52<14:57:17,  3.33s/batch]Batch 3800/20010 Done, mean position loss: 27.53710466146469\n",
      "Training growing_up:  19%|█▊       | 3900/20010 [3:10:54<13:03:41,  2.92s/batch]Batch 3800/20010 Done, mean position loss: 27.677050540447233\n",
      "Training growing_up:  19%|█▋       | 3885/20010 [3:10:56<14:47:06,  3.30s/batch]Batch 3900/20010 Done, mean position loss: 27.99607115983963\n",
      "Training growing_up:  19%|█▋       | 3879/20010 [3:10:56<12:13:08,  2.73s/batch]Batch 3900/20010 Done, mean position loss: 28.095706810951235\n",
      "Training growing_up:  19%|█▊       | 3898/20010 [3:11:19<11:53:21,  2.66s/batch]Batch 3900/20010 Done, mean position loss: 28.195018610954286\n",
      "Training growing_up:  20%|█▊       | 3924/20010 [3:11:21<13:42:44,  3.07s/batch]Batch 3900/20010 Done, mean position loss: 27.909986236095428\n",
      "Training growing_up:  19%|█▋       | 3884/20010 [3:11:27<11:56:17,  2.67s/batch]Batch 3900/20010 Done, mean position loss: 27.9381097817421\n",
      "Training growing_up:  20%|█▊       | 3917/20010 [3:11:43<14:01:37,  3.14s/batch]Batch 3900/20010 Done, mean position loss: 27.317361598014834\n",
      "Training growing_up:  19%|█▋       | 3830/20010 [3:11:45<13:49:50,  3.08s/batch]Batch 3900/20010 Done, mean position loss: 27.75816264629364\n",
      "Training growing_up:  20%|█▊       | 3919/20010 [3:11:45<13:52:22,  3.10s/batch]Batch 3900/20010 Done, mean position loss: 26.950299007892607\n",
      "Training growing_up:  20%|█▊       | 3912/20010 [3:11:53<13:31:44,  3.03s/batch]Batch 3900/20010 Done, mean position loss: 27.74598685503006\n",
      "Training growing_up:  20%|█▊       | 3924/20010 [3:12:00<12:37:10,  2.82s/batch]Batch 3900/20010 Done, mean position loss: 27.217242617607116\n",
      "Training growing_up:  19%|█▊       | 3899/20010 [3:12:16<14:15:26,  3.19s/batch]Batch 3900/20010 Done, mean position loss: 27.620213956832885\n",
      "Training growing_up:  20%|█▊       | 3913/20010 [3:12:19<14:03:33,  3.14s/batch]Batch 3900/20010 Done, mean position loss: 30.581661689281464\n",
      "Training growing_up:  20%|█▊       | 3962/20010 [3:12:23<13:02:49,  2.93s/batch]Batch 3900/20010 Done, mean position loss: 28.045538597106933\n",
      "Training growing_up:  20%|█▊       | 3922/20010 [3:12:28<12:28:19,  2.79s/batch]Batch 3900/20010 Done, mean position loss: 28.15066369533539\n",
      "Training growing_up:  20%|█▊       | 3932/20010 [3:12:29<14:36:07,  3.27s/batch]Batch 3900/20010 Done, mean position loss: 27.86521735906601\n",
      "Training growing_up:  19%|█▋       | 3885/20010 [3:12:56<13:29:40,  3.01s/batch]Batch 3900/20010 Done, mean position loss: 29.646081881523134\n",
      "Training growing_up:  19%|█▋       | 3855/20010 [3:13:02<14:07:02,  3.15s/batch]Batch 3900/20010 Done, mean position loss: 27.90212208032608\n",
      "Training growing_up:  20%|█▊       | 3918/20010 [3:13:14<14:12:20,  3.18s/batch]Batch 3900/20010 Done, mean position loss: 29.32235519170761\n",
      "Training growing_up:  20%|█▊       | 3944/20010 [3:13:33<12:10:09,  2.73s/batch]Batch 4000/20010 Done, mean position loss: 28.117558166980743\n",
      "Training growing_up:  20%|█▊       | 3928/20010 [3:13:34<12:38:25,  2.83s/batch]Batch 3900/20010 Done, mean position loss: 27.224043018817902\n",
      "Training growing_up:  20%|█▊       | 3987/20010 [3:13:35<13:02:35,  2.93s/batch]Batch 3900/20010 Done, mean position loss: 27.29196619749069\n",
      "Training growing_up:  20%|█▊       | 3945/20010 [3:13:35<12:01:12,  2.69s/batch]Batch 3900/20010 Done, mean position loss: 27.341401658058167\n",
      "Training growing_up:  20%|█▊       | 3981/20010 [3:13:42<12:53:43,  2.90s/batch]Batch 3900/20010 Done, mean position loss: 28.836910107135772\n",
      "Training growing_up:  19%|█▋       | 3879/20010 [3:14:15<13:27:33,  3.00s/batch]Batch 4000/20010 Done, mean position loss: 27.416557912826537\n",
      "Training growing_up:  20%|█▊       | 3972/20010 [3:14:31<13:30:54,  3.03s/batch]Batch 3900/20010 Done, mean position loss: 34.34714027166366\n",
      "Training growing_up:  20%|█▊       | 3955/20010 [3:14:42<11:52:35,  2.66s/batch]Batch 4000/20010 Done, mean position loss: 27.258191170692445\n",
      "Training growing_up:  20%|█▊       | 3923/20010 [3:14:46<14:31:43,  3.25s/batch]Batch 4000/20010 Done, mean position loss: 27.748309755325316\n",
      "Training growing_up:  20%|█▊       | 3996/20010 [3:14:48<13:46:40,  3.10s/batch]Batch 3900/20010 Done, mean position loss: 29.250362520217895\n",
      "Training growing_up:  19%|█▋       | 3881/20010 [3:14:53<14:50:19,  3.31s/batch]Batch 3900/20010 Done, mean position loss: 26.76615317106247\n",
      "Training growing_up:  20%|█▊       | 3989/20010 [3:14:59<13:27:33,  3.02s/batch]Batch 4000/20010 Done, mean position loss: 26.394223444461822\n",
      "Training growing_up:  20%|█▊       | 3977/20010 [3:15:03<12:46:26,  2.87s/batch]Batch 4000/20010 Done, mean position loss: 26.390934603214262\n",
      "Training growing_up:  19%|█▋       | 3888/20010 [3:15:04<13:41:55,  3.06s/batch]Batch 4000/20010 Done, mean position loss: 29.67908753633499\n",
      "Training growing_up:  20%|█▊       | 3959/20010 [3:15:22<13:06:09,  2.94s/batch]Batch 3900/20010 Done, mean position loss: 27.859746963977813\n",
      "Training growing_up:  19%|█▊       | 3891/20010 [3:15:24<13:39:01,  3.05s/batch]Batch 4000/20010 Done, mean position loss: 28.909739022254943\n",
      "Training growing_up:  20%|█▊       | 4022/20010 [3:15:38<12:36:15,  2.84s/batch]Batch 4000/20010 Done, mean position loss: 26.67126668214798\n",
      "Training growing_up:  20%|█▊       | 3968/20010 [3:15:44<12:27:29,  2.80s/batch]Batch 3900/20010 Done, mean position loss: 27.692636797428133\n",
      "Training growing_up:  19%|█▊       | 3900/20010 [3:15:45<12:40:56,  2.83s/batch]Batch 4000/20010 Done, mean position loss: 27.563465151786804\n",
      "Training growing_up:  20%|█▊       | 3946/20010 [3:15:46<13:12:25,  2.96s/batch]Batch 4000/20010 Done, mean position loss: 25.854577052593232\n",
      "Training growing_up:  20%|█▊       | 3970/20010 [3:15:47<13:11:14,  2.96s/batch]Batch 3900/20010 Done, mean position loss: 27.65041859865189\n",
      "Training growing_up:  20%|█▊       | 4017/20010 [3:15:48<12:56:51,  2.91s/batch]Batch 4000/20010 Done, mean position loss: 27.914844474792478\n",
      "Training growing_up:  20%|█▊       | 3976/20010 [3:15:55<14:11:27,  3.19s/batch]Batch 4000/20010 Done, mean position loss: 27.789310657978056\n",
      "Training growing_up:  20%|█▊       | 3982/20010 [3:15:55<14:07:38,  3.17s/batch]Batch 3900/20010 Done, mean position loss: 27.588862063884736\n",
      "Training growing_up:  20%|█▊       | 3928/20010 [3:16:16<14:45:19,  3.30s/batch]Batch 4000/20010 Done, mean position loss: 27.46883628368378\n",
      "Training growing_up:  20%|█▊       | 3986/20010 [3:16:18<13:07:35,  2.95s/batch]Batch 4000/20010 Done, mean position loss: 27.48288049697876\n",
      "Training growing_up:  20%|█▊       | 3985/20010 [3:16:22<13:43:09,  3.08s/batch]Batch 4000/20010 Done, mean position loss: 30.509070641994477\n",
      "Training growing_up:  20%|█▊       | 3989/20010 [3:16:28<14:05:43,  3.17s/batch]Batch 4000/20010 Done, mean position loss: 26.58454243659973\n",
      "Training growing_up:  20%|█▊       | 4024/20010 [3:16:33<14:14:20,  3.21s/batch]Batch 4000/20010 Done, mean position loss: 26.050402069091795\n",
      "Training growing_up:  20%|█▊       | 4051/20010 [3:16:40<11:43:47,  2.65s/batch]Batch 4000/20010 Done, mean position loss: 26.753612306118008\n",
      "Training growing_up:  20%|█▊       | 4011/20010 [3:16:48<12:38:19,  2.84s/batch]Batch 4000/20010 Done, mean position loss: 28.187519457340244\n",
      "Training growing_up:  20%|█▊       | 4049/20010 [3:17:03<13:26:57,  3.03s/batch]Batch 4000/20010 Done, mean position loss: 26.74383163928986\n",
      "Training growing_up:  20%|█▊       | 4012/20010 [3:17:10<12:14:28,  2.75s/batch]Batch 4000/20010 Done, mean position loss: 27.168550748825073\n",
      "Training growing_up:  20%|█▊       | 3984/20010 [3:17:20<12:20:07,  2.77s/batch]Batch 4000/20010 Done, mean position loss: 28.844460864067077\n",
      "Training growing_up:  20%|█▊       | 3958/20010 [3:17:21<13:20:34,  2.99s/batch]Batch 4000/20010 Done, mean position loss: 27.23579690217972\n",
      "Training growing_up:  20%|█▊       | 4033/20010 [3:17:24<13:22:06,  3.01s/batch]Batch 4000/20010 Done, mean position loss: 27.294718074798585\n",
      "Training growing_up:  20%|█▊       | 4024/20010 [3:17:24<12:09:15,  2.74s/batch]Batch 4000/20010 Done, mean position loss: 26.792321412563325\n",
      "Training growing_up:  20%|█▊       | 4033/20010 [3:17:51<11:45:37,  2.65s/batch]Batch 4000/20010 Done, mean position loss: 29.060460445880892\n",
      "Training growing_up:  20%|█▊       | 3989/20010 [3:17:53<13:18:13,  2.99s/batch]Batch 4000/20010 Done, mean position loss: 27.84153581380844\n",
      "Training growing_up:  20%|█▊       | 3975/20010 [3:18:05<11:51:02,  2.66s/batch]Batch 4000/20010 Done, mean position loss: 29.530369753837583\n",
      "Training growing_up:  20%|█▊       | 4044/20010 [3:18:15<12:38:58,  2.85s/batch]Batch 4000/20010 Done, mean position loss: 26.882006230354307\n",
      "Training growing_up:  20%|█▊       | 4020/20010 [3:18:16<11:58:14,  2.70s/batch]Batch 4000/20010 Done, mean position loss: 26.699971032142642\n",
      "Training growing_up:  20%|█▊       | 4033/20010 [3:18:19<12:16:45,  2.77s/batch]Batch 4100/20010 Done, mean position loss: 27.483919699192047\n",
      "Training growing_up:  20%|█▊       | 4008/20010 [3:18:24<12:47:35,  2.88s/batch]Batch 4000/20010 Done, mean position loss: 27.673405716419218\n",
      "Training growing_up:  20%|█▊       | 4006/20010 [3:18:29<11:19:27,  2.55s/batch]Batch 4000/20010 Done, mean position loss: 28.0769749212265\n",
      "Training growing_up:  20%|█▊       | 4059/20010 [3:18:59<12:53:34,  2.91s/batch]Batch 4100/20010 Done, mean position loss: 28.02588891506195\n",
      "Training growing_up:  21%|█▊       | 4122/20010 [3:19:21<13:38:59,  3.09s/batch]Batch 4000/20010 Done, mean position loss: 30.916923089027403\n",
      "Training growing_up:  20%|█▊       | 3994/20010 [3:19:23<13:02:19,  2.93s/batch]Batch 4100/20010 Done, mean position loss: 26.429054915905\n",
      "Training growing_up:  20%|█▊       | 4021/20010 [3:19:27<12:59:45,  2.93s/batch]Batch 4100/20010 Done, mean position loss: 27.771768097877505\n",
      "Training growing_up:  20%|█▊       | 4027/20010 [3:19:40<13:04:32,  2.95s/batch]Batch 4100/20010 Done, mean position loss: 28.164876158237455\n",
      "Training growing_up:  20%|█▊       | 4074/20010 [3:19:43<12:52:40,  2.91s/batch]Batch 4000/20010 Done, mean position loss: 27.894273345470427\n",
      "Training growing_up:  20%|█▊       | 4031/20010 [3:19:45<13:18:13,  3.00s/batch]Batch 4100/20010 Done, mean position loss: 26.632166030406953\n",
      "Training growing_up:  20%|█▊       | 4034/20010 [3:19:49<13:34:07,  3.06s/batch]Batch 4000/20010 Done, mean position loss: 26.27609464883804\n",
      "Training growing_up:  20%|█▊       | 4031/20010 [3:19:51<12:51:17,  2.90s/batch]Batch 4100/20010 Done, mean position loss: 26.800943396091462\n",
      "Training growing_up:  21%|█▊       | 4117/20010 [3:20:09<13:16:20,  3.01s/batch]Batch 4100/20010 Done, mean position loss: 27.988596365451812\n",
      "Training growing_up:  20%|█▊       | 4048/20010 [3:20:10<12:22:39,  2.79s/batch]Batch 4000/20010 Done, mean position loss: 28.473459661006927\n",
      "Training growing_up:  20%|█▊       | 3997/20010 [3:20:20<13:04:06,  2.94s/batch]Batch 4100/20010 Done, mean position loss: 27.94172287464142\n",
      "Training growing_up:  20%|█▊       | 4064/20010 [3:20:22<13:06:58,  2.96s/batch]Batch 4100/20010 Done, mean position loss: 27.11373391151428\n",
      "Training growing_up:  20%|█▊       | 4046/20010 [3:20:32<12:58:36,  2.93s/batch]Batch 4000/20010 Done, mean position loss: 28.807428863048553\n",
      "Training growing_up:  21%|█▊       | 4150/20010 [3:20:36<11:06:23,  2.52s/batch]Batch 4100/20010 Done, mean position loss: 27.61804785013199\n",
      "Training growing_up:  20%|█▊       | 4101/20010 [3:20:37<12:24:28,  2.81s/batch]Batch 4100/20010 Done, mean position loss: 27.976088683605195\n",
      "Training growing_up:  20%|█▊       | 4089/20010 [3:20:41<12:30:08,  2.83s/batch]Batch 4000/20010 Done, mean position loss: 27.244800355434418\n",
      "Training growing_up:  21%|█▊       | 4109/20010 [3:20:42<12:47:13,  2.89s/batch]Batch 4100/20010 Done, mean position loss: 28.986069688796995\n",
      "Training growing_up:  20%|█▊       | 4058/20010 [3:20:52<12:54:27,  2.91s/batch]Batch 4000/20010 Done, mean position loss: 27.167528781890866\n",
      "Training growing_up:  20%|█▊       | 4008/20010 [3:21:00<11:46:56,  2.65s/batch]Batch 4100/20010 Done, mean position loss: 27.32000732898712\n",
      "Training growing_up:  21%|█▊       | 4130/20010 [3:21:03<12:53:26,  2.92s/batch]Batch 4100/20010 Done, mean position loss: 28.67186922311783\n",
      "Training growing_up:  20%|█▊       | 4068/20010 [3:21:07<12:40:11,  2.86s/batch]Batch 4100/20010 Done, mean position loss: 27.76987958431244\n",
      "Training growing_up:  20%|█▊       | 4080/20010 [3:21:09<12:49:13,  2.90s/batch]Batch 4100/20010 Done, mean position loss: 27.26009593009949\n",
      "Training growing_up:  21%|█▊       | 4103/20010 [3:21:14<12:07:36,  2.74s/batch]Batch 4100/20010 Done, mean position loss: 27.19804526090622\n",
      "Training growing_up:  20%|█▊       | 4072/20010 [3:21:20<12:29:44,  2.82s/batch]Batch 4100/20010 Done, mean position loss: 27.13093787431717\n",
      "Training growing_up:  20%|█▊       | 4088/20010 [3:21:32<12:49:25,  2.90s/batch]Batch 4100/20010 Done, mean position loss: 27.109906706809998\n",
      "Training growing_up:  20%|█▊       | 4020/20010 [3:21:48<12:59:36,  2.93s/batch]Batch 4100/20010 Done, mean position loss: 29.33281256914139\n",
      "Training growing_up:  20%|█▊       | 4095/20010 [3:21:53<12:18:31,  2.78s/batch]Batch 4100/20010 Done, mean position loss: 27.458832302093505\n",
      "Training growing_up:  21%|█▊       | 4118/20010 [3:22:07<12:09:40,  2.75s/batch]Batch 4100/20010 Done, mean position loss: 27.37432754993439\n",
      "Training growing_up:  21%|█▊       | 4125/20010 [3:22:09<11:49:40,  2.68s/batch]Batch 4100/20010 Done, mean position loss: 27.85554104804993\n",
      "Training growing_up:  20%|█▊       | 4089/20010 [3:22:11<13:39:50,  3.09s/batch]Batch 4100/20010 Done, mean position loss: 28.046799602508546\n",
      "Training growing_up:  20%|█▊       | 4101/20010 [3:22:11<13:05:37,  2.96s/batch]Batch 4100/20010 Done, mean position loss: 27.485453476905825\n",
      "Training growing_up:  20%|█▊       | 4095/20010 [3:22:36<12:43:51,  2.88s/batch]Batch 4100/20010 Done, mean position loss: 28.037235238552093\n",
      "Training growing_up:  21%|█▊       | 4112/20010 [3:22:43<13:25:50,  3.04s/batch]Batch 4100/20010 Done, mean position loss: 28.075894756317137\n",
      "Training growing_up:  20%|█▊       | 4073/20010 [3:22:54<14:21:10,  3.24s/batch]Batch 4100/20010 Done, mean position loss: 28.979192967414857\n",
      "Training growing_up:  20%|█▊       | 4097/20010 [3:22:56<13:07:27,  2.97s/batch]Batch 4200/20010 Done, mean position loss: 26.586721544265746\n",
      "Training growing_up:  21%|█▊       | 4133/20010 [3:23:04<13:15:31,  3.01s/batch]Batch 4100/20010 Done, mean position loss: 28.87994587659836\n",
      "Training growing_up:  21%|█▊       | 4152/20010 [3:23:07<15:07:04,  3.43s/batch]Batch 4100/20010 Done, mean position loss: 27.242808973789217\n",
      "Training growing_up:  21%|█▊       | 4154/20010 [3:23:15<13:57:11,  3.17s/batch]Batch 4100/20010 Done, mean position loss: 28.278908081054688\n",
      "Training growing_up:  21%|█▊       | 4162/20010 [3:23:19<13:42:06,  3.11s/batch]Batch 4100/20010 Done, mean position loss: 27.951990830898286\n",
      "Training growing_up:  21%|█▉       | 4183/20010 [3:23:48<13:12:53,  3.01s/batch]Batch 4200/20010 Done, mean position loss: 27.902648825645446\n",
      "Training growing_up:  21%|█▉       | 4226/20010 [3:24:11<13:03:41,  2.98s/batch]Batch 4200/20010 Done, mean position loss: 26.24979637384415\n",
      "Training growing_up:  21%|█▊       | 4166/20010 [3:24:11<13:02:17,  2.96s/batch]Batch 4200/20010 Done, mean position loss: 26.79294021844864\n",
      "Training growing_up:  21%|█▊       | 4146/20010 [3:24:17<12:39:06,  2.87s/batch]Batch 4100/20010 Done, mean position loss: 29.13887853384018\n",
      "Training growing_up:  20%|█▊       | 4094/20010 [3:24:23<12:42:40,  2.88s/batch]Batch 4200/20010 Done, mean position loss: 28.709364650249483\n",
      "Training growing_up:  21%|█▊       | 4108/20010 [3:24:38<13:16:42,  3.01s/batch]Batch 4200/20010 Done, mean position loss: 26.03634801864624\n",
      "Training growing_up:  21%|█▉       | 4185/20010 [3:24:39<12:45:53,  2.90s/batch]Batch 4100/20010 Done, mean position loss: 26.759958846569063\n",
      "Training growing_up:  21%|█▉       | 4212/20010 [3:24:42<11:51:16,  2.70s/batch]Batch 4200/20010 Done, mean position loss: 26.697434010505678\n",
      "Training growing_up:  21%|█▊       | 4151/20010 [3:24:44<13:27:18,  3.05s/batch]Batch 4100/20010 Done, mean position loss: 27.532023322582248\n",
      "Training growing_up:  21%|█▉       | 4191/20010 [3:25:08<14:35:49,  3.32s/batch]Batch 4100/20010 Done, mean position loss: 28.784225227832795\n",
      "Training growing_up:  21%|█▉       | 4228/20010 [3:25:09<14:11:06,  3.24s/batch]Batch 4200/20010 Done, mean position loss: 27.69511131763458\n",
      "Training growing_up:  21%|█▉       | 4176/20010 [3:25:12<12:33:39,  2.86s/batch]Batch 4200/20010 Done, mean position loss: 27.439606180191042\n",
      "Training growing_up:  21%|█▉       | 4202/20010 [3:25:15<12:40:32,  2.89s/batch]Batch 4200/20010 Done, mean position loss: 26.231246123313902\n",
      "Training growing_up:  21%|█▊       | 4154/20010 [3:25:26<15:48:41,  3.59s/batch]Batch 4200/20010 Done, mean position loss: 28.115198526382446\n",
      "Training growing_up:  21%|█▊       | 4162/20010 [3:25:37<12:47:18,  2.91s/batch]Batch 4100/20010 Done, mean position loss: 28.82200532913208\n",
      "Training growing_up:  21%|█▊       | 4119/20010 [3:25:37<13:43:42,  3.11s/batch]Batch 4200/20010 Done, mean position loss: 28.679071865081788\n",
      "Training growing_up:  21%|█▉       | 4230/20010 [3:25:38<14:05:41,  3.22s/batch]Batch 4100/20010 Done, mean position loss: 26.58075573682785\n",
      "Training growing_up:  21%|█▉       | 4171/20010 [3:25:39<13:04:58,  2.97s/batch]Batch 4200/20010 Done, mean position loss: 27.737166848182678\n",
      "Training growing_up:  21%|█▉       | 4206/20010 [3:25:52<12:32:43,  2.86s/batch]Batch 4100/20010 Done, mean position loss: 26.530010998249054\n",
      "Training growing_up:  21%|█▉       | 4178/20010 [3:25:58<12:29:47,  2.84s/batch]Batch 4200/20010 Done, mean position loss: 27.523187489509585\n",
      "Training growing_up:  21%|█▉       | 4216/20010 [3:26:00<14:40:41,  3.35s/batch]Batch 4200/20010 Done, mean position loss: 27.857958443164826\n",
      "Training growing_up:  21%|█▉       | 4234/20010 [3:26:01<12:32:42,  2.86s/batch]Batch 4200/20010 Done, mean position loss: 27.516126115322116\n",
      "Training growing_up:  21%|█▉       | 4203/20010 [3:26:06<12:44:35,  2.90s/batch]Batch 4200/20010 Done, mean position loss: 28.702744143009184\n",
      "Training growing_up:  21%|█▊       | 4107/20010 [3:26:09<12:22:33,  2.80s/batch]Batch 4200/20010 Done, mean position loss: 26.531692049503327\n",
      "Training growing_up:  21%|█▉       | 4203/20010 [3:26:10<11:26:03,  2.60s/batch]Batch 4200/20010 Done, mean position loss: 26.43818128824234\n",
      "Training growing_up:  21%|█▉       | 4217/20010 [3:26:26<12:24:38,  2.83s/batch]Batch 4200/20010 Done, mean position loss: 27.55513947963715\n",
      "Training growing_up:  21%|█▉       | 4243/20010 [3:26:46<13:06:02,  2.99s/batch]Batch 4200/20010 Done, mean position loss: 26.785008316040038\n",
      "Training growing_up:  21%|█▉       | 4194/20010 [3:26:46<13:25:55,  3.06s/batch]Batch 4200/20010 Done, mean position loss: 28.13801892757416\n",
      "Training growing_up:  21%|█▉       | 4253/20010 [3:27:00<12:57:37,  2.96s/batch]Batch 4200/20010 Done, mean position loss: 26.921867899894714\n",
      "Training growing_up:  21%|█▉       | 4208/20010 [3:27:08<13:17:54,  3.03s/batch]Batch 4200/20010 Done, mean position loss: 28.12978687286377\n",
      "Training growing_up:  21%|█▉       | 4287/20010 [3:27:08<12:46:45,  2.93s/batch]Batch 4200/20010 Done, mean position loss: 27.20362103700638\n",
      "Training growing_up:  21%|█▊       | 4131/20010 [3:27:19<14:38:46,  3.32s/batch]Batch 4200/20010 Done, mean position loss: 28.00132426261902\n",
      "Training growing_up:  21%|█▉       | 4261/20010 [3:27:34<13:15:06,  3.03s/batch]Batch 4200/20010 Done, mean position loss: 27.683623006343844\n",
      "Training growing_up:  21%|█▉       | 4216/20010 [3:27:45<12:06:07,  2.76s/batch]Batch 4200/20010 Done, mean position loss: 28.39229334831238\n",
      "Training growing_up:  21%|█▊       | 4163/20010 [3:27:50<12:46:57,  2.90s/batch]Batch 4300/20010 Done, mean position loss: 26.698628742694858\n",
      "Training growing_up:  21%|█▉       | 4205/20010 [3:27:57<12:14:47,  2.79s/batch]Batch 4200/20010 Done, mean position loss: 28.373747520446777\n",
      "Training growing_up:  21%|█▉       | 4243/20010 [3:28:01<12:09:42,  2.78s/batch]Batch 4200/20010 Done, mean position loss: 27.756755805015565\n",
      "Training growing_up:  21%|█▉       | 4212/20010 [3:28:07<12:00:01,  2.73s/batch]Batch 4200/20010 Done, mean position loss: 27.20810741186142\n",
      "Training growing_up:  21%|█▉       | 4209/20010 [3:28:08<12:02:36,  2.74s/batch]Batch 4200/20010 Done, mean position loss: 27.718022367954255\n",
      "Training growing_up:  21%|█▉       | 4213/20010 [3:28:20<12:55:52,  2.95s/batch]Batch 4200/20010 Done, mean position loss: 28.52295536994934\n",
      "Training growing_up:  21%|█▉       | 4210/20010 [3:28:47<12:54:35,  2.94s/batch]Batch 4300/20010 Done, mean position loss: 28.15788330078125\n",
      "Training growing_up:  21%|█▉       | 4221/20010 [3:29:07<11:50:08,  2.70s/batch]Batch 4300/20010 Done, mean position loss: 26.698597190380095\n",
      "Training growing_up:  21%|█▉       | 4297/20010 [3:29:12<13:29:36,  3.09s/batch]Batch 4300/20010 Done, mean position loss: 26.04542725801468\n",
      "Training growing_up:  21%|█▉       | 4286/20010 [3:29:20<11:56:20,  2.73s/batch]Batch 4200/20010 Done, mean position loss: 28.23426906108856\n",
      "Training growing_up:  21%|█▉       | 4285/20010 [3:29:23<13:18:03,  3.05s/batch]Batch 4300/20010 Done, mean position loss: 28.213072423934936\n",
      "Training growing_up:  21%|█▉       | 4241/20010 [3:29:37<13:16:49,  3.03s/batch]Batch 4300/20010 Done, mean position loss: 26.935237431526183\n",
      "Training growing_up:  21%|█▉       | 4275/20010 [3:29:39<13:21:52,  3.06s/batch]Batch 4300/20010 Done, mean position loss: 27.111057016849518\n",
      "Training growing_up:  21%|█▉       | 4284/20010 [3:29:44<12:25:17,  2.84s/batch]Batch 4200/20010 Done, mean position loss: 26.98747500181198\n",
      "Training growing_up:  21%|█▉       | 4234/20010 [3:29:45<12:04:30,  2.76s/batch]Batch 4200/20010 Done, mean position loss: 26.3715066409111\n",
      "Training growing_up:  21%|█▉       | 4208/20010 [3:30:05<12:53:25,  2.94s/batch]Batch 4300/20010 Done, mean position loss: 27.433764061927796\n",
      "Training growing_up:  21%|█▉       | 4191/20010 [3:30:10<12:29:15,  2.84s/batch]Batch 4300/20010 Done, mean position loss: 27.41893816947937\n",
      "Training growing_up:  21%|█▉       | 4292/20010 [3:30:11<15:08:21,  3.47s/batch]Batch 4300/20010 Done, mean position loss: 28.17732877731323\n",
      "Training growing_up:  22%|█▉       | 4349/20010 [3:30:11<12:37:44,  2.90s/batch]Batch 4200/20010 Done, mean position loss: 27.4649374294281\n",
      "Training growing_up:  21%|█▉       | 4270/20010 [3:30:27<11:24:37,  2.61s/batch]Batch 4300/20010 Done, mean position loss: 27.740163278579708\n",
      "Training growing_up:  21%|█▉       | 4245/20010 [3:30:30<12:26:06,  2.84s/batch]Batch 4200/20010 Done, mean position loss: 27.72191206932068\n",
      "Training growing_up:  22%|█▉       | 4321/20010 [3:30:35<11:43:58,  2.69s/batch]Batch 4300/20010 Done, mean position loss: 28.061121215820314\n",
      "Training growing_up:  22%|█▉       | 4357/20010 [3:30:37<13:26:19,  3.09s/batch]Batch 4300/20010 Done, mean position loss: 27.864268231391907\n",
      "Training growing_up:  21%|█▉       | 4302/20010 [3:30:40<11:57:33,  2.74s/batch]Batch 4200/20010 Done, mean position loss: 27.34146782159805\n",
      "Training growing_up:  21%|█▉       | 4300/20010 [3:30:54<12:55:33,  2.96s/batch]Batch 4200/20010 Done, mean position loss: 26.195901288986207\n",
      "Training growing_up:  22%|█▉       | 4363/20010 [3:30:54<12:26:30,  2.86s/batch]Batch 4300/20010 Done, mean position loss: 26.959282565116883\n",
      "Training growing_up:  22%|█▉       | 4311/20010 [3:30:56<12:20:02,  2.83s/batch]Batch 4300/20010 Done, mean position loss: 26.87170033454895\n",
      "Training growing_up:  21%|█▉       | 4280/20010 [3:30:57<13:14:31,  3.03s/batch]Batch 4300/20010 Done, mean position loss: 28.561329038143157\n",
      "Training growing_up:  21%|█▉       | 4273/20010 [3:31:01<14:51:30,  3.40s/batch]Batch 4300/20010 Done, mean position loss: 27.278463351726533\n",
      "Training growing_up:  22%|█▉       | 4331/20010 [3:31:07<13:28:38,  3.09s/batch]Batch 4300/20010 Done, mean position loss: 26.74439956188202\n",
      "Training growing_up:  22%|█▉       | 4341/20010 [3:31:10<13:06:42,  3.01s/batch]Batch 4300/20010 Done, mean position loss: 26.37601563692093\n",
      "Training growing_up:  21%|█▉       | 4265/20010 [3:31:27<14:01:39,  3.21s/batch]Batch 4300/20010 Done, mean position loss: 27.32531129360199\n",
      "Training growing_up:  22%|█▉       | 4376/20010 [3:31:33<12:33:10,  2.89s/batch]Batch 4300/20010 Done, mean position loss: 27.731373131275177\n",
      "Training growing_up:  22%|█▉       | 4330/20010 [3:31:38<12:07:47,  2.78s/batch]Batch 4300/20010 Done, mean position loss: 27.626503796577452\n",
      "Training growing_up:  21%|█▉       | 4251/20010 [3:31:52<13:21:42,  3.05s/batch]Batch 4300/20010 Done, mean position loss: 27.115642292499544\n",
      "Training growing_up:  22%|█▉       | 4348/20010 [3:31:53<13:52:25,  3.19s/batch]Batch 4300/20010 Done, mean position loss: 26.667517049312593\n",
      "Training growing_up:  21%|█▉       | 4231/20010 [3:32:05<10:13:26,  2.33s/batch]Batch 4300/20010 Done, mean position loss: 26.680414855480194\n",
      "Training growing_up:  22%|█▉       | 4344/20010 [3:32:18<12:20:22,  2.84s/batch]Batch 4300/20010 Done, mean position loss: 28.09079804420471\n",
      "Training growing_up:  22%|█▉       | 4370/20010 [3:32:33<11:45:22,  2.71s/batch]Batch 4300/20010 Done, mean position loss: 27.43125471353531\n",
      "Training growing_up:  22%|█▉       | 4382/20010 [3:32:39<12:31:24,  2.88s/batch]Batch 4300/20010 Done, mean position loss: 26.89700459957123\n",
      "Training growing_up:  22%|█▉       | 4364/20010 [3:32:39<11:47:02,  2.71s/batch]Batch 4400/20010 Done, mean position loss: 26.51339203119278\n",
      "Training growing_up:  21%|█▉       | 4269/20010 [3:32:44<13:07:27,  3.00s/batch]Batch 4300/20010 Done, mean position loss: 29.77900962352753\n",
      "Training growing_up:  22%|█▉       | 4356/20010 [3:32:44<11:20:51,  2.61s/batch]Batch 4300/20010 Done, mean position loss: 26.277563252449035\n",
      "Training growing_up:  22%|█▉       | 4316/20010 [3:32:57<11:23:09,  2.61s/batch]Batch 4300/20010 Done, mean position loss: 27.841013643741608\n",
      "Training growing_up:  22%|█▉       | 4336/20010 [3:33:01<13:13:17,  3.04s/batch]Batch 4300/20010 Done, mean position loss: 27.259648241996764\n",
      "Training growing_up:  22%|█▉       | 4345/20010 [3:33:06<11:15:03,  2.59s/batch]Batch 4300/20010 Done, mean position loss: 27.97069125890732\n",
      "Training growing_up:  22%|█▉       | 4338/20010 [3:33:27<10:15:31,  2.36s/batch]Batch 4400/20010 Done, mean position loss: 28.287217943668367\n",
      "Training growing_up:  22%|█▉       | 4338/20010 [3:33:42<11:56:21,  2.74s/batch]Batch 4400/20010 Done, mean position loss: 27.414507508277893\n",
      "Training growing_up:  21%|█▉       | 4271/20010 [3:33:47<11:49:57,  2.71s/batch]Batch 4400/20010 Done, mean position loss: 27.49164710998535\n",
      "Training growing_up:  22%|█▉       | 4366/20010 [3:33:51<11:36:14,  2.67s/batch]Batch 4400/20010 Done, mean position loss: 28.099724071025847\n",
      "Training growing_up:  22%|█▉       | 4336/20010 [3:34:07<13:29:02,  3.10s/batch]Batch 4300/20010 Done, mean position loss: 27.99617207527161\n",
      "Training growing_up:  22%|█▉       | 4363/20010 [3:34:13<11:49:16,  2.72s/batch]Batch 4400/20010 Done, mean position loss: 26.420232198238374\n",
      "Training growing_up:  22%|█▉       | 4330/20010 [3:34:20<12:29:00,  2.87s/batch]Batch 4300/20010 Done, mean position loss: 27.727837541103362\n",
      "Training growing_up:  22%|█▉       | 4366/20010 [3:34:20<13:30:06,  3.11s/batch]Batch 4400/20010 Done, mean position loss: 26.73414088964462\n",
      "Training growing_up:  22%|█▉       | 4384/20010 [3:34:27<13:45:29,  3.17s/batch]Batch 4300/20010 Done, mean position loss: 27.52935061454773\n",
      "Training growing_up:  22%|█▉       | 4411/20010 [3:34:49<11:40:57,  2.70s/batch]Batch 4300/20010 Done, mean position loss: 26.49372456789017\n",
      "Training growing_up:  21%|█▉       | 4301/20010 [3:34:49<14:42:48,  3.37s/batch]Batch 4400/20010 Done, mean position loss: 28.31896803379059\n",
      "Training growing_up:  22%|█▉       | 4336/20010 [3:34:49<13:19:08,  3.06s/batch]Batch 4400/20010 Done, mean position loss: 26.7205983877182\n",
      "Training growing_up:  21%|█▉       | 4296/20010 [3:34:57<13:21:15,  3.06s/batch]Batch 4400/20010 Done, mean position loss: 27.75866517066956\n",
      "Training growing_up:  22%|█▉       | 4376/20010 [3:35:10<12:39:23,  2.91s/batch]Batch 4300/20010 Done, mean position loss: 27.078864476680756\n",
      "Training growing_up:  22%|█▉       | 4374/20010 [3:35:15<12:52:55,  2.97s/batch]Batch 4400/20010 Done, mean position loss: 27.61464872121811\n",
      "Training growing_up:  22%|█▉       | 4319/20010 [3:35:16<12:59:34,  2.98s/batch]Batch 4400/20010 Done, mean position loss: 26.84576925754547\n",
      "Training growing_up:  22%|██       | 4460/20010 [3:35:21<11:54:22,  2.76s/batch]Batch 4300/20010 Done, mean position loss: 28.001775884628294\n",
      "Training growing_up:  22%|█▉       | 4410/20010 [3:35:22<12:00:17,  2.77s/batch]Batch 4400/20010 Done, mean position loss: 30.291548256874087\n",
      "Training growing_up:  22%|█▉       | 4372/20010 [3:35:34<11:11:00,  2.57s/batch]Batch 4400/20010 Done, mean position loss: 27.340939645767214\n",
      "Training growing_up:  22%|█▉       | 4415/20010 [3:35:34<11:02:28,  2.55s/batch]Batch 4400/20010 Done, mean position loss: 29.02238347053528\n",
      "Training growing_up:  22%|█▉       | 4419/20010 [3:35:38<11:08:45,  2.57s/batch]Batch 4300/20010 Done, mean position loss: 27.948973252773285\n",
      "Training growing_up:  22%|█▉       | 4356/20010 [3:35:44<11:13:08,  2.58s/batch]Batch 4400/20010 Done, mean position loss: 27.49088164806366\n",
      "Training growing_up:  22%|██▏       | 4442/20010 [3:35:44<9:49:03,  2.27s/batch]Batch 4400/20010 Done, mean position loss: 26.834656753540038\n",
      "Training growing_up:  22%|█▉       | 4402/20010 [3:35:45<10:27:03,  2.41s/batch]Batch 4400/20010 Done, mean position loss: 26.757715566158296\n",
      "Training growing_up:  22%|█▉       | 4397/20010 [3:35:52<12:23:41,  2.86s/batch]Batch 4400/20010 Done, mean position loss: 26.045239100456236\n",
      "Training growing_up:  22%|█▉       | 4365/20010 [3:36:01<11:12:49,  2.58s/batch]Batch 4400/20010 Done, mean position loss: 27.160456919670104\n",
      "Training growing_up:  22%|██▏       | 4441/20010 [3:36:04<9:17:56,  2.15s/batch]Batch 4400/20010 Done, mean position loss: 29.099095120429993\n",
      "Training growing_up:  22%|█▉       | 4410/20010 [3:36:17<11:25:54,  2.64s/batch]Batch 4400/20010 Done, mean position loss: 27.663933663368226\n",
      "Training growing_up:  22%|█▉       | 4410/20010 [3:36:28<11:13:19,  2.59s/batch]Batch 4400/20010 Done, mean position loss: 27.939608149528503\n",
      "Training growing_up:  22%|█▉       | 4408/20010 [3:36:34<11:43:29,  2.71s/batch]Batch 4400/20010 Done, mean position loss: 26.89724190711975\n",
      "Training growing_up:  22%|█▉       | 4396/20010 [3:36:40<12:53:41,  2.97s/batch]Batch 4400/20010 Done, mean position loss: 27.136631612777713\n",
      "Training growing_up:  22%|█▉       | 4393/20010 [3:36:53<12:03:52,  2.78s/batch]Batch 4400/20010 Done, mean position loss: 27.28713707208633\n",
      "Training growing_up:  22%|█▉       | 4436/20010 [3:37:08<11:37:47,  2.69s/batch]Batch 4500/20010 Done, mean position loss: 26.826137573719024\n",
      "Training growing_up:  22%|█▉       | 4400/20010 [3:37:14<12:04:19,  2.78s/batch]Batch 4400/20010 Done, mean position loss: 28.194927277565004\n",
      "Training growing_up:  22%|██       | 4447/20010 [3:37:17<10:50:06,  2.51s/batch]Batch 4400/20010 Done, mean position loss: 26.963863997459413\n",
      "Training growing_up:  22%|██       | 4477/20010 [3:37:20<12:38:41,  2.93s/batch]Batch 4400/20010 Done, mean position loss: 27.938800106048582\n",
      "Training growing_up:  22%|██       | 4467/20010 [3:37:23<14:26:42,  3.35s/batch]Batch 4400/20010 Done, mean position loss: 28.489116833209994\n",
      "Training growing_up:  22%|██       | 4485/20010 [3:37:33<12:31:36,  2.90s/batch]Batch 4400/20010 Done, mean position loss: 27.295360860824587\n",
      "Training growing_up:  22%|█▉       | 4424/20010 [3:37:37<13:43:41,  3.17s/batch]Batch 4400/20010 Done, mean position loss: 28.055339460372927\n",
      "Training growing_up:  23%|██       | 4516/20010 [3:37:49<11:46:59,  2.74s/batch]Batch 4400/20010 Done, mean position loss: 27.541108531951906\n",
      "Training growing_up:  22%|█▉       | 4411/20010 [3:38:06<12:26:18,  2.87s/batch]Batch 4500/20010 Done, mean position loss: 27.206266403198242\n",
      "Training growing_up:  22%|█▉       | 4421/20010 [3:38:18<11:41:51,  2.70s/batch]Batch 4500/20010 Done, mean position loss: 27.725547273159027\n",
      "Training growing_up:  22%|██       | 4492/20010 [3:38:25<12:53:34,  2.99s/batch]Batch 4500/20010 Done, mean position loss: 28.699051954746245\n",
      "Training growing_up:  22%|█▉       | 4368/20010 [3:38:26<13:08:49,  3.03s/batch]Batch 4500/20010 Done, mean position loss: 27.153293375968936\n",
      "Training growing_up:  22%|█▉       | 4433/20010 [3:38:51<12:31:09,  2.89s/batch]Batch 4500/20010 Done, mean position loss: 27.070853142738343\n",
      "Training growing_up:  22%|█▉       | 4429/20010 [3:38:52<11:55:55,  2.76s/batch]Batch 4400/20010 Done, mean position loss: 28.55234126806259\n",
      "Training growing_up:  22%|██       | 4490/20010 [3:38:59<12:46:26,  2.96s/batch]Batch 4400/20010 Done, mean position loss: 26.27547857761383\n",
      "Batch 4500/20010 Done, mean position loss: 26.692276527881624\n",
      "Training growing_up:  22%|█▉       | 4442/20010 [3:39:17<12:09:56,  2.81s/batch]Batch 4400/20010 Done, mean position loss: 26.258877789974214\n",
      "Training growing_up:  22%|█▉       | 4412/20010 [3:39:25<13:00:30,  3.00s/batch]Batch 4500/20010 Done, mean position loss: 28.369965810775756\n",
      "Training growing_up:  22%|██       | 4494/20010 [3:39:27<12:18:14,  2.85s/batch]Batch 4500/20010 Done, mean position loss: 27.039294006824495\n",
      "Training growing_up:  22%|██       | 4468/20010 [3:39:27<13:24:25,  3.11s/batch]Batch 4500/20010 Done, mean position loss: 27.762321646213532\n",
      "Training growing_up:  22%|█▉       | 4444/20010 [3:39:39<11:22:23,  2.63s/batch]Batch 4400/20010 Done, mean position loss: 26.89599817276001\n",
      "Training growing_up:  22%|██       | 4451/20010 [3:39:45<12:42:44,  2.94s/batch]Batch 4500/20010 Done, mean position loss: 27.99904564142227\n",
      "Training growing_up:  22%|██       | 4489/20010 [3:39:50<11:50:31,  2.75s/batch]Batch 4500/20010 Done, mean position loss: 26.165149147510526\n",
      "Training growing_up:  22%|██       | 4456/20010 [3:39:56<12:34:20,  2.91s/batch]Batch 4400/20010 Done, mean position loss: 26.438124690055847\n",
      "Training growing_up:  23%|██       | 4563/20010 [3:40:00<11:09:38,  2.60s/batch]Batch 4400/20010 Done, mean position loss: 27.122262871265413\n",
      "Training growing_up:  22%|██       | 4487/20010 [3:40:02<12:27:17,  2.89s/batch]Batch 4500/20010 Done, mean position loss: 28.111453680992128\n",
      "Training growing_up:  23%|██       | 4510/20010 [3:40:09<11:31:31,  2.68s/batch]Batch 4500/20010 Done, mean position loss: 27.28069947719574\n",
      "Training growing_up:  22%|██       | 4463/20010 [3:40:16<13:21:35,  3.09s/batch]Batch 4500/20010 Done, mean position loss: 28.016641030311586\n",
      "Training growing_up:  22%|██       | 4464/20010 [3:40:19<12:44:06,  2.95s/batch]Batch 4400/20010 Done, mean position loss: 26.68261435508728\n",
      "Training growing_up:  22%|██       | 4493/20010 [3:40:21<13:51:29,  3.22s/batch]Batch 4500/20010 Done, mean position loss: 27.737730338573456\n",
      "Training growing_up:  22%|██       | 4494/20010 [3:40:23<12:52:06,  2.99s/batch]Batch 4500/20010 Done, mean position loss: 26.80786145210266\n",
      "Training growing_up:  23%|██       | 4504/20010 [3:40:24<12:03:54,  2.80s/batch]Batch 4500/20010 Done, mean position loss: 26.610559062957762\n",
      "Training growing_up:  22%|█▉       | 4406/20010 [3:40:34<12:43:17,  2.93s/batch]Batch 4500/20010 Done, mean position loss: 27.458631913661957\n",
      "Training growing_up:  22%|█▉       | 4423/20010 [3:40:42<13:50:46,  3.20s/batch]Batch 4500/20010 Done, mean position loss: 27.42511171340942\n",
      "Training growing_up:  23%|██       | 4519/20010 [3:40:44<12:39:13,  2.94s/batch]Batch 4500/20010 Done, mean position loss: 27.16362713098526\n",
      "Training growing_up:  23%|██       | 4584/20010 [3:41:03<12:09:45,  2.84s/batch]Batch 4500/20010 Done, mean position loss: 26.252497375011444\n",
      "Training growing_up:  23%|██       | 4512/20010 [3:41:16<13:32:31,  3.15s/batch]Batch 4500/20010 Done, mean position loss: 27.25115782260895\n",
      "Training growing_up:  23%|██       | 4554/20010 [3:41:20<13:09:47,  3.07s/batch]Batch 4500/20010 Done, mean position loss: 26.960981285572053\n",
      "Training growing_up:  23%|██       | 4524/20010 [3:41:21<12:26:11,  2.89s/batch]Batch 4500/20010 Done, mean position loss: 27.160126733779908\n",
      "Training growing_up:  23%|██       | 4595/20010 [3:41:35<12:25:10,  2.90s/batch]Batch 4500/20010 Done, mean position loss: 27.696026833057402\n",
      "Training growing_up:  23%|██       | 4524/20010 [3:41:52<12:05:18,  2.81s/batch]Batch 4600/20010 Done, mean position loss: 25.973152217864993\n",
      "Training growing_up:  23%|██       | 4581/20010 [3:42:06<12:28:25,  2.91s/batch]Batch 4500/20010 Done, mean position loss: 28.203207049369812\n",
      "Training growing_up:  22%|██       | 4453/20010 [3:42:11<13:10:04,  3.05s/batch]Batch 4500/20010 Done, mean position loss: 27.879516611099245\n",
      "Training growing_up:  23%|██       | 4533/20010 [3:42:15<12:09:40,  2.83s/batch]Batch 4500/20010 Done, mean position loss: 27.526716222763064\n",
      "Batch 4500/20010 Done, mean position loss: 27.656274814605712\n",
      "Training growing_up:  23%|██       | 4503/20010 [3:42:20<12:16:16,  2.85s/batch]Batch 4500/20010 Done, mean position loss: 27.872018704414366\n",
      "Training growing_up:  23%|██       | 4578/20010 [3:42:30<12:31:34,  2.92s/batch]Batch 4500/20010 Done, mean position loss: 26.700842189788816\n",
      "Training growing_up:  23%|██       | 4547/20010 [3:42:42<14:06:43,  3.29s/batch]Batch 4500/20010 Done, mean position loss: 27.779412589073182\n",
      "Training growing_up:  22%|██       | 4475/20010 [3:42:52<13:05:27,  3.03s/batch]Batch 4600/20010 Done, mean position loss: 27.260948202610017\n",
      "Training growing_up:  23%|██       | 4577/20010 [3:43:05<11:44:44,  2.74s/batch]Batch 4600/20010 Done, mean position loss: 26.340362708568573\n",
      "Training growing_up:  23%|██       | 4539/20010 [3:43:15<13:16:19,  3.09s/batch]Batch 4600/20010 Done, mean position loss: 28.511865026950836\n",
      "Training growing_up:  22%|██       | 4476/20010 [3:43:24<13:50:06,  3.21s/batch]Batch 4600/20010 Done, mean position loss: 26.77207491159439\n",
      "Training growing_up:  23%|██       | 4525/20010 [3:43:41<11:23:06,  2.65s/batch]Batch 4600/20010 Done, mean position loss: 25.98963395357132\n",
      "Training growing_up:  22%|██       | 4477/20010 [3:43:47<13:22:02,  3.10s/batch]Batch 4600/20010 Done, mean position loss: 26.69181671380997\n",
      "Training growing_up:  23%|██       | 4573/20010 [3:43:48<13:23:20,  3.12s/batch]Batch 4500/20010 Done, mean position loss: 29.10491572380066\n",
      "Training growing_up:  23%|██       | 4617/20010 [3:44:01<13:45:10,  3.22s/batch]Batch 4500/20010 Done, mean position loss: 27.667876613140105\n",
      "Training growing_up:  23%|██       | 4562/20010 [3:44:09<12:02:44,  2.81s/batch]Batch 4500/20010 Done, mean position loss: 25.909474618434906\n",
      "Training growing_up:  22%|██       | 4486/20010 [3:44:15<13:03:33,  3.03s/batch]Batch 4600/20010 Done, mean position loss: 26.257174077033998\n",
      "Training growing_up:  23%|██       | 4541/20010 [3:44:19<13:51:32,  3.23s/batch]Batch 4600/20010 Done, mean position loss: 28.209148104190827\n",
      "Training growing_up:  23%|██       | 4572/20010 [3:44:26<12:53:38,  3.01s/batch]Batch 4600/20010 Done, mean position loss: 27.59791362762451\n",
      "Training growing_up:  23%|██       | 4539/20010 [3:44:37<12:17:45,  2.86s/batch]Batch 4500/20010 Done, mean position loss: 27.408910222053528\n",
      "Training growing_up:  23%|██       | 4620/20010 [3:44:40<12:25:28,  2.91s/batch]Batch 4600/20010 Done, mean position loss: 27.485746128559114\n",
      "Training growing_up:  23%|██       | 4589/20010 [3:44:42<13:29:04,  3.15s/batch]Batch 4600/20010 Done, mean position loss: 28.362572441101076\n",
      "Training growing_up:  23%|██       | 4569/20010 [3:44:48<12:52:16,  3.00s/batch]Batch 4500/20010 Done, mean position loss: 27.268183312416078\n",
      "Training growing_up:  23%|██       | 4604/20010 [3:44:49<11:43:49,  2.74s/batch]Batch 4600/20010 Done, mean position loss: 28.750637135505677\n",
      "Training growing_up:  23%|██       | 4583/20010 [3:45:02<13:29:08,  3.15s/batch]Batch 4500/20010 Done, mean position loss: 26.840758001804353\n",
      "Training growing_up:  23%|██       | 4591/20010 [3:45:12<14:20:57,  3.35s/batch]Batch 4600/20010 Done, mean position loss: 27.016987011432647\n",
      "Training growing_up:  23%|██       | 4575/20010 [3:45:15<12:45:02,  2.97s/batch]Batch 4600/20010 Done, mean position loss: 27.640946016311645\n",
      "Training growing_up:  23%|██       | 4613/20010 [3:45:16<12:24:19,  2.90s/batch]Batch 4500/20010 Done, mean position loss: 25.754242844581604\n",
      "Training growing_up:  23%|██       | 4598/20010 [3:45:17<14:30:39,  3.39s/batch]Batch 4600/20010 Done, mean position loss: 26.372821509838104\n",
      "Training growing_up:  23%|██       | 4582/20010 [3:45:25<11:31:06,  2.69s/batch]Batch 4600/20010 Done, mean position loss: 27.287485840320585\n",
      "Training growing_up:  23%|██       | 4648/20010 [3:45:26<12:14:53,  2.87s/batch]Batch 4600/20010 Done, mean position loss: 26.621891787052153\n",
      "Training growing_up:  23%|██       | 4583/20010 [3:45:27<10:40:35,  2.49s/batch]Batch 4600/20010 Done, mean position loss: 26.512839233875273\n",
      "Training growing_up:  23%|██       | 4678/20010 [3:45:42<11:35:24,  2.72s/batch]Batch 4600/20010 Done, mean position loss: 26.195838367938997\n",
      "Training growing_up:  23%|██       | 4632/20010 [3:45:55<12:16:51,  2.88s/batch]Batch 4600/20010 Done, mean position loss: 27.224098761081695\n",
      "Training growing_up:  23%|██       | 4581/20010 [3:46:09<13:13:16,  3.08s/batch]Batch 4600/20010 Done, mean position loss: 26.982084493637085\n",
      "Training growing_up:  23%|██       | 4640/20010 [3:46:18<12:55:59,  3.03s/batch]Batch 4600/20010 Done, mean position loss: 28.571507618427276\n",
      "Training growing_up:  23%|██       | 4598/20010 [3:46:20<12:46:12,  2.98s/batch]Batch 4600/20010 Done, mean position loss: 27.793121650218964\n",
      "Training growing_up:  23%|██       | 4532/20010 [3:46:20<13:21:09,  3.11s/batch]Batch 4600/20010 Done, mean position loss: 27.20015678167343\n",
      "Training growing_up:  23%|██       | 4604/20010 [3:46:28<12:17:53,  2.87s/batch]Batch 4600/20010 Done, mean position loss: 26.939452009201048\n",
      "Training growing_up:  23%|██       | 4664/20010 [3:46:48<12:49:50,  3.01s/batch]Batch 4700/20010 Done, mean position loss: 26.174648642539978\n",
      "Training growing_up:  23%|██       | 4682/20010 [3:47:08<13:35:31,  3.19s/batch]Batch 4600/20010 Done, mean position loss: 26.89707013130188\n",
      "Training growing_up:  23%|██       | 4567/20010 [3:47:09<13:05:27,  3.05s/batch]Batch 4600/20010 Done, mean position loss: 26.98558176279068\n",
      "Training growing_up:  23%|██       | 4639/20010 [3:47:14<11:35:14,  2.71s/batch]Batch 4600/20010 Done, mean position loss: 26.926876685619355\n",
      "Training growing_up:  23%|██       | 4555/20010 [3:47:18<11:05:31,  2.58s/batch]Batch 4600/20010 Done, mean position loss: 27.015383331775666\n",
      "Training growing_up:  23%|██       | 4659/20010 [3:47:24<11:40:41,  2.74s/batch]Batch 4600/20010 Done, mean position loss: 28.078689074516298\n",
      "Training growing_up:  23%|██       | 4643/20010 [3:47:27<11:34:20,  2.71s/batch]Batch 4600/20010 Done, mean position loss: 26.38245772123337\n",
      "Training growing_up:  23%|██       | 4662/20010 [3:47:41<12:19:58,  2.89s/batch]Batch 4600/20010 Done, mean position loss: 27.248532040119173\n",
      "Training growing_up:  23%|██       | 4684/20010 [3:47:46<11:36:40,  2.73s/batch]Batch 4700/20010 Done, mean position loss: 27.899838809967044\n",
      "Training growing_up:  23%|██       | 4654/20010 [3:48:00<12:58:17,  3.04s/batch]Batch 4700/20010 Done, mean position loss: 26.807395467758177\n",
      "Training growing_up:  23%|██       | 4692/20010 [3:48:08<11:26:22,  2.69s/batch]Batch 4700/20010 Done, mean position loss: 28.06593310594559\n",
      "Training growing_up:  23%|██       | 4668/20010 [3:48:27<13:09:59,  3.09s/batch]Batch 4700/20010 Done, mean position loss: 26.52782279253006\n",
      "Training growing_up:  23%|██       | 4685/20010 [3:48:35<12:48:55,  3.01s/batch]Batch 4700/20010 Done, mean position loss: 26.364390377998355\n",
      "Training growing_up:  24%|██       | 4719/20010 [3:48:38<13:04:29,  3.08s/batch]Batch 4700/20010 Done, mean position loss: 25.7176158952713\n",
      "Training growing_up:  23%|██       | 4599/20010 [3:48:50<14:06:19,  3.30s/batch]Batch 4600/20010 Done, mean position loss: 27.546904265880585\n",
      "Training growing_up:  23%|██       | 4668/20010 [3:48:56<12:28:44,  2.93s/batch]Batch 4600/20010 Done, mean position loss: 28.810087544918062\n",
      "Training growing_up:  23%|██       | 4588/20010 [3:49:03<12:43:37,  2.97s/batch]Batch 4700/20010 Done, mean position loss: 26.23323653459549\n",
      "Training growing_up:  23%|██       | 4637/20010 [3:49:10<13:12:33,  3.09s/batch]Batch 4700/20010 Done, mean position loss: 26.92919453382492\n",
      "Training growing_up:  23%|██       | 4669/20010 [3:49:14<11:40:00,  2.74s/batch]Batch 4600/20010 Done, mean position loss: 26.351667976379392\n",
      "Training growing_up:  24%|██▏      | 4755/20010 [3:49:24<12:32:39,  2.96s/batch]Batch 4700/20010 Done, mean position loss: 28.579193921089175\n",
      "Training growing_up:  23%|██       | 4635/20010 [3:49:26<13:59:49,  3.28s/batch]Batch 4700/20010 Done, mean position loss: 27.02961145401001\n",
      "Training growing_up:  24%|██       | 4720/20010 [3:49:31<12:15:53,  2.89s/batch]Batch 4700/20010 Done, mean position loss: 27.190191705226898\n",
      "Training growing_up:  23%|██       | 4648/20010 [3:49:43<13:02:54,  3.06s/batch]Batch 4600/20010 Done, mean position loss: 26.826881256103515\n",
      "Training growing_up:  23%|██       | 4691/20010 [3:49:49<13:16:55,  3.12s/batch]Batch 4600/20010 Done, mean position loss: 26.73706523180008\n",
      "Training growing_up:  23%|██       | 4654/20010 [3:49:51<12:53:10,  3.02s/batch]Batch 4700/20010 Done, mean position loss: 27.463563933372498\n",
      "Training growing_up:  23%|██       | 4621/20010 [3:50:00<13:56:11,  3.26s/batch]Batch 4600/20010 Done, mean position loss: 27.36256911754608\n",
      "Training growing_up:  23%|██       | 4697/20010 [3:50:04<11:54:53,  2.80s/batch]Batch 4700/20010 Done, mean position loss: 26.778881330490115\n",
      "Training growing_up:  23%|██       | 4700/20010 [3:50:11<13:15:33,  3.12s/batch]Batch 4700/20010 Done, mean position loss: 26.62531085729599\n",
      "Training growing_up:  24%|██▏      | 4747/20010 [3:50:14<11:05:10,  2.61s/batch]Batch 4700/20010 Done, mean position loss: 25.90061990022659\n",
      "Training growing_up:  23%|██       | 4680/20010 [3:50:14<10:25:08,  2.45s/batch]Batch 4700/20010 Done, mean position loss: 28.947924554347992\n",
      "Training growing_up:  24%|██▏      | 4735/20010 [3:50:15<12:37:09,  2.97s/batch]Batch 4700/20010 Done, mean position loss: 28.861643950939182\n",
      "Training growing_up:  23%|██       | 4624/20010 [3:50:18<10:23:15,  2.43s/batch]Batch 4600/20010 Done, mean position loss: 26.367682411670685\n",
      "Training growing_up:  24%|██▏      | 4737/20010 [3:50:19<12:10:19,  2.87s/batch]Batch 4700/20010 Done, mean position loss: 27.309374577999115\n",
      "Training growing_up:  23%|██       | 4657/20010 [3:50:31<12:39:22,  2.97s/batch]Batch 4700/20010 Done, mean position loss: 26.34339367389679\n",
      "Training growing_up:  24%|██▏      | 4737/20010 [3:50:43<10:48:40,  2.55s/batch]Batch 4700/20010 Done, mean position loss: 28.135300612449647\n",
      "Training growing_up:  24%|██▏      | 4732/20010 [3:50:55<11:40:49,  2.75s/batch]Batch 4700/20010 Done, mean position loss: 26.347766222953794\n",
      "Training growing_up:  24%|██▏      | 4753/20010 [3:51:00<10:26:23,  2.46s/batch]Batch 4700/20010 Done, mean position loss: 27.421355509757994\n",
      "Training growing_up:  23%|██       | 4681/20010 [3:51:06<10:59:38,  2.58s/batch]Batch 4700/20010 Done, mean position loss: 28.692336726188657\n",
      "Training growing_up:  24%|██       | 4724/20010 [3:51:11<10:13:35,  2.41s/batch]Batch 4700/20010 Done, mean position loss: 26.51631846189499\n",
      "Training growing_up:  24%|██       | 4710/20010 [3:51:18<11:44:31,  2.76s/batch]Batch 4700/20010 Done, mean position loss: 27.34754692316055\n",
      "Training growing_up:  24%|██▎       | 4728/20010 [3:51:33<9:22:50,  2.21s/batch]Batch 4800/20010 Done, mean position loss: 27.43924606800079\n",
      "Training growing_up:  24%|██▏      | 4737/20010 [3:51:53<11:02:08,  2.60s/batch]Batch 4700/20010 Done, mean position loss: 26.247392497062684\n",
      "Training growing_up:  23%|██       | 4647/20010 [3:51:54<11:13:22,  2.63s/batch]Batch 4700/20010 Done, mean position loss: 26.498136861324312\n",
      "Training growing_up:  24%|██▏      | 4764/20010 [3:51:55<10:28:43,  2.47s/batch]Batch 4700/20010 Done, mean position loss: 26.82887722492218\n",
      "Training growing_up:  24%|██▏      | 4773/20010 [3:51:56<10:29:17,  2.48s/batch]Batch 4700/20010 Done, mean position loss: 28.755837070941922\n",
      "Training growing_up:  24%|██▏      | 4761/20010 [3:52:08<10:49:10,  2.55s/batch]Batch 4700/20010 Done, mean position loss: 26.669812552928924\n",
      "Training growing_up:  24%|██▏      | 4765/20010 [3:52:15<10:10:32,  2.40s/batch]Batch 4700/20010 Done, mean position loss: 28.30742755174637\n",
      "Training growing_up:  24%|██▏      | 4750/20010 [3:52:21<11:48:21,  2.79s/batch]Batch 4800/20010 Done, mean position loss: 26.44256383895874\n",
      "Training growing_up:  24%|██▏      | 4768/20010 [3:52:26<11:07:35,  2.63s/batch]Batch 4700/20010 Done, mean position loss: 29.239707112312317\n",
      "Training growing_up:  24%|██▏      | 4777/20010 [3:52:32<11:13:38,  2.65s/batch]Batch 4800/20010 Done, mean position loss: 26.816068549156192\n",
      "Training growing_up:  23%|██       | 4681/20010 [3:52:42<12:01:07,  2.82s/batch]Batch 4800/20010 Done, mean position loss: 28.78622277736664\n",
      "Training growing_up:  24%|██▏      | 4787/20010 [3:53:01<12:12:33,  2.89s/batch]Batch 4800/20010 Done, mean position loss: 26.637229962348936\n",
      "Training growing_up:  23%|██       | 4674/20010 [3:53:07<11:26:49,  2.69s/batch]Batch 4800/20010 Done, mean position loss: 26.234689788818358\n",
      "Training growing_up:  23%|██       | 4667/20010 [3:53:09<11:26:53,  2.69s/batch]Batch 4800/20010 Done, mean position loss: 25.742794218063352\n",
      "Training growing_up:  23%|██       | 4677/20010 [3:53:28<12:44:01,  2.99s/batch]Batch 4700/20010 Done, mean position loss: 28.196315073966982\n",
      "Training growing_up:  23%|██       | 4697/20010 [3:53:34<11:30:57,  2.71s/batch]Batch 4700/20010 Done, mean position loss: 28.10918171405792\n",
      "Training growing_up:  24%|██▏      | 4740/20010 [3:53:36<11:23:52,  2.69s/batch]Batch 4800/20010 Done, mean position loss: 25.38931336402893\n",
      "Training growing_up:  23%|██       | 4699/20010 [3:53:40<12:57:06,  3.05s/batch]Batch 4800/20010 Done, mean position loss: 26.79295808553696\n",
      "Training growing_up:  24%|██▍       | 4785/20010 [3:53:47<8:59:42,  2.13s/batch]Batch 4700/20010 Done, mean position loss: 25.338574509620667\n",
      "Training growing_up:  24%|██▏      | 4836/20010 [3:53:51<10:34:27,  2.51s/batch]Batch 4800/20010 Done, mean position loss: 27.27064161300659\n",
      "Training growing_up:  24%|██▏      | 4764/20010 [3:53:52<10:28:48,  2.47s/batch]Batch 4800/20010 Done, mean position loss: 27.700374610424042\n",
      "Training growing_up:  24%|██▏      | 4740/20010 [3:53:57<12:33:53,  2.96s/batch]Batch 4800/20010 Done, mean position loss: 26.817266149520876\n",
      "Training growing_up:  24%|██▏      | 4796/20010 [3:54:18<13:14:39,  3.13s/batch]Batch 4800/20010 Done, mean position loss: 27.530817806720734\n",
      "Training growing_up:  24%|██▏      | 4866/20010 [3:54:18<11:44:12,  2.79s/batch]Batch 4700/20010 Done, mean position loss: 25.938894481658934\n",
      "Training growing_up:  24%|██▏      | 4755/20010 [3:54:22<13:57:39,  3.29s/batch]Batch 4700/20010 Done, mean position loss: 26.424431324005127\n",
      "Training growing_up:  23%|██       | 4698/20010 [3:54:29<12:33:24,  2.95s/batch]Batch 4800/20010 Done, mean position loss: 27.071807279586793\n",
      "Training growing_up:  24%|██       | 4705/20010 [3:54:32<11:57:48,  2.81s/batch]Batch 4800/20010 Done, mean position loss: 26.72717955112457\n",
      "Training growing_up:  24%|██▏      | 4844/20010 [3:54:39<13:03:42,  3.10s/batch]Batch 4700/20010 Done, mean position loss: 27.396632685661316\n",
      "Training growing_up:  24%|██▏      | 4800/20010 [3:54:40<11:49:20,  2.80s/batch]Batch 4800/20010 Done, mean position loss: 26.942680690288544\n",
      "Training growing_up:  24%|██▏      | 4818/20010 [3:54:43<12:30:50,  2.97s/batch]Batch 4800/20010 Done, mean position loss: 27.983906168937686\n",
      "Training growing_up:  24%|██▏      | 4800/20010 [3:54:47<11:46:28,  2.79s/batch]Batch 4700/20010 Done, mean position loss: 25.63850046634674\n",
      "Batch 4800/20010 Done, mean position loss: 25.621927855014803\n",
      "Training growing_up:  24%|██▏      | 4779/20010 [3:54:49<12:18:48,  2.91s/batch]Batch 4800/20010 Done, mean position loss: 26.454713227748872\n",
      "Training growing_up:  24%|██▏      | 4840/20010 [3:54:50<11:29:06,  2.73s/batch]Batch 4800/20010 Done, mean position loss: 26.474672389030456\n",
      "Training growing_up:  24%|██▏      | 4772/20010 [3:55:21<12:51:32,  3.04s/batch]Batch 4800/20010 Done, mean position loss: 27.968411600589754\n",
      "Training growing_up:  24%|██▏      | 4798/20010 [3:55:28<12:33:48,  2.97s/batch]Batch 4800/20010 Done, mean position loss: 26.525227668285368\n",
      "Training growing_up:  24%|██▏      | 4780/20010 [3:55:35<13:01:09,  3.08s/batch]Batch 4800/20010 Done, mean position loss: 27.226836717128755\n",
      "Training growing_up:  24%|██▏      | 4821/20010 [3:55:38<12:32:52,  2.97s/batch]Batch 4800/20010 Done, mean position loss: 26.85741480827332\n",
      "Training growing_up:  24%|██▏      | 4784/20010 [3:55:46<11:49:42,  2.80s/batch]Batch 4800/20010 Done, mean position loss: 26.298137998580934\n",
      "Training growing_up:  24%|██▏      | 4824/20010 [3:55:53<11:52:56,  2.82s/batch]Batch 4800/20010 Done, mean position loss: 27.30090765237808\n",
      "Training growing_up:  24%|██▏      | 4876/20010 [3:55:54<11:56:35,  2.84s/batch]Batch 4900/20010 Done, mean position loss: 27.013648765087126\n",
      "Training growing_up:  24%|██▏      | 4789/20010 [3:56:29<12:13:13,  2.89s/batch]Batch 4800/20010 Done, mean position loss: 27.001852281093598\n",
      "Training growing_up:  24%|██▏      | 4841/20010 [3:56:30<12:32:03,  2.97s/batch]Batch 4800/20010 Done, mean position loss: 27.491806540489193\n",
      "Training growing_up:  24%|██▏      | 4821/20010 [3:56:34<11:37:39,  2.76s/batch]Batch 4800/20010 Done, mean position loss: 26.870199842453005\n",
      "Training growing_up:  24%|██▏      | 4894/20010 [3:56:36<12:04:33,  2.88s/batch]Batch 4800/20010 Done, mean position loss: 27.64733404636383\n",
      "Training growing_up:  24%|██▏      | 4849/20010 [3:56:44<11:29:17,  2.73s/batch]Batch 4800/20010 Done, mean position loss: 27.17955919265747\n",
      "Training growing_up:  24%|██▏      | 4871/20010 [3:56:52<12:44:31,  3.03s/batch]Batch 4800/20010 Done, mean position loss: 27.806089410781862\n",
      "Training growing_up:  24%|██▏      | 4797/20010 [3:56:55<12:58:24,  3.07s/batch]Batch 4900/20010 Done, mean position loss: 26.460504019260405\n",
      "Training growing_up:  24%|██▏      | 4824/20010 [3:57:02<12:59:21,  3.08s/batch]Batch 4900/20010 Done, mean position loss: 26.133590092658995\n",
      "Training growing_up:  24%|██▏      | 4855/20010 [3:57:06<12:38:21,  3.00s/batch]Batch 4800/20010 Done, mean position loss: 26.65185398340225\n",
      "Training growing_up:  24%|██▏      | 4783/20010 [3:57:18<12:47:19,  3.02s/batch]Batch 4900/20010 Done, mean position loss: 27.72443030357361\n",
      "Training growing_up:  24%|██▏      | 4838/20010 [3:57:43<11:56:39,  2.83s/batch]Batch 4900/20010 Done, mean position loss: 26.32080862998962\n",
      "Training growing_up:  24%|██▏      | 4864/20010 [3:57:49<11:41:43,  2.78s/batch]Batch 4900/20010 Done, mean position loss: 25.5452703166008\n",
      "Training growing_up:  24%|██▏      | 4828/20010 [3:57:50<12:18:46,  2.92s/batch]Batch 4900/20010 Done, mean position loss: 26.248248069286348\n",
      "Training growing_up:  24%|██▏      | 4833/20010 [3:58:09<13:18:58,  3.16s/batch]Batch 4800/20010 Done, mean position loss: 26.488594722747806\n",
      "Training growing_up:  25%|██▏      | 4951/20010 [3:58:12<11:43:46,  2.80s/batch]Batch 4900/20010 Done, mean position loss: 25.857958900928498\n",
      "Training growing_up:  24%|██▏      | 4836/20010 [3:58:16<13:44:23,  3.26s/batch]Batch 4900/20010 Done, mean position loss: 26.42920336484909\n",
      "Training growing_up:  24%|██▏      | 4776/20010 [3:58:23<12:29:34,  2.95s/batch]Batch 4800/20010 Done, mean position loss: 27.363514897823332\n",
      "Training growing_up:  24%|██▏      | 4863/20010 [3:58:36<13:39:41,  3.25s/batch]Batch 4900/20010 Done, mean position loss: 27.553259680271147\n",
      "Training growing_up:  24%|██▏      | 4901/20010 [3:58:35<12:44:34,  3.04s/batch]Batch 4900/20010 Done, mean position loss: 27.714933450222016\n",
      "Training growing_up:  24%|██▏      | 4846/20010 [3:58:39<11:04:22,  2.63s/batch]Batch 4800/20010 Done, mean position loss: 25.69735358476639\n",
      "Training growing_up:  25%|██▏      | 4931/20010 [3:58:48<14:59:33,  3.58s/batch]Batch 4900/20010 Done, mean position loss: 27.2350320148468\n",
      "Training growing_up:  25%|██▏      | 4948/20010 [3:59:08<12:18:44,  2.94s/batch]Batch 4900/20010 Done, mean position loss: 27.00519602537155\n",
      "Training growing_up:  25%|██▏      | 4970/20010 [3:59:08<12:09:53,  2.91s/batch]Batch 4800/20010 Done, mean position loss: 26.762703821659088\n",
      "Training growing_up:  24%|██▏      | 4868/20010 [3:59:13<12:37:31,  3.00s/batch]Batch 4800/20010 Done, mean position loss: 26.144450676441195\n",
      "Training growing_up:  24%|██▏      | 4875/20010 [3:59:14<12:20:09,  2.93s/batch]Batch 4900/20010 Done, mean position loss: 27.39919593572617\n",
      "Training growing_up:  25%|██▏      | 4943/20010 [3:59:21<11:31:05,  2.75s/batch]Batch 4900/20010 Done, mean position loss: 27.37083248615265\n",
      "Training growing_up:  24%|██▏      | 4816/20010 [3:59:26<13:41:20,  3.24s/batch]Batch 4900/20010 Done, mean position loss: 26.5868776679039\n",
      "Training growing_up:  24%|██▏      | 4808/20010 [3:59:29<12:30:53,  2.96s/batch]Batch 4900/20010 Done, mean position loss: 27.33976508378983\n",
      "Training growing_up:  25%|██▏      | 4937/20010 [3:59:33<11:55:29,  2.85s/batch]Batch 4900/20010 Done, mean position loss: 26.09682277917862\n",
      "Training growing_up:  24%|██▏      | 4882/20010 [3:59:37<13:28:18,  3.21s/batch]Batch 4900/20010 Done, mean position loss: 25.82902769088745\n",
      "Training growing_up:  24%|██▏      | 4799/20010 [3:59:38<13:37:31,  3.22s/batch]Batch 4900/20010 Done, mean position loss: 27.20132923364639\n",
      "Training growing_up:  24%|██▏      | 4867/20010 [3:59:39<11:49:44,  2.81s/batch]Batch 4800/20010 Done, mean position loss: 26.4349803519249\n",
      "Training growing_up:  24%|██▏      | 4857/20010 [3:59:44<13:53:35,  3.30s/batch]Batch 4800/20010 Done, mean position loss: 26.19804897069931\n",
      "Training growing_up:  24%|██▏      | 4890/20010 [4:00:01<12:14:38,  2.92s/batch]Batch 4900/20010 Done, mean position loss: 28.22192785978317\n",
      "Training growing_up:  24%|██▏      | 4864/20010 [4:00:17<11:55:35,  2.83s/batch]Batch 4900/20010 Done, mean position loss: 25.94222307920456\n",
      "Training growing_up:  25%|██▏      | 4918/20010 [4:00:24<13:51:20,  3.31s/batch]Batch 4900/20010 Done, mean position loss: 26.365673704147337\n",
      "Training growing_up:  25%|██▏      | 4949/20010 [4:00:37<12:18:22,  2.94s/batch]Batch 4900/20010 Done, mean position loss: 27.70166888475418\n",
      "Training growing_up:  25%|██▏      | 4930/20010 [4:00:39<12:35:36,  3.01s/batch]Batch 4900/20010 Done, mean position loss: 27.0694392824173\n",
      "Training growing_up:  25%|██▏      | 4927/20010 [4:00:43<12:16:07,  2.93s/batch]Batch 5000/20010 Done, mean position loss: 27.03479758262634\n",
      "Training growing_up:  25%|██▏      | 4963/20010 [4:00:50<13:05:14,  3.13s/batch]Batch 4900/20010 Done, mean position loss: 26.96729790210724\n",
      "Training growing_up:  24%|██▏      | 4843/20010 [4:01:25<13:28:13,  3.20s/batch]Batch 4900/20010 Done, mean position loss: 27.284718387126922\n",
      "Training growing_up:  25%|██▏      | 4904/20010 [4:01:33<11:45:31,  2.80s/batch]Batch 4900/20010 Done, mean position loss: 26.8361500787735\n",
      "Training growing_up:  25%|██▏      | 4917/20010 [4:01:35<12:35:53,  3.00s/batch]Batch 4900/20010 Done, mean position loss: 28.022568747997283\n",
      "Training growing_up:  25%|██▏      | 4925/20010 [4:01:37<11:38:00,  2.78s/batch]Batch 4900/20010 Done, mean position loss: 26.421175830364227\n",
      "Training growing_up:  24%|██▏      | 4893/20010 [4:01:49<14:45:41,  3.52s/batch]Batch 5000/20010 Done, mean position loss: 26.527808315753937\n",
      "Training growing_up:  25%|██▏      | 4955/20010 [4:01:51<13:00:53,  3.11s/batch]Batch 4900/20010 Done, mean position loss: 26.920079445838926\n",
      "Training growing_up:  25%|██▏      | 4939/20010 [4:01:55<12:08:21,  2.90s/batch]Batch 4900/20010 Done, mean position loss: 26.86600596666336\n",
      "Training growing_up:  24%|██▏      | 4845/20010 [4:01:57<13:42:20,  3.25s/batch]Batch 5000/20010 Done, mean position loss: 25.523039069175717\n",
      "Training growing_up:  24%|██▏      | 4849/20010 [4:02:12<13:30:42,  3.21s/batch]Batch 5000/20010 Done, mean position loss: 27.52822154045105\n",
      "Training growing_up:  25%|██▏      | 4917/20010 [4:02:12<11:59:53,  2.86s/batch]Batch 4900/20010 Done, mean position loss: 26.70066421508789\n",
      "Training growing_up:  24%|██▏      | 4889/20010 [4:02:41<10:43:44,  2.55s/batch]Batch 5000/20010 Done, mean position loss: 26.796236827373505\n",
      "Training growing_up:  25%|██▏      | 4918/20010 [4:02:44<11:46:33,  2.81s/batch]Batch 5000/20010 Done, mean position loss: 25.654812409877778\n",
      "Training growing_up:  25%|██▏      | 4995/20010 [4:02:50<13:15:00,  3.18s/batch]Batch 5000/20010 Done, mean position loss: 25.62162213563919\n",
      "Training growing_up:  25%|██▏      | 4973/20010 [4:03:05<13:11:08,  3.16s/batch]Batch 5000/20010 Done, mean position loss: 25.400696461200713\n",
      "Training growing_up:  25%|██▏      | 4928/20010 [4:03:09<11:18:16,  2.70s/batch]Batch 5000/20010 Done, mean position loss: 26.47410245895386\n",
      "Training growing_up:  25%|██▎      | 5009/20010 [4:03:12<12:36:59,  3.03s/batch]Batch 4900/20010 Done, mean position loss: 25.705399024486542\n",
      "Training growing_up:  25%|██▎      | 5013/20010 [4:03:16<10:50:29,  2.60s/batch]Batch 4900/20010 Done, mean position loss: 27.662142107486723\n",
      "Training growing_up:  24%|██▏      | 4875/20010 [4:03:33<13:24:48,  3.19s/batch]Batch 5000/20010 Done, mean position loss: 27.331369144916533\n",
      "Training growing_up:  25%|██▏      | 4993/20010 [4:03:44<12:39:05,  3.03s/batch]Batch 5000/20010 Done, mean position loss: 26.9926148557663\n",
      "Training growing_up:  25%|██▏      | 4988/20010 [4:03:47<11:04:38,  2.65s/batch]Batch 4900/20010 Done, mean position loss: 25.120105147361755\n",
      "Training growing_up:  25%|██▏      | 4963/20010 [4:03:50<11:28:05,  2.74s/batch]Batch 5000/20010 Done, mean position loss: 27.679188389778137\n",
      "Training growing_up:  25%|██▏      | 4917/20010 [4:04:08<14:26:37,  3.45s/batch]Batch 5000/20010 Done, mean position loss: 27.576630532741547\n",
      "Training growing_up:  25%|██▏      | 4973/20010 [4:04:14<12:15:43,  2.94s/batch]Batch 5000/20010 Done, mean position loss: 26.00086716890335\n",
      "Training growing_up:  25%|██▏      | 4978/20010 [4:04:16<12:36:50,  3.02s/batch]Batch 4900/20010 Done, mean position loss: 26.422289378643036\n",
      "Training growing_up:  25%|██▏      | 4975/20010 [4:04:20<11:54:05,  2.85s/batch]Batch 4900/20010 Done, mean position loss: 27.391693563461303\n",
      "Training growing_up:  25%|██▏      | 4983/20010 [4:04:24<13:30:43,  3.24s/batch]Batch 5000/20010 Done, mean position loss: 26.995116868019103\n",
      "Training growing_up:  25%|██▏      | 5001/20010 [4:04:24<12:40:12,  3.04s/batch]Batch 5000/20010 Done, mean position loss: 26.294922215938566\n",
      "Training growing_up:  25%|██▏      | 4915/20010 [4:04:28<12:53:03,  3.07s/batch]Batch 5000/20010 Done, mean position loss: 27.58010036945343\n",
      "Training growing_up:  25%|██▎      | 5048/20010 [4:04:29<13:38:30,  3.28s/batch]Batch 5000/20010 Done, mean position loss: 27.101596493721008\n",
      "Training growing_up:  25%|██▎      | 5007/20010 [4:04:34<13:58:48,  3.35s/batch]Batch 5000/20010 Done, mean position loss: 26.706218805313114\n",
      "Training growing_up:  25%|██▏      | 4909/20010 [4:04:38<10:59:25,  2.62s/batch]Batch 5000/20010 Done, mean position loss: 25.866159658432007\n",
      "Training growing_up:  25%|██▏      | 4930/20010 [4:04:46<12:23:27,  2.96s/batch]Batch 4900/20010 Done, mean position loss: 29.207989320755004\n",
      "Training growing_up:  25%|██▎      | 5010/20010 [4:04:50<11:59:19,  2.88s/batch]Batch 4900/20010 Done, mean position loss: 25.912039177417753\n",
      "Training growing_up:  25%|██▎      | 5058/20010 [4:04:56<11:53:33,  2.86s/batch]Batch 5000/20010 Done, mean position loss: 27.341302449703214\n",
      "Training growing_up:  25%|██▏      | 4963/20010 [4:05:13<11:13:07,  2.68s/batch]Batch 5000/20010 Done, mean position loss: 26.532523715496062\n",
      "Training growing_up:  25%|██▏      | 4970/20010 [4:05:16<12:40:26,  3.03s/batch]Batch 5000/20010 Done, mean position loss: 26.4320219039917\n",
      "Training growing_up:  25%|██▏      | 4976/20010 [4:05:33<12:27:47,  2.98s/batch]Batch 5000/20010 Done, mean position loss: 26.974222087860106\n",
      "Training growing_up:  25%|██▏      | 4947/20010 [4:05:36<11:39:49,  2.79s/batch]Batch 5100/20010 Done, mean position loss: 25.755557117462157\n",
      "Training growing_up:  25%|██▎      | 5063/20010 [4:05:37<11:03:58,  2.67s/batch]Batch 5000/20010 Done, mean position loss: 26.854195575714115\n",
      "Training growing_up:  25%|██▎      | 5065/20010 [4:05:42<11:08:25,  2.68s/batch]Batch 5000/20010 Done, mean position loss: 26.815704152584075\n",
      "Training growing_up:  25%|██▏      | 4942/20010 [4:06:21<11:45:56,  2.81s/batch]Batch 5000/20010 Done, mean position loss: 26.91661732673645\n",
      "Training growing_up:  25%|██▏      | 4996/20010 [4:06:33<11:23:17,  2.73s/batch]Batch 5000/20010 Done, mean position loss: 27.471386733055112\n",
      "Training growing_up:  25%|██▎      | 5079/20010 [4:06:34<12:04:22,  2.91s/batch]Batch 5000/20010 Done, mean position loss: 26.3882666349411\n",
      "Training growing_up:  25%|██▎      | 5074/20010 [4:06:38<10:43:07,  2.58s/batch]Batch 5000/20010 Done, mean position loss: 26.363119835853574\n",
      "Training growing_up:  25%|██▎      | 5045/20010 [4:06:43<11:19:40,  2.73s/batch]Batch 5000/20010 Done, mean position loss: 27.98288146972656\n",
      "Training growing_up:  25%|██▎      | 5067/20010 [4:06:44<11:09:13,  2.69s/batch]Batch 5100/20010 Done, mean position loss: 25.586359124183655\n",
      "Training growing_up:  25%|██▎      | 5087/20010 [4:06:45<10:53:21,  2.63s/batch]Batch 5100/20010 Done, mean position loss: 27.23149165391922\n",
      "Training growing_up:  25%|██▌       | 5069/20010 [4:06:48<9:37:55,  2.32s/batch]Batch 5000/20010 Done, mean position loss: 26.637334527969358\n",
      "Training growing_up:  25%|██▏      | 4974/20010 [4:06:56<11:11:50,  2.68s/batch]Batch 5100/20010 Done, mean position loss: 27.47682159900665\n",
      "Training growing_up:  25%|██▎      | 5029/20010 [4:07:03<11:19:27,  2.72s/batch]Batch 5000/20010 Done, mean position loss: 26.671841931343078\n",
      "Training growing_up:  25%|██▎      | 5040/20010 [4:07:21<10:37:36,  2.56s/batch]Batch 5100/20010 Done, mean position loss: 25.827357635498046\n",
      "Training growing_up:  26%|██▎      | 5120/20010 [4:07:33<11:56:33,  2.89s/batch]Batch 5100/20010 Done, mean position loss: 26.631561374664305\n",
      "Training growing_up:  25%|██▎      | 5082/20010 [4:07:40<10:22:59,  2.50s/batch]Batch 5100/20010 Done, mean position loss: 26.29126934289932\n",
      "Training growing_up:  25%|██▏      | 4986/20010 [4:07:49<11:21:10,  2.72s/batch]Batch 5100/20010 Done, mean position loss: 25.460846147537232\n",
      "Training growing_up:  25%|██▏      | 4965/20010 [4:07:52<13:16:49,  3.18s/batch]Batch 5100/20010 Done, mean position loss: 26.393270761966704\n",
      "Training growing_up:  26%|██▎      | 5111/20010 [4:08:02<12:50:16,  3.10s/batch]Batch 5000/20010 Done, mean position loss: 26.00625489473343\n",
      "Training growing_up:  25%|██▎      | 5087/20010 [4:08:13<11:38:40,  2.81s/batch]Batch 5000/20010 Done, mean position loss: 27.628378608226775\n",
      "Training growing_up:  25%|██▎      | 5080/20010 [4:08:16<12:11:33,  2.94s/batch]Batch 5100/20010 Done, mean position loss: 27.03040936231613\n",
      "Training growing_up:  25%|██▎      | 5005/20010 [4:08:24<11:27:15,  2.75s/batch]Batch 5100/20010 Done, mean position loss: 26.403428361415862\n",
      "Training growing_up:  25%|██▎      | 5084/20010 [4:08:33<11:14:53,  2.71s/batch]Batch 5000/20010 Done, mean position loss: 25.6197261595726\n",
      "Training growing_up:  25%|██▎      | 5045/20010 [4:08:35<11:13:51,  2.70s/batch]Batch 5100/20010 Done, mean position loss: 27.651898612976073\n",
      "Training growing_up:  25%|██▎      | 5072/20010 [4:08:51<12:40:44,  3.06s/batch]Batch 5100/20010 Done, mean position loss: 27.956312904357908\n",
      "Batch 5100/20010 Done, mean position loss: 26.4903901720047\n",
      "Training growing_up:  26%|██▎      | 5109/20010 [4:08:57<11:30:26,  2.78s/batch]Batch 5000/20010 Done, mean position loss: 26.566282155513765\n",
      "Training growing_up:  25%|██▎      | 5096/20010 [4:09:06<12:10:02,  2.94s/batch]Batch 5100/20010 Done, mean position loss: 25.916878421306613\n",
      "Training growing_up:  26%|██▎      | 5134/20010 [4:09:08<11:15:29,  2.72s/batch]Batch 5100/20010 Done, mean position loss: 26.160093441009522\n",
      "Training growing_up:  25%|██▎      | 5101/20010 [4:09:08<11:50:49,  2.86s/batch]Batch 5100/20010 Done, mean position loss: 25.649215586185456\n",
      "Training growing_up:  25%|██▎      | 5050/20010 [4:09:09<11:00:39,  2.65s/batch]Batch 5000/20010 Done, mean position loss: 26.469156272411347\n",
      "Training growing_up:  26%|██▎      | 5156/20010 [4:09:15<11:47:59,  2.86s/batch]Batch 5100/20010 Done, mean position loss: 27.491852984428405\n",
      "Training growing_up:  26%|██▎      | 5106/20010 [4:09:21<10:34:31,  2.55s/batch]Batch 5100/20010 Done, mean position loss: 25.80846832990646\n",
      "Training growing_up:  25%|██▎      | 5082/20010 [4:09:24<12:13:17,  2.95s/batch]Batch 5100/20010 Done, mean position loss: 28.001108062267306\n",
      "Training growing_up:  25%|██▎      | 5024/20010 [4:09:34<11:27:24,  2.75s/batch]Batch 5000/20010 Done, mean position loss: 26.231276464462283\n",
      "Training growing_up:  26%|██▎      | 5139/20010 [4:09:41<12:26:37,  3.01s/batch]Batch 5100/20010 Done, mean position loss: 27.23474714756012\n",
      "Training growing_up:  25%|██▎      | 5094/20010 [4:09:41<11:49:52,  2.86s/batch]Batch 5000/20010 Done, mean position loss: 48.24464340686798\n",
      "Training growing_up:  26%|██▎      | 5158/20010 [4:09:59<12:16:21,  2.97s/batch]Batch 5100/20010 Done, mean position loss: 26.854266924858095\n",
      "Training growing_up:  25%|██▎      | 5008/20010 [4:10:03<12:40:01,  3.04s/batch]Batch 5100/20010 Done, mean position loss: 25.453380908966064\n",
      "Training growing_up:  25%|██▎      | 5078/20010 [4:10:09<11:45:42,  2.84s/batch]Batch 5200/20010 Done, mean position loss: 25.871159822940825\n",
      "Training growing_up:  26%|██▎      | 5131/20010 [4:10:14<11:10:43,  2.70s/batch]Batch 5100/20010 Done, mean position loss: 26.40697341442108\n",
      "Training growing_up:  26%|██▎      | 5157/20010 [4:10:18<12:29:26,  3.03s/batch]Batch 5100/20010 Done, mean position loss: 25.904577684402465\n",
      "Training growing_up:  25%|██▎      | 5083/20010 [4:10:29<11:30:20,  2.77s/batch]Batch 5100/20010 Done, mean position loss: 26.42526849269867\n",
      "Training growing_up:  25%|██▎      | 5096/20010 [4:11:08<11:01:22,  2.66s/batch]Batch 5100/20010 Done, mean position loss: 26.85552541732788\n",
      "Training growing_up:  26%|██▎      | 5184/20010 [4:11:12<12:03:13,  2.93s/batch]Batch 5100/20010 Done, mean position loss: 26.19995011806488\n",
      "Training growing_up:  25%|██▎      | 5065/20010 [4:11:16<12:51:10,  3.10s/batch]Batch 5100/20010 Done, mean position loss: 26.969257400035858\n",
      "Training growing_up:  25%|██▎      | 5060/20010 [4:11:19<12:19:12,  2.97s/batch]Batch 5200/20010 Done, mean position loss: 27.46724828720093\n",
      "Training growing_up:  26%|██▎      | 5124/20010 [4:11:21<12:12:43,  2.95s/batch]Batch 5200/20010 Done, mean position loss: 25.43452419281006\n",
      "Training growing_up:  26%|██▎      | 5104/20010 [4:11:23<11:36:56,  2.81s/batch]Batch 5100/20010 Done, mean position loss: 26.406325397491454\n",
      "Training growing_up:  26%|██▎      | 5121/20010 [4:11:24<11:37:53,  2.81s/batch]Batch 5100/20010 Done, mean position loss: 27.824904854297635\n",
      "Training growing_up:  26%|██▎      | 5153/20010 [4:11:35<11:01:30,  2.67s/batch]Batch 5100/20010 Done, mean position loss: 27.477038133144376\n",
      "Training growing_up:  26%|██▎      | 5141/20010 [4:11:36<11:41:39,  2.83s/batch]Batch 5200/20010 Done, mean position loss: 27.11763459920883\n",
      "Training growing_up:  25%|██▎      | 5079/20010 [4:11:51<12:45:01,  3.07s/batch]Batch 5100/20010 Done, mean position loss: 27.002392971515654\n",
      "Training growing_up:  25%|██▎      | 5079/20010 [4:12:00<13:29:16,  3.25s/batch]Batch 5200/20010 Done, mean position loss: 26.31991184234619\n",
      "Training growing_up:  25%|██▎      | 5085/20010 [4:12:17<11:45:49,  2.84s/batch]Batch 5200/20010 Done, mean position loss: 25.785545563697816\n",
      "Training growing_up:  26%|██▎      | 5169/20010 [4:12:24<11:58:27,  2.90s/batch]Batch 5200/20010 Done, mean position loss: 25.755917689800263\n",
      "Training growing_up:  26%|██▎      | 5174/20010 [4:12:32<12:40:59,  3.08s/batch]Batch 5200/20010 Done, mean position loss: 25.74367931127548\n",
      "Training growing_up:  25%|██▎      | 5094/20010 [4:12:38<13:18:56,  3.21s/batch]Batch 5200/20010 Done, mean position loss: 26.511015350818635\n",
      "Training growing_up:  26%|██▎      | 5208/20010 [4:13:01<13:07:59,  3.19s/batch]Batch 5100/20010 Done, mean position loss: 25.89067774295807\n",
      "Training growing_up:  26%|██▎      | 5193/20010 [4:13:06<14:07:44,  3.43s/batch]Batch 5200/20010 Done, mean position loss: 26.980500898361207\n",
      "Training growing_up:  26%|██▎      | 5156/20010 [4:13:11<16:14:27,  3.94s/batch]Batch 5100/20010 Done, mean position loss: 27.08374313354492\n",
      "Training growing_up:  25%|██▎      | 5073/20010 [4:13:15<12:34:40,  3.03s/batch]Batch 5200/20010 Done, mean position loss: 27.61766617536545\n",
      "Training growing_up:  26%|██▎      | 5135/20010 [4:13:22<13:50:30,  3.35s/batch]Batch 5100/20010 Done, mean position loss: 25.947172467708587\n",
      "Training growing_up:  26%|██▎      | 5166/20010 [4:13:29<13:11:28,  3.20s/batch]Batch 5200/20010 Done, mean position loss: 27.640695085525515\n",
      "Training growing_up:  25%|██▎      | 5097/20010 [4:13:38<12:43:59,  3.07s/batch]Batch 5200/20010 Done, mean position loss: 27.332495934963227\n",
      "Training growing_up:  26%|██▎      | 5172/20010 [4:13:44<12:42:27,  3.08s/batch]Batch 5200/20010 Done, mean position loss: 27.077574734687804\n",
      "Training growing_up:  26%|██▎      | 5232/20010 [4:13:52<12:52:37,  3.14s/batch]Batch 5100/20010 Done, mean position loss: 25.887972700595856\n",
      "Training growing_up:  25%|██▎      | 5086/20010 [4:13:56<13:13:04,  3.19s/batch]Batch 5200/20010 Done, mean position loss: 27.020340068340303\n",
      "Training growing_up:  26%|██▎      | 5229/20010 [4:13:58<12:02:49,  2.93s/batch]Batch 5200/20010 Done, mean position loss: 26.986524221897128\n",
      "Training growing_up:  26%|██▎      | 5184/20010 [4:14:02<11:41:01,  2.84s/batch]Batch 5200/20010 Done, mean position loss: 25.937317876815797\n",
      "Training growing_up:  26%|██▎      | 5260/20010 [4:14:10<11:15:22,  2.75s/batch]Batch 5100/20010 Done, mean position loss: 25.7526141500473\n",
      "Training growing_up:  25%|██▎      | 5091/20010 [4:14:12<12:48:36,  3.09s/batch]Batch 5200/20010 Done, mean position loss: 27.121018898487094\n",
      "Training growing_up:  26%|██▎      | 5201/20010 [4:14:12<14:42:00,  3.57s/batch]Batch 5200/20010 Done, mean position loss: 26.700712802410123\n",
      "Training growing_up:  26%|██▎      | 5163/20010 [4:14:20<12:30:41,  3.03s/batch]Batch 5200/20010 Done, mean position loss: 26.451994833946227\n",
      "Training growing_up:  26%|██▎      | 5266/20010 [4:14:39<11:06:54,  2.71s/batch]Batch 5100/20010 Done, mean position loss: 26.171180160045623\n",
      "Training growing_up:  26%|██▎      | 5126/20010 [4:14:40<12:19:32,  2.98s/batch]Batch 5200/20010 Done, mean position loss: 26.36227870941162\n",
      "Training growing_up:  26%|██▎      | 5272/20010 [4:14:44<10:57:27,  2.68s/batch]Batch 5100/20010 Done, mean position loss: 28.796558618545532\n",
      "Training growing_up:  26%|██▎      | 5248/20010 [4:14:56<12:59:49,  3.17s/batch]Batch 5200/20010 Done, mean position loss: 25.838333299160006\n",
      "Training growing_up:  26%|██▎      | 5108/20010 [4:15:01<13:21:45,  3.23s/batch]Batch 5200/20010 Done, mean position loss: 26.909824726581576\n",
      "Training growing_up:  26%|██▎      | 5269/20010 [4:15:02<13:34:58,  3.32s/batch]Batch 5300/20010 Done, mean position loss: 26.812131004333494\n",
      "Training growing_up:  26%|██▎      | 5181/20010 [4:15:10<11:51:18,  2.88s/batch]Batch 5200/20010 Done, mean position loss: 27.315578536987303\n",
      "Training growing_up:  26%|██▎      | 5179/20010 [4:15:12<11:32:20,  2.80s/batch]Batch 5200/20010 Done, mean position loss: 26.93693779706955\n",
      "Training growing_up:  26%|██▎      | 5258/20010 [4:15:24<10:51:58,  2.65s/batch]Batch 5200/20010 Done, mean position loss: 27.177763090133666\n",
      "Training growing_up:  26%|██▎      | 5277/20010 [4:16:06<12:57:29,  3.17s/batch]Batch 5300/20010 Done, mean position loss: 27.01752140522003\n",
      "Training growing_up:  26%|██▎      | 5274/20010 [4:16:08<13:13:37,  3.23s/batch]Batch 5200/20010 Done, mean position loss: 26.672124991416933\n",
      "Training growing_up:  26%|██▎      | 5190/20010 [4:16:13<13:01:32,  3.16s/batch]Batch 5200/20010 Done, mean position loss: 26.06705759048462\n",
      "Training growing_up:  26%|██▎      | 5198/20010 [4:16:19<13:09:34,  3.20s/batch]Batch 5200/20010 Done, mean position loss: 26.593560726642608\n",
      "Training growing_up:  26%|██▎      | 5192/20010 [4:16:20<13:43:38,  3.34s/batch]Batch 5200/20010 Done, mean position loss: 26.843221068382263\n",
      "Training growing_up:  26%|██▎      | 5202/20010 [4:16:23<12:09:37,  2.96s/batch]Batch 5300/20010 Done, mean position loss: 27.03283069372177\n",
      "Training growing_up:  26%|██▎      | 5170/20010 [4:16:27<11:53:08,  2.88s/batch]Batch 5200/20010 Done, mean position loss: 27.12565898180008\n",
      "Training growing_up:  26%|██▎      | 5250/20010 [4:16:41<11:39:49,  2.84s/batch]Batch 5300/20010 Done, mean position loss: 26.963526368141174\n",
      "Training growing_up:  27%|██▍      | 5310/20010 [4:16:47<11:20:00,  2.78s/batch]Batch 5200/20010 Done, mean position loss: 26.748711607456208\n",
      "Training growing_up:  26%|██▎      | 5275/20010 [4:16:51<14:37:18,  3.57s/batch]Batch 5200/20010 Done, mean position loss: 26.140987505912783\n",
      "Training growing_up:  26%|██▎      | 5245/20010 [4:17:16<13:11:10,  3.22s/batch]Batch 5300/20010 Done, mean position loss: 26.96483517408371\n",
      "Training growing_up:  26%|██▎      | 5276/20010 [4:17:21<11:49:08,  2.89s/batch]Batch 5300/20010 Done, mean position loss: 26.170890803337095\n",
      "Training growing_up:  27%|██▍      | 5306/20010 [4:17:35<11:24:42,  2.79s/batch]Batch 5300/20010 Done, mean position loss: 25.66868399143219\n",
      "Training growing_up:  27%|██▍      | 5303/20010 [4:17:39<11:34:45,  2.83s/batch]Batch 5300/20010 Done, mean position loss: 26.48416086435318\n",
      "Training growing_up:  26%|██▎      | 5273/20010 [4:17:41<11:43:24,  2.86s/batch]Batch 5300/20010 Done, mean position loss: 25.531797277927396\n",
      "Training growing_up:  26%|██▎      | 5234/20010 [4:18:05<12:16:18,  2.99s/batch]Batch 5200/20010 Done, mean position loss: 25.569065642356872\n",
      "Training growing_up:  27%|██▍      | 5320/20010 [4:18:10<12:09:05,  2.98s/batch]Batch 5300/20010 Done, mean position loss: 26.791889314651492\n",
      "Training growing_up:  26%|██▎      | 5280/20010 [4:18:15<10:57:29,  2.68s/batch]Batch 5200/20010 Done, mean position loss: 26.952699229717254\n",
      "Training growing_up:  26%|██▎      | 5267/20010 [4:18:17<11:36:10,  2.83s/batch]Batch 5300/20010 Done, mean position loss: 27.004855856895446\n",
      "Training growing_up:  26%|██▎      | 5243/20010 [4:18:31<12:09:26,  2.96s/batch]Batch 5200/20010 Done, mean position loss: 25.328322336673736\n",
      "Training growing_up:  26%|██▎      | 5277/20010 [4:18:35<12:08:11,  2.97s/batch]Batch 5300/20010 Done, mean position loss: 26.955103795528412\n",
      "Training growing_up:  26%|██▎      | 5249/20010 [4:18:35<12:04:20,  2.94s/batch]Batch 5300/20010 Done, mean position loss: 27.156332912445066\n",
      "Training growing_up:  26%|██▍      | 5291/20010 [4:18:45<11:15:18,  2.75s/batch]Batch 5300/20010 Done, mean position loss: 26.97962310552597\n",
      "Training growing_up:  26%|██▎      | 5276/20010 [4:18:53<10:29:51,  2.56s/batch]Batch 5300/20010 Done, mean position loss: 26.895338871479034\n",
      "Training growing_up:  27%|██▍      | 5309/20010 [4:18:58<11:24:46,  2.79s/batch]Batch 5200/20010 Done, mean position loss: 25.58983362913132\n",
      "Training growing_up:  26%|██▎      | 5278/20010 [4:19:00<11:58:42,  2.93s/batch]Batch 5300/20010 Done, mean position loss: 26.439328179359435\n",
      "Training growing_up:  26%|██▎      | 5185/20010 [4:19:03<13:24:05,  3.25s/batch]Batch 5300/20010 Done, mean position loss: 25.41492012500763\n",
      "Training growing_up:  26%|██▎      | 5257/20010 [4:19:12<12:28:42,  3.04s/batch]Batch 5300/20010 Done, mean position loss: 26.16673102617264\n",
      "Training growing_up:  27%|██▍      | 5314/20010 [4:19:13<11:52:58,  2.91s/batch]Batch 5200/20010 Done, mean position loss: 26.54921448945999\n",
      "Training growing_up:  27%|██▍      | 5306/20010 [4:19:16<10:27:00,  2.56s/batch]Batch 5300/20010 Done, mean position loss: 26.071267936229706\n",
      "Training growing_up:  27%|██▍      | 5316/20010 [4:19:27<12:45:02,  3.12s/batch]Batch 5300/20010 Done, mean position loss: 26.52644176721573\n",
      "Training growing_up:  27%|██▍      | 5316/20010 [4:19:43<12:51:16,  3.15s/batch]Batch 5300/20010 Done, mean position loss: 26.757983727455137\n",
      "Training growing_up:  27%|██▍      | 5354/20010 [4:19:50<11:46:24,  2.89s/batch]Batch 5200/20010 Done, mean position loss: 25.85579334974289\n",
      "Training growing_up:  27%|██▍      | 5379/20010 [4:19:54<10:53:20,  2.68s/batch]Batch 5300/20010 Done, mean position loss: 26.759071135520934\n",
      "Training growing_up:  26%|██▎      | 5264/20010 [4:19:55<11:29:33,  2.81s/batch]Batch 5400/20010 Done, mean position loss: 26.66069412946701\n",
      "Training growing_up:  27%|██▍      | 5380/20010 [4:19:58<11:58:25,  2.95s/batch]Batch 5300/20010 Done, mean position loss: 25.641063010692598\n",
      "Training growing_up:  27%|██▍      | 5322/20010 [4:20:00<11:47:10,  2.89s/batch]Batch 5200/20010 Done, mean position loss: 26.66461933851242\n",
      "Training growing_up:  27%|██▍      | 5303/20010 [4:20:03<12:01:43,  2.94s/batch]Batch 5300/20010 Done, mean position loss: 27.015704624652862\n",
      "Training growing_up:  27%|██▍      | 5322/20010 [4:20:18<11:16:51,  2.76s/batch]Batch 5300/20010 Done, mean position loss: 25.58828461647034\n",
      "Training growing_up:  27%|██▍      | 5375/20010 [4:20:25<11:32:42,  2.84s/batch]Batch 5300/20010 Done, mean position loss: 27.11166757583618\n",
      "Training growing_up:  27%|██▍      | 5328/20010 [4:20:59<10:56:02,  2.68s/batch]Batch 5400/20010 Done, mean position loss: 26.43757778644562\n",
      "Training growing_up:  26%|██▎      | 5259/20010 [4:21:06<10:38:46,  2.60s/batch]Batch 5300/20010 Done, mean position loss: 27.257578077316282\n",
      "Training growing_up:  27%|██▍      | 5345/20010 [4:21:10<11:18:28,  2.78s/batch]Batch 5400/20010 Done, mean position loss: 25.597965240478516\n",
      "Training growing_up:  27%|██▋       | 5384/20010 [4:21:12<9:38:17,  2.37s/batch]Batch 5300/20010 Done, mean position loss: 27.182251744270324\n",
      "Training growing_up:  27%|██▍      | 5305/20010 [4:21:15<10:03:41,  2.46s/batch]Batch 5300/20010 Done, mean position loss: 28.971232614517213\n",
      "Training growing_up:  27%|██▍      | 5378/20010 [4:21:18<10:25:16,  2.56s/batch]Batch 5300/20010 Done, mean position loss: 28.977366943359375\n",
      "Training growing_up:  27%|██▍      | 5381/20010 [4:21:28<11:30:35,  2.83s/batch]Batch 5300/20010 Done, mean position loss: 26.76347630739212\n",
      "Training growing_up:  26%|██▎      | 5265/20010 [4:21:37<11:52:23,  2.90s/batch]Batch 5400/20010 Done, mean position loss: 27.540731024742126\n",
      "Training growing_up:  27%|██▋       | 5331/20010 [4:21:42<9:34:18,  2.35s/batch]Batch 5300/20010 Done, mean position loss: 28.211506145000456\n",
      "Training growing_up:  27%|██▍      | 5395/20010 [4:21:42<10:46:06,  2.65s/batch]Batch 5300/20010 Done, mean position loss: 26.94684452533722\n",
      "Training growing_up:  27%|██▍      | 5370/20010 [4:21:59<10:01:17,  2.46s/batch]Batch 5400/20010 Done, mean position loss: 25.6706418299675\n",
      "Training growing_up:  27%|██▍      | 5375/20010 [4:22:00<10:49:47,  2.66s/batch]Batch 5400/20010 Done, mean position loss: 27.086976132392884\n",
      "Training growing_up:  27%|██▍      | 5430/20010 [4:22:24<16:28:26,  4.07s/batch]Batch 5400/20010 Done, mean position loss: 25.50880646467209\n",
      "Training growing_up:  26%|██▎      | 5268/20010 [4:22:27<14:13:49,  3.48s/batch]Batch 5400/20010 Done, mean position loss: 26.166474573612213\n",
      "Training growing_up:  27%|██▍      | 5391/20010 [4:22:32<12:07:05,  2.98s/batch]Batch 5400/20010 Done, mean position loss: 27.270352547168734\n",
      "Training growing_up:  27%|██▍      | 5400/20010 [4:22:56<10:43:32,  2.64s/batch]Batch 5300/20010 Done, mean position loss: 25.90766218662262\n",
      "Training growing_up:  26%|██▋       | 5293/20010 [4:22:58<9:43:39,  2.38s/batch]Batch 5400/20010 Done, mean position loss: 26.569768941402437\n",
      "Training growing_up:  27%|██▍      | 5340/20010 [4:23:01<10:41:48,  2.62s/batch]Batch 5400/20010 Done, mean position loss: 26.404540815353393\n",
      "Training growing_up:  27%|██▍      | 5396/20010 [4:23:03<10:02:30,  2.47s/batch]Batch 5300/20010 Done, mean position loss: 27.0561675453186\n",
      "Training growing_up:  27%|██▍      | 5420/20010 [4:23:15<10:41:01,  2.64s/batch]Batch 5400/20010 Done, mean position loss: 26.3588209438324\n",
      "Training growing_up:  27%|██▍      | 5424/20010 [4:23:19<11:15:34,  2.78s/batch]Batch 5400/20010 Done, mean position loss: 26.550789024829864\n",
      "Training growing_up:  27%|██▍      | 5353/20010 [4:23:25<14:41:06,  3.61s/batch]Batch 5300/20010 Done, mean position loss: 25.81367176055908\n",
      "Training growing_up:  27%|██▍      | 5339/20010 [4:23:37<16:08:39,  3.96s/batch]Batch 5400/20010 Done, mean position loss: 26.197103478908538\n",
      "Training growing_up:  27%|██▍      | 5376/20010 [4:23:37<14:45:07,  3.63s/batch]Batch 5400/20010 Done, mean position loss: 25.89871139526367\n",
      "Training growing_up:  27%|██▍      | 5403/20010 [4:23:42<11:30:50,  2.84s/batch]Batch 5400/20010 Done, mean position loss: 26.7167475771904\n",
      "Training growing_up:  27%|██▍      | 5382/20010 [4:23:50<11:20:32,  2.79s/batch]Batch 5300/20010 Done, mean position loss: 25.686587686538694\n",
      "Training growing_up:  27%|██▋       | 5381/20010 [4:23:53<9:48:15,  2.41s/batch]Batch 5400/20010 Done, mean position loss: 25.490842509269715\n",
      "Training growing_up:  27%|██▍      | 5313/20010 [4:23:59<11:13:12,  2.75s/batch]Batch 5400/20010 Done, mean position loss: 27.03200183868408\n",
      "Training growing_up:  27%|██▍      | 5416/20010 [4:24:04<10:13:47,  2.52s/batch]Batch 5400/20010 Done, mean position loss: 25.536269030570985\n",
      "Training growing_up:  27%|██▍      | 5362/20010 [4:24:04<10:19:59,  2.54s/batch]Batch 5300/20010 Done, mean position loss: 26.91073846578598\n",
      "Training growing_up:  27%|██▍      | 5415/20010 [4:24:14<10:06:45,  2.49s/batch]Batch 5400/20010 Done, mean position loss: 26.2221337556839\n",
      "Training growing_up:  27%|██▍      | 5359/20010 [4:24:31<13:51:09,  3.40s/batch]Batch 5400/20010 Done, mean position loss: 26.631427166461943\n",
      "Training growing_up:  27%|██▍      | 5389/20010 [4:24:37<12:56:01,  3.18s/batch]Batch 5500/20010 Done, mean position loss: 25.74738915681839\n",
      "Training growing_up:  27%|██▍      | 5372/20010 [4:24:39<15:18:59,  3.77s/batch]Batch 5300/20010 Done, mean position loss: 26.760473380088804\n",
      "Training growing_up:  27%|██▍      | 5424/20010 [4:24:41<12:29:24,  3.08s/batch]Batch 5400/20010 Done, mean position loss: 26.413855881690978\n",
      "Training growing_up:  27%|██▍      | 5451/20010 [4:24:43<12:40:50,  3.14s/batch]Batch 5400/20010 Done, mean position loss: 26.886685128211973\n",
      "Training growing_up:  27%|██▍      | 5451/20010 [4:24:49<12:51:26,  3.18s/batch]Batch 5300/20010 Done, mean position loss: 27.18028603553772\n",
      "Training growing_up:  27%|██▍      | 5405/20010 [4:24:51<11:00:24,  2.71s/batch]Batch 5400/20010 Done, mean position loss: 26.6913439822197\n",
      "Training growing_up:  27%|██▍      | 5473/20010 [4:25:10<11:10:22,  2.77s/batch]Batch 5400/20010 Done, mean position loss: 25.80690486192703\n",
      "Training growing_up:  27%|██▍      | 5348/20010 [4:25:14<10:51:25,  2.67s/batch]Batch 5400/20010 Done, mean position loss: 27.21380333185196\n",
      "Training growing_up:  27%|██▍      | 5477/20010 [4:25:39<11:44:29,  2.91s/batch]Batch 5500/20010 Done, mean position loss: 26.96052051305771\n",
      "Training growing_up:  27%|██▍      | 5395/20010 [4:25:41<10:40:31,  2.63s/batch]Batch 5400/20010 Done, mean position loss: 25.51995123386383\n",
      "Training growing_up:  27%|██▍      | 5461/20010 [4:25:54<11:11:01,  2.77s/batch]Batch 5400/20010 Done, mean position loss: 26.041495971679687\n",
      "Training growing_up:  28%|██▍      | 5530/20010 [4:25:57<11:22:19,  2.83s/batch]Batch 5400/20010 Done, mean position loss: 27.01597487926483\n",
      "Training growing_up:  27%|██▍      | 5365/20010 [4:25:59<10:18:48,  2.54s/batch]Batch 5500/20010 Done, mean position loss: 26.031330168247223\n",
      "Training growing_up:  27%|██▍      | 5433/20010 [4:26:01<11:24:48,  2.82s/batch]Batch 5400/20010 Done, mean position loss: 27.68046702623367\n",
      "Training growing_up:  27%|██▍      | 5412/20010 [4:26:17<14:54:01,  3.67s/batch]Batch 5400/20010 Done, mean position loss: 26.31478162527084\n",
      "Training growing_up:  27%|██▍      | 5332/20010 [4:26:24<16:07:07,  3.95s/batch]Batch 5400/20010 Done, mean position loss: 26.404736070632936\n",
      "Training growing_up:  27%|██▍      | 5486/20010 [4:26:32<13:42:33,  3.40s/batch]Batch 5500/20010 Done, mean position loss: 27.084212608337403\n",
      "Training growing_up:  27%|██▍      | 5469/20010 [4:26:38<13:22:29,  3.31s/batch]Batch 5400/20010 Done, mean position loss: 27.875322713851926\n",
      "Training growing_up:  27%|██▍      | 5453/20010 [4:26:48<11:36:21,  2.87s/batch]Batch 5500/20010 Done, mean position loss: 26.35674908876419\n",
      "Training growing_up:  28%|██▍      | 5504/20010 [4:26:56<11:38:40,  2.89s/batch]Batch 5500/20010 Done, mean position loss: 26.307678101062773\n",
      "Training growing_up:  27%|██▍      | 5416/20010 [4:27:10<12:50:54,  3.17s/batch]Batch 5500/20010 Done, mean position loss: 25.193825924396513\n",
      "Training growing_up:  27%|██▍      | 5421/20010 [4:27:15<10:46:29,  2.66s/batch]Batch 5500/20010 Done, mean position loss: 26.792292654514313\n",
      "Training growing_up:  27%|██▍      | 5443/20010 [4:27:17<12:03:10,  2.98s/batch]Batch 5500/20010 Done, mean position loss: 26.781127648353575\n",
      "Training growing_up:  27%|██▍      | 5463/20010 [4:27:45<11:13:46,  2.78s/batch]Batch 5500/20010 Done, mean position loss: 25.943932309150696\n",
      "Training growing_up:  28%|██▍      | 5528/20010 [4:27:46<10:35:57,  2.63s/batch]Batch 5400/20010 Done, mean position loss: 26.091982576847077\n",
      "Training growing_up:  27%|██▍      | 5377/20010 [4:27:52<11:25:55,  2.81s/batch]Batch 5500/20010 Done, mean position loss: 27.946641464233398\n",
      "Training growing_up:  27%|██▍      | 5487/20010 [4:28:02<11:27:20,  2.84s/batch]Batch 5400/20010 Done, mean position loss: 27.30678253173828\n",
      "Training growing_up:  27%|██▍      | 5461/20010 [4:28:07<11:30:20,  2.85s/batch]Batch 5500/20010 Done, mean position loss: 26.452589638233185\n",
      "Training growing_up:  27%|██▍      | 5437/20010 [4:28:11<11:57:03,  2.95s/batch]Batch 5500/20010 Done, mean position loss: 26.85840273618698\n",
      "Training growing_up:  28%|██▍      | 5524/20010 [4:28:16<11:54:32,  2.96s/batch]Batch 5500/20010 Done, mean position loss: 26.5932403755188\n",
      "Training growing_up:  28%|██▍      | 5550/20010 [4:28:21<11:44:41,  2.92s/batch]Batch 5500/20010 Done, mean position loss: 25.91153218269348\n",
      "Training growing_up:  27%|██▍      | 5414/20010 [4:28:23<11:21:28,  2.80s/batch]Batch 5400/20010 Done, mean position loss: 26.57914544582367\n",
      "Training growing_up:  28%|██▍      | 5515/20010 [4:28:32<11:36:29,  2.88s/batch]Batch 5500/20010 Done, mean position loss: 26.27155879974365\n",
      "Training growing_up:  28%|██▌      | 5586/20010 [4:28:42<12:30:00,  3.12s/batch]Batch 5500/20010 Done, mean position loss: 26.107931942939757\n",
      "Training growing_up:  28%|██▍      | 5521/20010 [4:28:42<10:53:07,  2.70s/batch]Batch 5400/20010 Done, mean position loss: 26.69403807640076\n",
      "Training growing_up:  27%|██▍      | 5460/20010 [4:28:42<11:43:19,  2.90s/batch]Batch 5500/20010 Done, mean position loss: 27.19668091773987\n",
      "Training growing_up:  28%|██▌      | 5589/20010 [4:28:49<10:43:50,  2.68s/batch]Batch 5500/20010 Done, mean position loss: 25.691996214389803\n",
      "Training growing_up:  27%|██▍      | 5465/20010 [4:29:04<12:35:33,  3.12s/batch]Batch 5500/20010 Done, mean position loss: 25.664804182052613\n",
      "Batch 5400/20010 Done, mean position loss: 27.555966277122497\n",
      "Training growing_up:  27%|██▍      | 5394/20010 [4:29:24<11:20:20,  2.79s/batch]Batch 5600/20010 Done, mean position loss: 26.499264624118805\n",
      "Training growing_up:  27%|██▍      | 5495/20010 [4:29:25<11:32:29,  2.86s/batch]Batch 5500/20010 Done, mean position loss: 26.20319368124008\n",
      "Training growing_up:  28%|██▌      | 5566/20010 [4:29:35<12:11:40,  3.04s/batch]Batch 5500/20010 Done, mean position loss: 25.39220301628113\n",
      "Training growing_up:  27%|██▍      | 5439/20010 [4:29:37<11:48:03,  2.92s/batch]Batch 5500/20010 Done, mean position loss: 26.244044127464296\n",
      "Training growing_up:  28%|██▍      | 5551/20010 [4:29:41<11:18:46,  2.82s/batch]Batch 5500/20010 Done, mean position loss: 26.58213637828827\n",
      "Training growing_up:  28%|██▍      | 5521/20010 [4:29:41<11:57:29,  2.97s/batch]Batch 5400/20010 Done, mean position loss: 26.104097955226898\n",
      "Training growing_up:  27%|██▍      | 5481/20010 [4:29:45<11:10:24,  2.77s/batch]Batch 5400/20010 Done, mean position loss: 26.141693122386933\n",
      "Training growing_up:  28%|██▍      | 5548/20010 [4:30:05<11:54:28,  2.96s/batch]Batch 5500/20010 Done, mean position loss: 26.224768340587616\n",
      "Training growing_up:  28%|██▌      | 5571/20010 [4:30:14<11:50:59,  2.95s/batch]Batch 5500/20010 Done, mean position loss: 27.571702706813813\n",
      "Training growing_up:  27%|██▍      | 5451/20010 [4:30:32<11:08:06,  2.75s/batch]Batch 5600/20010 Done, mean position loss: 27.117186498641967\n",
      "Training growing_up:  28%|██▌      | 5575/20010 [4:30:45<11:08:31,  2.78s/batch]Batch 5500/20010 Done, mean position loss: 25.530628702640534\n",
      "Training growing_up:  27%|██▍      | 5489/20010 [4:30:51<11:13:13,  2.78s/batch]Batch 5500/20010 Done, mean position loss: 26.4836074924469\n",
      "Training growing_up:  28%|██▌      | 5585/20010 [4:30:51<12:26:02,  3.10s/batch]Batch 5500/20010 Done, mean position loss: 26.171118698120118\n",
      "Training growing_up:  28%|██▍      | 5503/20010 [4:30:51<13:04:15,  3.24s/batch]Batch 5600/20010 Done, mean position loss: 26.015933678150176\n",
      "Training growing_up:  28%|██▍      | 5543/20010 [4:31:09<11:43:54,  2.92s/batch]Batch 5500/20010 Done, mean position loss: 28.21210521936417\n",
      "Training growing_up:  28%|██▍      | 5520/20010 [4:31:09<11:38:41,  2.89s/batch]Batch 5500/20010 Done, mean position loss: 26.400364537239078\n",
      "Training growing_up:  27%|██▍      | 5430/20010 [4:31:17<12:20:22,  3.05s/batch]Batch 5600/20010 Done, mean position loss: 27.027322816848756\n",
      "Training growing_up:  28%|██▍      | 5514/20010 [4:31:26<10:42:30,  2.66s/batch]Batch 5500/20010 Done, mean position loss: 25.753542959690094\n",
      "Training growing_up:  28%|██▌      | 5567/20010 [4:31:27<12:17:06,  3.06s/batch]Batch 5500/20010 Done, mean position loss: 25.744581809043886\n",
      "Training growing_up:  28%|██▌      | 5590/20010 [4:31:39<12:46:34,  3.19s/batch]Batch 5600/20010 Done, mean position loss: 26.044251475334168\n",
      "Training growing_up:  28%|██▍      | 5519/20010 [4:31:45<11:53:24,  2.95s/batch]Batch 5600/20010 Done, mean position loss: 25.556673827171323\n",
      "Training growing_up:  28%|██▌      | 5633/20010 [4:32:05<11:54:11,  2.98s/batch]Batch 5600/20010 Done, mean position loss: 25.567331538200378\n",
      "Training growing_up:  28%|██▍      | 5515/20010 [4:32:06<11:26:51,  2.84s/batch]Batch 5600/20010 Done, mean position loss: 24.844585502147673\n",
      "Training growing_up:  28%|██▌      | 5564/20010 [4:32:10<11:45:42,  2.93s/batch]Batch 5600/20010 Done, mean position loss: 26.52405044078827\n",
      "Training growing_up:  28%|██▌      | 5590/20010 [4:32:42<11:55:20,  2.98s/batch]Batch 5500/20010 Done, mean position loss: 25.290191924571992\n",
      "Training growing_up:  28%|██▌      | 5631/20010 [4:32:43<11:32:49,  2.89s/batch]Batch 5600/20010 Done, mean position loss: 25.667591824531556\n",
      "Training growing_up:  28%|██▍      | 5534/20010 [4:32:50<12:32:21,  3.12s/batch]Batch 5600/20010 Done, mean position loss: 26.86897836923599\n",
      "Training growing_up:  28%|██▍      | 5539/20010 [4:33:00<11:44:59,  2.92s/batch]Batch 5500/20010 Done, mean position loss: 26.704939785003663\n",
      "Training growing_up:  28%|██▌      | 5599/20010 [4:33:06<12:12:34,  3.05s/batch]Batch 5600/20010 Done, mean position loss: 26.885177295207978\n",
      "Training growing_up:  27%|██▍      | 5482/20010 [4:33:10<11:10:28,  2.77s/batch]Batch 5600/20010 Done, mean position loss: 27.139416105747223\n",
      "Training growing_up:  28%|██▌      | 5597/20010 [4:33:11<11:16:01,  2.81s/batch]Batch 5600/20010 Done, mean position loss: 25.38577442407608\n",
      "Training growing_up:  28%|██▌      | 5631/20010 [4:33:12<11:03:40,  2.77s/batch]Batch 5600/20010 Done, mean position loss: 26.60190856933594\n",
      "Training growing_up:  28%|██▍      | 5546/20010 [4:33:22<11:56:27,  2.97s/batch]Batch 5600/20010 Done, mean position loss: 26.547867991924285\n",
      "Training growing_up:  27%|██▍      | 5475/20010 [4:33:35<13:11:39,  3.27s/batch]Batch 5500/20010 Done, mean position loss: 25.039364387989046\n",
      "Training growing_up:  28%|██▌      | 5690/20010 [4:33:39<11:24:30,  2.87s/batch]Batch 5600/20010 Done, mean position loss: 26.714382693767547\n",
      "Training growing_up:  28%|██▌      | 5595/20010 [4:33:39<11:52:35,  2.97s/batch]Batch 5600/20010 Done, mean position loss: 25.93159449338913\n",
      "Training growing_up:  28%|██▍      | 5521/20010 [4:33:44<11:39:29,  2.90s/batch]Batch 5600/20010 Done, mean position loss: 26.613611583709716\n",
      "Training growing_up:  28%|██▌      | 5612/20010 [4:33:44<12:04:07,  3.02s/batch]Batch 5500/20010 Done, mean position loss: 26.19044468641281\n",
      "Training growing_up:  28%|██▍      | 5505/20010 [4:33:56<11:56:33,  2.96s/batch]Batch 5600/20010 Done, mean position loss: 25.843946664333345\n",
      "Training growing_up:  28%|██▌      | 5629/20010 [4:34:04<12:37:24,  3.16s/batch]Batch 5500/20010 Done, mean position loss: 25.12537813425064\n",
      "Training growing_up:  28%|██▍      | 5557/20010 [4:34:12<12:04:23,  3.01s/batch]Batch 5700/20010 Done, mean position loss: 25.478432278633118\n",
      "Training growing_up:  29%|██▌      | 5706/20010 [4:34:24<10:36:28,  2.67s/batch]Batch 5600/20010 Done, mean position loss: 26.662510685920715\n",
      "Training growing_up:  28%|██▌      | 5577/20010 [4:34:32<11:02:59,  2.76s/batch]Batch 5600/20010 Done, mean position loss: 24.840413584709168\n",
      "Training growing_up:  28%|██▌      | 5600/20010 [4:34:36<12:03:44,  3.01s/batch]Batch 5600/20010 Done, mean position loss: 26.421480665206907\n",
      "Training growing_up:  28%|██▌      | 5579/20010 [4:34:38<11:13:20,  2.80s/batch]Batch 5600/20010 Done, mean position loss: 26.574379665851595\n",
      "Training growing_up:  28%|██▌      | 5633/20010 [4:34:44<12:33:17,  3.14s/batch]Batch 5500/20010 Done, mean position loss: 26.457759068012237\n",
      "Training growing_up:  28%|██▍      | 5545/20010 [4:34:53<12:58:16,  3.23s/batch]Batch 5500/20010 Done, mean position loss: 25.99519237279892\n",
      "Training growing_up:  28%|██▌      | 5597/20010 [4:35:00<12:06:58,  3.03s/batch]Batch 5600/20010 Done, mean position loss: 26.371099798679353\n",
      "Training growing_up:  28%|██▌      | 5645/20010 [4:35:12<12:14:40,  3.07s/batch]Batch 5600/20010 Done, mean position loss: 26.011753344535826\n",
      "Training growing_up:  28%|██▌      | 5654/20010 [4:35:25<11:48:37,  2.96s/batch]Batch 5700/20010 Done, mean position loss: 26.22753504037857\n",
      "Training growing_up:  28%|██▌      | 5674/20010 [4:35:42<12:09:54,  3.05s/batch]Batch 5600/20010 Done, mean position loss: 26.09676897287369\n",
      "Training growing_up:  28%|██▌      | 5561/20010 [4:35:42<11:55:06,  2.97s/batch]Batch 5600/20010 Done, mean position loss: 25.4642666888237\n",
      "Training growing_up:  28%|██▌      | 5651/20010 [4:35:45<11:51:53,  2.97s/batch]Batch 5600/20010 Done, mean position loss: 28.111098141670226\n",
      "Training growing_up:  28%|██▌      | 5687/20010 [4:35:51<11:29:12,  2.89s/batch]Batch 5700/20010 Done, mean position loss: 25.38157907009125\n",
      "Training growing_up:  28%|██▌      | 5667/20010 [4:36:02<10:26:39,  2.62s/batch]Batch 5600/20010 Done, mean position loss: 26.680853366851807\n",
      "Training growing_up:  28%|██▌      | 5660/20010 [4:36:10<10:26:20,  2.62s/batch]Batch 5600/20010 Done, mean position loss: 27.510176928043364\n",
      "Training growing_up:  28%|██▍      | 5529/20010 [4:36:12<12:21:38,  3.07s/batch]Batch 5700/20010 Done, mean position loss: 27.216485853195188\n",
      "Training growing_up:  28%|██▍      | 5554/20010 [4:36:20<12:17:33,  3.06s/batch]Batch 5600/20010 Done, mean position loss: 25.978827395439147\n",
      "Training growing_up:  28%|██▍      | 5548/20010 [4:36:25<12:21:02,  3.07s/batch]Batch 5600/20010 Done, mean position loss: 26.563729689121246\n",
      "Training growing_up:  28%|██▍      | 5551/20010 [4:36:33<11:25:19,  2.84s/batch]Batch 5700/20010 Done, mean position loss: 27.09772219657898\n",
      "Training growing_up:  28%|██▌      | 5672/20010 [4:36:37<11:45:59,  2.95s/batch]Batch 5700/20010 Done, mean position loss: 24.980330405235293\n",
      "Training growing_up:  29%|██▌      | 5757/20010 [4:36:50<11:43:01,  2.96s/batch]Batch 5700/20010 Done, mean position loss: 25.38278987646103\n",
      "Training growing_up:  28%|██▌      | 5678/20010 [4:37:00<11:38:22,  2.92s/batch]Batch 5700/20010 Done, mean position loss: 26.291255080699923\n",
      "Training growing_up:  29%|██▌      | 5735/20010 [4:37:06<11:52:44,  3.00s/batch]Batch 5700/20010 Done, mean position loss: 25.539448771476742\n",
      "Training growing_up:  28%|██▌      | 5662/20010 [4:37:28<10:17:41,  2.58s/batch]Batch 5700/20010 Done, mean position loss: 25.691350803375244\n",
      "Training growing_up:  28%|██▊       | 5664/20010 [4:37:32<9:07:18,  2.29s/batch]Batch 5600/20010 Done, mean position loss: 25.512414610385896\n",
      "Training growing_up:  28%|██▊       | 5684/20010 [4:37:36<8:49:45,  2.22s/batch]Batch 5700/20010 Done, mean position loss: 27.279427618980407\n",
      "Training growing_up:  29%|██▌      | 5728/20010 [4:37:46<10:00:08,  2.52s/batch]Batch 5700/20010 Done, mean position loss: 27.268173658847807\n",
      "Training growing_up:  28%|██▌      | 5645/20010 [4:37:54<12:00:34,  3.01s/batch]Batch 5700/20010 Done, mean position loss: 27.27098834514618\n",
      "Training growing_up:  28%|██▌      | 5699/20010 [4:37:59<10:13:27,  2.57s/batch]Batch 5700/20010 Done, mean position loss: 25.904405579566955\n",
      "Training growing_up:  29%|██▊       | 5748/20010 [4:38:00<9:43:03,  2.45s/batch]Batch 5600/20010 Done, mean position loss: 27.16641494512558\n",
      "Training growing_up:  29%|██▌      | 5733/20010 [4:38:02<10:50:43,  2.73s/batch]Batch 5700/20010 Done, mean position loss: 25.953600134849545\n",
      "Training growing_up:  28%|██▌      | 5636/20010 [4:38:06<11:55:56,  2.99s/batch]Batch 5700/20010 Done, mean position loss: 25.85466995716095\n",
      "Training growing_up:  29%|██▌      | 5734/20010 [4:38:19<10:09:05,  2.56s/batch]Batch 5700/20010 Done, mean position loss: 25.633120727539062\n",
      "Training growing_up:  28%|██▌      | 5609/20010 [4:38:23<12:06:46,  3.03s/batch]Batch 5600/20010 Done, mean position loss: 24.699096665382385\n",
      "Training growing_up:  29%|██▌      | 5748/20010 [4:38:25<11:04:21,  2.79s/batch]Batch 5700/20010 Done, mean position loss: 25.621824843883516\n",
      "Training growing_up:  29%|██▌      | 5738/20010 [4:38:29<10:04:25,  2.54s/batch]Batch 5700/20010 Done, mean position loss: 26.919310090541842\n",
      "Training growing_up:  28%|██▌      | 5646/20010 [4:38:32<11:40:33,  2.93s/batch]Batch 5600/20010 Done, mean position loss: 26.93030638217926\n",
      "Training growing_up:  28%|██▌      | 5661/20010 [4:38:39<12:00:44,  3.01s/batch]Batch 5700/20010 Done, mean position loss: 25.890659623146057\n",
      "Training growing_up:  28%|██▌      | 5666/20010 [4:38:45<11:42:03,  2.94s/batch]Batch 5800/20010 Done, mean position loss: 25.734161002635958\n",
      "Training growing_up:  28%|██▌      | 5690/20010 [4:38:50<11:39:33,  2.93s/batch]Batch 5600/20010 Done, mean position loss: 25.665912330150604\n",
      "Training growing_up:  28%|██▌      | 5612/20010 [4:39:04<11:02:58,  2.76s/batch]Batch 5700/20010 Done, mean position loss: 27.221746265888214\n",
      "Training growing_up:  29%|██▌      | 5746/20010 [4:39:11<11:19:38,  2.86s/batch]Batch 5700/20010 Done, mean position loss: 25.689079146385193\n",
      "Training growing_up:  29%|██▌      | 5730/20010 [4:39:15<10:45:38,  2.71s/batch]Batch 5700/20010 Done, mean position loss: 26.44221958398819\n",
      "Training growing_up:  29%|██▌      | 5716/20010 [4:39:21<10:42:42,  2.70s/batch]Batch 5700/20010 Done, mean position loss: 27.17218241930008\n",
      "Training growing_up:  28%|██▌      | 5619/20010 [4:39:39<11:24:43,  2.85s/batch]Batch 5600/20010 Done, mean position loss: 26.296936094760895\n",
      "Training growing_up:  29%|██▌      | 5739/20010 [4:39:41<11:59:48,  3.03s/batch]Batch 5600/20010 Done, mean position loss: 26.03731330394745\n",
      "Training growing_up:  29%|██▌      | 5775/20010 [4:39:43<11:05:27,  2.80s/batch]Batch 5700/20010 Done, mean position loss: 25.85191497325897\n",
      "Training growing_up:  29%|██▌      | 5733/20010 [4:39:58<11:03:40,  2.79s/batch]Batch 5700/20010 Done, mean position loss: 27.45507448196411\n",
      "Training growing_up:  29%|██▌      | 5754/20010 [4:40:04<11:22:07,  2.87s/batch]Batch 5800/20010 Done, mean position loss: 26.138701419830323\n",
      "Training growing_up:  28%|██▌      | 5690/20010 [4:40:24<10:25:47,  2.62s/batch]Batch 5700/20010 Done, mean position loss: 26.021898934841154\n",
      "Training growing_up:  29%|██▌      | 5753/20010 [4:40:24<11:13:00,  2.83s/batch]Batch 5800/20010 Done, mean position loss: 24.859897174835204\n",
      "Training growing_up:  29%|██▌      | 5759/20010 [4:40:26<12:33:22,  3.17s/batch]Batch 5700/20010 Done, mean position loss: 26.77214689254761\n",
      "Training growing_up:  29%|██▌      | 5791/20010 [4:40:35<10:44:57,  2.72s/batch]Batch 5700/20010 Done, mean position loss: 26.33268474817276\n",
      "Training growing_up:  29%|██▌      | 5706/20010 [4:40:38<11:48:00,  2.97s/batch]Batch 5700/20010 Done, mean position loss: 26.537679281234738\n",
      "Training growing_up:  29%|██▌      | 5707/20010 [4:40:42<10:23:21,  2.61s/batch]Batch 5700/20010 Done, mean position loss: 27.335525817871094\n",
      "Training growing_up:  28%|██▌      | 5652/20010 [4:40:54<10:11:53,  2.56s/batch]Batch 5800/20010 Done, mean position loss: 27.095515074729917\n",
      "Training growing_up:  29%|██▌      | 5821/20010 [4:40:56<10:42:55,  2.72s/batch]Batch 5700/20010 Done, mean position loss: 26.84954416275024\n",
      "Training growing_up:  29%|██▌      | 5704/20010 [4:41:04<10:30:44,  2.65s/batch]Batch 5800/20010 Done, mean position loss: 25.991211788654326\n",
      "Training growing_up:  29%|██▌      | 5740/20010 [4:41:03<10:42:34,  2.70s/batch]Batch 5800/20010 Done, mean position loss: 26.400481293201448\n",
      "Training growing_up:  29%|██▌      | 5705/20010 [4:41:07<11:02:00,  2.78s/batch]Batch 5700/20010 Done, mean position loss: 26.042316789627076\n",
      "Training growing_up:  29%|██▉       | 5859/20010 [4:41:20<9:28:15,  2.41s/batch]Batch 5800/20010 Done, mean position loss: 27.087626073360443\n",
      "Training growing_up:  29%|██▋      | 5861/20010 [4:41:26<10:14:35,  2.61s/batch]Batch 5800/20010 Done, mean position loss: 25.62048786640167\n",
      "Training growing_up:  29%|██▌      | 5717/20010 [4:41:41<11:30:55,  2.90s/batch]Batch 5800/20010 Done, mean position loss: 25.3263991355896\n",
      "Training growing_up:  28%|██▌      | 5688/20010 [4:42:07<10:57:38,  2.76s/batch]Batch 5800/20010 Done, mean position loss: 25.420143234729768\n",
      "Training growing_up:  29%|██▋      | 5840/20010 [4:42:09<11:01:42,  2.80s/batch]Batch 5700/20010 Done, mean position loss: 25.58957666158676\n",
      "Training growing_up:  29%|██▌      | 5793/20010 [4:42:14<10:48:30,  2.74s/batch]Batch 5800/20010 Done, mean position loss: 26.283492276668547\n",
      "Training growing_up:  29%|██▌      | 5740/20010 [4:42:24<11:00:14,  2.78s/batch]Batch 5800/20010 Done, mean position loss: 27.786082105636595\n",
      "Training growing_up:  29%|██▌      | 5791/20010 [4:42:35<11:16:50,  2.86s/batch]Batch 5800/20010 Done, mean position loss: 26.23418278694153\n",
      "Training growing_up:  28%|██▌      | 5688/20010 [4:42:39<11:38:35,  2.93s/batch]Batch 5800/20010 Done, mean position loss: 26.83146501302719\n",
      "Training growing_up:  29%|██▌      | 5804/20010 [4:42:43<10:35:37,  2.68s/batch]Batch 5700/20010 Done, mean position loss: 26.87191520690918\n",
      "Training growing_up:  29%|██▌      | 5836/20010 [4:42:44<10:53:57,  2.77s/batch]Batch 5800/20010 Done, mean position loss: 26.087215921878812\n",
      "Training growing_up:  29%|██▌      | 5808/20010 [4:42:53<10:02:35,  2.55s/batch]Batch 5800/20010 Done, mean position loss: 26.28075135946274\n",
      "Training growing_up:  29%|██▌      | 5780/20010 [4:42:54<12:25:35,  3.14s/batch]Batch 5800/20010 Done, mean position loss: 25.35456943511963\n",
      "Training growing_up:  29%|██▌      | 5805/20010 [4:43:04<11:22:51,  2.88s/batch]Batch 5800/20010 Done, mean position loss: 26.903161911964418\n",
      "Training growing_up:  29%|██▌      | 5759/20010 [4:43:09<10:45:40,  2.72s/batch]Batch 5700/20010 Done, mean position loss: 24.890280628204344\n",
      "Training growing_up:  29%|██▌      | 5753/20010 [4:43:11<11:16:12,  2.85s/batch]Batch 5800/20010 Done, mean position loss: 26.172366070747376\n",
      "Training growing_up:  29%|██▌      | 5779/20010 [4:43:17<11:18:21,  2.86s/batch]Batch 5900/20010 Done, mean position loss: 25.50816781282425\n",
      "Training growing_up:  29%|██▌      | 5774/20010 [4:43:20<12:18:45,  3.11s/batch]Batch 5700/20010 Done, mean position loss: 27.140904741287233\n",
      "Training growing_up:  29%|██▌      | 5765/20010 [4:43:27<12:06:33,  3.06s/batch]Batch 5800/20010 Done, mean position loss: 25.430503921508787\n",
      "Training growing_up:  29%|██▌      | 5826/20010 [4:43:33<11:17:32,  2.87s/batch]Batch 5700/20010 Done, mean position loss: 25.670097134113313\n",
      "Training growing_up:  29%|██▌      | 5772/20010 [4:43:47<11:58:47,  3.03s/batch]Batch 5800/20010 Done, mean position loss: 26.303140034675597\n",
      "Training growing_up:  29%|██▌      | 5818/20010 [4:43:56<12:40:01,  3.21s/batch]Batch 5800/20010 Done, mean position loss: 26.509278233051297\n",
      "Training growing_up:  29%|██▌      | 5777/20010 [4:44:01<10:36:06,  2.68s/batch]Batch 5800/20010 Done, mean position loss: 26.249465942382812\n",
      "Training growing_up:  29%|██▌      | 5808/20010 [4:44:10<11:43:49,  2.97s/batch]Batch 5800/20010 Done, mean position loss: 26.35671811103821\n",
      "Training growing_up:  29%|██▌      | 5720/20010 [4:44:26<10:45:09,  2.71s/batch]Batch 5800/20010 Done, mean position loss: 26.223878726959228\n",
      "Training growing_up:  29%|██▌      | 5787/20010 [4:44:32<13:02:10,  3.30s/batch]Batch 5700/20010 Done, mean position loss: 25.753198232650753\n",
      "Training growing_up:  29%|██▋      | 5867/20010 [4:44:39<11:32:48,  2.94s/batch]Batch 5700/20010 Done, mean position loss: 24.934362175464628\n",
      "Training growing_up:  29%|██▌      | 5752/20010 [4:44:39<13:27:30,  3.40s/batch]Batch 5800/20010 Done, mean position loss: 25.91525540113449\n",
      "Training growing_up:  29%|██▋      | 5869/20010 [4:44:45<12:06:08,  3.08s/batch]Batch 5900/20010 Done, mean position loss: 25.919531795978546\n",
      "Training growing_up:  29%|██▋      | 5849/20010 [4:45:12<10:54:04,  2.77s/batch]Batch 5900/20010 Done, mean position loss: 26.118258054256437\n",
      "Training growing_up:  29%|██▌      | 5788/20010 [4:45:15<10:58:49,  2.78s/batch]Batch 5800/20010 Done, mean position loss: 25.546561226844787\n",
      "Training growing_up:  29%|██▌      | 5826/20010 [4:45:15<11:57:15,  3.03s/batch]Batch 5800/20010 Done, mean position loss: 26.83891826629639\n",
      "Training growing_up:  29%|██▋      | 5851/20010 [4:45:27<11:49:54,  3.01s/batch]Batch 5800/20010 Done, mean position loss: 26.723253548145294\n",
      "Training growing_up:  29%|██▌      | 5744/20010 [4:45:29<11:30:05,  2.90s/batch]Batch 5800/20010 Done, mean position loss: 25.892859189510347\n",
      "Training growing_up:  29%|██▋      | 5854/20010 [4:45:37<12:20:27,  3.14s/batch]Batch 5800/20010 Done, mean position loss: 26.607544691562655\n",
      "Training growing_up:  29%|██▌      | 5833/20010 [4:45:44<11:40:43,  2.97s/batch]Batch 5900/20010 Done, mean position loss: 27.25999254465103\n",
      "Training growing_up:  30%|██▋      | 5914/20010 [4:45:48<12:00:41,  3.07s/batch]Batch 5800/20010 Done, mean position loss: 25.975800490379335\n",
      "Training growing_up:  29%|██▋      | 5857/20010 [4:45:53<10:50:01,  2.76s/batch]Batch 5800/20010 Done, mean position loss: 26.19625850439072\n",
      "Training growing_up:  29%|██▌      | 5779/20010 [4:45:56<10:33:13,  2.67s/batch]Batch 5900/20010 Done, mean position loss: 25.793285977840423\n",
      "Training growing_up:  29%|██▋      | 5866/20010 [4:45:59<11:51:40,  3.02s/batch]Batch 5900/20010 Done, mean position loss: 25.868590381145477\n",
      "Training growing_up:  29%|██▌      | 5757/20010 [4:46:15<11:12:03,  2.83s/batch]Batch 5900/20010 Done, mean position loss: 26.077094013690946\n",
      "Training growing_up:  29%|██▋      | 5859/20010 [4:46:18<11:54:04,  3.03s/batch]Batch 5900/20010 Done, mean position loss: 25.506041939258573\n",
      "Training growing_up:  29%|██▌      | 5818/20010 [4:46:38<10:41:25,  2.71s/batch]Batch 5900/20010 Done, mean position loss: 24.864336087703705\n",
      "Training growing_up:  29%|██▋      | 5885/20010 [4:46:58<11:44:16,  2.99s/batch]Batch 5900/20010 Done, mean position loss: 25.874596524238587\n",
      "Training growing_up:  30%|██▋      | 5938/20010 [4:46:58<10:46:11,  2.76s/batch]Batch 5800/20010 Done, mean position loss: 25.271184678077695\n",
      "Training growing_up:  29%|██▋      | 5849/20010 [4:46:59<11:08:37,  2.83s/batch]Batch 5900/20010 Done, mean position loss: 25.76370697021484\n",
      "Training growing_up:  29%|██▋      | 5879/20010 [4:47:15<10:56:26,  2.79s/batch]Batch 5900/20010 Done, mean position loss: 26.530560352802276\n",
      "Training growing_up:  29%|██▌      | 5833/20010 [4:47:26<11:58:27,  3.04s/batch]Batch 5900/20010 Done, mean position loss: 27.556566491127015\n",
      "Training growing_up:  29%|██▋      | 5846/20010 [4:47:38<11:39:02,  2.96s/batch]Batch 5900/20010 Done, mean position loss: 26.15622139930725\n",
      "Training growing_up:  29%|██▌      | 5815/20010 [4:47:39<11:40:22,  2.96s/batch]Batch 5900/20010 Done, mean position loss: 25.86189083099365\n",
      "Training growing_up:  30%|██▋      | 5924/20010 [4:47:45<11:11:52,  2.86s/batch]Batch 5800/20010 Done, mean position loss: 27.660988981723783\n",
      "Training growing_up:  29%|██▋      | 5869/20010 [4:47:46<11:36:52,  2.96s/batch]Batch 5900/20010 Done, mean position loss: 25.36854688167572\n",
      "Training growing_up:  30%|██▋      | 5903/20010 [4:47:52<11:41:13,  2.98s/batch]Batch 5900/20010 Done, mean position loss: 26.287480752468106\n",
      "Training growing_up:  30%|██▋      | 5943/20010 [4:48:03<12:05:57,  3.10s/batch]Batch 5900/20010 Done, mean position loss: 26.942742495536805\n",
      "Training growing_up:  30%|██▋      | 5961/20010 [4:48:04<10:14:25,  2.62s/batch]Batch 5900/20010 Done, mean position loss: 26.908780629634855\n",
      "Training growing_up:  30%|██▋      | 5916/20010 [4:48:08<11:43:25,  2.99s/batch]Batch 6000/20010 Done, mean position loss: 25.997598078250885\n",
      "Training growing_up:  29%|██▋      | 5875/20010 [4:48:13<12:13:59,  3.12s/batch]Batch 5800/20010 Done, mean position loss: 25.871123962402343\n",
      "Training growing_up:  29%|██▌      | 5811/20010 [4:48:15<12:14:57,  3.11s/batch]Batch 5800/20010 Done, mean position loss: 25.97473322868347\n",
      "Training growing_up:  30%|██▋      | 5927/20010 [4:48:17<10:46:59,  2.76s/batch]Batch 5900/20010 Done, mean position loss: 25.81932305574417\n",
      "Training growing_up:  29%|██▋      | 5854/20010 [4:48:27<11:03:53,  2.81s/batch]Batch 5800/20010 Done, mean position loss: 25.196093220710758\n",
      "Training growing_up:  29%|██▋      | 5895/20010 [4:48:50<11:36:16,  2.96s/batch]Batch 5900/20010 Done, mean position loss: 26.746831369400024\n",
      "Training growing_up:  29%|██▋      | 5902/20010 [4:48:52<10:51:10,  2.77s/batch]Batch 5900/20010 Done, mean position loss: 25.443488261699677\n",
      "Training growing_up:  30%|██▋      | 5929/20010 [4:48:59<11:41:53,  2.99s/batch]Batch 5900/20010 Done, mean position loss: 25.836509089469907\n",
      "Training growing_up:  29%|██▌      | 5791/20010 [4:49:09<11:48:38,  2.99s/batch]Batch 5900/20010 Done, mean position loss: 25.395233843326565\n",
      "Training growing_up:  29%|██▌      | 5798/20010 [4:49:20<10:56:45,  2.77s/batch]Batch 5900/20010 Done, mean position loss: 26.9306406712532\n",
      "Training growing_up:  30%|██▋      | 5936/20010 [4:49:25<12:18:16,  3.15s/batch]Batch 5900/20010 Done, mean position loss: 25.721700499057768\n",
      "Training growing_up:  29%|██▋      | 5877/20010 [4:49:28<10:27:00,  2.66s/batch]Batch 5800/20010 Done, mean position loss: 25.837949450016023\n",
      "Training growing_up:  29%|██▋      | 5879/20010 [4:49:35<11:19:51,  2.89s/batch]Batch 6000/20010 Done, mean position loss: 25.26841993808746\n",
      "Training growing_up:  29%|██▋      | 5840/20010 [4:49:39<11:05:47,  2.82s/batch]Batch 5800/20010 Done, mean position loss: 25.40748240709305\n",
      "Training growing_up:  30%|██▉       | 5916/20010 [4:49:59<9:48:49,  2.51s/batch]Batch 6000/20010 Done, mean position loss: 25.454665191173554\n",
      "Training growing_up:  30%|██▋      | 5989/20010 [4:50:10<12:07:10,  3.11s/batch]Batch 5900/20010 Done, mean position loss: 25.92506484031677\n",
      "Batch 5900/20010 Done, mean position loss: 26.513817329406738\n",
      "Training growing_up:  29%|██▋      | 5898/20010 [4:50:16<12:08:44,  3.10s/batch]Batch 5900/20010 Done, mean position loss: 26.69081850528717\n",
      "Training growing_up:  30%|██▋      | 5933/20010 [4:50:23<10:32:04,  2.69s/batch]Batch 5900/20010 Done, mean position loss: 26.71759334087372\n",
      "Training growing_up:  29%|██▋      | 5846/20010 [4:50:29<11:07:22,  2.83s/batch]Batch 5900/20010 Done, mean position loss: 28.735649580955503\n",
      "Training growing_up:  29%|██▋      | 5846/20010 [4:50:32<12:58:45,  3.30s/batch]Batch 6000/20010 Done, mean position loss: 27.116087946891785\n",
      "Training growing_up:  29%|██▌      | 5826/20010 [4:50:40<11:38:56,  2.96s/batch]Batch 5900/20010 Done, mean position loss: 25.68630090236664\n",
      "Training growing_up:  30%|██▋      | 5959/20010 [4:50:43<10:27:03,  2.68s/batch]Batch 6000/20010 Done, mean position loss: 25.35237737894058\n",
      "Training growing_up:  30%|██▋      | 5937/20010 [4:50:45<12:19:46,  3.15s/batch]Batch 5900/20010 Done, mean position loss: 25.511139240264892\n",
      "Training growing_up:  30%|██▋      | 6004/20010 [4:50:53<11:29:05,  2.95s/batch]Batch 6000/20010 Done, mean position loss: 25.863803169727326\n",
      "Training growing_up:  30%|██▋      | 5909/20010 [4:51:05<13:17:57,  3.40s/batch]Batch 6000/20010 Done, mean position loss: 26.281655495166778\n",
      "Training growing_up:  30%|██▋      | 5970/20010 [4:51:13<11:35:37,  2.97s/batch]Batch 6000/20010 Done, mean position loss: 25.32050523996353\n",
      "Training growing_up:  30%|██▋      | 6031/20010 [4:51:28<10:32:10,  2.71s/batch]Batch 6000/20010 Done, mean position loss: 25.323705654144288\n",
      "Training growing_up:  30%|██▋      | 5977/20010 [4:51:45<10:56:00,  2.80s/batch]Batch 6000/20010 Done, mean position loss: 25.288509848117826\n",
      "Training growing_up:  29%|██▉       | 5844/20010 [4:51:46<9:49:41,  2.50s/batch]Batch 5900/20010 Done, mean position loss: 25.955028142929077\n",
      "Training growing_up:  30%|██▋      | 5928/20010 [4:51:46<10:31:56,  2.69s/batch]Batch 6000/20010 Done, mean position loss: 27.16587933540344\n",
      "Training growing_up:  29%|██▋      | 5856/20010 [4:52:05<10:38:47,  2.71s/batch]Batch 6000/20010 Done, mean position loss: 26.558282573223114\n",
      "Training growing_up:  29%|██▉       | 5896/20010 [4:52:12<9:45:37,  2.49s/batch]Batch 6000/20010 Done, mean position loss: 26.90923839569092\n",
      "Training growing_up:  30%|███       | 6044/20010 [4:52:25<8:44:23,  2.25s/batch]Batch 6000/20010 Done, mean position loss: 25.598518159389496\n",
      "Training growing_up:  30%|██▋      | 5938/20010 [4:52:26<10:32:23,  2.70s/batch]Batch 5900/20010 Done, mean position loss: 27.788921282291412\n",
      "Training growing_up:  29%|██▋      | 5865/20010 [4:52:28<10:20:57,  2.63s/batch]Batch 6000/20010 Done, mean position loss: 27.346284108161925\n",
      "Training growing_up:  30%|██▋      | 5940/20010 [4:52:30<11:34:33,  2.96s/batch]Batch 6000/20010 Done, mean position loss: 26.13626310825348\n",
      "Training growing_up:  30%|███       | 6038/20010 [4:52:36<9:50:43,  2.54s/batch]Batch 6000/20010 Done, mean position loss: 25.69156936407089\n",
      "Training growing_up:  30%|███       | 6028/20010 [4:52:38<9:55:39,  2.56s/batch]Batch 6000/20010 Done, mean position loss: 26.409839696884156\n",
      "Training growing_up:  29%|██▋      | 5871/20010 [4:52:43<10:21:43,  2.64s/batch]Batch 6100/20010 Done, mean position loss: 25.34045657634735\n",
      "Training growing_up:  30%|██▋      | 5999/20010 [4:52:51<10:03:03,  2.58s/batch]Batch 6000/20010 Done, mean position loss: 25.8185537815094\n",
      "Training growing_up:  30%|██▋      | 5928/20010 [4:52:56<10:13:25,  2.61s/batch]Batch 6000/20010 Done, mean position loss: 25.343726198673252\n",
      "Training growing_up:  30%|██▋      | 6009/20010 [4:52:58<10:03:04,  2.58s/batch]Batch 5900/20010 Done, mean position loss: 24.808478672504425\n",
      "Training growing_up:  30%|███       | 6050/20010 [4:53:08<9:55:08,  2.56s/batch]Batch 5900/20010 Done, mean position loss: 26.10041876554489\n",
      "Training growing_up:  30%|██▋      | 6080/20010 [4:53:09<10:06:41,  2.61s/batch]Batch 5900/20010 Done, mean position loss: 25.84362231492996\n",
      "Training growing_up:  30%|███       | 6029/20010 [4:53:23<8:55:52,  2.30s/batch]Batch 6000/20010 Done, mean position loss: 26.55093115568161\n",
      "Training growing_up:  30%|███       | 6021/20010 [4:53:27<9:48:14,  2.52s/batch]Batch 6000/20010 Done, mean position loss: 26.941194722652437\n",
      "Training growing_up:  29%|██▉       | 5886/20010 [4:53:39<9:47:12,  2.49s/batch]Batch 6000/20010 Done, mean position loss: 26.6153200340271\n",
      "Training growing_up:  30%|██▋      | 6039/20010 [4:53:48<12:06:39,  3.12s/batch]Batch 6000/20010 Done, mean position loss: 26.44121257066727\n",
      "Training growing_up:  30%|██▋      | 6000/20010 [4:53:51<11:20:08,  2.91s/batch]Batch 6000/20010 Done, mean position loss: 26.51289474248886\n",
      "Training growing_up:  30%|██▋      | 6039/20010 [4:53:54<11:52:19,  3.06s/batch]Batch 6000/20010 Done, mean position loss: 25.83929349899292\n",
      "Training growing_up:  30%|██▋      | 5982/20010 [4:54:06<11:30:39,  2.95s/batch]Batch 5900/20010 Done, mean position loss: 25.935642869472503\n",
      "Training growing_up:  30%|██▋      | 5922/20010 [4:54:08<10:54:48,  2.79s/batch]Batch 6100/20010 Done, mean position loss: 25.631425483226778\n",
      "Training growing_up:  30%|██▋      | 6022/20010 [4:54:26<10:37:08,  2.73s/batch]Batch 5900/20010 Done, mean position loss: 25.720883328914642\n",
      "Training growing_up:  30%|██▋      | 5989/20010 [4:54:32<11:57:37,  3.07s/batch]Batch 6100/20010 Done, mean position loss: 24.526389389038087\n",
      "Training growing_up:  30%|██▋      | 6042/20010 [4:54:41<11:30:52,  2.97s/batch]Batch 6000/20010 Done, mean position loss: 25.29377556324005\n",
      "Training growing_up:  30%|██▋      | 6050/20010 [4:54:48<11:17:09,  2.91s/batch]Batch 6000/20010 Done, mean position loss: 25.81462709188461\n",
      "Training growing_up:  30%|██▋      | 5937/20010 [4:54:53<11:18:28,  2.89s/batch]Batch 6100/20010 Done, mean position loss: 26.649796726703645\n",
      "Training growing_up:  30%|██▋      | 6102/20010 [4:54:55<10:41:56,  2.77s/batch]Batch 6000/20010 Done, mean position loss: 27.412780194282533\n",
      "Training growing_up:  31%|██▊      | 6151/20010 [4:55:00<10:45:21,  2.79s/batch]Batch 6000/20010 Done, mean position loss: 26.282467172145843\n",
      "Training growing_up:  31%|██▋      | 6105/20010 [4:55:04<11:22:43,  2.95s/batch]Batch 6000/20010 Done, mean position loss: 26.613640356063843\n",
      "Training growing_up:  30%|██▋      | 6077/20010 [4:55:17<11:30:06,  2.97s/batch]Batch 6000/20010 Done, mean position loss: 26.613086688518525\n",
      "Training growing_up:  30%|██▋      | 5925/20010 [4:55:18<12:52:18,  3.29s/batch]Batch 6100/20010 Done, mean position loss: 25.02981071472168\n",
      "Training growing_up:  31%|██▊      | 6129/20010 [4:55:25<10:15:24,  2.66s/batch]Batch 6000/20010 Done, mean position loss: 25.629445631504062\n",
      "Training growing_up:  30%|██▋      | 5950/20010 [4:55:32<11:34:16,  2.96s/batch]Batch 6100/20010 Done, mean position loss: 26.05890623092651\n",
      "Training growing_up:  30%|██▋      | 6048/20010 [4:55:36<10:47:18,  2.78s/batch]Batch 6100/20010 Done, mean position loss: 25.828378639221192\n",
      "Training growing_up:  30%|██▋      | 6042/20010 [4:55:46<10:58:02,  2.83s/batch]Batch 6100/20010 Done, mean position loss: 24.85397335767746\n",
      "Training growing_up:  31%|██▋      | 6110/20010 [4:55:56<10:40:56,  2.77s/batch]Batch 6100/20010 Done, mean position loss: 24.83571930885315\n",
      "Training growing_up:  31%|██▊      | 6115/20010 [4:56:17<11:44:22,  3.04s/batch]Batch 6100/20010 Done, mean position loss: 26.322602179050445\n",
      "Training growing_up:  31%|██▊      | 6125/20010 [4:56:25<11:24:37,  2.96s/batch]Batch 6000/20010 Done, mean position loss: 25.30951652765274\n",
      "Training growing_up:  30%|██▋      | 6059/20010 [4:56:27<11:45:10,  3.03s/batch]Batch 6100/20010 Done, mean position loss: 24.923963449001313\n",
      "Training growing_up:  30%|██▋      | 6036/20010 [4:56:43<11:54:50,  3.07s/batch]Batch 6100/20010 Done, mean position loss: 26.10233129501343\n",
      "Training growing_up:  31%|██▊      | 6128/20010 [4:56:47<10:14:21,  2.66s/batch]Batch 6100/20010 Done, mean position loss: 26.9022394156456\n",
      "Training growing_up:  30%|██▋      | 5954/20010 [4:57:04<11:39:21,  2.99s/batch]Batch 6100/20010 Done, mean position loss: 26.667900738716124\n",
      "Training growing_up:  30%|██▋      | 5981/20010 [4:57:05<10:54:17,  2.80s/batch]Batch 6100/20010 Done, mean position loss: 25.57777827978134\n",
      "Training growing_up:  30%|██▋      | 5988/20010 [4:57:09<11:34:03,  2.97s/batch]Batch 6100/20010 Done, mean position loss: 25.769392540454866\n",
      "Training growing_up:  31%|██▊      | 6120/20010 [4:57:12<11:55:01,  3.09s/batch]Batch 6000/20010 Done, mean position loss: 27.65678085565567\n",
      "Training growing_up:  30%|██▋      | 6098/20010 [4:57:19<10:09:41,  2.63s/batch]Batch 6100/20010 Done, mean position loss: 26.96215022087097\n",
      "Training growing_up:  31%|██▊      | 6200/20010 [4:57:21<10:23:42,  2.71s/batch]Batch 6100/20010 Done, mean position loss: 26.977387599945068\n",
      "Training growing_up:  31%|███       | 6172/20010 [4:57:24<9:40:01,  2.51s/batch]Batch 6200/20010 Done, mean position loss: 25.31785479545593\n",
      "Training growing_up:  30%|██▋      | 6059/20010 [4:57:29<10:47:49,  2.79s/batch]Batch 6100/20010 Done, mean position loss: 26.384312739372255\n",
      "Training growing_up:  30%|██▋      | 6079/20010 [4:57:41<10:34:59,  2.73s/batch]Batch 6100/20010 Done, mean position loss: 25.572046377658843\n",
      "Training growing_up:  31%|██▊      | 6163/20010 [4:57:50<10:09:31,  2.64s/batch]Batch 6000/20010 Done, mean position loss: 24.95930160522461\n",
      "Training growing_up:  31%|██▋      | 6112/20010 [4:58:00<11:22:32,  2.95s/batch]Batch 6000/20010 Done, mean position loss: 25.52091569662094\n",
      "Training growing_up:  31%|██▊      | 6214/20010 [4:58:03<11:48:31,  3.08s/batch]Batch 6000/20010 Done, mean position loss: 25.404598107337954\n",
      "Training growing_up:  31%|██▊      | 6216/20010 [4:58:08<11:16:16,  2.94s/batch]Batch 6100/20010 Done, mean position loss: 25.10602973937988\n",
      "Training growing_up:  30%|██▋      | 6060/20010 [4:58:14<11:04:27,  2.86s/batch]Batch 6100/20010 Done, mean position loss: 27.061719110012053\n",
      "Training growing_up:  31%|██▊      | 6130/20010 [4:58:28<10:01:45,  2.60s/batch]Batch 6100/20010 Done, mean position loss: 26.458177750110625\n",
      "Training growing_up:  31%|██▊      | 6224/20010 [4:58:31<10:32:50,  2.75s/batch]Batch 6100/20010 Done, mean position loss: 25.446731112003327\n",
      "Training growing_up:  30%|██▋      | 6077/20010 [4:58:39<12:47:46,  3.31s/batch]Batch 6100/20010 Done, mean position loss: 26.8382675743103\n",
      "Training growing_up:  31%|██▊      | 6143/20010 [4:58:46<11:13:28,  2.91s/batch]Batch 6100/20010 Done, mean position loss: 26.444276580810545\n",
      "Training growing_up:  31%|██▊      | 6131/20010 [4:58:48<10:54:11,  2.83s/batch]Batch 6200/20010 Done, mean position loss: 27.047305347919462\n",
      "Training growing_up:  31%|██▊      | 6166/20010 [4:58:59<10:47:39,  2.81s/batch]Batch 6000/20010 Done, mean position loss: 26.150336384773254\n",
      "Training growing_up:  31%|██▊      | 6179/20010 [4:59:16<12:41:15,  3.30s/batch]Batch 6200/20010 Done, mean position loss: 25.551158792972565\n",
      "Training growing_up:  31%|██▊      | 6177/20010 [4:59:28<14:13:06,  3.70s/batch]Batch 6000/20010 Done, mean position loss: 25.565680282115935\n",
      "Training growing_up:  30%|██▋      | 6095/20010 [4:59:35<11:55:51,  3.09s/batch]Batch 6100/20010 Done, mean position loss: 25.10929709672928\n",
      "Training growing_up:  30%|██▋      | 6089/20010 [4:59:37<12:21:10,  3.19s/batch]Batch 6200/20010 Done, mean position loss: 26.4964785695076\n",
      "Training growing_up:  31%|██▊      | 6181/20010 [4:59:39<11:31:38,  3.00s/batch]Batch 6100/20010 Done, mean position loss: 27.093225514888765\n",
      "Training growing_up:  31%|██▊      | 6135/20010 [4:59:51<11:37:15,  3.02s/batch]Batch 6100/20010 Done, mean position loss: 26.141680738925935\n",
      "Training growing_up:  30%|██▋      | 6038/20010 [4:59:55<11:16:47,  2.91s/batch]Batch 6100/20010 Done, mean position loss: 25.944938979148866\n",
      "Training growing_up:  31%|██▊      | 6157/20010 [5:00:00<11:26:21,  2.97s/batch]Batch 6100/20010 Done, mean position loss: 26.40126294374466\n",
      "Training growing_up:  31%|██▋      | 6106/20010 [5:00:10<11:25:40,  2.96s/batch]Batch 6200/20010 Done, mean position loss: 26.355155680179596\n",
      "Training growing_up:  31%|██▋      | 6108/20010 [5:00:15<13:27:10,  3.48s/batch]Batch 6100/20010 Done, mean position loss: 25.76696846485138\n",
      "Training growing_up:  30%|██▋      | 6027/20010 [5:00:19<11:52:05,  3.06s/batch]Batch 6100/20010 Done, mean position loss: 25.690665819644927\n",
      "Training growing_up:  31%|███       | 6184/20010 [5:00:21<9:45:03,  2.54s/batch]Batch 6200/20010 Done, mean position loss: 24.76329628705978\n",
      "Training growing_up:  31%|██▊      | 6205/20010 [5:00:32<10:35:05,  2.76s/batch]Batch 6200/20010 Done, mean position loss: 25.92682729244232\n",
      "Training growing_up:  30%|██▋      | 6071/20010 [5:00:40<11:13:52,  2.90s/batch]Batch 6200/20010 Done, mean position loss: 25.21641125679016\n",
      "Training growing_up:  30%|██▋      | 6072/20010 [5:00:45<13:18:43,  3.44s/batch]Batch 6200/20010 Done, mean position loss: 25.155651926994324\n",
      "Training growing_up:  31%|██▊      | 6277/20010 [5:01:11<11:14:22,  2.95s/batch]Batch 6200/20010 Done, mean position loss: 25.66070189476013\n",
      "Training growing_up:  30%|██▋      | 6068/20010 [5:01:17<11:51:21,  3.06s/batch]Batch 6200/20010 Done, mean position loss: 25.031710259914398\n",
      "Training growing_up:  31%|██▊      | 6281/20010 [5:01:21<10:28:41,  2.75s/batch]Batch 6100/20010 Done, mean position loss: 25.320334458351134\n",
      "Training growing_up:  31%|██▊      | 6130/20010 [5:01:39<10:54:21,  2.83s/batch]Batch 6200/20010 Done, mean position loss: 26.016325936317443\n",
      "Training growing_up:  31%|██▊      | 6136/20010 [5:01:39<10:36:31,  2.75s/batch]Batch 6200/20010 Done, mean position loss: 26.404238653182983\n",
      "Training growing_up:  31%|███▏      | 6294/20010 [5:01:59<9:57:46,  2.61s/batch]Batch 6200/20010 Done, mean position loss: 25.302672169208527\n",
      "Training growing_up:  31%|██▊      | 6193/20010 [5:02:00<10:47:27,  2.81s/batch]Batch 6200/20010 Done, mean position loss: 26.15487825155258\n",
      "Training growing_up:  31%|██▊      | 6117/20010 [5:02:08<12:10:27,  3.15s/batch]Batch 6200/20010 Done, mean position loss: 25.937938129901887\n",
      "Training growing_up:  31%|██▊      | 6141/20010 [5:02:11<11:54:18,  3.09s/batch]Batch 6100/20010 Done, mean position loss: 28.0787256526947\n",
      "Training growing_up:  31%|██▊      | 6214/20010 [5:02:16<10:50:52,  2.83s/batch]Batch 6200/20010 Done, mean position loss: 25.608443052768706\n",
      "Training growing_up:  31%|██▊      | 6222/20010 [5:02:21<11:09:40,  2.91s/batch]Batch 6200/20010 Done, mean position loss: 26.53495831012726\n",
      "Training growing_up:  31%|██▊      | 6145/20010 [5:02:23<11:07:11,  2.89s/batch]Batch 6300/20010 Done, mean position loss: 24.95569015264511\n",
      "Training growing_up:  31%|██▊      | 6186/20010 [5:02:23<11:39:31,  3.04s/batch]Batch 6200/20010 Done, mean position loss: 25.42097416639328\n",
      "Training growing_up:  31%|██▊      | 6151/20010 [5:02:39<10:29:34,  2.73s/batch]Batch 6200/20010 Done, mean position loss: 25.59184205532074\n",
      "Training growing_up:  30%|██▋      | 6097/20010 [5:02:58<11:29:58,  2.98s/batch]Batch 6100/20010 Done, mean position loss: 25.29584925413132\n",
      "Training growing_up:  31%|██▊      | 6221/20010 [5:02:58<10:30:12,  2.74s/batch]Batch 6100/20010 Done, mean position loss: 25.887994687557217\n",
      "Training growing_up:  31%|██▊      | 6239/20010 [5:03:04<10:57:54,  2.87s/batch]Batch 6200/20010 Done, mean position loss: 27.44818937778473\n",
      "Training growing_up:  31%|██▊      | 6162/20010 [5:03:06<10:43:52,  2.79s/batch]Batch 6200/20010 Done, mean position loss: 24.786305644512176\n",
      "Training growing_up:  31%|██▊      | 6225/20010 [5:03:09<10:55:08,  2.85s/batch]Batch 6100/20010 Done, mean position loss: 25.48317429304123\n",
      "Training growing_up:  31%|██▊      | 6243/20010 [5:03:23<12:38:22,  3.31s/batch]Batch 6200/20010 Done, mean position loss: 26.363927717208863\n",
      "Training growing_up:  30%|██▋      | 6080/20010 [5:03:34<12:30:35,  3.23s/batch]Batch 6200/20010 Done, mean position loss: 25.15093613624573\n",
      "Training growing_up:  31%|██▊      | 6172/20010 [5:03:36<11:51:12,  3.08s/batch]Batch 6200/20010 Done, mean position loss: 25.83449764966965\n",
      "Training growing_up:  31%|██▊      | 6232/20010 [5:03:41<11:26:18,  2.99s/batch]Batch 6200/20010 Done, mean position loss: 25.914921360015867\n",
      "Training growing_up:  31%|██▊      | 6150/20010 [5:03:46<10:49:23,  2.81s/batch]Batch 6300/20010 Done, mean position loss: 26.017149658203124\n",
      "Training growing_up:  31%|██▊      | 6179/20010 [5:03:58<11:27:48,  2.98s/batch]Batch 6100/20010 Done, mean position loss: 25.540775780677798\n",
      "Training growing_up:  31%|██▊      | 6194/20010 [5:04:11<11:09:40,  2.91s/batch]Batch 6300/20010 Done, mean position loss: 24.967392110824584\n",
      "Training growing_up:  31%|██▊      | 6243/20010 [5:04:30<10:47:13,  2.82s/batch]Batch 6300/20010 Done, mean position loss: 26.342557208538054\n",
      "Training growing_up:  31%|██▊      | 6254/20010 [5:04:31<11:27:55,  3.00s/batch]Batch 6200/20010 Done, mean position loss: 25.630116531848905\n",
      "Training growing_up:  31%|██▊      | 6254/20010 [5:04:36<12:08:51,  3.18s/batch]Batch 6200/20010 Done, mean position loss: 26.16496675014496\n",
      "Training growing_up:  31%|██▊      | 6136/20010 [5:04:37<10:35:25,  2.75s/batch]Batch 6100/20010 Done, mean position loss: 25.01848727464676\n",
      "Training growing_up:  31%|██▊      | 6140/20010 [5:04:47<10:19:18,  2.68s/batch]Batch 6200/20010 Done, mean position loss: 26.60839626789093\n",
      "Training growing_up:  31%|██▊      | 6289/20010 [5:04:52<10:31:07,  2.76s/batch]Batch 6200/20010 Done, mean position loss: 27.658313829898834\n",
      "Training growing_up:  31%|██▊      | 6240/20010 [5:05:00<10:28:37,  2.74s/batch]Batch 6300/20010 Done, mean position loss: 25.25439786195755\n",
      "Training growing_up:  32%|██▊      | 6312/20010 [5:05:02<11:47:59,  3.10s/batch]Batch 6200/20010 Done, mean position loss: 26.402601809501647\n",
      "Training growing_up:  31%|██▊      | 6259/20010 [5:05:02<11:20:14,  2.97s/batch]Batch 6300/20010 Done, mean position loss: 25.236146676540375\n",
      "Training growing_up:  31%|██▊      | 6292/20010 [5:05:04<10:45:06,  2.82s/batch]Batch 6200/20010 Done, mean position loss: 26.066751568317414\n",
      "Training growing_up:  31%|██▊      | 6233/20010 [5:05:15<12:18:16,  3.22s/batch]Batch 6200/20010 Done, mean position loss: 25.41537894010544\n",
      "Training growing_up:  31%|██▊      | 6250/20010 [5:05:26<10:32:52,  2.76s/batch]Batch 6300/20010 Done, mean position loss: 25.239941132068633\n",
      "Training growing_up:  31%|███▏      | 6266/20010 [5:05:31<9:36:16,  2.52s/batch]Batch 6300/20010 Done, mean position loss: 25.75718620300293\n",
      "Training growing_up:  32%|██▊      | 6312/20010 [5:05:34<10:26:27,  2.74s/batch]Batch 6300/20010 Done, mean position loss: 24.68291374206543\n",
      "Training growing_up:  31%|██▊      | 6166/20010 [5:06:06<11:06:20,  2.89s/batch]Batch 6300/20010 Done, mean position loss: 26.011413741111753\n",
      "Training growing_up:  31%|██▊      | 6300/20010 [5:06:16<10:36:27,  2.79s/batch]Batch 6300/20010 Done, mean position loss: 25.696581749916078\n",
      "Training growing_up:  31%|██▊      | 6255/20010 [5:06:19<10:53:50,  2.85s/batch]Batch 6300/20010 Done, mean position loss: 27.43116787672043\n",
      "Training growing_up:  31%|███▏      | 6292/20010 [5:06:19<9:55:36,  2.61s/batch]Batch 6200/20010 Done, mean position loss: 25.27694378614426\n",
      "Training growing_up:  32%|██▊      | 6325/20010 [5:06:40<10:48:18,  2.84s/batch]Batch 6300/20010 Done, mean position loss: 26.752118175029757\n",
      "Training growing_up:  31%|██▊      | 6283/20010 [5:06:42<10:50:30,  2.84s/batch]Batch 6300/20010 Done, mean position loss: 25.152912387847902\n",
      "Training growing_up:  32%|██▊      | 6317/20010 [5:06:48<10:27:57,  2.75s/batch]Batch 6300/20010 Done, mean position loss: 25.840158703327177\n",
      "Training growing_up:  32%|██▊      | 6332/20010 [5:06:59<10:25:58,  2.75s/batch]Batch 6200/20010 Done, mean position loss: 26.295105724334714\n",
      "Training growing_up:  32%|███▏      | 6363/20010 [5:07:00<9:40:05,  2.55s/batch]Batch 6300/20010 Done, mean position loss: 25.355322391986846\n",
      "Training growing_up:  31%|██▊      | 6256/20010 [5:07:00<10:02:08,  2.63s/batch]Batch 6300/20010 Done, mean position loss: 25.711708562374117\n",
      "Training growing_up:  32%|██▊      | 6371/20010 [5:07:03<11:36:32,  3.06s/batch]Batch 6400/20010 Done, mean position loss: 24.859373106956483\n",
      "Training growing_up:  31%|██▊      | 6189/20010 [5:07:09<10:44:54,  2.80s/batch]Batch 6300/20010 Done, mean position loss: 26.535851144790648\n",
      "Training growing_up:  31%|██▊      | 6252/20010 [5:07:16<10:21:30,  2.71s/batch]Batch 6300/20010 Done, mean position loss: 25.49018543243408\n",
      "Training growing_up:  31%|███▏      | 6260/20010 [5:07:28<9:56:16,  2.60s/batch]Batch 6300/20010 Done, mean position loss: 25.35252186536789\n",
      "Training growing_up:  31%|██▊      | 6270/20010 [5:07:42<11:58:03,  3.14s/batch]Batch 6200/20010 Done, mean position loss: 25.62526397228241\n",
      "Training growing_up:  32%|███▏      | 6417/20010 [5:07:44<9:48:10,  2.60s/batch]Batch 6200/20010 Done, mean position loss: 24.89464591264725\n",
      "Training growing_up:  31%|███       | 6235/20010 [5:07:49<9:55:26,  2.59s/batch]Batch 6300/20010 Done, mean position loss: 25.520270330905912\n",
      "Training growing_up:  32%|██▉      | 6420/20010 [5:07:53<11:24:00,  3.02s/batch]Batch 6300/20010 Done, mean position loss: 26.23958896398544\n",
      "Training growing_up:  31%|██▊      | 6270/20010 [5:07:59<10:56:43,  2.87s/batch]Batch 6200/20010 Done, mean position loss: 25.241253049373626\n",
      "Training growing_up:  31%|██▊      | 6245/20010 [5:08:19<10:50:00,  2.83s/batch]Batch 6300/20010 Done, mean position loss: 25.132830057144165\n",
      "Training growing_up:  32%|██▊      | 6351/20010 [5:08:19<10:09:02,  2.68s/batch]Batch 6300/20010 Done, mean position loss: 25.735683665275573\n",
      "Training growing_up:  32%|██▊      | 6331/20010 [5:08:22<10:03:43,  2.65s/batch]Batch 6300/20010 Done, mean position loss: 26.097349491119388\n",
      "Training growing_up:  31%|██▊      | 6284/20010 [5:08:23<10:25:33,  2.73s/batch]Batch 6400/20010 Done, mean position loss: 25.528200526237487\n",
      "Training growing_up:  32%|██▊      | 6305/20010 [5:08:32<10:42:00,  2.81s/batch]Batch 6300/20010 Done, mean position loss: 25.616890106201172\n",
      "Training growing_up:  31%|██▊      | 6277/20010 [5:08:37<10:44:45,  2.82s/batch]Batch 6200/20010 Done, mean position loss: 25.639858534336092\n",
      "Training growing_up:  32%|███▏      | 6347/20010 [5:08:41<9:30:10,  2.50s/batch]Batch 6400/20010 Done, mean position loss: 25.039665615558626\n",
      "Training growing_up:  32%|██▊      | 6365/20010 [5:09:07<11:11:23,  2.95s/batch]Batch 6400/20010 Done, mean position loss: 26.69770007610321\n",
      "Training growing_up:  32%|██▊      | 6379/20010 [5:09:10<10:02:37,  2.65s/batch]Batch 6300/20010 Done, mean position loss: 25.607381520271304\n",
      "Training growing_up:  31%|██▊      | 6249/20010 [5:09:13<10:32:40,  2.76s/batch]Batch 6300/20010 Done, mean position loss: 25.98355487585068\n",
      "Training growing_up:  31%|██▊      | 6236/20010 [5:09:23<11:09:26,  2.92s/batch]Batch 6200/20010 Done, mean position loss: 25.480512120723724\n",
      "Training growing_up:  32%|██▊      | 6342/20010 [5:09:27<11:33:32,  3.04s/batch]Batch 6300/20010 Done, mean position loss: 26.4032271027565\n",
      "Training growing_up:  32%|██▉      | 6456/20010 [5:09:33<10:53:46,  2.89s/batch]Batch 6300/20010 Done, mean position loss: 27.74971525192261\n",
      "Training growing_up:  32%|███▏      | 6356/20010 [5:09:40<9:31:21,  2.51s/batch]Batch 6400/20010 Done, mean position loss: 25.05406426668167\n",
      "Training growing_up:  32%|██▊      | 6375/20010 [5:09:40<10:04:50,  2.66s/batch]Batch 6400/20010 Done, mean position loss: 25.978049154281617\n",
      "Training growing_up:  31%|██▊      | 6276/20010 [5:09:45<10:04:13,  2.64s/batch]Batch 6300/20010 Done, mean position loss: 26.15165758609772\n",
      "Training growing_up:  32%|██▊      | 6391/20010 [5:09:46<11:03:57,  2.93s/batch]Batch 6300/20010 Done, mean position loss: 25.72498384952545\n",
      "Training growing_up:  31%|██▊      | 6229/20010 [5:09:57<11:39:21,  3.04s/batch]Batch 6300/20010 Done, mean position loss: 25.14122624397278\n",
      "Training growing_up:  32%|██▉      | 6409/20010 [5:10:03<10:37:51,  2.81s/batch]Batch 6400/20010 Done, mean position loss: 25.09605420827866\n",
      "Training growing_up:  32%|██▊      | 6308/20010 [5:10:06<10:31:46,  2.77s/batch]Batch 6400/20010 Done, mean position loss: 25.36622371673584\n",
      "Training growing_up:  32%|██▉      | 6413/20010 [5:10:15<11:17:29,  2.99s/batch]Batch 6400/20010 Done, mean position loss: 24.598851275444034\n",
      "Training growing_up:  32%|██▊      | 6366/20010 [5:10:36<11:02:09,  2.91s/batch]Batch 6400/20010 Done, mean position loss: 26.48427301645279\n",
      "Training growing_up:  32%|██▉      | 6424/20010 [5:10:47<11:13:35,  2.97s/batch]Batch 6400/20010 Done, mean position loss: 25.72056221961975\n",
      "Training growing_up:  32%|██▊      | 6339/20010 [5:10:58<11:16:40,  2.97s/batch]Batch 6400/20010 Done, mean position loss: 26.332000739574433\n",
      "Training growing_up:  32%|██▊      | 6374/20010 [5:10:59<10:04:35,  2.66s/batch]Batch 6300/20010 Done, mean position loss: 25.120564229488373\n",
      "Training growing_up:  32%|██▉      | 6458/20010 [5:11:16<10:11:04,  2.71s/batch]Batch 6400/20010 Done, mean position loss: 26.266512327194214\n",
      "Training growing_up:  32%|██▉      | 6436/20010 [5:11:19<10:46:57,  2.86s/batch]Batch 6400/20010 Done, mean position loss: 25.03275044441223\n",
      "Training growing_up:  31%|██▊      | 6245/20010 [5:11:31<11:12:24,  2.93s/batch]Batch 6400/20010 Done, mean position loss: 25.349299113750455\n",
      "Training growing_up:  31%|██▊      | 6247/20010 [5:11:38<12:08:25,  3.18s/batch]Batch 6400/20010 Done, mean position loss: 27.489320154190064\n",
      "Training growing_up:  32%|██▊      | 6381/20010 [5:11:39<11:04:08,  2.92s/batch]Batch 6400/20010 Done, mean position loss: 26.053248345851898\n",
      "Training growing_up:  32%|██▊      | 6337/20010 [5:11:40<10:24:55,  2.74s/batch]Batch 6300/20010 Done, mean position loss: 27.498468630313873\n",
      "Training growing_up:  32%|██▉      | 6396/20010 [5:11:44<10:35:42,  2.80s/batch]Batch 6400/20010 Done, mean position loss: 26.634874413013456\n",
      "Training growing_up:  32%|██▉      | 6401/20010 [5:11:44<10:24:02,  2.75s/batch]Batch 6500/20010 Done, mean position loss: 25.035172123908996\n",
      "Training growing_up:  32%|██▉      | 6394/20010 [5:11:57<10:18:05,  2.72s/batch]Batch 6400/20010 Done, mean position loss: 26.2429581451416\n",
      "Training growing_up:  32%|██▉      | 6466/20010 [5:12:16<12:50:58,  3.42s/batch]Batch 6400/20010 Done, mean position loss: 26.030359375476834\n",
      "Training growing_up:  32%|██▉      | 6418/20010 [5:12:28<11:00:42,  2.92s/batch]Batch 6400/20010 Done, mean position loss: 25.390314955711364\n",
      "Training growing_up:  31%|██▊      | 6264/20010 [5:12:29<12:04:43,  3.16s/batch]Batch 6300/20010 Done, mean position loss: 24.94766084432602\n",
      "Training growing_up:  32%|███▏      | 6429/20010 [5:12:36<9:21:32,  2.48s/batch]Batch 6400/20010 Done, mean position loss: 26.44878950595856\n",
      "Training growing_up:  32%|██▉      | 6463/20010 [5:12:37<13:02:15,  3.46s/batch]Batch 6300/20010 Done, mean position loss: 25.55295019865036\n",
      "Training growing_up:  32%|██▉      | 6466/20010 [5:12:48<11:56:32,  3.17s/batch]Batch 6300/20010 Done, mean position loss: 25.61186931848526\n",
      "Training growing_up:  32%|██▉      | 6470/20010 [5:12:59<11:27:49,  3.05s/batch]Batch 6500/20010 Done, mean position loss: 25.499791610240933\n",
      "Training growing_up:  32%|██▉      | 6438/20010 [5:13:03<10:31:26,  2.79s/batch]Batch 6400/20010 Done, mean position loss: 26.321281566619874\n",
      "Training growing_up:  32%|██▉      | 6439/20010 [5:13:07<11:27:37,  3.04s/batch]Batch 6400/20010 Done, mean position loss: 25.65061800956726\n",
      "Training growing_up:  32%|██▉      | 6411/20010 [5:13:09<12:24:59,  3.29s/batch]Batch 6400/20010 Done, mean position loss: 26.6130527305603\n",
      "Training growing_up:  32%|██▊      | 6373/20010 [5:13:25<11:38:05,  3.07s/batch]Batch 6500/20010 Done, mean position loss: 24.931891705989838\n",
      "Training growing_up:  32%|██▉      | 6453/20010 [5:13:27<10:40:43,  2.84s/batch]Batch 6400/20010 Done, mean position loss: 26.174500391483306\n",
      "Training growing_up:  32%|██▉      | 6421/20010 [5:13:30<12:33:18,  3.33s/batch]Batch 6300/20010 Done, mean position loss: 25.268269774913787\n",
      "Training growing_up:  33%|██▉      | 6521/20010 [5:14:03<12:12:11,  3.26s/batch]Batch 6400/20010 Done, mean position loss: 26.088736629486085\n",
      "Training growing_up:  32%|██▉      | 6431/20010 [5:14:02<11:58:54,  3.18s/batch]Batch 6500/20010 Done, mean position loss: 26.847582948207855\n",
      "Training growing_up:  32%|██▉      | 6503/20010 [5:14:08<10:48:19,  2.88s/batch]Batch 6400/20010 Done, mean position loss: 25.73609869480133\n",
      "Training growing_up:  32%|██▉      | 6487/20010 [5:14:18<11:38:00,  3.10s/batch]Batch 6400/20010 Done, mean position loss: 25.918146283626555\n",
      "Training growing_up:  32%|██▊      | 6331/20010 [5:14:23<11:33:16,  3.04s/batch]Batch 6400/20010 Done, mean position loss: 26.464915215969086\n",
      "Training growing_up:  32%|██▊      | 6359/20010 [5:14:29<11:23:56,  3.01s/batch]Batch 6300/20010 Done, mean position loss: 27.070199711322786\n",
      "Training growing_up:  32%|██▊      | 6360/20010 [5:14:32<11:57:37,  3.15s/batch]Batch 6500/20010 Done, mean position loss: 24.964653248786924\n",
      "Training growing_up:  33%|██▉      | 6558/20010 [5:14:33<11:06:28,  2.97s/batch]Batch 6500/20010 Done, mean position loss: 25.815904095172883\n",
      "Training growing_up:  32%|██▉      | 6459/20010 [5:14:40<11:21:58,  3.02s/batch]Batch 6400/20010 Done, mean position loss: 26.194677793979643\n",
      "Training growing_up:  33%|██▉      | 6504/20010 [5:14:43<11:56:23,  3.18s/batch]Batch 6400/20010 Done, mean position loss: 25.538507034778593\n",
      "Training growing_up:  33%|██▉      | 6518/20010 [5:14:53<10:41:27,  2.85s/batch]Batch 6500/20010 Done, mean position loss: 25.18793352127075\n",
      "Training growing_up:  32%|██▉      | 6463/20010 [5:14:54<12:14:22,  3.25s/batch]Batch 6400/20010 Done, mean position loss: 25.617403502464295\n",
      "Training growing_up:  32%|██▊      | 6383/20010 [5:15:02<11:11:09,  2.96s/batch]Batch 6500/20010 Done, mean position loss: 25.677822980880737\n",
      "Training growing_up:  32%|██▉      | 6419/20010 [5:15:12<10:25:06,  2.76s/batch]Batch 6500/20010 Done, mean position loss: 24.672992897033694\n",
      "Training growing_up:  32%|██▉      | 6456/20010 [5:15:29<12:33:20,  3.33s/batch]Batch 6500/20010 Done, mean position loss: 25.889822778701785\n",
      "Training growing_up:  32%|██▉      | 6493/20010 [5:15:46<11:12:02,  2.98s/batch]Batch 6500/20010 Done, mean position loss: 26.0187846493721\n",
      "Training growing_up:  33%|██▉      | 6550/20010 [5:15:51<11:27:41,  3.07s/batch]Batch 6500/20010 Done, mean position loss: 25.565513169765474\n",
      "Training growing_up:  33%|██▉      | 6509/20010 [5:15:54<11:16:21,  3.01s/batch]Batch 6400/20010 Done, mean position loss: 25.051383943557738\n",
      "Training growing_up:  32%|██▉      | 6499/20010 [5:16:09<11:07:04,  2.96s/batch]Batch 6500/20010 Done, mean position loss: 25.67607832431793\n",
      "Training growing_up:  33%|██▉      | 6526/20010 [5:16:16<10:12:34,  2.73s/batch]Batch 6500/20010 Done, mean position loss: 25.107570927143097\n",
      "Training growing_up:  32%|██▉      | 6467/20010 [5:16:26<10:08:40,  2.70s/batch]Batch 6500/20010 Done, mean position loss: 25.436435444355013\n",
      "Training growing_up:  33%|██▉      | 6541/20010 [5:16:28<10:33:27,  2.82s/batch]Batch 6500/20010 Done, mean position loss: 26.85437480688095\n",
      "Training growing_up:  32%|██▉      | 6439/20010 [5:16:34<11:22:53,  3.02s/batch]Batch 6400/20010 Done, mean position loss: 27.32200197696686\n",
      "Training growing_up:  33%|██▉      | 6508/20010 [5:16:34<10:25:43,  2.78s/batch]Batch 6500/20010 Done, mean position loss: 25.85673388719559\n",
      "Training growing_up:  32%|██▊      | 6382/20010 [5:16:37<11:42:04,  3.09s/batch]Batch 6600/20010 Done, mean position loss: 25.35259966611862\n",
      "Training growing_up:  33%|██▉      | 6545/20010 [5:16:41<12:06:11,  3.24s/batch]Batch 6500/20010 Done, mean position loss: 26.82452319860458\n",
      "Training growing_up:  32%|██▉      | 6492/20010 [5:16:54<10:42:34,  2.85s/batch]Batch 6500/20010 Done, mean position loss: 25.996766972541806\n",
      "Training growing_up:  32%|██▉      | 6460/20010 [5:17:17<11:53:27,  3.16s/batch]Batch 6500/20010 Done, mean position loss: 25.304221115112306\n",
      "Training growing_up:  32%|██▉      | 6469/20010 [5:17:33<10:13:47,  2.72s/batch]Batch 6500/20010 Done, mean position loss: 25.030644798278807\n",
      "Training growing_up:  33%|██▉      | 6552/20010 [5:17:37<10:00:01,  2.68s/batch]Batch 6400/20010 Done, mean position loss: 24.50460352897644\n",
      "Training growing_up:  32%|██▉      | 6500/20010 [5:17:42<11:09:19,  2.97s/batch]Batch 6400/20010 Done, mean position loss: 24.983845608234404\n",
      "Training growing_up:  33%|██▉      | 6577/20010 [5:17:45<11:35:27,  3.11s/batch]Batch 6500/20010 Done, mean position loss: 26.240255513191222\n",
      "Training growing_up:  33%|██▉      | 6535/20010 [5:17:54<11:40:19,  3.12s/batch]Batch 6400/20010 Done, mean position loss: 25.39916691303253\n",
      "Training growing_up:  33%|██▉      | 6509/20010 [5:17:57<11:22:20,  3.03s/batch]Batch 6600/20010 Done, mean position loss: 25.524624857902527\n",
      "Training growing_up:  32%|██▉      | 6409/20010 [5:18:00<10:39:30,  2.82s/batch]Batch 6500/20010 Done, mean position loss: 25.20283319473267\n",
      "Training growing_up:  33%|██▉      | 6584/20010 [5:18:06<11:16:16,  3.02s/batch]Batch 6500/20010 Done, mean position loss: 26.921395905017853\n",
      "Batch 6500/20010 Done, mean position loss: 26.15488466978073\n",
      "Training growing_up:  32%|██▉      | 6499/20010 [5:18:24<11:38:03,  3.10s/batch]Batch 6600/20010 Done, mean position loss: 25.200546987056732\n",
      "Training growing_up:  32%|██▉      | 6483/20010 [5:18:26<10:38:08,  2.83s/batch]Batch 6400/20010 Done, mean position loss: 25.268027741909027\n",
      "Training growing_up:  32%|██▉      | 6418/20010 [5:18:31<11:00:41,  2.92s/batch]Batch 6500/20010 Done, mean position loss: 25.84199648141861\n",
      "Training growing_up:  32%|██▉      | 6496/20010 [5:18:53<10:40:33,  2.84s/batch]Batch 6600/20010 Done, mean position loss: 26.578172459602357\n",
      "Training growing_up:  33%|██▉      | 6587/20010 [5:19:06<10:13:02,  2.74s/batch]Batch 6500/20010 Done, mean position loss: 25.226652793884277\n",
      "Training growing_up:  32%|██▉      | 6431/20010 [5:19:07<11:22:42,  3.02s/batch]Batch 6500/20010 Done, mean position loss: 26.602059381008146\n",
      "Training growing_up:  33%|██▉      | 6557/20010 [5:19:08<11:06:55,  2.97s/batch]Batch 6500/20010 Done, mean position loss: 25.94810529232025\n",
      "Training growing_up:  33%|███▎      | 6568/20010 [5:19:19<9:55:39,  2.66s/batch]Batch 6500/20010 Done, mean position loss: 25.956091122627257\n",
      "Training growing_up:  33%|██▉      | 6506/20010 [5:19:22<11:00:01,  2.93s/batch]Batch 6400/20010 Done, mean position loss: 25.00908788919449\n",
      "Training growing_up:  33%|██▉      | 6507/20010 [5:19:24<10:21:52,  2.76s/batch]Batch 6600/20010 Done, mean position loss: 25.88888788700104\n",
      "Training growing_up:  33%|██▉      | 6544/20010 [5:19:26<10:34:20,  2.83s/batch]Batch 6600/20010 Done, mean position loss: 26.092024788856506\n",
      "Training growing_up:  33%|██▉      | 6534/20010 [5:19:40<10:59:47,  2.94s/batch]Batch 6500/20010 Done, mean position loss: 25.869354488849638\n",
      "Training growing_up:  33%|██▉      | 6583/20010 [5:19:43<11:41:00,  3.13s/batch]Batch 6500/20010 Done, mean position loss: 25.08601497888565\n",
      "Training growing_up:  33%|██▉      | 6526/20010 [5:19:48<10:45:42,  2.87s/batch]Batch 6500/20010 Done, mean position loss: 25.580606541633607\n",
      "Training growing_up:  33%|██▉      | 6537/20010 [5:19:48<10:22:56,  2.77s/batch]Batch 6600/20010 Done, mean position loss: 25.162512011528015\n",
      "Training growing_up:  33%|███▎      | 6540/20010 [5:19:55<9:08:38,  2.44s/batch]Batch 6600/20010 Done, mean position loss: 24.97709960460663\n",
      "Training growing_up:  33%|██▉      | 6644/20010 [5:20:05<11:43:05,  3.16s/batch]Batch 6600/20010 Done, mean position loss: 24.744951946735384\n",
      "Training growing_up:  33%|███▎      | 6623/20010 [5:20:28<9:28:19,  2.55s/batch]Batch 6600/20010 Done, mean position loss: 27.811169130802156\n",
      "Training growing_up:  32%|███▏      | 6456/20010 [5:20:38<9:41:44,  2.58s/batch]Batch 6600/20010 Done, mean position loss: 25.915803289413454\n",
      "Training growing_up:  33%|██▉      | 6530/20010 [5:20:44<10:21:27,  2.77s/batch]Batch 6600/20010 Done, mean position loss: 25.556143081188203\n",
      "Training growing_up:  33%|██▉      | 6590/20010 [5:20:53<12:14:52,  3.29s/batch]Batch 6600/20010 Done, mean position loss: 26.130143656730652\n",
      "Training growing_up:  33%|██▉      | 6560/20010 [5:20:56<10:15:51,  2.75s/batch]Batch 6500/20010 Done, mean position loss: 25.389702970981595\n",
      "Training growing_up:  33%|███▎      | 6605/20010 [5:21:03<9:44:16,  2.62s/batch]Batch 6600/20010 Done, mean position loss: 25.20690963983536\n",
      "Training growing_up:  33%|██▉      | 6508/20010 [5:21:15<10:46:44,  2.87s/batch]Batch 6600/20010 Done, mean position loss: 25.187634954452516\n",
      "Training growing_up:  33%|███▎      | 6651/20010 [5:21:15<9:13:55,  2.49s/batch]Batch 6600/20010 Done, mean position loss: 25.768360028266905\n",
      "Training growing_up:  32%|███▏      | 6460/20010 [5:21:18<9:41:20,  2.57s/batch]Batch 6700/20010 Done, mean position loss: 23.869405364990236\n",
      "Training growing_up:  33%|██▉      | 6535/20010 [5:21:21<10:04:07,  2.69s/batch]Batch 6600/20010 Done, mean position loss: 25.703869745731353\n",
      "Training growing_up:  33%|███▎      | 6672/20010 [5:21:26<9:08:22,  2.47s/batch]Batch 6500/20010 Done, mean position loss: 26.313223195075988\n",
      "Training growing_up:  33%|██▉      | 6573/20010 [5:21:32<10:05:55,  2.71s/batch]Batch 6600/20010 Done, mean position loss: 26.3581627869606\n",
      "Training growing_up:  33%|███▎      | 6556/20010 [5:21:42<9:17:57,  2.49s/batch]Batch 6600/20010 Done, mean position loss: 25.539445419311523\n",
      "Training growing_up:  33%|██▉      | 6618/20010 [5:22:06<10:23:50,  2.79s/batch]Batch 6600/20010 Done, mean position loss: 25.323739552497862\n",
      "Training growing_up:  33%|███▎      | 6566/20010 [5:22:12<9:46:54,  2.62s/batch]Batch 6600/20010 Done, mean position loss: 24.302461717128754\n",
      "Training growing_up:  33%|██▉      | 6590/20010 [5:22:18<10:02:23,  2.69s/batch]Batch 6500/20010 Done, mean position loss: 25.428218779563903\n",
      "Training growing_up:  33%|███▎      | 6665/20010 [5:22:21<8:59:01,  2.42s/batch]Batch 6600/20010 Done, mean position loss: 26.30418918609619\n",
      "Training growing_up:  33%|███▎      | 6576/20010 [5:22:32<9:17:06,  2.49s/batch]Batch 6500/20010 Done, mean position loss: 24.639057993888855\n",
      "Training growing_up:  33%|███▎      | 6529/20010 [5:22:37<9:38:35,  2.58s/batch]Batch 6500/20010 Done, mean position loss: 24.741608057022095\n",
      "Training growing_up:  33%|███▎      | 6626/20010 [5:22:39<9:42:26,  2.61s/batch]Batch 6600/20010 Done, mean position loss: 25.665456023216247\n",
      "Batch 6700/20010 Done, mean position loss: 25.443475947380065\n",
      "Training growing_up:  33%|███▎      | 6654/20010 [5:22:47<7:57:46,  2.15s/batch]Batch 6600/20010 Done, mean position loss: 26.36828394174576\n",
      "Training growing_up:  33%|██▉      | 6629/20010 [5:22:48<10:40:52,  2.87s/batch]Batch 6600/20010 Done, mean position loss: 26.019617371559143\n",
      "Training growing_up:  33%|███▎      | 6646/20010 [5:22:57<9:41:27,  2.61s/batch]Batch 6700/20010 Done, mean position loss: 24.72079535007477\n",
      "Training growing_up:  33%|███▎      | 6609/20010 [5:23:08<9:17:36,  2.50s/batch]Batch 6500/20010 Done, mean position loss: 25.482238819599154\n",
      "Training growing_up:  33%|███▎      | 6588/20010 [5:23:10<9:43:06,  2.61s/batch]Batch 6600/20010 Done, mean position loss: 25.353348367214203\n",
      "Training growing_up:  33%|███▎      | 6617/20010 [5:23:25<8:35:54,  2.31s/batch]Batch 6700/20010 Done, mean position loss: 26.019976334571837\n",
      "Training growing_up:  33%|███▎      | 6588/20010 [5:23:36<9:32:49,  2.56s/batch]Batch 6600/20010 Done, mean position loss: 25.324562733173373\n",
      "Training growing_up:  33%|██▉      | 6647/20010 [5:23:42<12:03:06,  3.25s/batch]Batch 6600/20010 Done, mean position loss: 25.938048932552338\n",
      "Training growing_up:  33%|███▎      | 6687/20010 [5:23:44<9:51:31,  2.66s/batch]Batch 6600/20010 Done, mean position loss: 26.31186232805252\n",
      "Training growing_up:  33%|███      | 6676/20010 [5:23:49<10:47:41,  2.91s/batch]Batch 6500/20010 Done, mean position loss: 25.59946698665619\n",
      "Training growing_up:  33%|██▉      | 6629/20010 [5:23:52<10:18:19,  2.77s/batch]Batch 6700/20010 Done, mean position loss: 26.035252709388732\n",
      "Training growing_up:  33%|███▎      | 6520/20010 [5:23:54<9:45:45,  2.61s/batch]Batch 6600/20010 Done, mean position loss: 25.966355590820314\n",
      "Training growing_up:  33%|███▎      | 6568/20010 [5:23:54<9:58:24,  2.67s/batch]Batch 6700/20010 Done, mean position loss: 25.583906166553497\n",
      "Training growing_up:  33%|██▉      | 6544/20010 [5:24:12<10:43:59,  2.87s/batch]Batch 6600/20010 Done, mean position loss: 26.30885115623474\n",
      "Training growing_up:  34%|███      | 6736/20010 [5:24:12<10:38:09,  2.88s/batch]Batch 6600/20010 Done, mean position loss: 24.944628262519835\n",
      "Training growing_up:  33%|██▉      | 6547/20010 [5:24:23<12:52:58,  3.44s/batch]Batch 6700/20010 Done, mean position loss: 24.635794603824614\n",
      "Training growing_up:  33%|██▉      | 6605/20010 [5:24:24<11:26:58,  3.07s/batch]Batch 6700/20010 Done, mean position loss: 24.506942517757416\n",
      "Training growing_up:  34%|███      | 6711/20010 [5:24:24<12:29:49,  3.38s/batch]Batch 6700/20010 Done, mean position loss: 24.853811836242677\n",
      "Training growing_up:  33%|██▉      | 6654/20010 [5:24:24<12:36:16,  3.40s/batch]Batch 6600/20010 Done, mean position loss: 25.626590223312377\n",
      "Training growing_up:  34%|███      | 6736/20010 [5:25:01<11:04:04,  3.00s/batch]Batch 6700/20010 Done, mean position loss: 26.1514777469635\n",
      "Training growing_up:  33%|██▉      | 6619/20010 [5:25:05<11:00:06,  2.96s/batch]Batch 6700/20010 Done, mean position loss: 26.57475468635559\n",
      "Training growing_up:  33%|██▉      | 6655/20010 [5:25:16<10:19:28,  2.78s/batch]Batch 6700/20010 Done, mean position loss: 25.096581492424015\n",
      "Training growing_up:  34%|███▎      | 6722/20010 [5:25:22<9:48:37,  2.66s/batch]Batch 6700/20010 Done, mean position loss: 26.02893530368805\n",
      "Training growing_up:  33%|███      | 6696/20010 [5:25:31<10:41:00,  2.89s/batch]Batch 6600/20010 Done, mean position loss: 25.120428264141083\n",
      "Training growing_up:  34%|███      | 6736/20010 [5:25:35<10:20:08,  2.80s/batch]Batch 6700/20010 Done, mean position loss: 24.87379646539688\n",
      "Training growing_up:  34%|███▎      | 6705/20010 [5:25:44<9:12:26,  2.49s/batch]Batch 6700/20010 Done, mean position loss: 25.075643048286437\n",
      "Training growing_up:  33%|██▉      | 6559/20010 [5:25:46<10:32:33,  2.82s/batch]Batch 6700/20010 Done, mean position loss: 25.454633982181548\n",
      "Training growing_up:  33%|██▉      | 6609/20010 [5:25:53<11:03:51,  2.97s/batch]Batch 6800/20010 Done, mean position loss: 24.743580214977264\n",
      "Training growing_up:  33%|██▉      | 6563/20010 [5:25:57<10:08:15,  2.71s/batch]Batch 6700/20010 Done, mean position loss: 25.26732711076737\n",
      "Training growing_up:  33%|██▉      | 6650/20010 [5:26:03<11:13:18,  3.02s/batch]Batch 6600/20010 Done, mean position loss: 26.755390973091124\n",
      "Training growing_up:  34%|███      | 6725/20010 [5:26:10<10:20:53,  2.80s/batch]Batch 6700/20010 Done, mean position loss: 25.904218366146086\n",
      "Training growing_up:  33%|██▉      | 6643/20010 [5:26:16<11:35:27,  3.12s/batch]Batch 6700/20010 Done, mean position loss: 25.726624586582183\n",
      "Training growing_up:  34%|███      | 6729/20010 [5:26:40<10:17:13,  2.79s/batch]Batch 6700/20010 Done, mean position loss: 24.84122144460678\n",
      "Training growing_up:  33%|██▉      | 6618/20010 [5:26:54<11:26:00,  3.07s/batch]Batch 6700/20010 Done, mean position loss: 24.623581242561343\n",
      "Training growing_up:  33%|███▎      | 6664/20010 [5:26:55<9:58:13,  2.69s/batch]Batch 6700/20010 Done, mean position loss: 26.033950126171113\n",
      "Training growing_up:  34%|███▎      | 6720/20010 [5:27:03<9:48:09,  2.66s/batch]Batch 6600/20010 Done, mean position loss: 25.650586931705472\n",
      "Training growing_up:  34%|███      | 6770/20010 [5:27:13<11:15:21,  3.06s/batch]Batch 6700/20010 Done, mean position loss: 25.786462631225586\n",
      "Training growing_up:  34%|███      | 6762/20010 [5:27:15<10:52:54,  2.96s/batch]Batch 6600/20010 Done, mean position loss: 25.247842102050782\n",
      "Training growing_up:  33%|███      | 6699/20010 [5:27:17<10:10:43,  2.75s/batch]Batch 6600/20010 Done, mean position loss: 25.346199722290038\n",
      "Training growing_up:  34%|███      | 6749/20010 [5:27:22<10:23:56,  2.82s/batch]Batch 6700/20010 Done, mean position loss: 27.08000497341156\n",
      "Training growing_up:  33%|██▉      | 6604/20010 [5:27:24<11:26:33,  3.07s/batch]Batch 6800/20010 Done, mean position loss: 25.061588516235354\n",
      "Training growing_up:  34%|███      | 6799/20010 [5:27:30<11:15:32,  3.07s/batch]Batch 6700/20010 Done, mean position loss: 25.677691562175752\n",
      "Training growing_up:  34%|███      | 6805/20010 [5:27:35<10:46:31,  2.94s/batch]Batch 6800/20010 Done, mean position loss: 24.547718813419344\n",
      "Training growing_up:  34%|███      | 6753/20010 [5:27:50<10:14:49,  2.78s/batch]Batch 6600/20010 Done, mean position loss: 25.628668792247773\n",
      "Training growing_up:  33%|██▉      | 6638/20010 [5:27:53<10:11:57,  2.75s/batch]Batch 6700/20010 Done, mean position loss: 25.119112014770508\n",
      "Training growing_up:  33%|██▉      | 6586/20010 [5:28:02<11:25:01,  3.06s/batch]Batch 6800/20010 Done, mean position loss: 26.176219165325165\n",
      "Training growing_up:  34%|███      | 6820/20010 [5:28:21<10:41:55,  2.92s/batch]Batch 6700/20010 Done, mean position loss: 25.1808949136734\n",
      "Training growing_up:  33%|███      | 6686/20010 [5:28:32<10:55:12,  2.95s/batch]Batch 6700/20010 Done, mean position loss: 25.69495175600052\n",
      "Training growing_up:  33%|██▉      | 6632/20010 [5:28:34<10:08:59,  2.73s/batch]Batch 6700/20010 Done, mean position loss: 26.19494358778\n",
      "Training growing_up:  34%|███      | 6790/20010 [5:28:40<10:17:56,  2.80s/batch]Batch 6800/20010 Done, mean position loss: 25.034929902553557\n",
      "Training growing_up:  33%|███      | 6690/20010 [5:28:44<11:09:42,  3.02s/batch]Batch 6700/20010 Done, mean position loss: 26.16116910457611\n",
      "Training growing_up:  34%|███      | 6816/20010 [5:28:44<10:41:25,  2.92s/batch]Batch 6800/20010 Done, mean position loss: 24.770202317237853\n",
      "Training growing_up:  34%|███      | 6740/20010 [5:28:45<10:47:05,  2.93s/batch]Batch 6600/20010 Done, mean position loss: 24.472099103927615\n",
      "Training growing_up:  33%|███      | 6697/20010 [5:29:05<10:49:32,  2.93s/batch]Batch 6700/20010 Done, mean position loss: 25.01777723789215\n",
      "Training growing_up:  34%|███▎      | 6734/20010 [5:29:07<9:59:16,  2.71s/batch]Batch 6800/20010 Done, mean position loss: 25.618372321128845\n",
      "Training growing_up:  34%|███      | 6738/20010 [5:29:09<10:36:22,  2.88s/batch]Batch 6800/20010 Done, mean position loss: 24.61746745347977\n",
      "Training growing_up:  34%|███▍      | 6826/20010 [5:29:13<9:53:03,  2.70s/batch]Batch 6800/20010 Done, mean position loss: 24.48968604326248\n",
      "Training growing_up:  34%|███      | 6811/20010 [5:29:13<10:31:44,  2.87s/batch]Batch 6700/20010 Done, mean position loss: 26.46840415239334\n",
      "Training growing_up:  33%|██▉      | 6640/20010 [5:29:17<11:55:09,  3.21s/batch]Batch 6700/20010 Done, mean position loss: 25.12923460483551\n",
      "Training growing_up:  34%|███      | 6817/20010 [5:29:58<10:39:52,  2.91s/batch]Batch 6800/20010 Done, mean position loss: 26.693451035022733\n",
      "Training growing_up:  33%|██▉      | 6626/20010 [5:29:59<10:23:28,  2.80s/batch]Batch 6800/20010 Done, mean position loss: 25.945444989204404\n",
      "Training growing_up:  34%|███      | 6790/20010 [5:30:09<10:24:10,  2.83s/batch]Batch 6800/20010 Done, mean position loss: 26.15219852924347\n",
      "Training growing_up:  33%|██▉      | 6633/20010 [5:30:18<10:03:19,  2.71s/batch]Batch 6800/20010 Done, mean position loss: 25.769962060451505\n",
      "Training growing_up:  34%|███      | 6825/20010 [5:30:25<11:44:46,  3.21s/batch]Batch 6800/20010 Done, mean position loss: 25.159764149188995\n",
      "Training growing_up:  34%|███      | 6773/20010 [5:30:27<10:59:38,  2.99s/batch]Batch 6700/20010 Done, mean position loss: 24.936500315666198\n",
      "Training growing_up:  34%|███      | 6807/20010 [5:30:36<12:21:39,  3.37s/batch]Batch 6800/20010 Done, mean position loss: 25.44786529302597\n",
      "Training growing_up:  34%|███      | 6768/20010 [5:30:46<11:22:48,  3.09s/batch]Batch 6800/20010 Done, mean position loss: 25.666734807491302\n",
      "Training growing_up:  34%|███      | 6774/20010 [5:30:49<10:35:19,  2.88s/batch]Batch 6800/20010 Done, mean position loss: 25.796067390441895\n",
      "Training growing_up:  34%|███      | 6803/20010 [5:30:51<10:24:19,  2.84s/batch]Batch 6900/20010 Done, mean position loss: 24.294730546474455\n",
      "Training growing_up:  34%|███      | 6785/20010 [5:31:02<10:41:52,  2.91s/batch]Batch 6800/20010 Done, mean position loss: 26.097079446315764\n",
      "Training growing_up:  34%|███      | 6781/20010 [5:31:05<11:03:56,  3.01s/batch]Batch 6800/20010 Done, mean position loss: 25.543500096797942\n",
      "Training growing_up:  34%|███      | 6750/20010 [5:31:10<11:56:53,  3.24s/batch]Batch 6700/20010 Done, mean position loss: 26.81005377292633\n",
      "Training growing_up:  34%|███▍      | 6786/20010 [5:31:35<9:17:34,  2.53s/batch]Batch 6800/20010 Done, mean position loss: 25.429567050933837\n",
      "Training growing_up:  34%|███      | 6887/20010 [5:31:48<10:05:31,  2.77s/batch]Batch 6800/20010 Done, mean position loss: 26.065325598716736\n",
      "Training growing_up:  34%|███      | 6865/20010 [5:31:54<10:55:28,  2.99s/batch]Batch 6800/20010 Done, mean position loss: 25.68746676206589\n",
      "Training growing_up:  34%|███      | 6844/20010 [5:32:04<10:53:13,  2.98s/batch]Batch 6800/20010 Done, mean position loss: 25.088217153549195\n",
      "Batch 6700/20010 Done, mean position loss: 25.02363829612732\n",
      "Training growing_up:  34%|███      | 6833/20010 [5:32:08<10:18:10,  2.81s/batch]Batch 6800/20010 Done, mean position loss: 25.95626140117645\n",
      "Training growing_up:  33%|███      | 6699/20010 [5:32:14<11:51:56,  3.21s/batch]Batch 6700/20010 Done, mean position loss: 25.002146089076994\n",
      "Training growing_up:  34%|███      | 6832/20010 [5:32:15<10:47:39,  2.95s/batch]Batch 6900/20010 Done, mean position loss: 25.785884420871735\n",
      "Training growing_up:  34%|███      | 6830/20010 [5:32:19<10:46:55,  2.95s/batch]Batch 6800/20010 Done, mean position loss: 25.232399439811708\n",
      "Training growing_up:  33%|███      | 6688/20010 [5:32:20<10:49:19,  2.92s/batch]Batch 6700/20010 Done, mean position loss: 25.095028581619264\n",
      "Training growing_up:  34%|███      | 6876/20010 [5:32:26<11:05:27,  3.04s/batch]Batch 6900/20010 Done, mean position loss: 24.796248183250427\n",
      "Training growing_up:  34%|███      | 6861/20010 [5:32:52<10:21:35,  2.84s/batch]Batch 6800/20010 Done, mean position loss: 25.440091917514803\n",
      "Training growing_up:  34%|███      | 6788/20010 [5:32:53<10:17:53,  2.80s/batch]Batch 6900/20010 Done, mean position loss: 25.98344799041748\n",
      "Training growing_up:  34%|███      | 6819/20010 [5:32:59<10:36:04,  2.89s/batch]Batch 6700/20010 Done, mean position loss: 25.758850424289705\n",
      "Training growing_up:  34%|███      | 6891/20010 [5:33:28<11:40:26,  3.20s/batch]Batch 6800/20010 Done, mean position loss: 24.959703693389894\n",
      "Training growing_up:  34%|███      | 6899/20010 [5:33:30<10:14:51,  2.81s/batch]Batch 6800/20010 Done, mean position loss: 25.62318727731705\n",
      "Training growing_up:  34%|███      | 6889/20010 [5:33:33<11:02:58,  3.03s/batch]Batch 6900/20010 Done, mean position loss: 25.165279324054715\n",
      "Training growing_up:  34%|███      | 6816/20010 [5:33:33<10:12:09,  2.78s/batch]Batch 6800/20010 Done, mean position loss: 25.47841539144516\n",
      "Training growing_up:  34%|███      | 6894/20010 [5:33:36<10:25:51,  2.86s/batch]Batch 6900/20010 Done, mean position loss: 25.019480228424072\n",
      "Training growing_up:  34%|███▍      | 6892/20010 [5:33:40<9:43:59,  2.67s/batch]Batch 6800/20010 Done, mean position loss: 25.331890089511873\n",
      "Training growing_up:  34%|███▍      | 6839/20010 [5:33:43<9:48:21,  2.68s/batch]Batch 6700/20010 Done, mean position loss: 25.104777290821076\n",
      "Training growing_up:  34%|███      | 6882/20010 [5:33:56<10:53:57,  2.99s/batch]Batch 6900/20010 Done, mean position loss: 25.96939987421036\n",
      "Training growing_up:  34%|███      | 6759/20010 [5:34:01<10:20:40,  2.81s/batch]Batch 6800/20010 Done, mean position loss: 25.32954473733902\n",
      "Training growing_up:  35%|███▏     | 6969/20010 [5:34:06<10:36:31,  2.93s/batch]Batch 6900/20010 Done, mean position loss: 24.994834623336793\n",
      "Training growing_up:  34%|███      | 6709/20010 [5:34:07<11:09:11,  3.02s/batch]Batch 6900/20010 Done, mean position loss: 24.462167110443115\n",
      "Training growing_up:  34%|███      | 6799/20010 [5:34:07<10:10:56,  2.77s/batch]Batch 6800/20010 Done, mean position loss: 25.57551750421524\n",
      "Training growing_up:  34%|███▍      | 6903/20010 [5:34:12<9:45:36,  2.68s/batch]Batch 6800/20010 Done, mean position loss: 24.9919855761528\n",
      "Training growing_up:  35%|███      | 6939/20010 [5:34:48<11:29:55,  3.17s/batch]Batch 6900/20010 Done, mean position loss: 26.44987866163254\n",
      "Training growing_up:  34%|███      | 6740/20010 [5:34:51<10:11:50,  2.77s/batch]Batch 6900/20010 Done, mean position loss: 24.729480860233306\n",
      "Training growing_up:  34%|███      | 6755/20010 [5:34:53<12:28:13,  3.39s/batch]Batch 6900/20010 Done, mean position loss: 25.99546015977859\n",
      "Training growing_up:  35%|███      | 6926/20010 [5:35:20<10:44:27,  2.96s/batch]Batch 6800/20010 Done, mean position loss: 25.001762366294862\n",
      "Training growing_up:  34%|███      | 6734/20010 [5:35:22<11:40:40,  3.17s/batch]Batch 6900/20010 Done, mean position loss: 25.856949729919435\n",
      "Training growing_up:  34%|███▍      | 6897/20010 [5:35:28<8:41:53,  2.39s/batch]Batch 6900/20010 Done, mean position loss: 24.783472204208373\n",
      "Training growing_up:  34%|███      | 6865/20010 [5:35:29<11:08:31,  3.05s/batch]Batch 6900/20010 Done, mean position loss: 24.765503282546998\n",
      "Training growing_up:  34%|███      | 6902/20010 [5:35:32<10:27:59,  2.87s/batch]Batch 6900/20010 Done, mean position loss: 25.500372948646543\n",
      "Training growing_up:  35%|███▏     | 6956/20010 [5:35:38<10:00:25,  2.76s/batch]Batch 6900/20010 Done, mean position loss: 24.976140630245208\n",
      "Training growing_up:  34%|███      | 6869/20010 [5:35:40<10:29:34,  2.87s/batch]Batch 7000/20010 Done, mean position loss: 24.375874738693238\n",
      "Training growing_up:  34%|███▍      | 6875/20010 [5:35:56<9:40:53,  2.65s/batch]Batch 6900/20010 Done, mean position loss: 26.278881545066834\n",
      "Training growing_up:  34%|███      | 6878/20010 [5:35:58<10:57:17,  3.00s/batch]Batch 6900/20010 Done, mean position loss: 25.449566340446474\n",
      "Training growing_up:  34%|███      | 6766/20010 [5:36:07<10:05:57,  2.75s/batch]Batch 6800/20010 Done, mean position loss: 26.388997199535368\n",
      "Training growing_up:  34%|███▍      | 6861/20010 [5:36:27<9:48:06,  2.68s/batch]Batch 6900/20010 Done, mean position loss: 24.972439136505127\n",
      "Training growing_up:  34%|███▍      | 6852/20010 [5:36:41<9:41:10,  2.65s/batch]Batch 6900/20010 Done, mean position loss: 25.175796504020692\n",
      "Training growing_up:  35%|███▏     | 6953/20010 [5:36:45<11:31:43,  3.18s/batch]Batch 6900/20010 Done, mean position loss: 26.244981179237364\n",
      "Training growing_up:  34%|███      | 6861/20010 [5:36:53<10:49:49,  2.97s/batch]Batch 6800/20010 Done, mean position loss: 26.057007551193237\n",
      "Training growing_up:  35%|███      | 6947/20010 [5:37:01<10:16:49,  2.83s/batch]Batch 6900/20010 Done, mean position loss: 26.346575362682344\n",
      "Training growing_up:  35%|███▍      | 6972/20010 [5:37:05<9:08:21,  2.52s/batch]Batch 6900/20010 Done, mean position loss: 25.47148905038834\n",
      "Training growing_up:  35%|███      | 6925/20010 [5:37:07<11:32:52,  3.18s/batch]Batch 7000/20010 Done, mean position loss: 24.844357542991638\n",
      "Training growing_up:  34%|███▍      | 6864/20010 [5:37:07<9:11:33,  2.52s/batch]Batch 7000/20010 Done, mean position loss: 24.52306275367737\n",
      "Training growing_up:  35%|███▍      | 6936/20010 [5:37:09<9:55:21,  2.73s/batch]Batch 6900/20010 Done, mean position loss: 25.11897548675537\n",
      "Training growing_up:  34%|███▍      | 6868/20010 [5:37:12<9:53:57,  2.71s/batch]Batch 6800/20010 Done, mean position loss: 25.484899117946625\n",
      "Training growing_up:  35%|███▍      | 6990/20010 [5:37:18<9:21:51,  2.59s/batch]Batch 6800/20010 Done, mean position loss: 24.11785849094391\n",
      "Training growing_up:  35%|███▌      | 7011/20010 [5:37:32<8:01:25,  2.22s/batch]Batch 6900/20010 Done, mean position loss: 28.324080398082735\n",
      "Training growing_up:  35%|███▍      | 6906/20010 [5:37:45<9:47:56,  2.69s/batch]Batch 6800/20010 Done, mean position loss: 25.289526450634003\n",
      "Training growing_up:  34%|███▍      | 6891/20010 [5:37:46<8:00:04,  2.20s/batch]Batch 7000/20010 Done, mean position loss: 26.547668113708497\n",
      "Training growing_up:  35%|███▍      | 6927/20010 [5:38:09<8:46:36,  2.42s/batch]Batch 7000/20010 Done, mean position loss: 24.47951461315155\n",
      "Training growing_up:  35%|███▍      | 7001/20010 [5:38:09<8:53:15,  2.46s/batch]Batch 6900/20010 Done, mean position loss: 24.94086635351181\n",
      "Training growing_up:  35%|███▌      | 7059/20010 [5:38:13<8:23:06,  2.33s/batch]Batch 6900/20010 Done, mean position loss: 25.386431753635406\n",
      "Training growing_up:  34%|███▍      | 6899/20010 [5:38:15<9:27:51,  2.60s/batch]Batch 6900/20010 Done, mean position loss: 25.41137509584427\n",
      "Training growing_up:  35%|███▍      | 6937/20010 [5:38:20<9:10:17,  2.53s/batch]Batch 6900/20010 Done, mean position loss: 26.259559061527252\n",
      "Training growing_up:  35%|███▍      | 6977/20010 [5:38:22<9:15:30,  2.56s/batch]Batch 7000/20010 Done, mean position loss: 24.700460243225095\n",
      "Training growing_up:  35%|███▍      | 6981/20010 [5:38:34<9:54:16,  2.74s/batch]Batch 6800/20010 Done, mean position loss: 24.864117641448974\n",
      "Training growing_up:  34%|███▍      | 6801/20010 [5:38:34<8:33:25,  2.33s/batch]Batch 7000/20010 Done, mean position loss: 25.46460422515869\n",
      "Training growing_up:  35%|███▍      | 6946/20010 [5:38:42<8:46:47,  2.42s/batch]Batch 6900/20010 Done, mean position loss: 25.398485484123228\n",
      "Training growing_up:  34%|███▍      | 6901/20010 [5:38:42<8:02:19,  2.21s/batch]Batch 6900/20010 Done, mean position loss: 25.84008923768997\n",
      "Training growing_up:  35%|███▏     | 6969/20010 [5:38:44<11:24:32,  3.15s/batch]Batch 7000/20010 Done, mean position loss: 24.552679195404053\n",
      "Training growing_up:  35%|███      | 6919/20010 [5:38:57<10:00:39,  2.75s/batch]Batch 7000/20010 Done, mean position loss: 24.91248936891556\n",
      "Training growing_up:  34%|███      | 6862/20010 [5:39:00<10:35:58,  2.90s/batch]Batch 6900/20010 Done, mean position loss: 24.80877500534058\n",
      "Training growing_up:  35%|███▏     | 6959/20010 [5:39:23<11:00:40,  3.04s/batch]Batch 7000/20010 Done, mean position loss: 26.46202096939087\n",
      "Training growing_up:  35%|███▌      | 7023/20010 [5:39:29<9:33:51,  2.65s/batch]Batch 7000/20010 Done, mean position loss: 25.287849605083466\n",
      "Training growing_up:  35%|███▏     | 7016/20010 [5:39:38<11:08:37,  3.09s/batch]Batch 7000/20010 Done, mean position loss: 26.3537663435936\n",
      "Training growing_up:  35%|███▏     | 6966/20010 [5:39:58<10:15:28,  2.83s/batch]Batch 6900/20010 Done, mean position loss: 25.057008674144747\n",
      "Training growing_up:  35%|███▏     | 6998/20010 [5:40:02<11:02:11,  3.05s/batch]Batch 7000/20010 Done, mean position loss: 25.761063830852507\n",
      "Training growing_up:  34%|███      | 6834/20010 [5:40:07<10:49:31,  2.96s/batch]Batch 7100/20010 Done, mean position loss: 24.780281307697297\n",
      "Training growing_up:  35%|███▏     | 7101/20010 [5:40:06<10:28:46,  2.92s/batch]Batch 7000/20010 Done, mean position loss: 25.195753614902493\n",
      "Training growing_up:  35%|███▏     | 7012/20010 [5:40:09<10:41:35,  2.96s/batch]Batch 7000/20010 Done, mean position loss: 24.833621780872345\n",
      "Training growing_up:  35%|███▏     | 6992/20010 [5:40:10<10:03:37,  2.78s/batch]Batch 7000/20010 Done, mean position loss: 24.993329532146454\n",
      "Training growing_up:  35%|███▏     | 6978/20010 [5:40:12<10:02:42,  2.77s/batch]Batch 7000/20010 Done, mean position loss: 25.521348316669467\n",
      "Training growing_up:  35%|███▌      | 7011/20010 [5:40:35<9:45:11,  2.70s/batch]Batch 7000/20010 Done, mean position loss: 25.32153109550476\n",
      "Training growing_up:  35%|███▏     | 7013/20010 [5:40:40<10:18:30,  2.86s/batch]Batch 7000/20010 Done, mean position loss: 26.15428884983063\n",
      "Training growing_up:  35%|███▏     | 7041/20010 [5:40:49<10:09:13,  2.82s/batch]Batch 6900/20010 Done, mean position loss: 26.977933156490327\n",
      "Training growing_up:  36%|███▏     | 7123/20010 [5:41:04<10:03:56,  2.81s/batch]Batch 7000/20010 Done, mean position loss: 25.074088962078093\n",
      "Training growing_up:  35%|███▏     | 6993/20010 [5:41:17<10:17:17,  2.85s/batch]Batch 7000/20010 Done, mean position loss: 25.65582122325897\n",
      "Training growing_up:  35%|███▏     | 6995/20010 [5:41:23<10:35:02,  2.93s/batch]Batch 7000/20010 Done, mean position loss: 25.440321316719054\n",
      "Training growing_up:  35%|███▏     | 7053/20010 [5:41:23<10:33:46,  2.93s/batch]Batch 7000/20010 Done, mean position loss: 24.640893483161925\n",
      "Training growing_up:  35%|███▏     | 6997/20010 [5:41:30<11:16:57,  3.12s/batch]Batch 7100/20010 Done, mean position loss: 24.242164969444275\n",
      "Training growing_up:  35%|███▌      | 7071/20010 [5:41:33<8:49:44,  2.46s/batch]Batch 7100/20010 Done, mean position loss: 25.672617812156673\n",
      "Training growing_up:  35%|███▌      | 7013/20010 [5:41:37<9:05:57,  2.52s/batch]Batch 6900/20010 Done, mean position loss: 24.79034696817398\n",
      "Training growing_up:  35%|███▌      | 7074/20010 [5:41:42<9:41:11,  2.70s/batch]Batch 7000/20010 Done, mean position loss: 25.572824807167052\n",
      "Training growing_up:  35%|███▏     | 6977/20010 [5:41:47<10:29:26,  2.90s/batch]Batch 7000/20010 Done, mean position loss: 25.050092613697053\n",
      "Training growing_up:  35%|███▏     | 6978/20010 [5:41:50<10:14:45,  2.83s/batch]Batch 6900/20010 Done, mean position loss: 26.037675693035126\n",
      "Training growing_up:  36%|███▌      | 7115/20010 [5:42:08<9:59:14,  2.79s/batch]Batch 6900/20010 Done, mean position loss: 24.92429496049881\n",
      "Training growing_up:  35%|███▏     | 7059/20010 [5:42:14<11:12:18,  3.11s/batch]Batch 7000/20010 Done, mean position loss: 25.963761811256408\n",
      "Training growing_up:  35%|███▏     | 7058/20010 [5:42:22<11:26:49,  3.18s/batch]Batch 7100/20010 Done, mean position loss: 26.249062743186954\n",
      "Training growing_up:  35%|███      | 6909/20010 [5:42:33<11:42:10,  3.22s/batch]Batch 6900/20010 Done, mean position loss: 25.216931426525115\n",
      "Training growing_up:  35%|███▌      | 7048/20010 [5:42:47<9:38:16,  2.68s/batch]Batch 7100/20010 Done, mean position loss: 24.834903116226197\n",
      "Training growing_up:  35%|███▏     | 7058/20010 [5:42:56<10:27:35,  2.91s/batch]Batch 7000/20010 Done, mean position loss: 26.053254392147064\n",
      "Training growing_up:  35%|███▏     | 6991/20010 [5:42:56<10:53:32,  3.01s/batch]Batch 7000/20010 Done, mean position loss: 24.72110167503357\n",
      "Training growing_up:  35%|███▏     | 7061/20010 [5:43:00<10:45:16,  2.99s/batch]Batch 7100/20010 Done, mean position loss: 24.803652386665345\n",
      "Training growing_up:  36%|███▏     | 7134/20010 [5:43:06<10:09:45,  2.84s/batch]Batch 7000/20010 Done, mean position loss: 24.952377822399136\n",
      "Training growing_up:  35%|███▏     | 7053/20010 [5:43:09<10:24:02,  2.89s/batch]Batch 7100/20010 Done, mean position loss: 25.14626097202301\n",
      "Training growing_up:  35%|███▏     | 7090/20010 [5:43:11<11:04:48,  3.09s/batch]Batch 7000/20010 Done, mean position loss: 25.85328733444214\n",
      "Training growing_up:  36%|███▌      | 7109/20010 [5:43:22<9:48:25,  2.74s/batch]Batch 7000/20010 Done, mean position loss: 25.60136045694351\n",
      "Training growing_up:  35%|███▏     | 7051/20010 [5:43:27<10:53:48,  3.03s/batch]Batch 6900/20010 Done, mean position loss: 24.90404067993164\n",
      "Training growing_up:  35%|███▏     | 7003/20010 [5:43:29<10:29:42,  2.90s/batch]Batch 7000/20010 Done, mean position loss: 24.918097422122955\n",
      "Training growing_up:  35%|███▏     | 7004/20010 [5:43:31<10:05:17,  2.79s/batch]Batch 7100/20010 Done, mean position loss: 23.878079082965854\n",
      "Training growing_up:  35%|███▏     | 7042/20010 [5:43:41<11:11:06,  3.11s/batch]Batch 7100/20010 Done, mean position loss: 24.585376577377318\n",
      "Training growing_up:  35%|███▏     | 7075/20010 [5:43:47<10:55:50,  3.04s/batch]Batch 7000/20010 Done, mean position loss: 25.02950677394867\n",
      "Training growing_up:  35%|███      | 6935/20010 [5:44:13<12:18:39,  3.39s/batch]Batch 7100/20010 Done, mean position loss: 26.581169600486756\n",
      "Training growing_up:  35%|███▏     | 7076/20010 [5:44:18<11:38:06,  3.24s/batch]Batch 7100/20010 Done, mean position loss: 24.542117338180542\n",
      "Training growing_up:  36%|███▏     | 7127/20010 [5:44:26<10:12:12,  2.85s/batch]Batch 7100/20010 Done, mean position loss: 25.756774027347564\n",
      "Training growing_up:  35%|███▏     | 7100/20010 [5:44:47<11:16:25,  3.14s/batch]Batch 7000/20010 Done, mean position loss: 24.413634617328647\n",
      "Training growing_up:  35%|███▏     | 7022/20010 [5:44:48<11:44:27,  3.25s/batch]Batch 7200/20010 Done, mean position loss: 24.617963795661925\n",
      "Training growing_up:  35%|███▏     | 7029/20010 [5:44:49<10:44:44,  2.98s/batch]Batch 7100/20010 Done, mean position loss: 25.139405064582824\n",
      "Training growing_up:  36%|███▏     | 7143/20010 [5:44:54<10:59:25,  3.07s/batch]Batch 7100/20010 Done, mean position loss: 25.394310669898985\n",
      "Training growing_up:  36%|███▏     | 7172/20010 [5:44:57<10:45:59,  3.02s/batch]Batch 7100/20010 Done, mean position loss: 24.927791709899903\n",
      "Training growing_up:  36%|███▏     | 7116/20010 [5:45:02<10:39:41,  2.98s/batch]Batch 7100/20010 Done, mean position loss: 25.110628914833068\n",
      "Training growing_up:  36%|███▏     | 7175/20010 [5:45:05<10:07:20,  2.84s/batch]Batch 7100/20010 Done, mean position loss: 25.174942212104796\n",
      "Training growing_up:  35%|███▏     | 6957/20010 [5:45:24<12:33:35,  3.46s/batch]Batch 7100/20010 Done, mean position loss: 25.48938967227936\n",
      "Training growing_up:  36%|███▌      | 7127/20010 [5:45:35<9:39:30,  2.70s/batch]Batch 7100/20010 Done, mean position loss: 25.881304090023043\n",
      "Training growing_up:  35%|███▌      | 7098/20010 [5:45:50<9:54:54,  2.76s/batch]Batch 7000/20010 Done, mean position loss: 26.16759712457657\n",
      "Training growing_up:  36%|███▏     | 7148/20010 [5:45:58<10:27:13,  2.93s/batch]Batch 7100/20010 Done, mean position loss: 25.083229937553405\n",
      "Training growing_up:  36%|███▌      | 7141/20010 [5:46:11<9:57:52,  2.79s/batch]Batch 7100/20010 Done, mean position loss: 25.824371089935305\n",
      "Training growing_up:  36%|███▏     | 7126/20010 [5:46:12<11:21:32,  3.17s/batch]Batch 7100/20010 Done, mean position loss: 25.301624917984007\n",
      "Training growing_up:  35%|███▏     | 7085/20010 [5:46:24<10:20:19,  2.88s/batch]Batch 7100/20010 Done, mean position loss: 25.63131167650223\n",
      "Training growing_up:  36%|███▏     | 7157/20010 [5:46:24<10:02:45,  2.81s/batch]Batch 7200/20010 Done, mean position loss: 24.284373807907105\n",
      "Training growing_up:  35%|███▏     | 7064/20010 [5:46:24<10:11:11,  2.83s/batch]Batch 7200/20010 Done, mean position loss: 24.641393835544584\n",
      "Training growing_up:  36%|███▏     | 7165/20010 [5:46:41<10:45:34,  3.02s/batch]Batch 7000/20010 Done, mean position loss: 25.3457182264328\n",
      "Training growing_up:  35%|███▏     | 7017/20010 [5:46:41<11:30:34,  3.19s/batch]Batch 7100/20010 Done, mean position loss: 25.339008893966675\n",
      "Training growing_up:  35%|███▏     | 7063/20010 [5:46:52<11:13:01,  3.12s/batch]Batch 7100/20010 Done, mean position loss: 25.30157565355301\n",
      "Training growing_up:  35%|███▏     | 7072/20010 [5:47:01<11:06:25,  3.09s/batch]Batch 7000/20010 Done, mean position loss: 26.147933599948885\n",
      "Training growing_up:  36%|███▏     | 7120/20010 [5:47:09<11:47:48,  3.29s/batch]Batch 7100/20010 Done, mean position loss: 25.547019562721253\n",
      "Training growing_up:  36%|███▏     | 7175/20010 [5:47:10<10:38:09,  2.98s/batch]Batch 7000/20010 Done, mean position loss: 24.121565842628478\n",
      "Training growing_up:  36%|███▏     | 7125/20010 [5:47:23<10:18:11,  2.88s/batch]Batch 7200/20010 Done, mean position loss: 26.35506165742874\n",
      "Training growing_up:  36%|███▏     | 7121/20010 [5:47:39<11:15:44,  3.15s/batch]Batch 7000/20010 Done, mean position loss: 25.8439058637619\n",
      "Training growing_up:  36%|███▏     | 7173/20010 [5:47:46<10:29:28,  2.94s/batch]Batch 7200/20010 Done, mean position loss: 24.540972576141357\n",
      "Training growing_up:  35%|███▏     | 7037/20010 [5:47:46<11:48:24,  3.28s/batch]Batch 7100/20010 Done, mean position loss: 25.45441190004349\n",
      "Training growing_up:  36%|███▌      | 7230/20010 [5:47:53<9:58:55,  2.81s/batch]Batch 7200/20010 Done, mean position loss: 24.667713551521302\n",
      "Training growing_up:  36%|███▏     | 7178/20010 [5:48:00<10:23:15,  2.91s/batch]Batch 7100/20010 Done, mean position loss: 25.420798540115356\n",
      "Training growing_up:  35%|███▏     | 7101/20010 [5:48:00<10:32:08,  2.94s/batch]Batch 7100/20010 Done, mean position loss: 25.12541508436203\n",
      "Training growing_up:  36%|███▏     | 7104/20010 [5:48:10<10:49:01,  3.02s/batch]Batch 7200/20010 Done, mean position loss: 25.058437123298646\n",
      "Training growing_up:  35%|███▏     | 7097/20010 [5:48:17<10:44:14,  2.99s/batch]Batch 7100/20010 Done, mean position loss: 25.89429619073868\n",
      "Training growing_up:  36%|███▌      | 7115/20010 [5:48:25<9:44:24,  2.72s/batch]Batch 7200/20010 Done, mean position loss: 24.791860189437866\n",
      "Training growing_up:  36%|███▌      | 7173/20010 [5:48:26<8:49:42,  2.48s/batch]Batch 7000/20010 Done, mean position loss: 24.528594822883605\n",
      "Training growing_up:  36%|███▏     | 7168/20010 [5:48:27<10:47:57,  3.03s/batch]Batch 7100/20010 Done, mean position loss: 25.8098379778862\n",
      "Training growing_up:  35%|███▏     | 7037/20010 [5:48:29<11:34:02,  3.21s/batch]Batch 7100/20010 Done, mean position loss: 25.494879665374757\n",
      "Training growing_up:  36%|███▏     | 7135/20010 [5:48:37<11:39:56,  3.26s/batch]Batch 7200/20010 Done, mean position loss: 25.17297344923019\n",
      "Training growing_up:  35%|███▏     | 7024/20010 [5:48:48<10:08:40,  2.81s/batch]Batch 7100/20010 Done, mean position loss: 24.49362376689911\n",
      "Training growing_up:  36%|███▏     | 7119/20010 [5:49:08<10:36:37,  2.96s/batch]Batch 7200/20010 Done, mean position loss: 25.56965078353882\n",
      "Training growing_up:  36%|███▏     | 7167/20010 [5:49:14<11:00:40,  3.09s/batch]Batch 7200/20010 Done, mean position loss: 25.452723648548126\n",
      "Training growing_up:  36%|███▎     | 7233/20010 [5:49:28<10:27:13,  2.95s/batch]Batch 7200/20010 Done, mean position loss: 25.953940324783325\n",
      "Training growing_up:  35%|███▏     | 7060/20010 [5:49:40<10:47:57,  3.00s/batch]Batch 7300/20010 Done, mean position loss: 24.22787275314331\n",
      "Training growing_up:  36%|███▎     | 7269/20010 [5:49:46<10:04:28,  2.85s/batch]Batch 7100/20010 Done, mean position loss: 24.56461783647537\n",
      "Training growing_up:  36%|███▏     | 7161/20010 [5:49:50<10:45:40,  3.02s/batch]Batch 7200/20010 Done, mean position loss: 24.81665471553803\n",
      "Training growing_up:  36%|███▎     | 7227/20010 [5:49:54<10:30:28,  2.96s/batch]Batch 7200/20010 Done, mean position loss: 25.39088968753815\n",
      "Training growing_up:  35%|███▏     | 7057/20010 [5:49:55<11:21:13,  3.16s/batch]Batch 7200/20010 Done, mean position loss: 25.063733117580412\n",
      "Training growing_up:  36%|███▏     | 7204/20010 [5:49:59<10:16:34,  2.89s/batch]Batch 7200/20010 Done, mean position loss: 24.50711061954498\n",
      "Training growing_up:  36%|███▋      | 7276/20010 [5:50:03<9:20:31,  2.64s/batch]Batch 7200/20010 Done, mean position loss: 25.78008250951767\n",
      "Training growing_up:  36%|███▋      | 7280/20010 [5:50:13<8:56:08,  2.53s/batch]Batch 7200/20010 Done, mean position loss: 25.920645933151246\n",
      "Training growing_up:  35%|███▏     | 7070/20010 [5:50:35<10:26:56,  2.91s/batch]Batch 7200/20010 Done, mean position loss: 25.446035990715025\n",
      "Training growing_up:  36%|███▏     | 7189/20010 [5:50:59<10:23:49,  2.92s/batch]Batch 7100/20010 Done, mean position loss: 26.670185456275938\n",
      "Training growing_up:  36%|███▏     | 7184/20010 [5:51:00<10:25:55,  2.93s/batch]Batch 7200/20010 Done, mean position loss: 24.52979249715805\n",
      "Training growing_up:  36%|███▎     | 7251/20010 [5:51:04<11:01:53,  3.11s/batch]Batch 7200/20010 Done, mean position loss: 25.64849155187607\n",
      "Training growing_up:  36%|███▌      | 7167/20010 [5:51:11<9:58:16,  2.79s/batch]Batch 7200/20010 Done, mean position loss: 25.616137981414795\n",
      "Training growing_up:  36%|███▏     | 7193/20010 [5:51:13<12:05:47,  3.40s/batch]Batch 7300/20010 Done, mean position loss: 24.97775637626648\n",
      "Training growing_up:  36%|███▎     | 7230/20010 [5:51:19<10:40:04,  3.01s/batch]Batch 7300/20010 Done, mean position loss: 24.400012929439544\n",
      "Training growing_up:  36%|███▎     | 7270/20010 [5:51:21<11:29:29,  3.25s/batch]Batch 7200/20010 Done, mean position loss: 24.842562024593356\n",
      "Training growing_up:  36%|███▌      | 7224/20010 [5:51:37<9:55:06,  2.79s/batch]Batch 7200/20010 Done, mean position loss: 25.171800239086153\n",
      "Training growing_up:  36%|███▎     | 7269/20010 [5:51:45<11:10:38,  3.16s/batch]Batch 7100/20010 Done, mean position loss: 25.185045659542084\n",
      "Training growing_up:  36%|███▌      | 7164/20010 [5:51:52<9:52:13,  2.77s/batch]Batch 7200/20010 Done, mean position loss: 25.225983049869537\n",
      "Training growing_up:  36%|███▌      | 7184/20010 [5:52:00<9:45:49,  2.74s/batch]Batch 7100/20010 Done, mean position loss: 25.95109756708145\n",
      "Training growing_up:  36%|███▌      | 7149/20010 [5:52:02<9:03:00,  2.53s/batch]Batch 7200/20010 Done, mean position loss: 25.85984734296799\n",
      "Training growing_up:  36%|███▌      | 7219/20010 [5:52:05<9:59:07,  2.81s/batch]Batch 7100/20010 Done, mean position loss: 24.474339599609372\n",
      "Training growing_up:  36%|███▌      | 7151/20010 [5:52:07<8:59:00,  2.52s/batch]Batch 7300/20010 Done, mean position loss: 26.14609995365143\n",
      "Training growing_up:  36%|███▎     | 7226/20010 [5:52:26<10:28:18,  2.95s/batch]Batch 7300/20010 Done, mean position loss: 24.662808136940004\n",
      "Training growing_up:  36%|███▋      | 7254/20010 [5:52:35<9:30:51,  2.69s/batch]Batch 7200/20010 Done, mean position loss: 25.162642121315002\n",
      "Training growing_up:  36%|███▌      | 7135/20010 [5:52:38<9:15:20,  2.59s/batch]Batch 7100/20010 Done, mean position loss: 25.366638226509096\n",
      "Training growing_up:  36%|███▋      | 7300/20010 [5:52:45<8:46:40,  2.49s/batch]Batch 7200/20010 Done, mean position loss: 25.360224313735962\n",
      "Training growing_up:  36%|███▌      | 7202/20010 [5:52:46<8:00:50,  2.25s/batch]Batch 7300/20010 Done, mean position loss: 24.397902791500094\n",
      "Training growing_up:  35%|███▌      | 7091/20010 [5:52:47<9:32:19,  2.66s/batch]Batch 7200/20010 Done, mean position loss: 24.903744065761565\n",
      "Training growing_up:  36%|███▌      | 7206/20010 [5:52:48<9:24:12,  2.64s/batch]Batch 7300/20010 Done, mean position loss: 24.924291448593138\n",
      "Training growing_up:  36%|███▌      | 7240/20010 [5:53:03<7:58:57,  2.25s/batch]Batch 7200/20010 Done, mean position loss: 25.42123280763626\n",
      "Training growing_up:  36%|███▎     | 7297/20010 [5:53:12<10:11:18,  2.89s/batch]Batch 7300/20010 Done, mean position loss: 24.398474042415618\n",
      "Training growing_up:  37%|███▋      | 7312/20010 [5:53:14<8:29:39,  2.41s/batch]Batch 7100/20010 Done, mean position loss: 24.229669790267945\n",
      "Training growing_up:  37%|███▎     | 7314/20010 [5:53:21<10:24:52,  2.95s/batch]Batch 7200/20010 Done, mean position loss: 24.650499505996706\n",
      "Training growing_up:  37%|███▋      | 7381/20010 [5:53:22<9:18:50,  2.66s/batch]Batch 7200/20010 Done, mean position loss: 26.233799629211425\n",
      "Training growing_up:  36%|███▏     | 7209/20010 [5:53:24<10:03:05,  2.83s/batch]Batch 7300/20010 Done, mean position loss: 25.347912175655367\n",
      "Training growing_up:  36%|███▏     | 7140/20010 [5:53:33<10:03:14,  2.81s/batch]Batch 7200/20010 Done, mean position loss: 24.84528198003769\n",
      "Training growing_up:  37%|███▋      | 7333/20010 [5:53:53<9:37:03,  2.73s/batch]Batch 7300/20010 Done, mean position loss: 24.468266277313234\n",
      "Training growing_up:  36%|███▏     | 7128/20010 [5:53:57<12:05:32,  3.38s/batch]Batch 7300/20010 Done, mean position loss: 26.247476060390472\n",
      "Training growing_up:  36%|███▏     | 7198/20010 [5:54:17<10:53:20,  3.06s/batch]Batch 7400/20010 Done, mean position loss: 24.17102787017822\n",
      "Training growing_up:  36%|███▎     | 7253/20010 [5:54:18<10:56:28,  3.09s/batch]Batch 7300/20010 Done, mean position loss: 25.786154265403745\n",
      "Training growing_up:  37%|███▎     | 7336/20010 [5:54:25<10:04:44,  2.86s/batch]Batch 7200/20010 Done, mean position loss: 24.56298526287079\n",
      "Training growing_up:  36%|███▎     | 7263/20010 [5:54:32<10:03:33,  2.84s/batch]Batch 7300/20010 Done, mean position loss: 25.31561804294586\n",
      "Training growing_up:  36%|███▎     | 7239/20010 [5:54:35<10:38:55,  3.00s/batch]Batch 7300/20010 Done, mean position loss: 25.46507280111313\n",
      "Training growing_up:  36%|███▏     | 7129/20010 [5:54:36<10:07:17,  2.83s/batch]Batch 7300/20010 Done, mean position loss: 24.779294719696047\n",
      "Training growing_up:  36%|███▋      | 7302/20010 [5:54:38<9:15:54,  2.62s/batch]Batch 7300/20010 Done, mean position loss: 25.275299911499022\n",
      "Training growing_up:  37%|███▎     | 7351/20010 [5:54:45<10:07:03,  2.88s/batch]Batch 7300/20010 Done, mean position loss: 25.0054447555542\n",
      "Training growing_up:  36%|███▎     | 7230/20010 [5:54:58<10:09:39,  2.86s/batch]Batch 7300/20010 Done, mean position loss: 24.9408473443985\n",
      "Training growing_up:  36%|███▏     | 7172/20010 [5:55:08<10:23:50,  2.92s/batch]Batch 7300/20010 Done, mean position loss: 26.236740131378177\n",
      "Training growing_up:  37%|███▋      | 7323/20010 [5:55:39<9:47:34,  2.78s/batch]Batch 7300/20010 Done, mean position loss: 25.51283180475235\n",
      "Training growing_up:  36%|███▏     | 7200/20010 [5:55:45<10:01:34,  2.82s/batch]Batch 7300/20010 Done, mean position loss: 26.423609659671783\n",
      "Training growing_up:  36%|███▌      | 7253/20010 [5:55:47<9:56:13,  2.80s/batch]Batch 7200/20010 Done, mean position loss: 27.019714753627778\n",
      "Training growing_up:  36%|███▌      | 7253/20010 [5:55:52<8:35:50,  2.43s/batch]Batch 7400/20010 Done, mean position loss: 25.633400120735168\n",
      "Training growing_up:  37%|███▎     | 7435/20010 [5:55:54<10:14:09,  2.93s/batch]Batch 7400/20010 Done, mean position loss: 24.32251380443573\n",
      "Training growing_up:  36%|███▏     | 7188/20010 [5:55:56<10:26:38,  2.93s/batch]Batch 7300/20010 Done, mean position loss: 25.130853178501127\n",
      "Training growing_up:  37%|███▋      | 7370/20010 [5:56:05<9:28:45,  2.70s/batch]Batch 7300/20010 Done, mean position loss: 24.629560992717742\n",
      "Training growing_up:  37%|███▋      | 7410/20010 [5:56:20<9:45:00,  2.79s/batch]Batch 7300/20010 Done, mean position loss: 25.11170096874237\n",
      "Training growing_up:  36%|███▎     | 7280/20010 [5:56:35<10:19:38,  2.92s/batch]Batch 7200/20010 Done, mean position loss: 25.096046447753906\n",
      "Training growing_up:  37%|███▎     | 7397/20010 [5:56:38<10:28:32,  2.99s/batch]Batch 7300/20010 Done, mean position loss: 24.69559361219406\n",
      "Training growing_up:  37%|███▋      | 7455/20010 [5:56:50<9:42:26,  2.78s/batch]Batch 7400/20010 Done, mean position loss: 26.41291528224945\n",
      "Training growing_up:  37%|███▎     | 7363/20010 [5:56:51<10:21:07,  2.95s/batch]Batch 7300/20010 Done, mean position loss: 25.249861621856688\n",
      "Training growing_up:  37%|███▋      | 7350/20010 [5:56:53<9:51:39,  2.80s/batch]Batch 7200/20010 Done, mean position loss: 25.29230518579483\n",
      "Training growing_up:  37%|███▎     | 7328/20010 [5:56:57<10:55:23,  3.10s/batch]Batch 7200/20010 Done, mean position loss: 24.27736662864685\n",
      "Training growing_up:  36%|███▏     | 7181/20010 [5:57:11<10:37:20,  2.98s/batch]Batch 7400/20010 Done, mean position loss: 24.938053894042966\n",
      "Training growing_up:  36%|███▋      | 7296/20010 [5:57:19<9:29:03,  2.69s/batch]Batch 7300/20010 Done, mean position loss: 24.483666825294495\n",
      "Training growing_up:  37%|███▎     | 7413/20010 [5:57:25<10:38:26,  3.04s/batch]Batch 7400/20010 Done, mean position loss: 24.82188286781311\n",
      "Training growing_up:  37%|███▎     | 7407/20010 [5:57:29<10:55:07,  3.12s/batch]Batch 7300/20010 Done, mean position loss: 24.803010931015017\n",
      "Training growing_up:  37%|███▋      | 7350/20010 [5:57:32<9:10:03,  2.61s/batch]Batch 7400/20010 Done, mean position loss: 24.42991671562195\n",
      "Training growing_up:  36%|███▏     | 7221/20010 [5:57:33<10:13:51,  2.88s/batch]Batch 7300/20010 Done, mean position loss: 25.08595057487488\n",
      "Training growing_up:  36%|███▎     | 7290/20010 [5:57:38<10:16:40,  2.91s/batch]Batch 7200/20010 Done, mean position loss: 24.96761490106583\n",
      "Training growing_up:  37%|███▎     | 7309/20010 [5:57:54<11:06:05,  3.15s/batch]Batch 7300/20010 Done, mean position loss: 26.015400111675262\n",
      "Training growing_up:  37%|███▋      | 7388/20010 [5:58:01<9:30:06,  2.71s/batch]Batch 7400/20010 Done, mean position loss: 24.17108768939972\n",
      "Training growing_up:  37%|███▎     | 7363/20010 [5:58:13<11:00:52,  3.14s/batch]Batch 7200/20010 Done, mean position loss: 24.449066686630246\n",
      "Training growing_up:  37%|███▎     | 7353/20010 [5:58:14<10:26:26,  2.97s/batch]Batch 7300/20010 Done, mean position loss: 25.973552091121675\n",
      "Training growing_up:  37%|███▎     | 7422/20010 [5:58:15<10:58:21,  3.14s/batch]Batch 7300/20010 Done, mean position loss: 25.153174073696135\n",
      "Training growing_up:  36%|███▎     | 7302/20010 [5:58:18<10:39:41,  3.02s/batch]Batch 7400/20010 Done, mean position loss: 24.2766574549675\n",
      "Training growing_up:  37%|███▋      | 7421/20010 [5:58:23<9:47:22,  2.80s/batch]Batch 7300/20010 Done, mean position loss: 24.772944390773773\n",
      "Training growing_up:  37%|███▎     | 7408/20010 [5:58:39<10:43:17,  3.06s/batch]Batch 7400/20010 Done, mean position loss: 24.667663209438324\n",
      "Training growing_up:  36%|███▎     | 7244/20010 [5:58:43<10:55:22,  3.08s/batch]Batch 7400/20010 Done, mean position loss: 25.9982444357872\n",
      "Training growing_up:  36%|███▎     | 7267/20010 [5:59:06<11:25:36,  3.23s/batch]Batch 7500/20010 Done, mean position loss: 24.36735298871994\n",
      "Training growing_up:  36%|███▎     | 7230/20010 [5:59:09<11:06:33,  3.13s/batch]Batch 7400/20010 Done, mean position loss: 25.85960598230362\n",
      "Training growing_up:  36%|███▎     | 7246/20010 [5:59:16<10:42:40,  3.02s/batch]Batch 7300/20010 Done, mean position loss: 25.635580923557285\n",
      "Training growing_up:  37%|███▋      | 7363/20010 [5:59:22<9:35:10,  2.73s/batch]Batch 7400/20010 Done, mean position loss: 24.824240076541898\n",
      "Training growing_up:  36%|███▎     | 7249/20010 [5:59:26<12:15:02,  3.46s/batch]Batch 7400/20010 Done, mean position loss: 25.291478865146637\n",
      "Training growing_up:  37%|███▎     | 7398/20010 [5:59:29<10:35:44,  3.02s/batch]Batch 7400/20010 Done, mean position loss: 25.473344621658327\n",
      "Training growing_up:  37%|███▎     | 7475/20010 [5:59:33<11:26:58,  3.29s/batch]Batch 7400/20010 Done, mean position loss: 25.15885802268982\n",
      "Training growing_up:  37%|███▎     | 7403/20010 [5:59:38<10:41:59,  3.06s/batch]Batch 7400/20010 Done, mean position loss: 25.08390040874481\n",
      "Training growing_up:  37%|███▋      | 7340/20010 [5:59:49<9:52:31,  2.81s/batch]Batch 7400/20010 Done, mean position loss: 24.9300387763977\n",
      "Training growing_up:  36%|███▎     | 7286/20010 [6:00:07<10:54:46,  3.09s/batch]Batch 7400/20010 Done, mean position loss: 25.912598559856413\n",
      "Training growing_up:  37%|███▎     | 7431/20010 [6:00:38<10:08:07,  2.90s/batch]Batch 7400/20010 Done, mean position loss: 24.85912115097046\n",
      "Training growing_up:  37%|███▎     | 7363/20010 [6:00:40<10:38:10,  3.03s/batch]Batch 7400/20010 Done, mean position loss: 25.585065155029298\n",
      "Training growing_up:  37%|███▎     | 7432/20010 [6:00:42<11:00:24,  3.15s/batch]Batch 7500/20010 Done, mean position loss: 25.36508285045624\n",
      "Training growing_up:  37%|███▎     | 7467/20010 [6:00:50<10:10:28,  2.92s/batch]Batch 7500/20010 Done, mean position loss: 23.578517882823945\n",
      "Training growing_up:  37%|███▎     | 7435/20010 [6:00:50<10:15:01,  2.93s/batch]Batch 7300/20010 Done, mean position loss: 26.37039958953857\n",
      "Training growing_up:  37%|███▎     | 7397/20010 [6:00:53<10:32:36,  3.01s/batch]Batch 7400/20010 Done, mean position loss: 24.95878927230835\n",
      "Training growing_up:  37%|███▎     | 7376/20010 [6:01:05<11:36:44,  3.31s/batch]Batch 7400/20010 Done, mean position loss: 24.07484602212906\n",
      "Training growing_up:  37%|███▎     | 7367/20010 [6:01:10<11:20:57,  3.23s/batch]Batch 7400/20010 Done, mean position loss: 26.237411279678344\n",
      "Training growing_up:  37%|███▎     | 7459/20010 [6:01:34<10:52:29,  3.12s/batch]Batch 7300/20010 Done, mean position loss: 24.478717200756073\n",
      "Training growing_up:  38%|███▍     | 7518/20010 [6:01:39<10:19:34,  2.98s/batch]Batch 7400/20010 Done, mean position loss: 24.865698375701903\n",
      "Training growing_up:  38%|███▊      | 7556/20010 [6:01:39<9:29:32,  2.74s/batch]Batch 7500/20010 Done, mean position loss: 26.318503925800325\n",
      "Training growing_up:  36%|███▋      | 7297/20010 [6:01:50<9:58:39,  2.83s/batch]Batch 7400/20010 Done, mean position loss: 25.312562386989594\n",
      "Training growing_up:  37%|███▋      | 7421/20010 [6:02:01<9:34:02,  2.74s/batch]Batch 7300/20010 Done, mean position loss: 24.890553119182588\n",
      "Training growing_up:  37%|███▋      | 7497/20010 [6:02:01<9:45:41,  2.81s/batch]Batch 7300/20010 Done, mean position loss: 24.330496373176576\n",
      "Training growing_up:  37%|███▋      | 7413/20010 [6:02:13<9:41:42,  2.77s/batch]Batch 7500/20010 Done, mean position loss: 24.718053047657012\n",
      "Training growing_up:  37%|███▋      | 7395/20010 [6:02:16<9:32:42,  2.72s/batch]Batch 7500/20010 Done, mean position loss: 24.191422560214995\n",
      "Training growing_up:  36%|███▎     | 7295/20010 [6:02:21<10:44:34,  3.04s/batch]Batch 7400/20010 Done, mean position loss: 24.731511228084564\n",
      "Training growing_up:  38%|███▍     | 7504/20010 [6:02:26<11:29:39,  3.31s/batch]Batch 7500/20010 Done, mean position loss: 24.81535717725754\n",
      "Training growing_up:  37%|███▎     | 7466/20010 [6:02:35<10:10:54,  2.92s/batch]Batch 7400/20010 Done, mean position loss: 24.96151470184326\n",
      "Training growing_up:  38%|███▍     | 7507/20010 [6:02:36<11:23:54,  3.28s/batch]Batch 7400/20010 Done, mean position loss: 25.420436940193177\n",
      "Training growing_up:  37%|███▎     | 7391/20010 [6:02:39<10:14:30,  2.92s/batch]Batch 7300/20010 Done, mean position loss: 25.415048243999482\n",
      "Training growing_up:  37%|███▎     | 7405/20010 [6:02:49<11:41:58,  3.34s/batch]Batch 7400/20010 Done, mean position loss: 26.253190648555755\n",
      "Training growing_up:  37%|███▎     | 7318/20010 [6:02:51<11:27:21,  3.25s/batch]Batch 7500/20010 Done, mean position loss: 24.65380757570267\n",
      "Training growing_up:  37%|███▎     | 7334/20010 [6:03:09<11:02:41,  3.14s/batch]Batch 7400/20010 Done, mean position loss: 25.111868155002597\n",
      "Training growing_up:  37%|███▎     | 7312/20010 [6:03:10<10:05:38,  2.86s/batch]Batch 7500/20010 Done, mean position loss: 24.335581521987912\n",
      "Training growing_up:  36%|███▎     | 7300/20010 [6:03:11<11:30:27,  3.26s/batch]Batch 7400/20010 Done, mean position loss: 26.309892570972444\n",
      "Training growing_up:  37%|███▋      | 7494/20010 [6:03:15<9:12:35,  2.65s/batch]Batch 7300/20010 Done, mean position loss: 23.843344161510466\n",
      "Training growing_up:  38%|███▊      | 7513/20010 [6:03:27<9:44:24,  2.81s/batch]Batch 7400/20010 Done, mean position loss: 24.74682413816452\n",
      "Training growing_up:  37%|███▋      | 7485/20010 [6:03:37<9:23:55,  2.70s/batch]Batch 7500/20010 Done, mean position loss: 25.547287180423737\n",
      "Training growing_up:  38%|███▍     | 7529/20010 [6:03:38<11:24:52,  3.29s/batch]Batch 7500/20010 Done, mean position loss: 24.775878818035125\n",
      "Training growing_up:  38%|███▊      | 7545/20010 [6:03:46<9:31:48,  2.75s/batch]Batch 7600/20010 Done, mean position loss: 24.43063772201538\n",
      "Training growing_up:  37%|███▎     | 7350/20010 [6:04:00<11:14:10,  3.20s/batch]Batch 7500/20010 Done, mean position loss: 25.82997807264328\n",
      "Training growing_up:  37%|███▎     | 7453/20010 [6:04:15<11:46:28,  3.38s/batch]Batch 7400/20010 Done, mean position loss: 24.40585064411163\n",
      "Training growing_up:  37%|███▎     | 7442/20010 [6:04:22<11:33:35,  3.31s/batch]Batch 7500/20010 Done, mean position loss: 24.761159179210658\n",
      "Training growing_up:  37%|███▋      | 7349/20010 [6:04:23<9:51:24,  2.80s/batch]Batch 7500/20010 Done, mean position loss: 24.929572682380677\n",
      "Training growing_up:  37%|███▋      | 7488/20010 [6:04:24<9:48:06,  2.82s/batch]Batch 7500/20010 Done, mean position loss: 25.49399360895157\n",
      "Training growing_up:  38%|███▊      | 7546/20010 [6:04:27<9:28:49,  2.74s/batch]Batch 7500/20010 Done, mean position loss: 25.587424137592315\n",
      "Training growing_up:  38%|███▊      | 7519/20010 [6:04:29<9:56:38,  2.87s/batch]Batch 7500/20010 Done, mean position loss: 24.46907385110855\n",
      "Training growing_up:  37%|███▎     | 7497/20010 [6:04:52<11:12:20,  3.22s/batch]Batch 7500/20010 Done, mean position loss: 25.061216628551485\n",
      "Training growing_up:  37%|███▎     | 7350/20010 [6:05:03<10:00:18,  2.85s/batch]Batch 7500/20010 Done, mean position loss: 25.28393359899521\n",
      "Training growing_up:  38%|███▍     | 7541/20010 [6:05:32<10:49:03,  3.12s/batch]Batch 7500/20010 Done, mean position loss: 24.908919048309325\n",
      "Training growing_up:  38%|███▍     | 7515/20010 [6:05:35<11:38:04,  3.35s/batch]Batch 7600/20010 Done, mean position loss: 24.701872322559357\n",
      "Training growing_up:  38%|███▍     | 7570/20010 [6:05:38<10:27:42,  3.03s/batch]Batch 7500/20010 Done, mean position loss: 25.37691722393036\n",
      "Training growing_up:  37%|███▋      | 7466/20010 [6:05:44<8:25:33,  2.42s/batch]Batch 7600/20010 Done, mean position loss: 23.962973709106446\n",
      "Training growing_up:  38%|███▍     | 7556/20010 [6:05:52<10:51:23,  3.14s/batch]Batch 7400/20010 Done, mean position loss: 25.916246218681337\n",
      "Training growing_up:  38%|███▍     | 7507/20010 [6:05:53<12:14:15,  3.52s/batch]Batch 7500/20010 Done, mean position loss: 25.100115430355075\n",
      "Training growing_up:  37%|███▎     | 7451/20010 [6:05:55<10:05:28,  2.89s/batch]Batch 7500/20010 Done, mean position loss: 24.46520874738693\n",
      "Training growing_up:  37%|███▋      | 7457/20010 [6:06:12<9:54:18,  2.84s/batch]Batch 7500/20010 Done, mean position loss: 25.22749181985855\n",
      "Training growing_up:  38%|███▊      | 7562/20010 [6:06:30<7:57:35,  2.30s/batch]Batch 7400/20010 Done, mean position loss: 24.630704021453855\n",
      "Training growing_up:  38%|███▊      | 7576/20010 [6:06:31<9:40:57,  2.80s/batch]Batch 7600/20010 Done, mean position loss: 25.209904193878174\n",
      "Training growing_up:  38%|███▊      | 7603/20010 [6:06:36<9:27:40,  2.75s/batch]Batch 7500/20010 Done, mean position loss: 24.535799071788787\n",
      "Training growing_up:  37%|███▋      | 7371/20010 [6:06:43<9:45:06,  2.78s/batch]Batch 7500/20010 Done, mean position loss: 25.06387192487717\n",
      "Training growing_up:  37%|███▎     | 7375/20010 [6:06:55<10:49:33,  3.08s/batch]Batch 7400/20010 Done, mean position loss: 23.601025676727296\n",
      "Training growing_up:  38%|███▊      | 7507/20010 [6:06:58<9:04:07,  2.61s/batch]Batch 7400/20010 Done, mean position loss: 25.842838480472565\n",
      "Training growing_up:  38%|███▊      | 7556/20010 [6:07:02<8:46:03,  2.53s/batch]Batch 7600/20010 Done, mean position loss: 24.808928568363186\n",
      "Training growing_up:  38%|███▊      | 7672/20010 [6:07:04<7:46:55,  2.27s/batch]Batch 7600/20010 Done, mean position loss: 24.431634912490843\n",
      "Training growing_up:  38%|███▊      | 7514/20010 [6:07:10<8:46:41,  2.53s/batch]Batch 7500/20010 Done, mean position loss: 25.505018887519835\n",
      "Training growing_up:  37%|███▋      | 7382/20010 [6:07:15<9:58:24,  2.84s/batch]Batch 7600/20010 Done, mean position loss: 24.458800508975983\n",
      "Training growing_up:  37%|███▎     | 7489/20010 [6:07:20<10:14:05,  2.94s/batch]Batch 7500/20010 Done, mean position loss: 26.02632551431656\n",
      "Training growing_up:  37%|███▋      | 7400/20010 [6:07:28<9:34:28,  2.73s/batch]Batch 7500/20010 Done, mean position loss: 25.035931398868563\n",
      "Training growing_up:  38%|███▊      | 7608/20010 [6:07:32<8:19:17,  2.42s/batch]Batch 7400/20010 Done, mean position loss: 24.71088313579559\n",
      "Training growing_up:  37%|███▋      | 7403/20010 [6:07:35<8:09:20,  2.33s/batch]Batch 7600/20010 Done, mean position loss: 24.33673593044281\n",
      "Training growing_up:  38%|███▊      | 7646/20010 [6:07:37<8:26:00,  2.46s/batch]Batch 7500/20010 Done, mean position loss: 26.42712805747986\n",
      "Training growing_up:  38%|███▊      | 7620/20010 [6:07:52<8:34:00,  2.49s/batch]Batch 7500/20010 Done, mean position loss: 25.24477622509003\n",
      "Training growing_up:  38%|███▊      | 7529/20010 [6:07:53<9:27:13,  2.73s/batch]Batch 7500/20010 Done, mean position loss: 26.552595410346985\n",
      "Training growing_up:  38%|███▊      | 7693/20010 [6:07:57<9:12:11,  2.69s/batch]Batch 7600/20010 Done, mean position loss: 24.007571628093718\n",
      "Training growing_up:  38%|███▊      | 7657/20010 [6:08:04<9:19:05,  2.72s/batch]Batch 7600/20010 Done, mean position loss: 24.646903307437896\n",
      "Training growing_up:  38%|███▊      | 7508/20010 [6:08:10<9:22:06,  2.70s/batch]Batch 7400/20010 Done, mean position loss: 23.877462208271027\n",
      "Training growing_up:  37%|███▎     | 7452/20010 [6:08:11<11:32:13,  3.31s/batch]Batch 7500/20010 Done, mean position loss: 24.686219470500948\n",
      "Training growing_up:  38%|███▊      | 7586/20010 [6:08:20<9:35:22,  2.78s/batch]Batch 7700/20010 Done, mean position loss: 24.036389169692992\n",
      "Training growing_up:  38%|███▊      | 7517/20010 [6:08:20<9:48:36,  2.83s/batch]Batch 7600/20010 Done, mean position loss: 24.973421614170075\n",
      "Training growing_up:  38%|███▍     | 7566/20010 [6:08:42<10:28:48,  3.03s/batch]Batch 7600/20010 Done, mean position loss: 25.599718987941745\n",
      "Training growing_up:  37%|███▎     | 7446/20010 [6:09:03<10:15:41,  2.94s/batch]Batch 7600/20010 Done, mean position loss: 25.489317445755006\n",
      "Training growing_up:  38%|███▊      | 7574/20010 [6:09:04<9:42:19,  2.81s/batch]Batch 7600/20010 Done, mean position loss: 24.89614093780518\n",
      "Training growing_up:  37%|███▎     | 7434/20010 [6:09:05<11:07:05,  3.18s/batch]Batch 7600/20010 Done, mean position loss: 25.56362904548645\n",
      "Training growing_up:  37%|███▋      | 7420/20010 [6:09:07<9:53:01,  2.83s/batch]Batch 7500/20010 Done, mean position loss: 24.89020781517029\n",
      "Batch 7600/20010 Done, mean position loss: 24.586327471733092\n",
      "Training growing_up:  38%|███▊      | 7648/20010 [6:09:12<9:15:54,  2.70s/batch]Batch 7600/20010 Done, mean position loss: 24.523296055793764\n",
      "Training growing_up:  38%|███▊      | 7511/20010 [6:09:35<9:59:06,  2.88s/batch]Batch 7600/20010 Done, mean position loss: 25.235483145713808\n",
      "Training growing_up:  38%|███▊      | 7589/20010 [6:09:46<9:11:57,  2.67s/batch]Batch 7600/20010 Done, mean position loss: 25.29851899623871\n",
      "Training growing_up:  38%|███▊      | 7668/20010 [6:10:11<9:45:03,  2.84s/batch]Batch 7700/20010 Done, mean position loss: 24.515739107131957\n",
      "Training growing_up:  38%|███▊      | 7577/20010 [6:10:19<9:23:27,  2.72s/batch]Batch 7600/20010 Done, mean position loss: 24.938633041381834\n",
      "Training growing_up:  38%|███▍     | 7596/20010 [6:10:21<10:11:00,  2.95s/batch]Batch 7700/20010 Done, mean position loss: 23.740839540958405\n",
      "Training growing_up:  38%|███▊      | 7626/20010 [6:10:22<9:06:17,  2.65s/batch]Batch 7600/20010 Done, mean position loss: 24.82736210823059\n",
      "Training growing_up:  38%|███▊      | 7686/20010 [6:10:30<9:59:31,  2.92s/batch]Batch 7500/20010 Done, mean position loss: 26.286982083320616\n",
      "Training growing_up:  38%|███▍     | 7563/20010 [6:10:34<10:20:29,  2.99s/batch]Batch 7600/20010 Done, mean position loss: 25.245857419967653\n",
      "Training growing_up:  38%|███▍     | 7533/20010 [6:10:39<10:00:35,  2.89s/batch]Batch 7600/20010 Done, mean position loss: 25.27628888130188\n",
      "Training growing_up:  39%|███▊      | 7717/20010 [6:10:53<8:35:06,  2.51s/batch]Batch 7600/20010 Done, mean position loss: 25.375305485725406\n",
      "Training growing_up:  39%|███▊      | 7719/20010 [6:11:10<8:46:12,  2.57s/batch]Batch 7700/20010 Done, mean position loss: 25.308604149818418\n",
      "Training growing_up:  38%|███▊      | 7646/20010 [6:11:17<9:17:29,  2.71s/batch]Batch 7600/20010 Done, mean position loss: 24.71128194332123\n",
      "Training growing_up:  38%|███▊      | 7624/20010 [6:11:21<9:43:44,  2.83s/batch]Batch 7500/20010 Done, mean position loss: 24.66655530691147\n",
      "Training growing_up:  37%|███▎     | 7468/20010 [6:11:29<10:31:12,  3.02s/batch]Batch 7600/20010 Done, mean position loss: 25.784305238723753\n",
      "Training growing_up:  38%|███▊      | 7526/20010 [6:11:40<9:53:30,  2.85s/batch]Batch 7500/20010 Done, mean position loss: 24.632190759181974\n",
      "Training growing_up:  38%|███▊      | 7640/20010 [6:11:43<9:44:00,  2.83s/batch]Batch 7700/20010 Done, mean position loss: 24.81872648715973\n",
      "Training growing_up:  39%|███▍     | 7773/20010 [6:11:44<10:11:32,  3.00s/batch]Batch 7700/20010 Done, mean position loss: 25.068288691043854\n",
      "Training growing_up:  38%|███▍     | 7511/20010 [6:11:49<10:17:51,  2.97s/batch]Batch 7500/20010 Done, mean position loss: 25.107110898494717\n",
      "Training growing_up:  38%|███▊      | 7669/20010 [6:11:52<8:41:06,  2.53s/batch]Batch 7600/20010 Done, mean position loss: 25.1399489569664\n",
      "Training growing_up:  38%|███▊      | 7586/20010 [6:12:00<8:50:46,  2.56s/batch]Batch 7700/20010 Done, mean position loss: 24.932135529518128\n",
      "Training growing_up:  37%|███▎     | 7498/20010 [6:12:13<10:01:55,  2.89s/batch]Batch 7600/20010 Done, mean position loss: 24.78283327579498\n",
      "Training growing_up:  38%|███▊      | 7671/20010 [6:12:21<9:45:18,  2.85s/batch]Batch 7700/20010 Done, mean position loss: 24.054988124370574\n",
      "Training growing_up:  39%|███▊      | 7715/20010 [6:12:22<8:44:28,  2.56s/batch]Batch 7600/20010 Done, mean position loss: 24.66101864337921\n",
      "Training growing_up:  39%|███▊      | 7715/20010 [6:12:22<9:36:12,  2.81s/batch]Batch 7500/20010 Done, mean position loss: 24.728572363853452\n",
      "Training growing_up:  38%|███▊      | 7634/20010 [6:12:24<9:32:56,  2.78s/batch]Batch 7600/20010 Done, mean position loss: 26.58116853237152\n",
      "Training growing_up:  38%|███▊      | 7640/20010 [6:12:42<9:55:07,  2.89s/batch]Batch 7700/20010 Done, mean position loss: 24.1021258020401\n",
      "Training growing_up:  39%|███▊      | 7723/20010 [6:12:44<8:55:03,  2.61s/batch]Batch 7600/20010 Done, mean position loss: 24.44034628391266\n",
      "Training growing_up:  39%|███▊      | 7735/20010 [6:12:46<9:11:06,  2.69s/batch]Batch 7600/20010 Done, mean position loss: 25.532912170886995\n",
      "Training growing_up:  38%|███▍     | 7602/20010 [6:12:49<10:13:30,  2.97s/batch]Batch 7700/20010 Done, mean position loss: 24.220193815231326\n",
      "Training growing_up:  39%|███▊      | 7721/20010 [6:12:57<9:13:05,  2.70s/batch]Batch 7600/20010 Done, mean position loss: 24.105735073089598\n",
      "Training growing_up:  38%|███▊      | 7668/20010 [6:13:00<9:14:23,  2.70s/batch]Batch 7700/20010 Done, mean position loss: 25.34248634338379\n",
      "Training growing_up:  38%|███▍     | 7618/20010 [6:13:05<10:46:22,  3.13s/batch]Batch 7800/20010 Done, mean position loss: 23.74812701225281\n",
      "Training growing_up:  38%|███▍     | 7529/20010 [6:13:05<10:29:36,  3.03s/batch]Batch 7500/20010 Done, mean position loss: 23.910649106502532\n",
      "Training growing_up:  39%|███▊      | 7730/20010 [6:13:25<9:54:20,  2.90s/batch]Batch 7700/20010 Done, mean position loss: 25.609569201469423\n",
      "Training growing_up:  38%|███▊      | 7550/20010 [6:13:47<9:33:33,  2.76s/batch]Batch 7700/20010 Done, mean position loss: 24.789210052490233\n",
      "Training growing_up:  38%|███▊      | 7624/20010 [6:13:49<9:13:36,  2.68s/batch]Batch 7700/20010 Done, mean position loss: 26.07540500164032\n",
      "Training growing_up:  38%|███▍     | 7698/20010 [6:13:52<10:02:49,  2.94s/batch]Batch 7700/20010 Done, mean position loss: 25.851228148937224\n",
      "Training growing_up:  39%|███▊      | 7726/20010 [6:13:52<9:45:01,  2.86s/batch]Batch 7700/20010 Done, mean position loss: 24.44025118112564\n",
      "Training growing_up:  38%|███▊      | 7674/20010 [6:14:00<9:37:40,  2.81s/batch]Batch 7700/20010 Done, mean position loss: 24.44891498327255\n",
      "Training growing_up:  38%|███▍     | 7548/20010 [6:14:05<10:04:05,  2.91s/batch]Batch 7600/20010 Done, mean position loss: 24.682589552402497\n",
      "Training growing_up:  39%|███▍     | 7721/20010 [6:14:25<10:25:42,  3.05s/batch]Batch 7700/20010 Done, mean position loss: 25.64020755290985\n",
      "Training growing_up:  39%|███▊      | 7735/20010 [6:14:39<9:18:07,  2.73s/batch]Batch 7700/20010 Done, mean position loss: 25.641378519535067\n",
      "Training growing_up:  38%|███▍     | 7677/20010 [6:14:59<10:02:06,  2.93s/batch]Batch 7800/20010 Done, mean position loss: 25.2750808763504\n",
      "Training growing_up:  39%|███▍     | 7746/20010 [6:15:13<10:49:40,  3.18s/batch]Batch 7700/20010 Done, mean position loss: 25.527969517707824\n",
      "Training growing_up:  39%|███▍     | 7729/20010 [6:15:14<11:43:20,  3.44s/batch]Batch 7700/20010 Done, mean position loss: 25.273216516971587\n",
      "Training growing_up:  38%|███▍     | 7663/20010 [6:15:17<10:16:13,  2.99s/batch]Batch 7800/20010 Done, mean position loss: 23.884481718540194\n",
      "Training growing_up:  38%|███▊      | 7671/20010 [6:15:21<9:24:47,  2.75s/batch]Batch 7700/20010 Done, mean position loss: 24.971226480007168\n",
      "Training growing_up:  38%|███▍     | 7699/20010 [6:15:21<10:38:33,  3.11s/batch]Batch 7600/20010 Done, mean position loss: 25.78210346221924\n",
      "Training growing_up:  39%|███▊      | 7730/20010 [6:15:28<9:11:13,  2.69s/batch]Batch 7700/20010 Done, mean position loss: 24.64122176885605\n",
      "Training growing_up:  38%|███▊      | 7633/20010 [6:15:42<9:45:40,  2.84s/batch]Batch 7700/20010 Done, mean position loss: 25.498728303909303\n",
      "Training growing_up:  39%|███▌     | 7822/20010 [6:15:59<10:37:28,  3.14s/batch]Batch 7800/20010 Done, mean position loss: 25.25284698247909\n",
      "Training growing_up:  39%|███▍     | 7771/20010 [6:16:07<10:06:25,  2.97s/batch]Batch 7700/20010 Done, mean position loss: 25.023272054195402\n",
      "Training growing_up:  39%|███▉      | 7783/20010 [6:16:18<9:36:26,  2.83s/batch]Batch 7600/20010 Done, mean position loss: 24.93419692516327\n",
      "Training growing_up:  38%|███▍     | 7595/20010 [6:16:26<10:24:19,  3.02s/batch]Batch 7700/20010 Done, mean position loss: 25.46205857515335\n",
      "Training growing_up:  39%|███▊      | 7752/20010 [6:16:28<9:06:26,  2.67s/batch]Batch 7800/20010 Done, mean position loss: 23.679936742782594\n",
      "Training growing_up:  39%|███▉      | 7754/20010 [6:16:34<9:41:47,  2.85s/batch]Batch 7800/20010 Done, mean position loss: 24.845856170654294\n",
      "Training growing_up:  39%|███▍     | 7729/20010 [6:16:43<10:24:53,  3.05s/batch]Batch 7600/20010 Done, mean position loss: 24.971132917404177\n",
      "Training growing_up:  39%|███▊      | 7732/20010 [6:16:46<9:31:19,  2.79s/batch]Batch 7600/20010 Done, mean position loss: 23.996777534484863\n",
      "Training growing_up:  39%|███▊      | 7710/20010 [6:16:50<9:04:45,  2.66s/batch]Batch 7700/20010 Done, mean position loss: 25.52318571805954\n",
      "Training growing_up:  38%|███▊      | 7700/20010 [6:17:03<9:42:51,  2.84s/batch]Batch 7800/20010 Done, mean position loss: 24.18693169593811\n",
      "Training growing_up:  39%|███▉      | 7839/20010 [6:17:05<8:37:14,  2.55s/batch]Batch 7700/20010 Done, mean position loss: 24.364903478622438\n",
      "Training growing_up:  38%|███▊      | 7688/20010 [6:17:14<9:45:01,  2.85s/batch]Batch 7800/20010 Done, mean position loss: 24.50339083194733\n",
      "Training growing_up:  38%|███▊      | 7691/20010 [6:17:19<9:37:16,  2.81s/batch]Batch 7700/20010 Done, mean position loss: 24.999893753528596\n",
      "Training growing_up:  39%|███▊      | 7745/20010 [6:17:24<9:57:23,  2.92s/batch]Batch 7700/20010 Done, mean position loss: 25.756375460624696\n",
      "Training growing_up:  38%|███▍     | 7616/20010 [6:17:30<11:12:43,  3.26s/batch]Batch 7600/20010 Done, mean position loss: 24.389201257228848\n",
      "Training growing_up:  39%|███▉      | 7795/20010 [6:17:33<9:48:31,  2.89s/batch]Batch 7800/20010 Done, mean position loss: 24.414473528861997\n",
      "Training growing_up:  38%|███▊      | 7647/20010 [6:17:39<9:14:16,  2.69s/batch]Batch 7800/20010 Done, mean position loss: 24.671796901226045\n",
      "Training growing_up:  39%|███▊      | 7719/20010 [6:17:40<9:16:45,  2.72s/batch]Batch 7700/20010 Done, mean position loss: 24.875876767635347\n",
      "Training growing_up:  39%|███▉      | 7839/20010 [6:17:50<8:46:00,  2.59s/batch]Batch 7700/20010 Done, mean position loss: 25.370501708984374\n",
      "Training growing_up:  39%|███▉      | 7814/20010 [6:17:51<9:59:07,  2.95s/batch]Batch 7800/20010 Done, mean position loss: 25.264083535671233\n",
      "Training growing_up:  39%|███▉      | 7756/20010 [6:17:55<9:34:06,  2.81s/batch]Batch 7700/20010 Done, mean position loss: 24.709940831661225\n",
      "Training growing_up:  39%|███▌     | 7811/20010 [6:18:03<10:37:13,  3.13s/batch]Batch 7900/20010 Done, mean position loss: 24.029408609867097\n",
      "Training growing_up:  38%|███▍     | 7680/20010 [6:18:03<10:20:09,  3.02s/batch]Batch 7600/20010 Done, mean position loss: 24.438108232021335\n",
      "Training growing_up:  39%|███▌     | 7871/20010 [6:18:22<10:09:20,  3.01s/batch]Batch 7800/20010 Done, mean position loss: 25.51253982067108\n",
      "Training growing_up:  39%|███▌     | 7790/20010 [6:18:39<10:35:55,  3.12s/batch]Batch 7800/20010 Done, mean position loss: 25.7308470249176\n",
      "Training growing_up:  38%|███▍     | 7625/20010 [6:18:42<10:06:25,  2.94s/batch]Batch 7800/20010 Done, mean position loss: 24.524744029045102\n",
      "Training growing_up:  38%|███▍     | 7616/20010 [6:18:47<11:02:23,  3.21s/batch]Batch 7800/20010 Done, mean position loss: 25.24019922733307\n",
      "Training growing_up:  39%|███▍     | 7772/20010 [6:18:49<11:07:03,  3.27s/batch]Batch 7800/20010 Done, mean position loss: 25.07061104774475\n",
      "Training growing_up:  38%|███▍     | 7627/20010 [6:18:49<11:32:02,  3.35s/batch]Batch 7800/20010 Done, mean position loss: 24.233769297599792\n",
      "Training growing_up:  39%|███▍     | 7777/20010 [6:19:04<10:45:08,  3.16s/batch]Batch 7700/20010 Done, mean position loss: 24.594899291992185\n",
      "Training growing_up:  39%|███▍     | 7771/20010 [6:19:10<10:18:28,  3.03s/batch]Batch 7800/20010 Done, mean position loss: 24.829051144123078\n",
      "Training growing_up:  39%|███▉      | 7893/20010 [6:19:33<9:09:49,  2.72s/batch]Batch 7800/20010 Done, mean position loss: 24.840963749885557\n",
      "Training growing_up:  39%|███▍     | 7762/20010 [6:19:47<10:20:50,  3.04s/batch]Batch 7900/20010 Done, mean position loss: 24.50342495203018\n",
      "Training growing_up:  39%|███▉      | 7825/20010 [6:19:57<9:35:07,  2.83s/batch]Batch 7900/20010 Done, mean position loss: 24.03931936740875\n",
      "Training growing_up:  39%|███▍     | 7756/20010 [6:20:05<11:05:18,  3.26s/batch]Batch 7800/20010 Done, mean position loss: 24.33071729660034\n",
      "Training growing_up:  39%|███▌     | 7797/20010 [6:20:06<10:15:52,  3.03s/batch]Batch 7800/20010 Done, mean position loss: 25.358978741168976\n",
      "Training growing_up:  39%|███▉      | 7759/20010 [6:20:14<9:49:38,  2.89s/batch]Batch 7800/20010 Done, mean position loss: 25.69628466844559\n",
      "Training growing_up:  39%|███▍     | 7754/20010 [6:20:17<10:51:16,  3.19s/batch]Batch 7800/20010 Done, mean position loss: 23.714798078536987\n",
      "Batch 7700/20010 Done, mean position loss: 25.626912450790407\n",
      "Training growing_up:  39%|███▍     | 7761/20010 [6:20:37<10:25:26,  3.06s/batch]Batch 7800/20010 Done, mean position loss: 25.203949434757234\n",
      "Training growing_up:  39%|███▍     | 7711/20010 [6:20:44<10:14:13,  3.00s/batch]Batch 7900/20010 Done, mean position loss: 25.720476031303406\n",
      "Training growing_up:  39%|███▉      | 7854/20010 [6:21:00<9:39:40,  2.86s/batch]Batch 7800/20010 Done, mean position loss: 24.95494638442993\n",
      "Training growing_up:  38%|███▊      | 7690/20010 [6:21:15<8:21:15,  2.44s/batch]Batch 7900/20010 Done, mean position loss: 25.662912359237673\n",
      "Training growing_up:  39%|███▉      | 7850/20010 [6:21:17<8:31:22,  2.52s/batch]Batch 7700/20010 Done, mean position loss: 25.325841629505156\n",
      "Training growing_up:  39%|███▉      | 7851/20010 [6:21:20<9:30:48,  2.82s/batch]Batch 7800/20010 Done, mean position loss: 25.349973621368406\n",
      "Training growing_up:  40%|███▉      | 7973/20010 [6:21:24<9:10:53,  2.75s/batch]Batch 7900/20010 Done, mean position loss: 24.98225571155548\n",
      "Training growing_up:  39%|███▉      | 7777/20010 [6:21:35<8:46:22,  2.58s/batch]Batch 7800/20010 Done, mean position loss: 24.83064957141876\n",
      "Training growing_up:  39%|███▌     | 7883/20010 [6:21:37<10:23:04,  3.08s/batch]Batch 7700/20010 Done, mean position loss: 24.436190476417543\n",
      "Training growing_up:  39%|███▌     | 7833/20010 [6:21:44<10:05:08,  2.98s/batch]Batch 7700/20010 Done, mean position loss: 25.61603045463562\n",
      "Training growing_up:  39%|███▉      | 7791/20010 [6:21:48<8:47:01,  2.59s/batch]Batch 7800/20010 Done, mean position loss: 24.424427843093873\n",
      "Training growing_up:  39%|███▍     | 7714/20010 [6:21:50<10:02:07,  2.94s/batch]Batch 7900/20010 Done, mean position loss: 24.20377238035202\n",
      "Training growing_up:  39%|███▊      | 7739/20010 [6:22:00<8:08:58,  2.39s/batch]Batch 7900/20010 Done, mean position loss: 24.792811353206634\n",
      "Training growing_up:  39%|███▉      | 7864/20010 [6:22:05<8:05:51,  2.40s/batch]Batch 7800/20010 Done, mean position loss: 25.27708704948425\n",
      "Training growing_up:  39%|███▉      | 7792/20010 [6:22:14<7:22:42,  2.17s/batch]Batch 7800/20010 Done, mean position loss: 25.061938910484315\n",
      "Training growing_up:  40%|███▌     | 7935/20010 [6:22:20<10:31:31,  3.14s/batch]Batch 7900/20010 Done, mean position loss: 24.26206040143967\n",
      "Training growing_up:  39%|███▉      | 7815/20010 [6:22:27<8:47:53,  2.60s/batch]Batch 7700/20010 Done, mean position loss: 24.264431252479554\n",
      "Training growing_up:  39%|███▊      | 7718/20010 [6:22:27<8:49:23,  2.58s/batch]Batch 7800/20010 Done, mean position loss: 24.71469563484192\n",
      "Training growing_up:  39%|███▊      | 7750/20010 [6:22:28<8:59:33,  2.64s/batch]Batch 7900/20010 Done, mean position loss: 24.147804362773893\n",
      "Training growing_up:  40%|███▉      | 8000/20010 [6:22:36<8:05:17,  2.42s/batch]Batch 7800/20010 Done, mean position loss: 23.84318862438202\n",
      "Training growing_up:  39%|███▉      | 7854/20010 [6:22:37<8:09:36,  2.42s/batch]Batch 7900/20010 Done, mean position loss: 25.243089339733125\n",
      "Training growing_up:  40%|███▉      | 7928/20010 [6:22:38<8:54:34,  2.65s/batch]Batch 8000/20010 Done, mean position loss: 23.69050095319748\n",
      "Training growing_up:  39%|███▉      | 7883/20010 [6:22:38<7:39:56,  2.28s/batch]Batch 7800/20010 Done, mean position loss: 25.221405026912688\n",
      "Training growing_up:  39%|███▉      | 7887/20010 [6:22:54<7:41:40,  2.28s/batch]Batch 7700/20010 Done, mean position loss: 24.292463800907136\n",
      "Training growing_up:  40%|███▉      | 7915/20010 [6:23:04<8:08:55,  2.43s/batch]Batch 7900/20010 Done, mean position loss: 26.239630320072173\n",
      "Training growing_up:  39%|███▊      | 7736/20010 [6:23:15<9:20:01,  2.74s/batch]Batch 7900/20010 Done, mean position loss: 24.73738145828247\n",
      "Training growing_up:  40%|███▉      | 7905/20010 [6:23:26<9:24:13,  2.80s/batch]Batch 7900/20010 Done, mean position loss: 25.387221443653104\n",
      "Training growing_up:  39%|███▉      | 7856/20010 [6:23:27<9:53:57,  2.93s/batch]Batch 7900/20010 Done, mean position loss: 25.24829918622971\n",
      "Training growing_up:  40%|███▉      | 7921/20010 [6:23:30<9:53:45,  2.95s/batch]Batch 7900/20010 Done, mean position loss: 24.073664700984953\n",
      "Training growing_up:  39%|███▌     | 7872/20010 [6:23:33<10:02:11,  2.98s/batch]Batch 7900/20010 Done, mean position loss: 24.606476702690124\n",
      "Training growing_up:  40%|███▉      | 7905/20010 [6:23:41<9:20:50,  2.78s/batch]Batch 7800/20010 Done, mean position loss: 24.16583990573883\n",
      "Training growing_up:  40%|███▉      | 7932/20010 [6:23:48<9:50:16,  2.93s/batch]Batch 7900/20010 Done, mean position loss: 24.839482464790343\n",
      "Training growing_up:  40%|███▉      | 7925/20010 [6:24:11<7:59:17,  2.38s/batch]Batch 7900/20010 Done, mean position loss: 24.85227046251297\n",
      "Training growing_up:  40%|███▉      | 7904/20010 [6:24:20<9:12:37,  2.74s/batch]Batch 8000/20010 Done, mean position loss: 24.690322542190554\n",
      "Training growing_up:  40%|███▉      | 7924/20010 [6:24:33<9:16:23,  2.76s/batch]Batch 8000/20010 Done, mean position loss: 23.766646528244017\n",
      "Training growing_up:  39%|███▉      | 7896/20010 [6:24:42<9:35:03,  2.85s/batch]Batch 7900/20010 Done, mean position loss: 24.607536861896513\n",
      "Training growing_up:  40%|███▉      | 7929/20010 [6:24:46<9:07:57,  2.72s/batch]Batch 7900/20010 Done, mean position loss: 24.242420341968536\n",
      "Training growing_up:  40%|███▉      | 7930/20010 [6:24:49<8:43:37,  2.60s/batch]Batch 7900/20010 Done, mean position loss: 25.170139043331147\n",
      "Training growing_up:  39%|███▉      | 7850/20010 [6:24:51<9:33:18,  2.83s/batch]Batch 7800/20010 Done, mean position loss: 25.380394515991213\n",
      "Training growing_up:  39%|███▉      | 7871/20010 [6:24:58<9:37:57,  2.86s/batch]Batch 7900/20010 Done, mean position loss: 25.371162927150728\n",
      "Training growing_up:  40%|███▉      | 7911/20010 [6:25:09<9:50:00,  2.93s/batch]Batch 7900/20010 Done, mean position loss: 25.75050763368607\n",
      "Training growing_up:  40%|███▉      | 7905/20010 [6:25:21<9:51:57,  2.93s/batch]Batch 8000/20010 Done, mean position loss: 25.317558999061582\n",
      "Training growing_up:  40%|███▉      | 7992/20010 [6:25:32<8:48:47,  2.64s/batch]Batch 7900/20010 Done, mean position loss: 24.98636983156204\n",
      "Training growing_up:  40%|███▌     | 7971/20010 [6:25:50<10:26:19,  3.12s/batch]Batch 7900/20010 Done, mean position loss: 25.24395361661911\n",
      "Training growing_up:  40%|███▉      | 7958/20010 [6:25:57<9:46:36,  2.92s/batch]Batch 7800/20010 Done, mean position loss: 24.19934567451477\n",
      "Training growing_up:  40%|███▉      | 7957/20010 [6:25:59<8:53:37,  2.66s/batch]Batch 8000/20010 Done, mean position loss: 25.205818819999696\n",
      "Training growing_up:  40%|███▉      | 7956/20010 [6:25:59<9:30:03,  2.84s/batch]Batch 8000/20010 Done, mean position loss: 24.180484206676482\n",
      "Training growing_up:  40%|███▉      | 7928/20010 [6:26:14<9:22:37,  2.79s/batch]Batch 7900/20010 Done, mean position loss: 25.1453218126297\n",
      "Training growing_up:  39%|███▉      | 7887/20010 [6:26:17<9:18:27,  2.76s/batch]Batch 7800/20010 Done, mean position loss: 23.981853504180908\n",
      "Training growing_up:  39%|███▉      | 7802/20010 [6:26:20<9:28:46,  2.80s/batch]Batch 8000/20010 Done, mean position loss: 24.252312903404235\n",
      "Training growing_up:  40%|███▉      | 7995/20010 [6:26:22<8:39:27,  2.59s/batch]Batch 7800/20010 Done, mean position loss: 25.335697000026702\n",
      "Training growing_up:  40%|███▌     | 7936/20010 [6:26:23<10:16:41,  3.06s/batch]Batch 7900/20010 Done, mean position loss: 24.237057268619537\n",
      "Training growing_up:  40%|███▉      | 7971/20010 [6:26:37<9:52:03,  2.95s/batch]Batch 8000/20010 Done, mean position loss: 24.07944170713425\n",
      "Training growing_up:  39%|███▉      | 7896/20010 [6:26:41<9:15:45,  2.75s/batch]Batch 7900/20010 Done, mean position loss: 25.892294268608094\n",
      "Training growing_up:  40%|████      | 8098/20010 [6:26:57<9:18:46,  2.81s/batch]Batch 7900/20010 Done, mean position loss: 24.856552212238313\n",
      "Training growing_up:  40%|███▉      | 7933/20010 [6:27:01<9:35:36,  2.86s/batch]Batch 8000/20010 Done, mean position loss: 24.146835765838624\n",
      "Training growing_up:  39%|███▉      | 7817/20010 [6:27:02<9:21:05,  2.76s/batch]Batch 7900/20010 Done, mean position loss: 24.93929654121399\n",
      "Training growing_up:  40%|███▉      | 7952/20010 [6:27:03<8:41:56,  2.60s/batch]Batch 8000/20010 Done, mean position loss: 24.28332389831543\n",
      "Training growing_up:  39%|███▉      | 7871/20010 [6:27:05<9:24:24,  2.79s/batch]Batch 8100/20010 Done, mean position loss: 23.863505902290342\n",
      "Training growing_up:  40%|███▌     | 7922/20010 [6:27:13<10:13:36,  3.05s/batch]Batch 7800/20010 Done, mean position loss: 25.223908104896545\n",
      "Training growing_up:  40%|███▉      | 7979/20010 [6:27:14<9:55:17,  2.97s/batch]Batch 8000/20010 Done, mean position loss: 25.119260408878326\n",
      "Training growing_up:  40%|███▉      | 7920/20010 [6:27:17<9:55:30,  2.96s/batch]Batch 7900/20010 Done, mean position loss: 25.36467870950699\n",
      "Training growing_up:  40%|███▌     | 7975/20010 [6:27:20<10:19:15,  3.09s/batch]Batch 7900/20010 Done, mean position loss: 23.995805132389066\n",
      "Training growing_up:  40%|███▌     | 7948/20010 [6:27:45<10:47:44,  3.22s/batch]Batch 8000/20010 Done, mean position loss: 25.249186618328096\n",
      "Training growing_up:  40%|███▉      | 7935/20010 [6:27:50<9:25:13,  2.81s/batch]Batch 7800/20010 Done, mean position loss: 23.829227361679077\n",
      "Training growing_up:  40%|███▉      | 7928/20010 [6:28:00<9:13:54,  2.75s/batch]Batch 8000/20010 Done, mean position loss: 25.276631312370302\n",
      "Training growing_up:  40%|███▉      | 8003/20010 [6:28:05<8:22:42,  2.51s/batch]Batch 8000/20010 Done, mean position loss: 24.196740958690643\n",
      "Training growing_up:  40%|████      | 8038/20010 [6:28:10<9:37:55,  2.90s/batch]Batch 8000/20010 Done, mean position loss: 25.112321920394898\n",
      "Training growing_up:  40%|████      | 8004/20010 [6:28:18<9:34:32,  2.87s/batch]Batch 8000/20010 Done, mean position loss: 24.838190984725955\n",
      "Training growing_up:  40%|███▌     | 8029/20010 [6:28:23<10:54:36,  3.28s/batch]Batch 8000/20010 Done, mean position loss: 24.517181959152225\n",
      "Training growing_up:  40%|███▉      | 7979/20010 [6:28:35<9:57:26,  2.98s/batch]Batch 7900/20010 Done, mean position loss: 24.499564723968504\n",
      "Training growing_up:  40%|███▌     | 7950/20010 [6:28:36<10:22:07,  3.10s/batch]Batch 8000/20010 Done, mean position loss: 24.856350123882294\n",
      "Training growing_up:  40%|███▌     | 7987/20010 [6:28:55<10:30:09,  3.14s/batch]Batch 8000/20010 Done, mean position loss: 25.287252182960508\n",
      "Training growing_up:  40%|████      | 8017/20010 [6:28:57<9:07:45,  2.74s/batch]Batch 8100/20010 Done, mean position loss: 25.04692973136902\n",
      "Training growing_up:  40%|███▌     | 8034/20010 [6:29:27<10:48:40,  3.25s/batch]Batch 8100/20010 Done, mean position loss: 25.09493511199951\n",
      "Training growing_up:  40%|████      | 8026/20010 [6:29:34<9:18:03,  2.79s/batch]Batch 8000/20010 Done, mean position loss: 24.88579288482666\n",
      "Training growing_up:  40%|███▌     | 8021/20010 [6:29:41<11:26:05,  3.43s/batch]Batch 8000/20010 Done, mean position loss: 24.218368487358095\n",
      "Batch 8000/20010 Done, mean position loss: 25.296559562683107\n",
      "Training growing_up:  40%|███▋     | 8090/20010 [6:29:46<11:34:18,  3.49s/batch]Batch 7900/20010 Done, mean position loss: 26.149187295436857\n",
      "Training growing_up:  40%|███▌     | 7976/20010 [6:29:56<11:50:08,  3.54s/batch]Batch 8000/20010 Done, mean position loss: 24.71687504529953\n",
      "Training growing_up:  40%|███▉      | 7957/20010 [6:30:07<9:54:13,  2.96s/batch]Batch 8000/20010 Done, mean position loss: 25.147271139621736\n",
      "Training growing_up:  40%|████      | 8005/20010 [6:30:18<9:16:52,  2.78s/batch]Batch 8100/20010 Done, mean position loss: 25.845256276130677\n",
      "Training growing_up:  40%|███▋     | 8070/20010 [6:30:26<10:25:57,  3.15s/batch]Batch 8000/20010 Done, mean position loss: 24.599800388813016\n",
      "Training growing_up:  40%|███▋     | 8076/20010 [6:30:45<10:50:52,  3.27s/batch]Batch 8000/20010 Done, mean position loss: 25.291978077888487\n",
      "Training growing_up:  40%|███▌     | 7989/20010 [6:30:46<10:31:43,  3.15s/batch]Batch 8100/20010 Done, mean position loss: 24.227326495647432\n",
      "Training growing_up:  41%|████      | 8180/20010 [6:30:55<9:17:12,  2.83s/batch]Batch 8100/20010 Done, mean position loss: 24.084111409187315\n",
      "Training growing_up:  40%|████      | 8061/20010 [6:31:03<9:30:45,  2.87s/batch]Batch 7900/20010 Done, mean position loss: 24.110957164764404\n",
      "Training growing_up:  41%|███▋     | 8184/20010 [6:31:10<12:12:24,  3.72s/batch]Batch 8000/20010 Done, mean position loss: 25.063084654808044\n",
      "Training growing_up:  40%|███▋     | 8099/20010 [6:31:20<10:42:21,  3.24s/batch]Batch 8000/20010 Done, mean position loss: 24.474411442279816\n",
      "Training growing_up:  40%|███▌     | 7995/20010 [6:31:22<10:01:01,  3.00s/batch]Batch 7900/20010 Done, mean position loss: 24.743179266452792\n",
      "Training growing_up:  40%|███▋     | 8089/20010 [6:31:26<10:09:04,  3.07s/batch]Batch 8100/20010 Done, mean position loss: 24.30823408126831\n",
      "Training growing_up:  40%|████      | 8017/20010 [6:31:31<9:56:39,  2.99s/batch]Batch 7900/20010 Done, mean position loss: 23.84699426174164\n",
      "Training growing_up:  40%|████      | 8039/20010 [6:31:38<9:42:53,  2.92s/batch]Batch 8100/20010 Done, mean position loss: 23.924246513843535\n",
      "Training growing_up:  40%|███▉      | 7904/20010 [6:31:40<9:53:52,  2.94s/batch]Batch 8000/20010 Done, mean position loss: 25.463195044994354\n",
      "Training growing_up:  40%|███▋     | 8074/20010 [6:32:00<10:04:21,  3.04s/batch]Batch 8200/20010 Done, mean position loss: 23.631509144306186\n",
      "Training growing_up:  40%|███▋     | 8079/20010 [6:32:00<10:19:31,  3.12s/batch]Batch 8000/20010 Done, mean position loss: 24.687210383415223\n",
      "Training growing_up:  40%|███▉      | 7997/20010 [6:32:02<9:45:22,  2.92s/batch]Batch 8000/20010 Done, mean position loss: 25.51485371828079\n",
      "Training growing_up:  40%|███▌     | 7992/20010 [6:32:03<10:08:08,  3.04s/batch]Batch 8100/20010 Done, mean position loss: 24.151732342243193\n",
      "Training growing_up:  40%|███▋     | 8098/20010 [6:32:04<10:13:10,  3.09s/batch]Batch 8100/20010 Done, mean position loss: 23.515250611305234\n",
      "Training growing_up:  40%|███▌     | 8031/20010 [6:32:13<10:41:53,  3.22s/batch]Batch 8000/20010 Done, mean position loss: 26.01874016523361\n",
      "Training growing_up:  41%|███▋     | 8157/20010 [6:32:14<10:11:10,  3.09s/batch]Batch 8100/20010 Done, mean position loss: 25.20245245695114\n",
      "Training growing_up:  40%|███▋     | 8085/20010 [6:32:20<10:27:16,  3.16s/batch]Batch 7900/20010 Done, mean position loss: 24.618613500595092\n",
      "Training growing_up:  40%|███▌     | 8024/20010 [6:32:31<11:03:46,  3.32s/batch]Batch 8000/20010 Done, mean position loss: 24.325815048217773\n",
      "Training growing_up:  40%|████      | 8089/20010 [6:32:47<9:58:04,  3.01s/batch]Batch 8100/20010 Done, mean position loss: 25.08879357337952\n",
      "Training growing_up:  40%|███▋     | 8081/20010 [6:32:54<10:17:51,  3.11s/batch]Batch 7900/20010 Done, mean position loss: 24.115119149684904\n",
      "Training growing_up:  40%|████      | 8031/20010 [6:33:07<9:02:51,  2.72s/batch]Batch 8100/20010 Done, mean position loss: 24.055310912132263\n",
      "Training growing_up:  40%|███▋     | 8061/20010 [6:33:09<10:57:51,  3.30s/batch]Batch 8100/20010 Done, mean position loss: 25.50219480752945\n",
      "Training growing_up:  40%|████      | 8033/20010 [6:33:12<8:44:19,  2.63s/batch]Batch 8100/20010 Done, mean position loss: 26.389001579284667\n",
      "Training growing_up:  40%|███▋     | 8077/20010 [6:33:20<10:15:30,  3.09s/batch]Batch 8100/20010 Done, mean position loss: 24.518073258399966\n",
      "Training growing_up:  40%|████      | 8073/20010 [6:33:23<9:11:45,  2.77s/batch]Batch 8100/20010 Done, mean position loss: 24.75573204755783\n",
      "Training growing_up:  41%|███▋     | 8128/20010 [6:33:34<10:34:13,  3.20s/batch]Batch 8000/20010 Done, mean position loss: 24.814884757995607\n",
      "Training growing_up:  40%|███▋     | 8079/20010 [6:33:37<10:41:04,  3.22s/batch]Batch 8100/20010 Done, mean position loss: 24.827296051979065\n",
      "Training growing_up:  40%|████      | 8069/20010 [6:33:48<9:47:12,  2.95s/batch]Batch 8200/20010 Done, mean position loss: 24.232199492454527\n",
      "Training growing_up:  41%|████      | 8152/20010 [6:33:54<9:11:36,  2.79s/batch]Batch 8100/20010 Done, mean position loss: 25.640552718639377\n",
      "Training growing_up:  41%|████      | 8119/20010 [6:34:16<9:58:12,  3.02s/batch]Batch 8200/20010 Done, mean position loss: 24.084684159755703\n",
      "Training growing_up:  41%|████      | 8165/20010 [6:34:32<8:38:28,  2.63s/batch]Batch 8100/20010 Done, mean position loss: 24.985967128276826\n",
      "Training growing_up:  41%|███▋     | 8180/20010 [6:34:39<10:07:56,  3.08s/batch]Batch 8100/20010 Done, mean position loss: 24.792641062736514\n",
      "Training growing_up:  40%|████      | 8047/20010 [6:34:41<9:18:01,  2.80s/batch]Batch 8000/20010 Done, mean position loss: 26.197621886730193\n",
      "Training growing_up:  40%|███▌     | 7968/20010 [6:34:46<10:30:51,  3.14s/batch]Batch 8100/20010 Done, mean position loss: 25.324159386157987\n",
      "Training growing_up:  41%|███▋     | 8118/20010 [6:34:47<10:02:38,  3.04s/batch]Batch 8100/20010 Done, mean position loss: 24.7243474316597\n",
      "Training growing_up:  41%|████      | 8140/20010 [6:35:06<9:40:23,  2.93s/batch]Batch 8100/20010 Done, mean position loss: 25.833721635341643\n",
      "Training growing_up:  41%|███▋     | 8109/20010 [6:35:10<10:22:31,  3.14s/batch]Batch 8200/20010 Done, mean position loss: 25.434373362064363\n",
      "Training growing_up:  41%|███▋     | 8143/20010 [6:35:18<10:33:59,  3.21s/batch]Batch 8100/20010 Done, mean position loss: 25.145873339176177\n",
      "Training growing_up:  40%|███▌     | 8041/20010 [6:35:37<10:51:53,  3.27s/batch]Batch 8100/20010 Done, mean position loss: 25.512184200286868\n",
      "Training growing_up:  41%|████      | 8151/20010 [6:35:40<9:13:14,  2.80s/batch]Batch 8200/20010 Done, mean position loss: 24.24733773469925\n",
      "Training growing_up:  41%|███▋     | 8177/20010 [6:35:45<10:11:06,  3.10s/batch]Batch 8200/20010 Done, mean position loss: 23.905565309524533\n",
      "Training growing_up:  41%|████▏     | 8282/20010 [6:35:59<9:23:33,  2.88s/batch]Batch 8000/20010 Done, mean position loss: 24.618463187217714\n",
      "Training growing_up:  40%|███▌     | 7994/20010 [6:36:03<10:13:32,  3.06s/batch]Batch 8100/20010 Done, mean position loss: 24.765488567352296\n",
      "Training growing_up:  40%|███▋     | 8083/20010 [6:36:16<10:20:27,  3.12s/batch]Batch 8200/20010 Done, mean position loss: 24.601498341560365\n",
      "Training growing_up:  41%|████      | 8167/20010 [6:36:20<8:51:34,  2.69s/batch]Batch 8100/20010 Done, mean position loss: 24.567948200702666\n",
      "Training growing_up:  41%|████      | 8118/20010 [6:36:23<8:26:10,  2.55s/batch]Batch 8000/20010 Done, mean position loss: 24.840621018409728\n",
      "Training growing_up:  41%|████      | 8152/20010 [6:36:24<8:17:26,  2.52s/batch]Batch 8200/20010 Done, mean position loss: 24.144914524555205\n",
      "Training growing_up:  40%|███▉      | 7999/20010 [6:36:26<9:08:31,  2.74s/batch]Batch 8100/20010 Done, mean position loss: 25.426528887748717\n",
      "Training growing_up:  40%|████      | 8086/20010 [6:36:33<8:29:48,  2.57s/batch]Batch 8000/20010 Done, mean position loss: 23.567559161186217\n",
      "Training growing_up:  41%|████      | 8145/20010 [6:36:49<8:13:43,  2.50s/batch]Batch 8300/20010 Done, mean position loss: 23.72095966100693\n",
      "Training growing_up:  41%|████      | 8200/20010 [6:36:50<9:33:56,  2.92s/batch]Batch 8100/20010 Done, mean position loss: 25.020168635845184\n",
      "Training growing_up:  40%|████      | 8047/20010 [6:36:51<8:29:20,  2.55s/batch]Batch 8100/20010 Done, mean position loss: 25.16807937860489\n",
      "Training growing_up:  41%|████      | 8169/20010 [6:36:52<8:22:14,  2.54s/batch]Batch 8200/20010 Done, mean position loss: 24.603575985431668\n",
      "Training growing_up:  41%|████▏     | 8257/20010 [6:36:54<7:57:02,  2.44s/batch]Batch 8200/20010 Done, mean position loss: 23.57252289533615\n",
      "Training growing_up:  41%|████▏     | 8258/20010 [6:36:57<7:47:40,  2.39s/batch]Batch 8200/20010 Done, mean position loss: 24.78329794883728\n",
      "Training growing_up:  41%|████      | 8154/20010 [6:37:03<9:21:00,  2.84s/batch]Batch 8000/20010 Done, mean position loss: 24.834671227931977\n",
      "Training growing_up:  40%|████      | 8016/20010 [6:37:04<8:17:54,  2.49s/batch]Batch 8100/20010 Done, mean position loss: 25.172066328525545\n",
      "Training growing_up:  40%|███▉      | 7990/20010 [6:37:12<8:35:44,  2.57s/batch]Batch 8100/20010 Done, mean position loss: 24.588527421951294\n",
      "Training growing_up:  40%|████      | 8028/20010 [6:37:36<8:55:33,  2.68s/batch]Batch 8200/20010 Done, mean position loss: 25.000191485881807\n",
      "Training growing_up:  41%|████      | 8156/20010 [6:37:42<9:49:47,  2.99s/batch]Batch 8000/20010 Done, mean position loss: 24.144196321964262\n",
      "Training growing_up:  41%|████      | 8248/20010 [6:37:49<8:37:15,  2.64s/batch]Batch 8200/20010 Done, mean position loss: 25.380287823677065\n",
      "Training growing_up:  40%|████      | 8069/20010 [6:37:53<9:06:32,  2.75s/batch]Batch 8200/20010 Done, mean position loss: 24.363718872070315\n",
      "Training growing_up:  41%|████      | 8154/20010 [6:37:59<9:00:48,  2.74s/batch]Batch 8200/20010 Done, mean position loss: 25.309270277023316\n",
      "Training growing_up:  41%|████      | 8253/20010 [6:38:03<8:08:21,  2.49s/batch]Batch 8200/20010 Done, mean position loss: 23.874233345985413\n",
      "Training growing_up:  41%|████      | 8163/20010 [6:38:06<8:21:46,  2.54s/batch]Batch 8200/20010 Done, mean position loss: 24.426428184509277\n",
      "Training growing_up:  41%|████      | 8167/20010 [6:38:17<8:49:49,  2.68s/batch]Batch 8200/20010 Done, mean position loss: 24.645927183628082\n",
      "Training growing_up:  41%|████      | 8210/20010 [6:38:20<8:37:39,  2.63s/batch]Batch 8100/20010 Done, mean position loss: 24.1289324593544\n",
      "Training growing_up:  41%|████      | 8138/20010 [6:38:28<8:29:15,  2.57s/batch]Batch 8300/20010 Done, mean position loss: 24.64227722406387\n",
      "Training growing_up:  41%|███▋     | 8136/20010 [6:38:39<10:27:25,  3.17s/batch]Batch 8200/20010 Done, mean position loss: 26.357179477214814\n",
      "Training growing_up:  41%|████      | 8138/20010 [6:38:49<9:43:34,  2.95s/batch]Batch 8300/20010 Done, mean position loss: 24.118997926712037\n",
      "Training growing_up:  41%|████      | 8181/20010 [6:39:13<8:27:41,  2.58s/batch]Batch 8200/20010 Done, mean position loss: 25.213253498077393\n",
      "Training growing_up:  41%|████      | 8240/20010 [6:39:22<9:30:56,  2.91s/batch]Batch 8200/20010 Done, mean position loss: 24.868972425460818\n",
      "Training growing_up:  41%|████      | 8168/20010 [6:39:25<9:12:04,  2.80s/batch]Batch 8200/20010 Done, mean position loss: 25.173014988899233\n",
      "Batch 8200/20010 Done, mean position loss: 24.137268724441526\n",
      "Training growing_up:  41%|████      | 8186/20010 [6:39:27<9:25:13,  2.87s/batch]Batch 8100/20010 Done, mean position loss: 26.144279458522796\n",
      "Training growing_up:  41%|████      | 8166/20010 [6:39:46<9:57:01,  3.02s/batch]Batch 8200/20010 Done, mean position loss: 25.150779411792755\n",
      "Training growing_up:  41%|███▋     | 8109/20010 [6:39:50<10:17:16,  3.11s/batch]Batch 8300/20010 Done, mean position loss: 25.48888999938965\n",
      "Training growing_up:  41%|████      | 8185/20010 [6:39:56<9:38:24,  2.93s/batch]Batch 8200/20010 Done, mean position loss: 24.945818593502043\n",
      "Training growing_up:  42%|████▏     | 8374/20010 [6:40:07<7:30:49,  2.32s/batch]Batch 8200/20010 Done, mean position loss: 24.950950922966\n",
      "Training growing_up:  40%|████      | 8081/20010 [6:40:15<9:51:36,  2.98s/batch]Batch 8300/20010 Done, mean position loss: 23.912495722770693\n",
      "Training growing_up:  41%|████      | 8220/20010 [6:40:15<8:15:22,  2.52s/batch]Batch 8300/20010 Done, mean position loss: 24.165425138473513\n",
      "Training growing_up:  42%|████▏     | 8309/20010 [6:40:39<9:52:54,  3.04s/batch]Batch 8200/20010 Done, mean position loss: 24.81703156232834\n",
      "Training growing_up:  41%|████      | 8194/20010 [6:40:43<9:14:11,  2.81s/batch]Batch 8100/20010 Done, mean position loss: 24.11060175895691\n",
      "Training growing_up:  42%|████▏     | 8315/20010 [6:40:54<8:35:30,  2.64s/batch]Batch 8300/20010 Done, mean position loss: 24.03977933883667\n",
      "Training growing_up:  41%|████▏     | 8291/20010 [6:40:58<8:47:24,  2.70s/batch]Batch 8200/20010 Done, mean position loss: 24.48531742334366\n",
      "Training growing_up:  41%|████      | 8157/20010 [6:41:04<8:12:57,  2.50s/batch]Batch 8200/20010 Done, mean position loss: 25.366378812789918\n",
      "Training growing_up:  41%|████      | 8211/20010 [6:41:06<9:29:27,  2.90s/batch]Batch 8300/20010 Done, mean position loss: 23.67884244918823\n",
      "Training growing_up:  41%|████▏     | 8293/20010 [6:41:07<9:04:57,  2.79s/batch]Batch 8100/20010 Done, mean position loss: 25.156642355918883\n",
      "Training growing_up:  41%|████      | 8240/20010 [6:41:13<9:12:28,  2.82s/batch]Batch 8100/20010 Done, mean position loss: 24.582294707298278\n",
      "Training growing_up:  40%|████      | 8077/20010 [6:41:18<9:42:02,  2.93s/batch]Batch 8400/20010 Done, mean position loss: 23.53513368844986\n",
      "Training growing_up:  40%|████      | 8094/20010 [6:41:22<9:15:41,  2.80s/batch]Batch 8200/20010 Done, mean position loss: 24.696054263114927\n",
      "Training growing_up:  41%|████      | 8108/20010 [6:41:26<9:05:50,  2.75s/batch]Batch 8200/20010 Done, mean position loss: 24.825366957187654\n",
      "Training growing_up:  41%|████      | 8201/20010 [6:41:26<9:41:49,  2.96s/batch]Batch 8300/20010 Done, mean position loss: 24.443088436126708\n",
      "Training growing_up:  41%|████      | 8165/20010 [6:41:27<9:28:23,  2.88s/batch]Batch 8300/20010 Done, mean position loss: 23.56326855659485\n",
      "Training growing_up:  41%|████▏     | 8280/20010 [6:41:29<9:16:05,  2.84s/batch]Batch 8300/20010 Done, mean position loss: 24.540307524204255\n",
      "Training growing_up:  42%|███▋     | 8318/20010 [6:41:41<10:06:01,  3.11s/batch]Batch 8200/20010 Done, mean position loss: 25.10218727827072\n",
      "Training growing_up:  41%|████▏     | 8282/20010 [6:41:41<9:20:44,  2.87s/batch]Batch 8200/20010 Done, mean position loss: 23.93456158399582\n",
      "Training growing_up:  41%|████▏     | 8277/20010 [6:41:42<8:56:48,  2.75s/batch]Batch 8100/20010 Done, mean position loss: 24.921223702430723\n",
      "Training growing_up:  41%|████      | 8229/20010 [6:42:18<9:02:42,  2.76s/batch]Batch 8300/20010 Done, mean position loss: 25.0444043135643\n",
      "Training growing_up:  41%|████      | 8223/20010 [6:42:27<9:36:50,  2.94s/batch]Batch 8300/20010 Done, mean position loss: 24.27854767560959\n",
      "Training growing_up:  41%|████      | 8127/20010 [6:42:28<9:15:00,  2.80s/batch]Batch 8100/20010 Done, mean position loss: 24.106653559207917\n",
      "Training growing_up:  41%|████▏     | 8284/20010 [6:42:31<9:40:39,  2.97s/batch]Batch 8300/20010 Done, mean position loss: 24.780534846782682\n",
      "Training growing_up:  41%|████▏     | 8292/20010 [6:42:34<9:02:40,  2.78s/batch]Batch 8300/20010 Done, mean position loss: 25.105051877498628\n",
      "Training growing_up:  41%|████      | 8248/20010 [6:42:48<8:45:58,  2.68s/batch]Batch 8300/20010 Done, mean position loss: 24.502376768589016\n",
      "Training growing_up:  42%|████▏     | 8435/20010 [6:42:54<9:26:22,  2.94s/batch]Batch 8300/20010 Done, mean position loss: 24.454499831199648\n",
      "Training growing_up:  41%|████      | 8230/20010 [6:43:00<8:42:42,  2.66s/batch]Batch 8300/20010 Done, mean position loss: 24.555611436367034\n",
      "Training growing_up:  42%|████▏     | 8312/20010 [6:43:00<8:38:55,  2.66s/batch]Batch 8400/20010 Done, mean position loss: 24.284947290420533\n",
      "Training growing_up:  41%|████▏     | 8278/20010 [6:43:10<9:38:37,  2.96s/batch]Batch 8200/20010 Done, mean position loss: 25.085840885639193\n",
      "Training growing_up:  42%|████▏     | 8445/20010 [6:43:21<8:05:21,  2.52s/batch]Batch 8300/20010 Done, mean position loss: 25.138746347427368\n",
      "Training growing_up:  41%|███▋     | 8291/20010 [6:43:28<10:43:54,  3.30s/batch]Batch 8400/20010 Done, mean position loss: 23.810205280780792\n",
      "Training growing_up:  42%|████▏     | 8422/20010 [6:43:59<9:17:26,  2.89s/batch]Batch 8300/20010 Done, mean position loss: 24.870881214141846\n",
      "Training growing_up:  42%|████▏     | 8337/20010 [6:44:15<9:55:56,  3.06s/batch]Batch 8300/20010 Done, mean position loss: 24.963694088459015\n",
      "Training growing_up:  41%|███▋     | 8295/20010 [6:44:18<10:12:39,  3.14s/batch]Batch 8300/20010 Done, mean position loss: 24.033314654827116\n",
      "Training growing_up:  42%|███▊     | 8367/20010 [6:44:18<10:19:01,  3.19s/batch]Batch 8300/20010 Done, mean position loss: 25.001553237438202\n",
      "Training growing_up:  41%|███▋     | 8138/20010 [6:44:22<10:54:04,  3.31s/batch]Batch 8400/20010 Done, mean position loss: 25.23826350450516\n",
      "Training growing_up:  42%|████▏     | 8360/20010 [6:44:26<9:34:00,  2.96s/batch]Batch 8200/20010 Done, mean position loss: 26.01131612300873\n",
      "Training growing_up:  41%|███▋     | 8261/20010 [6:44:35<10:04:44,  3.09s/batch]Batch 8300/20010 Done, mean position loss: 24.665953829288483\n",
      "Training growing_up:  41%|████▏     | 8287/20010 [6:44:44<9:50:59,  3.02s/batch]Batch 8300/20010 Done, mean position loss: 25.34141401529312\n",
      "Training growing_up:  42%|████▏     | 8316/20010 [6:45:01<9:34:55,  2.95s/batch]Batch 8400/20010 Done, mean position loss: 24.26130285978317\n",
      "Training growing_up:  42%|████▏     | 8444/20010 [6:45:04<9:56:23,  3.09s/batch]Batch 8400/20010 Done, mean position loss: 24.453649561405182\n",
      "Training growing_up:  42%|███▋     | 8336/20010 [6:45:08<10:07:32,  3.12s/batch]Batch 8300/20010 Done, mean position loss: 25.149788773059846\n",
      "Training growing_up:  41%|████▏     | 8280/20010 [6:45:24<9:37:07,  2.95s/batch]Batch 8300/20010 Done, mean position loss: 25.221676452159883\n",
      "Training growing_up:  41%|████▏     | 8283/20010 [6:45:43<9:47:42,  3.01s/batch]Batch 8200/20010 Done, mean position loss: 24.42620365858078\n",
      "Training growing_up:  41%|████      | 8180/20010 [6:45:45<8:40:01,  2.64s/batch]Batch 8400/20010 Done, mean position loss: 24.38081475019455\n",
      "Training growing_up:  41%|████▏     | 8288/20010 [6:45:48<9:31:21,  2.92s/batch]Batch 8300/20010 Done, mean position loss: 24.250481770038604\n",
      "Training growing_up:  42%|███▋     | 8330/20010 [6:46:00<10:00:19,  3.08s/batch]Batch 8300/20010 Done, mean position loss: 25.710947442054746\n",
      "Training growing_up:  42%|████▏     | 8362/20010 [6:46:02<9:32:28,  2.95s/batch]Batch 8400/20010 Done, mean position loss: 23.457160267829895\n",
      "Training growing_up:  42%|███▋     | 8331/20010 [6:46:03<10:00:29,  3.08s/batch]Batch 8500/20010 Done, mean position loss: 23.515162653923035\n",
      "Training growing_up:  42%|████▏     | 8374/20010 [6:46:06<9:12:15,  2.85s/batch]Batch 8200/20010 Done, mean position loss: 24.994058606624606\n",
      "Training growing_up:  42%|███▋     | 8329/20010 [6:46:14<10:28:15,  3.23s/batch]Batch 8200/20010 Done, mean position loss: 23.681093735694887\n",
      "Training growing_up:  41%|████▏     | 8296/20010 [6:46:16<9:47:13,  3.01s/batch]Batch 8300/20010 Done, mean position loss: 24.47213813304901\n",
      "Training growing_up:  42%|███▊     | 8377/20010 [6:46:17<10:31:26,  3.26s/batch]Batch 8400/20010 Done, mean position loss: 23.94929825782776\n",
      "Training growing_up:  42%|████▏     | 8380/20010 [6:46:23<9:29:18,  2.94s/batch]Batch 8400/20010 Done, mean position loss: 23.974932944774626\n",
      "Training growing_up:  42%|████▏     | 8343/20010 [6:46:23<9:04:48,  2.80s/batch]Batch 8400/20010 Done, mean position loss: 24.919599661827085\n",
      "Training growing_up:  41%|████▏     | 8298/20010 [6:46:26<9:36:31,  2.95s/batch]Batch 8300/20010 Done, mean position loss: 24.446247289180754\n",
      "Training growing_up:  41%|████▏     | 8303/20010 [6:46:32<9:41:31,  2.98s/batch]Batch 8300/20010 Done, mean position loss: 25.564529411792755\n",
      "Training growing_up:  42%|████▏     | 8462/20010 [6:46:34<8:24:47,  2.62s/batch]Batch 8300/20010 Done, mean position loss: 24.074063432216644\n",
      "Training growing_up:  41%|███▋     | 8274/20010 [6:46:50<10:18:40,  3.16s/batch]Batch 8200/20010 Done, mean position loss: 25.306797261238096\n",
      "Training growing_up:  42%|████▏     | 8387/20010 [6:47:19<9:58:52,  3.09s/batch]Batch 8400/20010 Done, mean position loss: 24.845117948055268\n",
      "Training growing_up:  42%|████▏     | 8319/20010 [6:47:23<9:28:15,  2.92s/batch]Batch 8400/20010 Done, mean position loss: 25.125327520370483\n",
      "Training growing_up:  42%|████▏     | 8319/20010 [6:47:25<9:46:46,  3.01s/batch]Batch 8400/20010 Done, mean position loss: 23.95393012523651\n",
      "Training growing_up:  42%|████▏     | 8462/20010 [6:47:27<9:49:05,  3.06s/batch]Batch 8400/20010 Done, mean position loss: 25.446521079540254\n",
      "Training growing_up:  42%|████▏     | 8453/20010 [6:47:37<8:04:43,  2.52s/batch]Batch 8200/20010 Done, mean position loss: 23.70615041732788\n",
      "Training growing_up:  42%|████▏     | 8325/20010 [6:47:44<9:39:20,  2.97s/batch]Batch 8400/20010 Done, mean position loss: 24.374868936538697\n",
      "Training growing_up:  41%|████      | 8220/20010 [6:47:47<8:53:36,  2.72s/batch]Batch 8500/20010 Done, mean position loss: 24.567196552753447\n",
      "Training growing_up:  42%|████▏     | 8458/20010 [6:47:51<9:27:00,  2.95s/batch]Batch 8400/20010 Done, mean position loss: 24.60468845129013\n",
      "Training growing_up:  42%|████▏     | 8344/20010 [6:47:59<8:56:06,  2.76s/batch]Batch 8400/20010 Done, mean position loss: 24.080426342487335\n",
      "Training growing_up:  41%|████▏     | 8277/20010 [6:48:18<9:48:13,  3.01s/batch]Batch 8400/20010 Done, mean position loss: 24.641462438106537\n",
      "Training growing_up:  41%|███▋     | 8252/20010 [6:48:19<10:18:01,  3.15s/batch]Batch 8300/20010 Done, mean position loss: 24.510899803638456\n",
      "Training growing_up:  42%|████▏     | 8343/20010 [6:48:33<9:50:56,  3.04s/batch]Batch 8500/20010 Done, mean position loss: 23.8255482506752\n",
      "Training growing_up:  42%|████▏     | 8423/20010 [6:48:54<9:11:18,  2.85s/batch]Batch 8400/20010 Done, mean position loss: 24.655101358890533\n",
      "Training growing_up:  42%|████▏     | 8460/20010 [6:49:10<8:39:30,  2.70s/batch]Batch 8400/20010 Done, mean position loss: 25.635576756000518\n",
      "Training growing_up:  42%|████▏     | 8389/20010 [6:49:11<8:46:21,  2.72s/batch]Batch 8400/20010 Done, mean position loss: 23.77808474779129\n",
      "Training growing_up:  42%|████▏     | 8466/20010 [6:49:12<8:10:22,  2.55s/batch]Batch 8400/20010 Done, mean position loss: 25.000636794567107\n",
      "Training growing_up:  42%|████▏     | 8430/20010 [6:49:22<8:47:07,  2.73s/batch]Batch 8500/20010 Done, mean position loss: 25.437946908473968\n",
      "Training growing_up:  42%|████▏     | 8502/20010 [6:49:25<9:05:16,  2.84s/batch]Batch 8400/20010 Done, mean position loss: 24.84697102308273\n",
      "Training growing_up:  42%|████▏     | 8407/20010 [6:49:28<8:45:00,  2.71s/batch]Batch 8300/20010 Done, mean position loss: 26.38731156826019\n",
      "Training growing_up:  42%|███▊     | 8373/20010 [6:49:48<10:07:34,  3.13s/batch]Batch 8400/20010 Done, mean position loss: 24.814067420959475\n",
      "Training growing_up:  42%|████▏     | 8443/20010 [6:49:57<8:13:19,  2.56s/batch]Batch 8500/20010 Done, mean position loss: 24.137516057491304\n",
      "Training growing_up:  42%|████▏     | 8449/20010 [6:50:02<9:09:33,  2.85s/batch]Batch 8400/20010 Done, mean position loss: 25.02599220275879\n",
      "Training growing_up:  43%|████▎     | 8534/20010 [6:50:03<8:36:06,  2.70s/batch]Batch 8500/20010 Done, mean position loss: 24.1809774184227\n",
      "Training growing_up:  42%|████▏     | 8425/20010 [6:50:20<9:05:31,  2.83s/batch]Batch 8400/20010 Done, mean position loss: 24.707495443820953\n",
      "Training growing_up:  42%|████▏     | 8467/20010 [6:50:41<8:52:34,  2.77s/batch]Batch 8300/20010 Done, mean position loss: 23.97071178674698\n",
      "Training growing_up:  42%|████▏     | 8490/20010 [6:50:43<9:53:15,  3.09s/batch]Batch 8500/20010 Done, mean position loss: 24.224402368068695\n",
      "Training growing_up:  43%|████▎     | 8529/20010 [6:50:44<9:07:27,  2.86s/batch]Batch 8400/20010 Done, mean position loss: 25.2981813621521\n",
      "Training growing_up:  42%|████▏     | 8489/20010 [6:50:45<8:56:59,  2.80s/batch]Batch 8400/20010 Done, mean position loss: 23.916047859191895\n",
      "Training growing_up:  41%|████▏     | 8296/20010 [6:50:55<9:03:38,  2.78s/batch]Batch 8600/20010 Done, mean position loss: 24.468098320960998\n",
      "Training growing_up:  42%|████▏     | 8405/20010 [6:50:57<9:59:08,  3.10s/batch]Batch 8500/20010 Done, mean position loss: 24.01751442670822\n",
      "Training growing_up:  42%|████▏     | 8444/20010 [6:50:58<9:10:52,  2.86s/batch]Batch 8300/20010 Done, mean position loss: 24.208478949069978\n",
      "Training growing_up:  42%|████▏     | 8443/20010 [6:51:11<8:55:04,  2.78s/batch]Batch 8300/20010 Done, mean position loss: 24.26706137418747\n",
      "Training growing_up:  42%|████▏     | 8437/20010 [6:51:11<8:46:31,  2.73s/batch]Batch 8400/20010 Done, mean position loss: 24.315862941741944\n",
      "Training growing_up:  42%|████▏     | 8425/20010 [6:51:13<7:52:56,  2.45s/batch]Batch 8500/20010 Done, mean position loss: 23.33537872314453\n",
      "Training growing_up:  43%|████▎     | 8608/20010 [6:51:14<8:12:33,  2.59s/batch]Batch 8500/20010 Done, mean position loss: 23.627399873733523\n",
      "Training growing_up:  43%|████▎     | 8541/20010 [6:51:17<8:57:41,  2.81s/batch]Batch 8500/20010 Done, mean position loss: 24.361939527988433\n",
      "Training growing_up:  42%|████▏     | 8439/20010 [6:51:18<9:36:44,  2.99s/batch]Batch 8400/20010 Done, mean position loss: 25.14090211391449\n",
      "Batch 8400/20010 Done, mean position loss: 24.40349976539612\n",
      "Training growing_up:  42%|████▏     | 8443/20010 [6:51:29<9:38:02,  3.00s/batch]Batch 8400/20010 Done, mean position loss: 23.681991562843322\n",
      "Training growing_up:  43%|████▎     | 8510/20010 [6:51:38<8:08:37,  2.55s/batch]Batch 8300/20010 Done, mean position loss: 24.168626725673676\n",
      "Training growing_up:  42%|████▏     | 8413/20010 [6:52:02<8:10:52,  2.54s/batch]Batch 8500/20010 Done, mean position loss: 25.033978478908537\n",
      "Training growing_up:  41%|████▏     | 8298/20010 [6:52:10<7:29:03,  2.30s/batch]Batch 8500/20010 Done, mean position loss: 25.324921030998233\n",
      "Training growing_up:  42%|████▏     | 8425/20010 [6:52:13<8:04:29,  2.51s/batch]Batch 8500/20010 Done, mean position loss: 24.808297481536865\n",
      "Training growing_up:  43%|████▎     | 8535/20010 [6:52:15<9:00:52,  2.83s/batch]Batch 8500/20010 Done, mean position loss: 25.108524920940397\n",
      "Training growing_up:  43%|████▎     | 8526/20010 [6:52:17<8:49:13,  2.77s/batch]Batch 8300/20010 Done, mean position loss: 24.07476155757904\n",
      "Training growing_up:  42%|████▏     | 8440/20010 [6:52:23<8:02:34,  2.50s/batch]Batch 8500/20010 Done, mean position loss: 24.299842879772186\n",
      "Training growing_up:  41%|████▏     | 8304/20010 [6:52:26<9:27:15,  2.91s/batch]Batch 8600/20010 Done, mean position loss: 24.8330779838562\n",
      "Training growing_up:  42%|████▏     | 8442/20010 [6:52:28<8:02:16,  2.50s/batch]Batch 8500/20010 Done, mean position loss: 24.955867733955383\n",
      "Training growing_up:  42%|████▏     | 8369/20010 [6:52:41<8:18:37,  2.57s/batch]Batch 8500/20010 Done, mean position loss: 24.440731513500214\n",
      "Training growing_up:  43%|████▎     | 8507/20010 [6:52:46<9:00:30,  2.82s/batch]Batch 8500/20010 Done, mean position loss: 24.573939445018766\n",
      "Training growing_up:  43%|████▎     | 8510/20010 [6:53:04<8:40:23,  2.72s/batch]Batch 8400/20010 Done, mean position loss: 24.42555312871933\n",
      "Training growing_up:  42%|████▏     | 8439/20010 [6:53:05<8:01:12,  2.50s/batch]Batch 8600/20010 Done, mean position loss: 23.817130892276765\n",
      "Training growing_up:  43%|████▎     | 8519/20010 [6:53:31<9:36:27,  3.01s/batch]Batch 8500/20010 Done, mean position loss: 24.265142738819122\n",
      "Training growing_up:  43%|████▎     | 8665/20010 [6:53:43<9:00:12,  2.86s/batch]Batch 8500/20010 Done, mean position loss: 24.002869310379026\n",
      "Training growing_up:  42%|████▏     | 8483/20010 [6:53:45<9:56:28,  3.10s/batch]Batch 8500/20010 Done, mean position loss: 24.586396214962008\n",
      "Training growing_up:  43%|████▎     | 8669/20010 [6:53:55<9:20:56,  2.97s/batch]Batch 8600/20010 Done, mean position loss: 24.999338195323944\n",
      "Training growing_up:  42%|████▏     | 8365/20010 [6:54:00<9:55:46,  3.07s/batch]Batch 8500/20010 Done, mean position loss: 24.913003993034366\n",
      "Training growing_up:  42%|████▏     | 8490/20010 [6:54:06<9:12:03,  2.88s/batch]Batch 8500/20010 Done, mean position loss: 24.954790065288545\n",
      "Training growing_up:  43%|████▎     | 8513/20010 [6:54:15<8:45:10,  2.74s/batch]Batch 8400/20010 Done, mean position loss: 25.54592890501022\n",
      "Training growing_up:  43%|████▎     | 8630/20010 [6:54:27<9:00:19,  2.85s/batch]Batch 8500/20010 Done, mean position loss: 24.157895348072053\n",
      "Training growing_up:  43%|████▎     | 8615/20010 [6:54:34<9:12:10,  2.91s/batch]Batch 8600/20010 Done, mean position loss: 23.726628761291504\n",
      "Training growing_up:  43%|████▎     | 8574/20010 [6:54:37<9:13:31,  2.90s/batch]Batch 8600/20010 Done, mean position loss: 23.919973058700563\n",
      "Training growing_up:  43%|████▎     | 8515/20010 [6:54:38<9:19:57,  2.92s/batch]Batch 8500/20010 Done, mean position loss: 25.097282853126526\n",
      "Training growing_up:  43%|████▎     | 8587/20010 [6:54:54<8:55:55,  2.81s/batch]Batch 8500/20010 Done, mean position loss: 25.314730174541474\n",
      "Training growing_up:  43%|████▎     | 8592/20010 [6:55:23<9:42:24,  3.06s/batch]Batch 8700/20010 Done, mean position loss: 24.06013243675232\n",
      "Training growing_up:  42%|████▏     | 8491/20010 [6:55:24<9:29:05,  2.96s/batch]Batch 8600/20010 Done, mean position loss: 23.977911372184757\n",
      "Batch 8500/20010 Done, mean position loss: 25.331640458106996\n",
      "Training growing_up:  43%|████▎     | 8599/20010 [6:55:31<9:13:49,  2.91s/batch]Batch 8500/20010 Done, mean position loss: 24.136456422805786\n",
      "Batch 8400/20010 Done, mean position loss: 24.625981879234317\n",
      "Training growing_up:  43%|████▎     | 8597/20010 [6:55:37<9:07:40,  2.88s/batch]Batch 8600/20010 Done, mean position loss: 23.885151631832123\n",
      "Training growing_up:  43%|████▎     | 8565/20010 [6:55:44<9:10:54,  2.89s/batch]Batch 8400/20010 Done, mean position loss: 24.353734371662142\n",
      "Training growing_up:  43%|████▎     | 8605/20010 [6:55:46<8:10:01,  2.58s/batch]Batch 8400/20010 Done, mean position loss: 23.539862239360808\n",
      "Training growing_up:  43%|████▎     | 8507/20010 [6:55:49<9:24:11,  2.94s/batch]Batch 8600/20010 Done, mean position loss: 23.38569282531738\n",
      "Batch 8500/20010 Done, mean position loss: 23.81888666152954\n",
      "Training growing_up:  43%|████▎     | 8549/20010 [6:55:50<9:21:34,  2.94s/batch]Batch 8600/20010 Done, mean position loss: 23.39744799852371\n",
      "Training growing_up:  43%|████▎     | 8577/20010 [6:55:55<9:17:13,  2.92s/batch]Batch 8500/20010 Done, mean position loss: 24.833164615631105\n",
      "Training growing_up:  43%|████▎     | 8551/20010 [6:55:55<9:01:37,  2.84s/batch]Batch 8500/20010 Done, mean position loss: 24.212749068737033\n",
      "Training growing_up:  42%|████▏     | 8459/20010 [6:55:56<9:20:48,  2.91s/batch]Batch 8600/20010 Done, mean position loss: 24.94313314199448\n",
      "Training growing_up:  43%|████▎     | 8610/20010 [6:56:13<9:01:40,  2.85s/batch]Batch 8500/20010 Done, mean position loss: 24.074078176021576\n",
      "Training growing_up:  43%|████▎     | 8653/20010 [6:56:21<9:58:37,  3.16s/batch]Batch 8400/20010 Done, mean position loss: 23.878301174640654\n",
      "Training growing_up:  43%|████▎     | 8520/20010 [6:56:45<9:11:16,  2.88s/batch]Batch 8600/20010 Done, mean position loss: 24.73314006090164\n",
      "Training growing_up:  43%|████▎     | 8528/20010 [6:56:45<9:39:49,  3.03s/batch]Batch 8600/20010 Done, mean position loss: 24.996894025802614\n",
      "Training growing_up:  43%|████▎     | 8520/20010 [6:56:49<9:28:34,  2.97s/batch]Batch 8600/20010 Done, mean position loss: 26.070254843235013\n",
      "Training growing_up:  42%|████▏     | 8426/20010 [6:56:55<8:43:01,  2.71s/batch]Batch 8600/20010 Done, mean position loss: 24.454979994297027\n",
      "Training growing_up:  43%|████▎     | 8594/20010 [6:57:08<9:11:01,  2.90s/batch]Batch 8600/20010 Done, mean position loss: 24.214627656936646\n",
      "Training growing_up:  43%|███▉     | 8699/20010 [6:57:09<10:28:59,  3.34s/batch]Batch 8400/20010 Done, mean position loss: 23.870808827877045\n",
      "Training growing_up:  43%|████▎     | 8554/20010 [6:57:15<9:25:59,  2.96s/batch]Batch 8700/20010 Done, mean position loss: 24.210743889808654\n",
      "Training growing_up:  43%|████▎     | 8555/20010 [6:57:18<9:27:46,  2.97s/batch]Batch 8600/20010 Done, mean position loss: 24.561640441417694\n",
      "Training growing_up:  42%|████▏     | 8491/20010 [6:57:28<9:29:00,  2.96s/batch]Batch 8600/20010 Done, mean position loss: 24.43795529603958\n",
      "Training growing_up:  42%|███▊     | 8493/20010 [6:57:34<10:15:45,  3.21s/batch]Batch 8600/20010 Done, mean position loss: 25.264852159023285\n",
      "Training growing_up:  44%|████▎     | 8716/20010 [6:57:59<8:56:52,  2.85s/batch]Batch 8700/20010 Done, mean position loss: 23.89927618265152\n",
      "Training growing_up:  42%|███▊     | 8451/20010 [6:58:03<11:34:29,  3.60s/batch]Batch 8500/20010 Done, mean position loss: 24.653178966045378\n",
      "Training growing_up:  43%|████▎     | 8635/20010 [6:58:25<8:53:46,  2.82s/batch]Batch 8600/20010 Done, mean position loss: 24.46835332155228\n",
      "Training growing_up:  43%|████▎     | 8593/20010 [6:58:26<9:25:20,  2.97s/batch]Batch 8600/20010 Done, mean position loss: 24.49935291528702\n",
      "Training growing_up:  43%|████▎     | 8657/20010 [6:58:31<9:17:36,  2.95s/batch]Batch 8600/20010 Done, mean position loss: 24.922266843318937\n",
      "Training growing_up:  43%|████▎     | 8582/20010 [6:58:42<9:42:55,  3.06s/batch]Batch 8700/20010 Done, mean position loss: 25.47502682685852\n",
      "Training growing_up:  43%|████▎     | 8600/20010 [6:58:51<9:24:00,  2.97s/batch]Batch 8600/20010 Done, mean position loss: 24.243149456977847\n",
      "Training growing_up:  42%|███▊     | 8464/20010 [6:58:53<10:26:21,  3.25s/batch]Batch 8600/20010 Done, mean position loss: 25.66051800727844\n",
      "Training growing_up:  43%|████▎     | 8652/20010 [6:59:15<8:51:30,  2.81s/batch]Batch 8500/20010 Done, mean position loss: 25.102333612442017\n",
      "Training growing_up:  42%|███▊     | 8445/20010 [6:59:23<10:46:02,  3.35s/batch]Batch 8600/20010 Done, mean position loss: 23.967034447193143\n",
      "Training growing_up:  44%|████▍     | 8786/20010 [6:59:27<8:37:57,  2.77s/batch]Batch 8700/20010 Done, mean position loss: 24.64552887916565\n",
      "Training growing_up:  42%|████▏     | 8477/20010 [6:59:31<9:07:54,  2.85s/batch]Batch 8700/20010 Done, mean position loss: 23.99661653995514\n",
      "Training growing_up:  44%|████▎     | 8734/20010 [6:59:40<9:25:24,  3.01s/batch]Batch 8600/20010 Done, mean position loss: 24.58772805929184\n",
      "Training growing_up:  43%|████▎     | 8684/20010 [6:59:49<9:05:42,  2.89s/batch]Batch 8600/20010 Done, mean position loss: 24.374495120048522\n",
      "Training growing_up:  43%|████▎     | 8667/20010 [7:00:13<9:53:01,  3.14s/batch]Batch 8800/20010 Done, mean position loss: 23.36914229154587\n",
      "Training growing_up:  43%|████▎     | 8614/20010 [7:00:19<9:19:37,  2.95s/batch]Batch 8700/20010 Done, mean position loss: 23.748664331436157\n",
      "Training growing_up:  42%|████▏     | 8468/20010 [7:00:31<9:58:34,  3.11s/batch]Batch 8600/20010 Done, mean position loss: 24.019568066596985\n",
      "Training growing_up:  43%|████▎     | 8642/20010 [7:00:33<9:39:47,  3.06s/batch]Batch 8700/20010 Done, mean position loss: 24.44555349111557\n",
      "Training growing_up:  43%|████▎     | 8675/20010 [7:00:37<9:16:47,  2.95s/batch]Batch 8600/20010 Done, mean position loss: 24.643564348220824\n",
      "Training growing_up:  43%|████▎     | 8588/20010 [7:00:38<8:45:42,  2.76s/batch]Batch 8500/20010 Done, mean position loss: 23.969338545799253\n",
      "Training growing_up:  43%|████▎     | 8670/20010 [7:00:39<9:49:10,  3.12s/batch]Batch 8700/20010 Done, mean position loss: 23.25529928922653\n",
      "Training growing_up:  43%|████▎     | 8702/20010 [7:00:41<9:14:06,  2.94s/batch]Batch 8500/20010 Done, mean position loss: 24.414124999046326\n",
      "Training growing_up:  42%|████▏     | 8501/20010 [7:00:42<9:32:42,  2.99s/batch]Batch 8500/20010 Done, mean position loss: 23.332295079231262\n",
      "Training growing_up:  43%|████▎     | 8531/20010 [7:00:46<9:03:01,  2.84s/batch]Batch 8700/20010 Done, mean position loss: 23.578278169631957\n",
      "Training growing_up:  43%|████▎     | 8555/20010 [7:00:48<9:44:31,  3.06s/batch]Batch 8700/20010 Done, mean position loss: 24.56323641061783\n",
      "Training growing_up:  43%|████▎     | 8675/20010 [7:00:49<9:07:19,  2.90s/batch]Batch 8600/20010 Done, mean position loss: 24.23665236711502\n",
      "Training growing_up:  44%|████▍     | 8776/20010 [7:00:58<8:35:56,  2.76s/batch]Batch 8600/20010 Done, mean position loss: 25.061410682201384\n",
      "Training growing_up:  44%|████▍     | 8761/20010 [7:00:59<9:34:55,  3.07s/batch]Batch 8600/20010 Done, mean position loss: 23.961584789752962\n",
      "Training growing_up:  43%|████▎     | 8659/20010 [7:01:15<9:53:52,  3.14s/batch]Batch 8600/20010 Done, mean position loss: 24.024566040039062\n",
      "Training growing_up:  43%|███▊     | 8515/20010 [7:01:27<11:34:11,  3.62s/batch]Batch 8500/20010 Done, mean position loss: 23.71175814628601\n",
      "Training growing_up:  43%|███▊     | 8550/20010 [7:01:48<11:32:53,  3.63s/batch]Batch 8700/20010 Done, mean position loss: 24.18473650455475\n",
      "Training growing_up:  44%|███▉     | 8793/20010 [7:01:53<10:52:29,  3.49s/batch]Batch 8700/20010 Done, mean position loss: 24.910166928768156\n",
      "Training growing_up:  43%|███▉     | 8625/20010 [7:01:54<11:30:10,  3.64s/batch]Batch 8700/20010 Done, mean position loss: 24.998463082313542\n",
      "Training growing_up:  44%|███▉     | 8729/20010 [7:02:04<11:51:15,  3.78s/batch]Batch 8700/20010 Done, mean position loss: 24.19048171043396\n",
      "Training growing_up:  43%|████▎     | 8667/20010 [7:02:14<9:49:25,  3.12s/batch]Batch 8700/20010 Done, mean position loss: 25.006255655288697\n",
      "Training growing_up:  43%|████▎     | 8628/20010 [7:02:19<9:39:56,  3.06s/batch]Batch 8700/20010 Done, mean position loss: 24.517602055072786\n",
      "Training growing_up:  43%|████▎     | 8653/20010 [7:02:21<9:57:53,  3.16s/batch]Batch 8800/20010 Done, mean position loss: 24.32874555826187\n",
      "Training growing_up:  44%|████▎     | 8741/20010 [7:02:24<9:16:39,  2.96s/batch]Batch 8500/20010 Done, mean position loss: 24.27391615152359\n",
      "Training growing_up:  43%|████▎     | 8674/20010 [7:02:42<9:51:05,  3.13s/batch]Batch 8700/20010 Done, mean position loss: 25.05265448331833\n",
      "Training growing_up:  44%|████▍     | 8778/20010 [7:02:42<9:25:29,  3.02s/batch]Batch 8700/20010 Done, mean position loss: 24.34424951314926\n",
      "Training growing_up:  43%|████▎     | 8672/20010 [7:03:05<9:11:25,  2.92s/batch]Batch 8800/20010 Done, mean position loss: 23.51722796201706\n",
      "Training growing_up:  44%|███▉     | 8729/20010 [7:03:21<11:36:42,  3.71s/batch]Batch 8600/20010 Done, mean position loss: 24.35886656522751\n",
      "Training growing_up:  43%|████▎     | 8656/20010 [7:03:30<8:23:19,  2.66s/batch]Batch 8700/20010 Done, mean position loss: 23.709573147296908\n",
      "Training growing_up:  43%|████▎     | 8677/20010 [7:03:35<9:22:52,  2.98s/batch]Batch 8700/20010 Done, mean position loss: 24.700570905208586\n",
      "Training growing_up:  44%|████▍     | 8758/20010 [7:03:37<9:00:21,  2.88s/batch]Batch 8700/20010 Done, mean position loss: 24.397960770130158\n",
      "Training growing_up:  44%|████▎     | 8738/20010 [7:03:52<7:31:08,  2.40s/batch]Batch 8800/20010 Done, mean position loss: 25.176438579559324\n",
      "Training growing_up:  43%|████▎     | 8666/20010 [7:03:56<8:40:44,  2.75s/batch]Batch 8700/20010 Done, mean position loss: 24.703159971237184\n",
      "Training growing_up:  44%|████▍     | 8804/20010 [7:04:02<8:45:51,  2.82s/batch]Batch 8700/20010 Done, mean position loss: 25.194192826747894\n",
      "Training growing_up:  44%|███▉     | 8774/20010 [7:04:22<10:28:08,  3.35s/batch]Batch 8600/20010 Done, mean position loss: 25.676926250457765\n",
      "Training growing_up:  44%|████▎     | 8724/20010 [7:04:32<9:27:02,  3.01s/batch]Batch 8800/20010 Done, mean position loss: 24.47032153367996\n",
      "Training growing_up:  43%|████▎     | 8562/20010 [7:04:34<9:54:07,  3.11s/batch]Batch 8700/20010 Done, mean position loss: 25.808796424865722\n",
      "Training growing_up:  43%|███▊     | 8563/20010 [7:04:37<10:04:14,  3.17s/batch]Batch 8800/20010 Done, mean position loss: 23.784705605506893\n",
      "Training growing_up:  43%|████▎     | 8678/20010 [7:04:43<8:22:25,  2.66s/batch]Batch 8700/20010 Done, mean position loss: 24.80054893016815\n",
      "Training growing_up:  43%|████▎     | 8686/20010 [7:04:58<8:46:00,  2.79s/batch]Batch 8700/20010 Done, mean position loss: 24.203772518634796\n",
      "Training growing_up:  43%|████▎     | 8639/20010 [7:05:09<9:31:04,  3.01s/batch]Batch 8900/20010 Done, mean position loss: 23.992283458709714\n",
      "Training growing_up:  44%|████▍     | 8765/20010 [7:05:20<7:44:06,  2.48s/batch]Batch 8800/20010 Done, mean position loss: 24.06367435693741\n",
      "Training growing_up:  44%|████▍     | 8870/20010 [7:05:35<7:44:51,  2.50s/batch]Batch 8800/20010 Done, mean position loss: 23.09633718252182\n",
      "Training growing_up:  44%|████▍     | 8825/20010 [7:05:36<7:53:16,  2.54s/batch]Batch 8800/20010 Done, mean position loss: 25.078252999782563\n",
      "Training growing_up:  44%|████▍     | 8798/20010 [7:05:37<7:35:54,  2.44s/batch]Batch 8800/20010 Done, mean position loss: 24.361381304264068\n",
      "Training growing_up:  44%|████▍     | 8764/20010 [7:05:40<8:27:27,  2.71s/batch]Batch 8700/20010 Done, mean position loss: 25.28886311531067\n",
      "Training growing_up:  44%|████▍     | 8780/20010 [7:05:39<7:43:01,  2.47s/batch]Batch 8600/20010 Done, mean position loss: 23.949498205184934\n",
      "Training growing_up:  43%|████▎     | 8601/20010 [7:05:40<8:40:21,  2.74s/batch]Batch 8700/20010 Done, mean position loss: 23.986038384437563\n",
      "Training growing_up:  43%|████▎     | 8701/20010 [7:05:41<7:49:47,  2.49s/batch]Batch 8700/20010 Done, mean position loss: 23.92979179382324\n",
      "Training growing_up:  44%|████▎     | 8746/20010 [7:05:42<8:12:19,  2.62s/batch]Batch 8600/20010 Done, mean position loss: 24.118322477340698\n",
      "Training growing_up:  44%|████▎     | 8752/20010 [7:05:44<7:22:08,  2.36s/batch]Batch 8800/20010 Done, mean position loss: 24.32241129875183\n",
      "Training growing_up:  44%|████▍     | 8780/20010 [7:05:46<8:00:19,  2.57s/batch]Batch 8600/20010 Done, mean position loss: 24.417419140338897\n",
      "Training growing_up:  44%|████▍     | 8810/20010 [7:05:59<7:46:14,  2.50s/batch]Batch 8700/20010 Done, mean position loss: 24.600488705635073\n",
      "Training growing_up:  44%|████▍     | 8865/20010 [7:06:01<7:34:21,  2.45s/batch]Batch 8700/20010 Done, mean position loss: 23.824858815670012\n",
      "Training growing_up:  44%|████▍     | 8836/20010 [7:06:03<7:26:09,  2.40s/batch]Batch 8700/20010 Done, mean position loss: 25.337230460643767\n",
      "Training growing_up:  44%|████▎     | 8715/20010 [7:06:16<7:56:05,  2.53s/batch]Batch 8600/20010 Done, mean position loss: 23.43715342760086\n",
      "Training growing_up:  44%|████▍     | 8876/20010 [7:06:28<7:21:12,  2.38s/batch]Batch 8800/20010 Done, mean position loss: 24.788717856407168\n",
      "Training growing_up:  43%|████▎     | 8620/20010 [7:06:30<7:49:59,  2.48s/batch]Batch 8800/20010 Done, mean position loss: 24.143668444156646\n",
      "Training growing_up:  44%|████▍     | 8799/20010 [7:06:38<7:35:27,  2.44s/batch]Batch 8800/20010 Done, mean position loss: 24.663688671588897\n",
      "Training growing_up:  44%|████▍     | 8801/20010 [7:06:38<7:55:22,  2.54s/batch]Batch 8800/20010 Done, mean position loss: 24.164051892757417\n",
      "Training growing_up:  44%|████▍     | 8806/20010 [7:06:43<8:16:36,  2.66s/batch]Batch 8800/20010 Done, mean position loss: 24.105060620307924\n",
      "Training growing_up:  44%|████▍     | 8804/20010 [7:06:49<6:23:59,  2.06s/batch]Batch 8900/20010 Done, mean position loss: 24.340579447746276\n",
      "Training growing_up:  44%|████▍     | 8780/20010 [7:06:49<7:09:11,  2.29s/batch]Batch 8800/20010 Done, mean position loss: 24.875387337207794\n",
      "Training growing_up:  43%|████▎     | 8633/20010 [7:07:04<8:04:27,  2.55s/batch]Batch 8800/20010 Done, mean position loss: 24.564882071018218\n",
      "Training growing_up:  44%|████▎     | 8726/20010 [7:07:07<8:11:51,  2.62s/batch]Batch 8600/20010 Done, mean position loss: 23.56476953268051\n",
      "Training growing_up:  43%|████▎     | 8636/20010 [7:07:11<7:47:08,  2.46s/batch]Batch 8800/20010 Done, mean position loss: 24.698942375183105\n",
      "Training growing_up:  43%|████▎     | 8642/20010 [7:07:29<9:49:23,  3.11s/batch]Batch 8900/20010 Done, mean position loss: 24.377923963069918\n",
      "Training growing_up:  44%|████▍     | 8774/20010 [7:07:40<7:59:49,  2.56s/batch]Batch 8800/20010 Done, mean position loss: 23.678698241710663\n",
      "Training growing_up:  44%|████▍     | 8813/20010 [7:07:42<7:27:48,  2.40s/batch]Batch 8700/20010 Done, mean position loss: 24.51152158021927\n",
      "Training growing_up:  44%|████▍     | 8782/20010 [7:07:53<7:10:15,  2.30s/batch]Batch 8800/20010 Done, mean position loss: 24.995698726177217\n",
      "Training growing_up:  45%|████▍     | 8914/20010 [7:07:59<7:29:14,  2.43s/batch]Batch 8800/20010 Done, mean position loss: 24.675691237449648\n",
      "Training growing_up:  43%|████▎     | 8648/20010 [7:08:07<6:56:35,  2.20s/batch]Batch 8900/20010 Done, mean position loss: 24.97498063802719\n",
      "Training growing_up:  45%|████▍     | 8921/20010 [7:08:15<6:48:52,  2.21s/batch]Batch 8800/20010 Done, mean position loss: 24.74248765230179\n",
      "Training growing_up:  43%|████▎     | 8698/20010 [7:08:24<8:13:49,  2.62s/batch]Batch 8800/20010 Done, mean position loss: 24.895582020282745\n",
      "Training growing_up:  44%|████▎     | 8721/20010 [7:08:31<7:35:24,  2.42s/batch]Batch 8700/20010 Done, mean position loss: 25.366657302379608\n",
      "Training growing_up:  44%|████▍     | 8766/20010 [7:08:37<8:57:18,  2.87s/batch]Batch 8800/20010 Done, mean position loss: 24.254958868026733\n",
      "Training growing_up:  44%|████▍     | 8855/20010 [7:08:40<8:18:46,  2.68s/batch]Batch 8900/20010 Done, mean position loss: 24.19107486009598\n",
      "Training growing_up:  44%|████▎     | 8705/20010 [7:08:41<7:45:06,  2.47s/batch]Batch 8900/20010 Done, mean position loss: 23.210466775894165\n",
      "Training growing_up:  43%|████▎     | 8675/20010 [7:08:48<8:09:34,  2.59s/batch]Batch 8800/20010 Done, mean position loss: 24.254883985519406\n",
      "Training growing_up:  44%|████▍     | 8789/20010 [7:09:10<7:58:29,  2.56s/batch]Batch 9000/20010 Done, mean position loss: 23.52392865896225\n",
      "Training growing_up:  44%|████▍     | 8887/20010 [7:09:12<8:40:26,  2.81s/batch]Batch 8800/20010 Done, mean position loss: 24.99280202150345\n",
      "Training growing_up:  44%|████▍     | 8838/20010 [7:09:25<8:01:06,  2.58s/batch]Batch 8900/20010 Done, mean position loss: 23.771706993579862\n",
      "Training growing_up:  45%|████▍     | 8967/20010 [7:09:30<7:03:19,  2.30s/batch]Batch 8900/20010 Done, mean position loss: 23.381676564216612\n",
      "Training growing_up:  45%|████▍     | 8905/20010 [7:09:35<8:11:09,  2.65s/batch]Batch 8900/20010 Done, mean position loss: 24.597178976535794\n",
      "Training growing_up:  44%|████▍     | 8839/20010 [7:09:38<8:13:10,  2.65s/batch]Batch 8800/20010 Done, mean position loss: 23.810690290927887\n",
      "Training growing_up:  44%|████▍     | 8815/20010 [7:09:43<6:49:20,  2.19s/batch]Batch 8900/20010 Done, mean position loss: 23.755221683979038\n",
      "Training growing_up:  44%|████▍     | 8882/20010 [7:09:45<7:14:37,  2.34s/batch]Batch 8900/20010 Done, mean position loss: 23.49242388486862\n",
      "Training growing_up:  45%|████▍     | 8907/20010 [7:09:48<6:48:16,  2.21s/batch]Batch 8800/20010 Done, mean position loss: 24.971398355960847\n",
      "Training growing_up:  44%|████▍     | 8875/20010 [7:09:50<7:10:20,  2.32s/batch]Batch 8800/20010 Done, mean position loss: 23.89696411371231\n",
      "Training growing_up:  44%|████▍     | 8903/20010 [7:09:50<7:17:15,  2.36s/batch]Batch 8700/20010 Done, mean position loss: 23.665881969928744\n",
      "Training growing_up:  44%|████▍     | 8879/20010 [7:09:50<7:14:21,  2.34s/batch]Batch 8700/20010 Done, mean position loss: 23.658227753639224\n",
      "Training growing_up:  45%|████▍     | 8933/20010 [7:09:55<7:23:33,  2.40s/batch]Batch 8700/20010 Done, mean position loss: 24.816778235435486\n",
      "Training growing_up:  45%|████▍     | 8964/20010 [7:09:58<7:42:10,  2.51s/batch]Batch 8800/20010 Done, mean position loss: 23.863727481365203\n",
      "Training growing_up:  44%|████▍     | 8805/20010 [7:10:01<8:57:46,  2.88s/batch]Batch 8800/20010 Done, mean position loss: 25.025756413936616\n",
      "Training growing_up:  44%|████▎     | 8706/20010 [7:10:01<7:01:24,  2.24s/batch]Batch 8800/20010 Done, mean position loss: 24.40065218448639\n",
      "Training growing_up:  45%|████▍     | 8924/20010 [7:10:23<8:19:23,  2.70s/batch]Batch 8700/20010 Done, mean position loss: 24.280982224941255\n",
      "Training growing_up:  44%|████▍     | 8817/20010 [7:10:34<8:32:03,  2.74s/batch]Batch 8900/20010 Done, mean position loss: 24.58197306871414\n",
      "Training growing_up:  44%|████▍     | 8774/20010 [7:10:42<8:15:31,  2.65s/batch]Batch 8900/20010 Done, mean position loss: 24.837716562747957\n",
      "Training growing_up:  44%|████▎     | 8722/20010 [7:10:44<7:23:19,  2.36s/batch]Batch 8900/20010 Done, mean position loss: 25.083921525478363\n",
      "Training growing_up:  44%|████▍     | 8902/20010 [7:10:46<6:44:51,  2.19s/batch]Batch 8900/20010 Done, mean position loss: 24.081267120838167\n",
      "Training growing_up:  44%|████▎     | 8723/20010 [7:10:50<7:35:21,  2.42s/batch]Batch 8900/20010 Done, mean position loss: 25.063549821376803\n",
      "Training growing_up:  44%|████▍     | 8825/20010 [7:10:52<7:02:01,  2.26s/batch]Batch 8900/20010 Done, mean position loss: 24.645637600421907\n",
      "Training growing_up:  44%|████▍     | 8878/20010 [7:10:55<6:50:16,  2.21s/batch]Batch 9000/20010 Done, mean position loss: 24.42401468992233\n",
      "Training growing_up:  45%|████▍     | 8935/20010 [7:11:08<7:38:00,  2.48s/batch]Batch 8900/20010 Done, mean position loss: 24.728269591331483\n",
      "Training growing_up:  44%|████▍     | 8875/20010 [7:11:09<8:18:54,  2.69s/batch]Batch 8700/20010 Done, mean position loss: 24.217965259552003\n",
      "Training growing_up:  45%|████▍     | 8993/20010 [7:11:11<7:01:13,  2.29s/batch]Batch 8900/20010 Done, mean position loss: 24.048007464408876\n",
      "Training growing_up:  45%|████▍     | 8947/20010 [7:11:30<7:16:19,  2.37s/batch]Batch 9000/20010 Done, mean position loss: 23.73401833295822\n",
      "Training growing_up:  45%|████▌     | 9021/20010 [7:11:42<7:26:04,  2.44s/batch]Batch 8900/20010 Done, mean position loss: 23.520133843421934\n",
      "Training growing_up:  44%|████▍     | 8853/20010 [7:11:48<7:40:51,  2.48s/batch]Batch 8800/20010 Done, mean position loss: 24.193343110084534\n",
      "Training growing_up:  45%|████▍     | 8918/20010 [7:11:50<8:00:22,  2.60s/batch]Batch 8900/20010 Done, mean position loss: 25.04085299253464\n",
      "Training growing_up:  44%|████▎     | 8724/20010 [7:12:06<7:40:49,  2.45s/batch]Batch 9000/20010 Done, mean position loss: 24.47809350967407\n",
      "Training growing_up:  45%|████▍     | 8938/20010 [7:12:12<7:30:19,  2.44s/batch]Batch 8900/20010 Done, mean position loss: 24.27877312660217\n",
      "Training growing_up:  44%|████▍     | 8760/20010 [7:12:21<7:50:38,  2.51s/batch]Batch 8900/20010 Done, mean position loss: 24.214879834651946\n",
      "Training growing_up:  44%|████▍     | 8885/20010 [7:12:30<6:13:30,  2.01s/batch]Batch 8800/20010 Done, mean position loss: 25.366135661602023\n",
      "Training growing_up:  45%|████▍     | 9000/20010 [7:12:36<7:25:57,  2.43s/batch]Batch 8900/20010 Done, mean position loss: 24.663030824661256\n",
      "Training growing_up:  44%|████▍     | 8901/20010 [7:12:37<9:10:27,  2.97s/batch]Batch 9000/20010 Done, mean position loss: 23.826284766197205\n",
      "Training growing_up:  45%|████▍     | 8948/20010 [7:12:39<8:13:32,  2.68s/batch]Batch 9000/20010 Done, mean position loss: 23.4534380698204\n",
      "Training growing_up:  45%|████▍     | 8974/20010 [7:12:41<7:17:42,  2.38s/batch]Batch 8900/20010 Done, mean position loss: 24.37296532869339\n",
      "Training growing_up:  45%|████▍     | 8928/20010 [7:12:47<7:37:20,  2.48s/batch]Batch 8900/20010 Done, mean position loss: 24.384024255275726\n",
      "Training growing_up:  45%|████▍     | 8960/20010 [7:13:05<7:26:29,  2.42s/batch]Batch 9100/20010 Done, mean position loss: 23.18852600336075\n",
      "Training growing_up:  45%|████▍     | 8965/20010 [7:13:08<7:31:28,  2.45s/batch]Batch 8900/20010 Done, mean position loss: 24.469857158660886\n",
      "Training growing_up:  45%|████▍     | 8919/20010 [7:13:30<7:16:56,  2.36s/batch]Batch 9000/20010 Done, mean position loss: 24.070780568122863\n",
      "Training growing_up:  44%|████▍     | 8895/20010 [7:13:36<7:32:39,  2.44s/batch]Batch 9000/20010 Done, mean position loss: 23.89316879034042\n",
      "Training growing_up:  45%|████▌     | 9072/20010 [7:13:41<7:08:42,  2.35s/batch]Batch 9000/20010 Done, mean position loss: 24.601476566791536\n",
      "Training growing_up:  45%|████▍     | 8940/20010 [7:13:46<7:02:18,  2.29s/batch]Batch 8900/20010 Done, mean position loss: 24.741176483631136\n",
      "Training growing_up:  45%|████▍     | 8977/20010 [7:13:47<7:13:43,  2.36s/batch]Batch 9000/20010 Done, mean position loss: 23.98243551015854\n",
      "Training growing_up:  45%|████▍     | 9004/20010 [7:13:48<7:38:06,  2.50s/batch]Batch 9000/20010 Done, mean position loss: 23.337428491115574\n",
      "Training growing_up:  45%|████▍     | 8981/20010 [7:13:51<8:20:02,  2.72s/batch]Batch 8800/20010 Done, mean position loss: 23.58881617307663\n",
      "Training growing_up:  44%|████▍     | 8800/20010 [7:13:52<8:47:57,  2.83s/batch]Batch 8900/20010 Done, mean position loss: 23.783415458202363\n",
      "Training growing_up:  45%|████▍     | 8954/20010 [7:13:53<8:01:02,  2.61s/batch]Batch 8800/20010 Done, mean position loss: 24.11192970275879\n",
      "Training growing_up:  44%|████▍     | 8787/20010 [7:13:55<7:55:00,  2.54s/batch]Batch 8900/20010 Done, mean position loss: 24.128790662288665\n",
      "Training growing_up:  45%|████▍     | 8980/20010 [7:13:57<7:43:08,  2.52s/batch]Batch 8900/20010 Done, mean position loss: 25.287096476554872\n",
      "Training growing_up:  44%|████▍     | 8837/20010 [7:14:00<7:14:32,  2.33s/batch]Batch 8900/20010 Done, mean position loss: 24.287749428749084\n",
      "Training growing_up:  44%|████▍     | 8800/20010 [7:14:01<7:08:02,  2.29s/batch]Batch 8900/20010 Done, mean position loss: 24.327639276981355\n",
      "Training growing_up:  45%|████▌     | 9013/20010 [7:14:03<7:27:28,  2.44s/batch]Batch 8800/20010 Done, mean position loss: 24.10965823173523\n",
      "Training growing_up:  45%|████▌     | 9062/20010 [7:14:33<7:10:47,  2.36s/batch]Batch 8800/20010 Done, mean position loss: 24.04072776794434\n",
      "Training growing_up:  45%|████▍     | 8971/20010 [7:14:36<7:19:26,  2.39s/batch]Batch 9000/20010 Done, mean position loss: 24.982934441566464\n",
      "Training growing_up:  45%|████▍     | 8974/20010 [7:14:43<7:12:21,  2.35s/batch]Batch 9000/20010 Done, mean position loss: 24.64868090391159\n",
      "Training growing_up:  45%|████▍     | 8989/20010 [7:14:46<7:45:04,  2.53s/batch]Batch 9000/20010 Done, mean position loss: 23.806306567192077\n",
      "Training growing_up:  45%|████▌     | 9007/20010 [7:14:51<8:02:27,  2.63s/batch]Batch 9000/20010 Done, mean position loss: 25.57461170911789\n",
      "Training growing_up:  45%|████▍     | 8991/20010 [7:14:52<7:58:46,  2.61s/batch]Batch 9000/20010 Done, mean position loss: 25.025295162200926\n",
      "Training growing_up:  45%|████▌     | 9055/20010 [7:14:55<8:30:08,  2.79s/batch]Batch 9100/20010 Done, mean position loss: 24.248649492263795\n",
      "Training growing_up:  45%|████▌     | 9035/20010 [7:15:00<9:09:14,  3.00s/batch]Batch 9000/20010 Done, mean position loss: 24.521001799106596\n",
      "Training growing_up:  45%|████▍     | 8931/20010 [7:15:18<8:15:44,  2.68s/batch]Batch 9000/20010 Done, mean position loss: 24.692081780433657\n",
      "Training growing_up:  45%|████▌     | 9037/20010 [7:15:20<8:48:33,  2.89s/batch]Batch 9000/20010 Done, mean position loss: 24.44419568300247\n",
      "Training growing_up:  45%|████▍     | 8966/20010 [7:15:24<7:07:17,  2.32s/batch]Batch 8800/20010 Done, mean position loss: 24.131531710624696\n",
      "Training growing_up:  45%|████▍     | 8938/20010 [7:15:34<7:36:04,  2.47s/batch]Batch 9100/20010 Done, mean position loss: 24.453177642822265\n",
      "Training growing_up:  45%|████▍     | 8991/20010 [7:15:52<7:39:58,  2.50s/batch]Batch 9000/20010 Done, mean position loss: 23.563829851150512\n",
      "Training growing_up:  45%|████▍     | 8975/20010 [7:15:59<8:10:33,  2.67s/batch]Batch 9000/20010 Done, mean position loss: 25.246139090061188\n",
      "Training growing_up:  45%|████▌     | 9017/20010 [7:16:00<7:40:20,  2.51s/batch]Batch 8900/20010 Done, mean position loss: 24.22476218938828\n",
      "Training growing_up:  45%|████▍     | 8995/20010 [7:16:19<7:49:01,  2.55s/batch]Batch 9100/20010 Done, mean position loss: 24.690703163146974\n",
      "Training growing_up:  45%|████▌     | 9063/20010 [7:16:19<7:16:32,  2.39s/batch]Batch 9000/20010 Done, mean position loss: 24.99724499464035\n",
      "Training growing_up:  45%|████▌     | 9070/20010 [7:16:33<8:18:22,  2.73s/batch]Batch 9000/20010 Done, mean position loss: 24.184039642810824\n",
      "Training growing_up:  45%|████▌     | 9046/20010 [7:16:43<7:53:40,  2.59s/batch]Batch 9100/20010 Done, mean position loss: 24.2842732834816\n",
      "Training growing_up:  46%|████▌     | 9146/20010 [7:16:51<7:22:35,  2.44s/batch]Batch 9100/20010 Done, mean position loss: 23.85736437320709\n",
      "Training growing_up:  45%|████▌     | 9052/20010 [7:16:51<7:18:46,  2.40s/batch]Batch 8900/20010 Done, mean position loss: 25.422272322177886\n",
      "Training growing_up:  45%|████▌     | 9051/20010 [7:16:56<8:04:43,  2.65s/batch]Batch 9000/20010 Done, mean position loss: 24.27832287788391\n",
      "Training growing_up:  45%|████▌     | 9048/20010 [7:16:57<7:27:41,  2.45s/batch]Batch 9000/20010 Done, mean position loss: 24.778958880901335\n",
      "Training growing_up:  44%|████▍     | 8876/20010 [7:17:06<7:18:17,  2.36s/batch]Batch 9000/20010 Done, mean position loss: 24.454893126487733\n",
      "Training growing_up:  45%|████▍     | 8912/20010 [7:17:20<8:21:49,  2.71s/batch]Batch 9200/20010 Done, mean position loss: 22.864587798118592\n",
      "Training growing_up:  45%|████▍     | 8980/20010 [7:17:23<8:03:54,  2.63s/batch]Batch 9000/20010 Done, mean position loss: 24.63615044116974\n",
      "Training growing_up:  45%|████▌     | 9096/20010 [7:17:49<7:21:07,  2.43s/batch]Batch 9100/20010 Done, mean position loss: 24.27919954299927\n",
      "Training growing_up:  45%|████▌     | 9033/20010 [7:17:50<7:40:50,  2.52s/batch]Batch 9100/20010 Done, mean position loss: 23.63391013145447\n",
      "Training growing_up:  45%|████▌     | 9064/20010 [7:17:55<7:14:57,  2.38s/batch]Batch 9100/20010 Done, mean position loss: 24.330526649951935\n",
      "Training growing_up:  45%|████▍     | 8991/20010 [7:17:56<7:25:53,  2.43s/batch]Batch 9000/20010 Done, mean position loss: 24.478867511749264\n",
      "Training growing_up:  46%|████▌     | 9106/20010 [7:18:02<7:43:12,  2.55s/batch]Batch 9100/20010 Done, mean position loss: 24.106600680351256\n",
      "Training growing_up:  45%|████▍     | 8948/20010 [7:18:03<7:59:43,  2.60s/batch]Batch 9100/20010 Done, mean position loss: 23.133264706134796\n",
      "Training growing_up:  45%|████▌     | 9007/20010 [7:18:09<7:24:22,  2.42s/batch]Batch 8900/20010 Done, mean position loss: 22.931874539852142\n",
      "Training growing_up:  45%|████▌     | 9046/20010 [7:18:13<7:48:18,  2.56s/batch]Batch 9000/20010 Done, mean position loss: 24.22995596885681\n",
      "Training growing_up:  45%|████▌     | 9083/20010 [7:18:13<7:27:06,  2.46s/batch]Batch 8900/20010 Done, mean position loss: 24.00052281856537\n",
      "Training growing_up:  44%|████▍     | 8903/20010 [7:18:15<8:39:30,  2.81s/batch]Batch 8900/20010 Done, mean position loss: 23.898257031440735\n",
      "Training growing_up:  45%|████▌     | 9032/20010 [7:18:16<7:35:34,  2.49s/batch]Batch 9000/20010 Done, mean position loss: 24.765444040298462\n",
      "Training growing_up:  45%|████▌     | 9056/20010 [7:18:18<7:27:43,  2.45s/batch]Batch 9000/20010 Done, mean position loss: 24.693707792758943\n",
      "Training growing_up:  46%|████▌     | 9165/20010 [7:18:18<8:14:24,  2.74s/batch]Batch 9000/20010 Done, mean position loss: 23.656685943603513\n",
      "Training growing_up:  45%|████▌     | 9046/20010 [7:18:23<8:01:20,  2.63s/batch]Batch 9000/20010 Done, mean position loss: 24.352240738868712\n",
      "Training growing_up:  45%|████▌     | 9036/20010 [7:18:50<8:28:07,  2.78s/batch]Batch 9100/20010 Done, mean position loss: 24.84347758769989\n",
      "Training growing_up:  45%|████▌     | 9100/20010 [7:18:53<7:57:41,  2.63s/batch]Batch 8900/20010 Done, mean position loss: 24.218203818798067\n",
      "Training growing_up:  45%|████▌     | 9043/20010 [7:18:56<8:30:08,  2.79s/batch]Batch 9100/20010 Done, mean position loss: 24.102547466754913\n",
      "Training growing_up:  45%|████▌     | 9100/20010 [7:18:57<7:24:35,  2.45s/batch]Batch 9100/20010 Done, mean position loss: 24.12661185979843\n",
      "Training growing_up:  45%|████▍     | 8970/20010 [7:19:00<8:47:08,  2.86s/batch]Batch 9100/20010 Done, mean position loss: 24.66735229730606\n",
      "Training growing_up:  45%|████▌     | 9102/20010 [7:19:03<7:55:26,  2.62s/batch]Batch 9100/20010 Done, mean position loss: 24.328631598949432\n",
      "Training growing_up:  45%|████▌     | 9104/20010 [7:19:08<7:58:53,  2.63s/batch]Batch 9200/20010 Done, mean position loss: 23.849146988391873\n",
      "Training growing_up:  46%|████▌     | 9133/20010 [7:19:14<8:06:56,  2.69s/batch]Batch 9100/20010 Done, mean position loss: 24.50141074180603\n",
      "Training growing_up:  46%|████▌     | 9112/20010 [7:19:28<6:09:21,  2.03s/batch]Batch 9100/20010 Done, mean position loss: 24.80768584728241\n",
      "Training growing_up:  45%|████▌     | 9074/20010 [7:19:36<8:04:57,  2.66s/batch]Batch 9100/20010 Done, mean position loss: 24.928593904972075\n",
      "Training growing_up:  45%|████▌     | 9077/20010 [7:19:44<8:36:33,  2.83s/batch]Batch 8900/20010 Done, mean position loss: 23.937756192684173\n",
      "Training growing_up:  46%|████▋     | 9258/20010 [7:19:47<7:27:03,  2.49s/batch]Batch 9200/20010 Done, mean position loss: 23.437380058765413\n",
      "Training growing_up:  45%|████▍     | 8973/20010 [7:19:59<7:45:05,  2.53s/batch]Batch 9100/20010 Done, mean position loss: 23.322444825172425\n",
      "Training growing_up:  46%|████▌     | 9133/20010 [7:20:16<7:40:58,  2.54s/batch]Batch 9100/20010 Done, mean position loss: 24.753961946964264\n",
      "Training growing_up:  45%|████▌     | 9090/20010 [7:20:18<7:48:06,  2.57s/batch]Batch 9000/20010 Done, mean position loss: 24.264618833065036\n",
      "Training growing_up:  46%|████▌     | 9134/20010 [7:20:21<7:08:18,  2.36s/batch]Batch 9200/20010 Done, mean position loss: 24.754593613147733\n",
      "Training growing_up:  46%|████▌     | 9139/20010 [7:20:34<8:03:23,  2.67s/batch]Batch 9100/20010 Done, mean position loss: 24.292926774024963\n",
      "Training growing_up:  45%|████▌     | 9090/20010 [7:20:50<7:57:14,  2.62s/batch]Batch 9100/20010 Done, mean position loss: 24.189350090026856\n",
      "Training growing_up:  46%|████▌     | 9177/20010 [7:20:59<7:01:43,  2.34s/batch]Batch 9200/20010 Done, mean position loss: 24.325898277759553\n",
      "Training growing_up:  45%|████▌     | 9097/20010 [7:21:05<7:48:27,  2.58s/batch]Batch 9200/20010 Done, mean position loss: 23.549849474430083\n",
      "Training growing_up:  46%|████▋     | 9291/20010 [7:21:09<6:41:29,  2.25s/batch]Batch 9000/20010 Done, mean position loss: 25.01342215776444\n",
      "Training growing_up:  45%|████▍     | 8971/20010 [7:21:13<7:54:31,  2.58s/batch]Batch 9100/20010 Done, mean position loss: 24.59835000753403\n",
      "Training growing_up:  46%|████▌     | 9176/20010 [7:21:19<8:00:09,  2.66s/batch]Batch 9100/20010 Done, mean position loss: 24.64244706630707\n",
      "Training growing_up:  46%|████▌     | 9145/20010 [7:21:23<7:23:26,  2.45s/batch]Batch 9100/20010 Done, mean position loss: 24.29139524936676\n",
      "Training growing_up:  45%|████▌     | 9080/20010 [7:21:35<8:10:47,  2.69s/batch]Batch 9300/20010 Done, mean position loss: 23.105679514408113\n",
      "Training growing_up:  45%|████▍     | 8944/20010 [7:21:36<8:03:58,  2.62s/batch]Batch 9100/20010 Done, mean position loss: 24.464599642753598\n",
      "Training growing_up:  46%|████▌     | 9132/20010 [7:21:53<8:01:10,  2.65s/batch]Batch 9200/20010 Done, mean position loss: 23.46918672800064\n",
      "Training growing_up:  46%|████▌     | 9127/20010 [7:21:58<7:18:59,  2.42s/batch]Batch 9200/20010 Done, mean position loss: 24.55694866657257\n",
      "Training growing_up:  46%|████▋     | 9256/20010 [7:22:06<7:11:44,  2.41s/batch]Batch 9200/20010 Done, mean position loss: 23.92750314474106\n",
      "Training growing_up:  46%|████▌     | 9178/20010 [7:22:09<6:59:03,  2.32s/batch]Batch 9100/20010 Done, mean position loss: 24.514237616062168\n",
      "Training growing_up:  46%|████▌     | 9167/20010 [7:22:17<7:12:21,  2.39s/batch]Batch 9200/20010 Done, mean position loss: 23.77628923177719\n",
      "Training growing_up:  45%|████▌     | 9094/20010 [7:22:21<8:05:38,  2.67s/batch]Batch 9200/20010 Done, mean position loss: 23.509732329845427\n",
      "Training growing_up:  45%|████▌     | 9095/20010 [7:22:23<8:10:31,  2.70s/batch]Batch 9100/20010 Done, mean position loss: 23.810676863193514\n",
      "Training growing_up:  45%|████▌     | 9101/20010 [7:22:24<8:23:06,  2.77s/batch]Batch 9000/20010 Done, mean position loss: 24.225190508365632\n",
      "Training growing_up:  46%|████▌     | 9147/20010 [7:22:27<6:14:42,  2.07s/batch]Batch 9100/20010 Done, mean position loss: 25.433121004104613\n",
      "Training growing_up:  46%|████▌     | 9184/20010 [7:22:30<6:18:31,  2.10s/batch]Batch 9000/20010 Done, mean position loss: 23.61081690311432\n",
      "Training growing_up:  45%|████▍     | 9001/20010 [7:22:30<6:52:14,  2.25s/batch]Batch 9000/20010 Done, mean position loss: 24.27380877971649\n",
      "Training growing_up:  46%|████▌     | 9143/20010 [7:22:38<6:54:23,  2.29s/batch]Batch 9100/20010 Done, mean position loss: 23.701566581726077\n",
      "Training growing_up:  45%|████▌     | 9005/20010 [7:22:39<6:38:04,  2.17s/batch]Batch 9100/20010 Done, mean position loss: 24.61406529903412\n",
      "Training growing_up:  46%|████▌     | 9126/20010 [7:22:39<7:28:32,  2.47s/batch]Batch 9100/20010 Done, mean position loss: 24.02032062768936\n",
      "Training growing_up:  46%|████▌     | 9249/20010 [7:22:59<7:30:21,  2.51s/batch]Batch 9200/20010 Done, mean position loss: 24.608472657203677\n",
      "Training growing_up:  46%|████▌     | 9153/20010 [7:23:01<6:40:18,  2.21s/batch]Batch 9200/20010 Done, mean position loss: 24.52647784948349\n",
      "Training growing_up:  46%|████▌     | 9111/20010 [7:23:04<7:32:41,  2.49s/batch]Batch 9200/20010 Done, mean position loss: 24.021781396865844\n",
      "Training growing_up:  46%|████▌     | 9119/20010 [7:23:12<6:55:42,  2.29s/batch]Batch 9200/20010 Done, mean position loss: 24.027265853881836\n",
      "Training growing_up:  46%|████▋     | 9298/20010 [7:23:13<7:21:59,  2.48s/batch]Batch 9200/20010 Done, mean position loss: 25.40167768239975\n",
      "Training growing_up:  46%|████▌     | 9207/20010 [7:23:13<7:18:41,  2.44s/batch]Batch 9000/20010 Done, mean position loss: 23.68031347513199\n",
      "Training growing_up:  45%|████▌     | 9026/20010 [7:23:20<6:28:10,  2.12s/batch]Batch 9300/20010 Done, mean position loss: 24.23524078130722\n",
      "Training growing_up:  46%|████▌     | 9234/20010 [7:23:26<6:26:31,  2.15s/batch]Batch 9200/20010 Done, mean position loss: 23.749958550930025\n",
      "Training growing_up:  46%|████▋     | 9261/20010 [7:23:36<7:52:45,  2.64s/batch]Batch 9200/20010 Done, mean position loss: 24.45393690109253\n",
      "Training growing_up:  46%|████▌     | 9126/20010 [7:23:44<7:47:06,  2.58s/batch]Batch 9200/20010 Done, mean position loss: 24.29398068189621\n",
      "Training growing_up:  46%|████▌     | 9144/20010 [7:23:56<7:38:56,  2.53s/batch]Batch 9000/20010 Done, mean position loss: 23.469558265209198\n",
      "Training growing_up:  46%|████▌     | 9220/20010 [7:24:00<6:49:51,  2.28s/batch]Batch 9300/20010 Done, mean position loss: 23.4071301651001\n",
      "Training growing_up:  46%|████▌     | 9136/20010 [7:24:07<7:27:37,  2.47s/batch]Batch 9200/20010 Done, mean position loss: 23.3362767124176\n",
      "Training growing_up:  46%|████▌     | 9251/20010 [7:24:23<6:44:42,  2.26s/batch]Batch 9200/20010 Done, mean position loss: 24.752987923622133\n",
      "Training growing_up:  46%|████▋     | 9284/20010 [7:24:30<6:23:12,  2.14s/batch]Batch 9300/20010 Done, mean position loss: 24.750333845615387\n",
      "Training growing_up:  46%|████▌     | 9227/20010 [7:24:36<6:38:43,  2.22s/batch]Batch 9100/20010 Done, mean position loss: 23.646844489574434\n",
      "Training growing_up:  45%|████▌     | 9104/20010 [7:24:44<7:56:39,  2.62s/batch]Batch 9200/20010 Done, mean position loss: 24.352869400978086\n",
      "Training growing_up:  47%|████▋     | 9340/20010 [7:24:52<5:31:15,  1.86s/batch]Batch 9200/20010 Done, mean position loss: 24.724610605239867\n",
      "Training growing_up:  46%|████▌     | 9172/20010 [7:25:05<8:40:37,  2.88s/batch]Batch 9300/20010 Done, mean position loss: 23.92853849649429\n",
      "Training growing_up:  47%|████▋     | 9348/20010 [7:25:13<7:23:16,  2.49s/batch]Batch 9300/20010 Done, mean position loss: 23.501006646156313\n",
      "Training growing_up:  46%|████▌     | 9172/20010 [7:25:26<7:11:05,  2.39s/batch]Batch 9200/20010 Done, mean position loss: 24.519993484020233\n",
      "Training growing_up:  46%|████▌     | 9199/20010 [7:25:29<7:05:32,  2.36s/batch]Batch 9100/20010 Done, mean position loss: 24.958209991455078\n",
      "Training growing_up:  46%|████▌     | 9217/20010 [7:25:33<6:57:38,  2.32s/batch]Batch 9200/20010 Done, mean position loss: 24.11557083129883\n",
      "Training growing_up:  47%|████▋     | 9310/20010 [7:25:34<7:00:07,  2.36s/batch]Batch 9200/20010 Done, mean position loss: 24.298154525756836\n",
      "Training growing_up:  45%|████▌     | 9082/20010 [7:25:40<8:11:29,  2.70s/batch]Batch 9400/20010 Done, mean position loss: 23.12851548433304\n",
      "Training growing_up:  46%|████▌     | 9176/20010 [7:25:44<7:12:19,  2.39s/batch]Batch 9200/20010 Done, mean position loss: 24.848796083927155\n",
      "Training growing_up:  47%|████▋     | 9351/20010 [7:25:54<6:20:33,  2.14s/batch]Batch 9300/20010 Done, mean position loss: 23.465154075622557\n",
      "Training growing_up:  45%|████▌     | 9088/20010 [7:26:02<7:29:08,  2.47s/batch]Batch 9300/20010 Done, mean position loss: 24.306102488040924\n",
      "Training growing_up:  45%|████▌     | 9093/20010 [7:26:14<7:01:05,  2.31s/batch]Batch 9200/20010 Done, mean position loss: 24.50398988485336\n",
      "Training growing_up:  47%|████▋     | 9360/20010 [7:26:14<6:19:57,  2.14s/batch]Batch 9300/20010 Done, mean position loss: 23.802094368934632\n",
      "Training growing_up:  46%|████▌     | 9124/20010 [7:26:26<7:18:55,  2.42s/batch]Batch 9100/20010 Done, mean position loss: 23.81100074529648\n",
      "Batch 9300/20010 Done, mean position loss: 23.908521456718447\n",
      "Training growing_up:  47%|████▋     | 9379/20010 [7:26:28<7:16:32,  2.46s/batch]Batch 9300/20010 Done, mean position loss: 23.508750555515288\n",
      "Training growing_up:  46%|████▌     | 9225/20010 [7:26:30<7:18:39,  2.44s/batch]Batch 9200/20010 Done, mean position loss: 23.555340893268585\n",
      "Training growing_up:  47%|████▋     | 9309/20010 [7:26:33<7:19:28,  2.46s/batch]Batch 9100/20010 Done, mean position loss: 23.281442534923553\n",
      "Training growing_up:  45%|████▌     | 9063/20010 [7:26:35<7:30:00,  2.47s/batch]Batch 9200/20010 Done, mean position loss: 25.477424912452697\n",
      "Training growing_up:  46%|████▋     | 9285/20010 [7:26:38<7:07:46,  2.39s/batch]Batch 9100/20010 Done, mean position loss: 24.246290657520294\n",
      "Training growing_up:  46%|████▌     | 9247/20010 [7:26:45<7:29:49,  2.51s/batch]Batch 9200/20010 Done, mean position loss: 23.69333348751068\n",
      "Training growing_up:  46%|████▌     | 9216/20010 [7:26:49<7:26:09,  2.48s/batch]Batch 9200/20010 Done, mean position loss: 23.749597511291505\n",
      "Training growing_up:  46%|████▌     | 9106/20010 [7:26:50<7:40:23,  2.53s/batch]Batch 9200/20010 Done, mean position loss: 24.387851977348326\n",
      "Training growing_up:  47%|████▋     | 9329/20010 [7:26:58<6:39:11,  2.24s/batch]Batch 9300/20010 Done, mean position loss: 24.35390301465988\n",
      "Training growing_up:  47%|████▋     | 9352/20010 [7:27:04<6:46:53,  2.29s/batch]Batch 9300/20010 Done, mean position loss: 24.004519410133362\n",
      "Training growing_up:  47%|████▋     | 9395/20010 [7:27:05<6:40:51,  2.27s/batch]Batch 9300/20010 Done, mean position loss: 25.580878298282624\n",
      "Training growing_up:  47%|████▋     | 9306/20010 [7:27:16<7:27:59,  2.51s/batch]Batch 9300/20010 Done, mean position loss: 24.1276921582222\n",
      "Training growing_up:  47%|████▋     | 9369/20010 [7:27:19<7:53:32,  2.67s/batch]Batch 9300/20010 Done, mean position loss: 24.752898485660552\n",
      "Training growing_up:  46%|████▌     | 9216/20010 [7:27:19<6:35:28,  2.20s/batch]Batch 9400/20010 Done, mean position loss: 24.19586381435394\n",
      "Training growing_up:  46%|████▌     | 9251/20010 [7:27:24<7:16:05,  2.43s/batch]Batch 9100/20010 Done, mean position loss: 23.61396370649338\n",
      "Training growing_up:  46%|████▌     | 9128/20010 [7:27:31<7:13:46,  2.39s/batch]Batch 9300/20010 Done, mean position loss: 23.439762973785403\n",
      "Training growing_up:  47%|████▋     | 9307/20010 [7:27:34<7:33:01,  2.54s/batch]Batch 9300/20010 Done, mean position loss: 24.87471388101578\n",
      "Training growing_up:  46%|████▋     | 9287/20010 [7:27:46<7:39:49,  2.57s/batch]Batch 9300/20010 Done, mean position loss: 24.282051322460177\n",
      "Training growing_up:  47%|████▋     | 9321/20010 [7:27:53<7:17:49,  2.46s/batch]Batch 9400/20010 Done, mean position loss: 24.036086649894713\n",
      "Training growing_up:  46%|████▌     | 9139/20010 [7:28:10<7:13:33,  2.39s/batch]Batch 9100/20010 Done, mean position loss: 23.664274084568024\n",
      "Training growing_up:  47%|████▋     | 9381/20010 [7:28:12<6:58:59,  2.37s/batch]Batch 9300/20010 Done, mean position loss: 23.512295317649844\n",
      "Training growing_up:  47%|████▋     | 9384/20010 [7:28:19<6:49:53,  2.31s/batch]Batch 9300/20010 Done, mean position loss: 24.952252418994902\n",
      "Training growing_up:  46%|████▌     | 9151/20010 [7:28:34<7:32:53,  2.50s/batch]Batch 9400/20010 Done, mean position loss: 25.016888015270233\n",
      "Training growing_up:  47%|████▋     | 9396/20010 [7:28:47<7:13:51,  2.45s/batch]Batch 9200/20010 Done, mean position loss: 24.134418165683748\n",
      "Training growing_up:  46%|████▌     | 9250/20010 [7:28:49<6:56:06,  2.32s/batch]Batch 9300/20010 Done, mean position loss: 24.287257664203644\n",
      "Training growing_up:  47%|████▋     | 9411/20010 [7:28:58<6:42:13,  2.28s/batch]Batch 9300/20010 Done, mean position loss: 24.217896144390107\n",
      "Training growing_up:  47%|████▋     | 9444/20010 [7:28:59<6:46:49,  2.31s/batch]Batch 9400/20010 Done, mean position loss: 23.378586494922637\n",
      "Training growing_up:  46%|████▋     | 9268/20010 [7:29:15<7:42:47,  2.58s/batch]Batch 9400/20010 Done, mean position loss: 23.38522442340851\n",
      "Training growing_up:  47%|████▋     | 9311/20010 [7:29:22<7:06:46,  2.39s/batch]Batch 9300/20010 Done, mean position loss: 24.25616553068161\n",
      "Training growing_up:  47%|████▋     | 9442/20010 [7:29:32<7:08:17,  2.43s/batch]Batch 9200/20010 Done, mean position loss: 24.98356686592102\n",
      "Training growing_up:  46%|████▌     | 9155/20010 [7:29:36<7:07:47,  2.36s/batch]Batch 9500/20010 Done, mean position loss: 23.211767942905425\n",
      "Training growing_up:  47%|████▋     | 9501/20010 [7:29:36<7:14:28,  2.48s/batch]Batch 9300/20010 Done, mean position loss: 23.914415714740752\n",
      "Training growing_up:  47%|████▋     | 9349/20010 [7:29:40<8:20:43,  2.82s/batch]Batch 9300/20010 Done, mean position loss: 24.032645359039307\n",
      "Training growing_up:  46%|████▌     | 9207/20010 [7:29:47<7:15:32,  2.42s/batch]Batch 9300/20010 Done, mean position loss: 24.377819373607636\n",
      "Training growing_up:  47%|████▋     | 9423/20010 [7:29:48<7:02:15,  2.39s/batch]Batch 9400/20010 Done, mean position loss: 23.51846561431885\n",
      "Training growing_up:  47%|████▋     | 9363/20010 [7:30:01<7:45:00,  2.62s/batch]Batch 9400/20010 Done, mean position loss: 24.64775673151016\n",
      "Training growing_up:  46%|████▌     | 9152/20010 [7:30:15<7:32:29,  2.50s/batch]Batch 9300/20010 Done, mean position loss: 24.923809673786163\n",
      "Training growing_up:  47%|████▋     | 9377/20010 [7:30:18<6:55:10,  2.34s/batch]Batch 9400/20010 Done, mean position loss: 23.7949694108963\n",
      "Training growing_up:  47%|████▋     | 9439/20010 [7:30:28<7:56:49,  2.71s/batch]Batch 9400/20010 Done, mean position loss: 23.953748066425327\n",
      "Training growing_up:  47%|████▋     | 9412/20010 [7:30:31<7:32:07,  2.56s/batch]Batch 9400/20010 Done, mean position loss: 23.467430198192595\n",
      "Training growing_up:  46%|████▌     | 9243/20010 [7:30:31<8:14:46,  2.76s/batch]Batch 9200/20010 Done, mean position loss: 23.917259647846222\n",
      "Training growing_up:  46%|████▋     | 9295/20010 [7:30:34<8:14:20,  2.77s/batch]Batch 9300/20010 Done, mean position loss: 24.083743460178376\n",
      "Training growing_up:  47%|████▋     | 9404/20010 [7:30:37<7:57:58,  2.70s/batch]Batch 9300/20010 Done, mean position loss: 24.90882254123688\n",
      "Training growing_up:  47%|████▋     | 9391/20010 [7:30:37<7:39:44,  2.60s/batch]Batch 9200/20010 Done, mean position loss: 23.138507664203644\n",
      "Training growing_up:  47%|████▋     | 9361/20010 [7:30:43<7:19:00,  2.47s/batch]Batch 9200/20010 Done, mean position loss: 23.93096168756485\n",
      "Training growing_up:  47%|████▋     | 9381/20010 [7:30:48<8:43:08,  2.95s/batch]Batch 9300/20010 Done, mean position loss: 23.52389450073242\n",
      "Training growing_up:  47%|████▋     | 9489/20010 [7:30:54<7:40:46,  2.63s/batch]Batch 9300/20010 Done, mean position loss: 23.707136926651\n",
      "Training growing_up:  47%|████▋     | 9350/20010 [7:31:02<8:16:47,  2.80s/batch]Batch 9300/20010 Done, mean position loss: 24.17276793718338\n",
      "Training growing_up:  47%|████▋     | 9331/20010 [7:31:04<8:23:01,  2.83s/batch]Batch 9400/20010 Done, mean position loss: 24.16629878282547\n",
      "Training growing_up:  47%|████▋     | 9427/20010 [7:31:08<7:17:09,  2.48s/batch]Batch 9400/20010 Done, mean position loss: 23.79523414373398\n",
      "Batch 9400/20010 Done, mean position loss: 24.535881369113923\n",
      "Training growing_up:  46%|████▌     | 9242/20010 [7:31:17<7:18:31,  2.44s/batch]Batch 9400/20010 Done, mean position loss: 24.841817467212678\n",
      "Training growing_up:  47%|████▋     | 9343/20010 [7:31:23<8:32:05,  2.88s/batch]Batch 9400/20010 Done, mean position loss: 24.740336728096008\n",
      "Training growing_up:  47%|████▋     | 9358/20010 [7:31:23<8:23:02,  2.83s/batch]Batch 9500/20010 Done, mean position loss: 23.912936093807218\n",
      "Training growing_up:  47%|████▋     | 9326/20010 [7:31:36<6:50:31,  2.31s/batch]Batch 9200/20010 Done, mean position loss: 23.945248332023624\n",
      "Training growing_up:  47%|████▋     | 9367/20010 [7:31:38<7:32:04,  2.55s/batch]Batch 9400/20010 Done, mean position loss: 23.617775559425354\n",
      "Training growing_up:  47%|████▋     | 9383/20010 [7:31:37<7:46:12,  2.63s/batch]Batch 9400/20010 Done, mean position loss: 24.4555517244339\n",
      "Training growing_up:  47%|████▋     | 9354/20010 [7:31:56<7:48:00,  2.64s/batch]Batch 9500/20010 Done, mean position loss: 23.23812242269516\n",
      "Training growing_up:  47%|████▋     | 9442/20010 [7:32:01<7:41:15,  2.62s/batch]Batch 9400/20010 Done, mean position loss: 23.992264487743377\n",
      "Training growing_up:  47%|████▋     | 9367/20010 [7:32:20<6:45:37,  2.29s/batch]Batch 9400/20010 Done, mean position loss: 24.68210049152374\n",
      "Training growing_up:  47%|████▋     | 9368/20010 [7:32:23<7:20:18,  2.48s/batch]Batch 9200/20010 Done, mean position loss: 23.496918001174926\n",
      "Training growing_up:  47%|████▋     | 9383/20010 [7:32:23<7:05:48,  2.40s/batch]Batch 9400/20010 Done, mean position loss: 23.514251186847684\n",
      "Training growing_up:  47%|████▋     | 9492/20010 [7:32:40<7:08:18,  2.44s/batch]Batch 9500/20010 Done, mean position loss: 24.67588493347168\n",
      "Training growing_up:  47%|████▋     | 9444/20010 [7:33:03<7:02:06,  2.40s/batch]Batch 9500/20010 Done, mean position loss: 23.63300174236298\n",
      "Batch 9300/20010 Done, mean position loss: 23.798171441555024\n",
      "Training growing_up:  46%|████▋     | 9301/20010 [7:33:03<8:41:54,  2.92s/batch]Batch 9400/20010 Done, mean position loss: 24.325743701457977\n",
      "Training growing_up:  47%|████▋     | 9348/20010 [7:33:08<8:07:31,  2.74s/batch]Batch 9400/20010 Done, mean position loss: 23.928757441043853\n",
      "Training growing_up:  47%|████▋     | 9405/20010 [7:33:18<7:30:46,  2.55s/batch]Batch 9500/20010 Done, mean position loss: 23.526736433506013\n",
      "Training growing_up:  46%|████▌     | 9249/20010 [7:33:34<6:58:08,  2.33s/batch]Batch 9400/20010 Done, mean position loss: 24.880799658298493\n",
      "Training growing_up:  47%|████▋     | 9440/20010 [7:33:46<7:48:17,  2.66s/batch]Batch 9400/20010 Done, mean position loss: 24.39413609027863\n",
      "Training growing_up:  47%|████▋     | 9467/20010 [7:33:47<7:40:15,  2.62s/batch]Batch 9300/20010 Done, mean position loss: 24.920133492946626\n",
      "Training growing_up:  48%|████▊     | 9560/20010 [7:33:51<7:57:18,  2.74s/batch]Batch 9600/20010 Done, mean position loss: 23.416886982917788\n",
      "Training growing_up:  47%|████▋     | 9468/20010 [7:33:58<7:23:15,  2.52s/batch]Batch 9400/20010 Done, mean position loss: 23.99244952440262\n",
      "Training growing_up:  47%|████▋     | 9491/20010 [7:34:01<7:05:22,  2.43s/batch]Batch 9500/20010 Done, mean position loss: 23.210417218208313\n",
      "Training growing_up:  47%|████▋     | 9403/20010 [7:34:03<8:19:52,  2.83s/batch]Batch 9400/20010 Done, mean position loss: 23.951892561912537\n",
      "Training growing_up:  48%|████▊     | 9543/20010 [7:34:24<6:47:46,  2.34s/batch]Batch 9500/20010 Done, mean position loss: 24.4965217423439\n",
      "Training growing_up:  48%|████▊     | 9527/20010 [7:34:25<7:09:04,  2.46s/batch]Batch 9500/20010 Done, mean position loss: 23.914421072006228\n",
      "Training growing_up:  47%|████▋     | 9475/20010 [7:34:26<7:27:32,  2.55s/batch]Batch 9400/20010 Done, mean position loss: 24.981065475940703\n",
      "Training growing_up:  47%|████▋     | 9473/20010 [7:34:40<8:01:46,  2.74s/batch]Batch 9500/20010 Done, mean position loss: 23.305508255958557\n",
      "Training growing_up:  47%|████▋     | 9420/20010 [7:34:43<7:11:19,  2.44s/batch]Batch 9500/20010 Done, mean position loss: 23.542557961940766\n",
      "Training growing_up:  47%|████▋     | 9503/20010 [7:34:46<6:55:00,  2.37s/batch]Batch 9400/20010 Done, mean position loss: 25.099360089302063\n",
      "Training growing_up:  47%|████▋     | 9456/20010 [7:34:48<8:56:29,  3.05s/batch]Batch 9300/20010 Done, mean position loss: 23.481541061401366\n",
      "Training growing_up:  46%|████▋     | 9278/20010 [7:34:48<7:01:41,  2.36s/batch]Batch 9400/20010 Done, mean position loss: 23.662239871025086\n",
      "Training growing_up:  47%|████▋     | 9396/20010 [7:34:50<7:19:22,  2.48s/batch]Batch 9300/20010 Done, mean position loss: 24.194735107421874\n",
      "Training growing_up:  47%|████▋     | 9391/20010 [7:34:56<7:39:40,  2.60s/batch]Batch 9300/20010 Done, mean position loss: 23.182446618080135\n",
      "Training growing_up:  47%|████▋     | 9489/20010 [7:35:03<7:45:08,  2.65s/batch]Batch 9400/20010 Done, mean position loss: 23.939377002716064\n",
      "Training growing_up:  47%|████▋     | 9463/20010 [7:35:07<8:13:25,  2.81s/batch]Batch 9400/20010 Done, mean position loss: 23.931647644042968\n",
      "Training growing_up:  48%|████▊     | 9520/20010 [7:35:12<7:42:14,  2.64s/batch]Batch 9500/20010 Done, mean position loss: 24.327287554740906\n",
      "Training growing_up:  47%|████▋     | 9455/20010 [7:35:19<7:01:47,  2.40s/batch]Batch 9500/20010 Done, mean position loss: 23.947438983917237\n",
      "Training growing_up:  46%|████▋     | 9290/20010 [7:35:20<7:55:04,  2.66s/batch]Batch 9500/20010 Done, mean position loss: 24.553404512405393\n",
      "Training growing_up:  47%|████▋     | 9314/20010 [7:35:22<7:20:15,  2.47s/batch]Batch 9400/20010 Done, mean position loss: 24.06202448606491\n",
      "Training growing_up:  48%|████▊     | 9552/20010 [7:35:25<7:19:26,  2.52s/batch]Batch 9500/20010 Done, mean position loss: 24.40852326631546\n",
      "Training growing_up:  48%|████▊     | 9528/20010 [7:35:31<7:00:13,  2.41s/batch]Batch 9600/20010 Done, mean position loss: 23.769222569465636\n",
      "Training growing_up:  48%|████▊     | 9522/20010 [7:35:34<7:08:25,  2.45s/batch]Batch 9500/20010 Done, mean position loss: 24.49840867757797\n",
      "Training growing_up:  47%|████▋     | 9480/20010 [7:35:49<7:13:47,  2.47s/batch]Batch 9500/20010 Done, mean position loss: 24.315617282390594\n",
      "Training growing_up:  47%|████▋     | 9323/20010 [7:35:50<7:34:56,  2.55s/batch]Batch 9300/20010 Done, mean position loss: 23.545310893058776\n",
      "Training growing_up:  47%|████▋     | 9446/20010 [7:35:51<7:09:25,  2.44s/batch]Batch 9500/20010 Done, mean position loss: 23.97433348417282\n",
      "Training growing_up:  47%|████▋     | 9453/20010 [7:36:08<7:19:28,  2.50s/batch]Batch 9600/20010 Done, mean position loss: 23.38914973974228\n",
      "Training growing_up:  48%|████▊     | 9607/20010 [7:36:23<7:08:42,  2.47s/batch]Batch 9500/20010 Done, mean position loss: 24.064604148864746\n",
      "Training growing_up:  47%|████▋     | 9460/20010 [7:36:27<7:37:07,  2.60s/batch]Batch 9500/20010 Done, mean position loss: 24.834642465114595\n",
      "Training growing_up:  48%|████▊     | 9552/20010 [7:36:35<6:36:23,  2.27s/batch]Batch 9300/20010 Done, mean position loss: 23.75059318065643\n",
      "Training growing_up:  48%|████▊     | 9508/20010 [7:36:41<7:26:30,  2.55s/batch]Batch 9500/20010 Done, mean position loss: 23.322441771030427\n",
      "Training growing_up:  47%|████▋     | 9349/20010 [7:36:49<7:27:23,  2.52s/batch]Batch 9600/20010 Done, mean position loss: 24.645832977294923\n",
      "Training growing_up:  48%|████▊     | 9532/20010 [7:37:08<7:02:59,  2.42s/batch]Batch 9600/20010 Done, mean position loss: 23.507819638252258\n",
      "Training growing_up:  47%|████▋     | 9474/20010 [7:37:10<6:20:15,  2.17s/batch]Batch 9500/20010 Done, mean position loss: 24.245738565921783\n",
      "Training growing_up:  47%|████▋     | 9383/20010 [7:37:16<8:44:50,  2.96s/batch]Batch 9500/20010 Done, mean position loss: 24.223216779232025\n",
      "Training growing_up:  47%|████▋     | 9363/20010 [7:37:20<7:09:21,  2.42s/batch]Batch 9400/20010 Done, mean position loss: 24.081115279197697\n",
      "Training growing_up:  47%|████▋     | 9387/20010 [7:37:26<8:15:21,  2.80s/batch]Batch 9600/20010 Done, mean position loss: 23.052663767337798\n",
      "Training growing_up:  47%|████▋     | 9468/20010 [7:37:50<5:59:44,  2.05s/batch]Batch 9500/20010 Done, mean position loss: 24.18284779071808\n",
      "Training growing_up:  48%|████▊     | 9532/20010 [7:37:52<5:57:02,  2.04s/batch]Batch 9500/20010 Done, mean position loss: 24.80847011566162\n",
      "Training growing_up:  48%|████▊     | 9579/20010 [7:38:00<7:22:52,  2.55s/batch]Batch 9700/20010 Done, mean position loss: 23.06755008220673\n",
      "Training growing_up:  47%|████▋     | 9381/20010 [7:38:02<7:26:17,  2.52s/batch]Batch 9400/20010 Done, mean position loss: 25.213535406589507\n",
      "Training growing_up:  48%|████▊     | 9553/20010 [7:38:05<7:23:43,  2.55s/batch]Batch 9500/20010 Done, mean position loss: 24.124828050136564\n",
      "Training growing_up:  48%|████▊     | 9566/20010 [7:38:14<6:26:58,  2.22s/batch]Batch 9500/20010 Done, mean position loss: 24.117266566753386\n",
      "Training growing_up:  48%|████▊     | 9546/20010 [7:38:15<7:12:35,  2.48s/batch]Batch 9600/20010 Done, mean position loss: 22.93747338294983\n",
      "Training growing_up:  48%|████▊     | 9579/20010 [7:38:32<8:19:02,  2.87s/batch]Batch 9500/20010 Done, mean position loss: 24.764941620826722\n",
      "Training growing_up:  48%|████▊     | 9583/20010 [7:38:32<8:04:37,  2.79s/batch]Batch 9600/20010 Done, mean position loss: 24.84391727685928\n",
      "Training growing_up:  48%|████▊     | 9553/20010 [7:38:40<8:13:59,  2.83s/batch]Batch 9600/20010 Done, mean position loss: 24.17261433362961\n",
      "Training growing_up:  47%|████▋     | 9369/20010 [7:38:49<7:23:08,  2.50s/batch]Batch 9400/20010 Done, mean position loss: 23.62337421655655\n",
      "Training growing_up:  49%|████▊     | 9725/20010 [7:38:52<6:15:00,  2.19s/batch]Batch 9400/20010 Done, mean position loss: 23.8885693693161\n",
      "Training growing_up:  48%|████▊     | 9521/20010 [7:38:53<7:18:45,  2.51s/batch]Batch 9500/20010 Done, mean position loss: 23.96235425710678\n",
      "Training growing_up:  47%|████▋     | 9422/20010 [7:38:55<7:07:39,  2.42s/batch]Batch 9600/20010 Done, mean position loss: 23.6105251121521\n",
      "Training growing_up:  48%|████▊     | 9595/20010 [7:38:57<6:09:16,  2.13s/batch]Batch 9600/20010 Done, mean position loss: 23.559126098155975\n",
      "Training growing_up:  48%|████▊     | 9612/20010 [7:39:04<6:50:15,  2.37s/batch]Batch 9500/20010 Done, mean position loss: 25.176467876434323\n",
      "Training growing_up:  48%|████▊     | 9598/20010 [7:39:04<6:43:11,  2.32s/batch]Batch 9400/20010 Done, mean position loss: 23.561871984004974\n",
      "Training growing_up:  48%|████▊     | 9644/20010 [7:39:10<6:41:18,  2.32s/batch]Batch 9500/20010 Done, mean position loss: 24.04817432165146\n",
      "Training growing_up:  48%|████▊     | 9596/20010 [7:39:11<6:55:59,  2.40s/batch]Batch 9600/20010 Done, mean position loss: 24.327687487602233\n",
      "Training growing_up:  47%|████▋     | 9497/20010 [7:39:16<6:30:09,  2.23s/batch]Batch 9500/20010 Done, mean position loss: 23.981594936847685\n",
      "Training growing_up:  47%|████▋     | 9504/20010 [7:39:23<6:47:33,  2.33s/batch]Batch 9600/20010 Done, mean position loss: 24.30134313106537\n",
      "Training growing_up:  48%|████▊     | 9531/20010 [7:39:23<7:19:15,  2.52s/batch]Batch 9600/20010 Done, mean position loss: 24.134754235744474\n",
      "Training growing_up:  48%|████▊     | 9660/20010 [7:39:25<6:28:19,  2.25s/batch]Batch 9600/20010 Done, mean position loss: 24.275018825531006\n",
      "Training growing_up:  48%|████▊     | 9632/20010 [7:39:25<7:05:37,  2.46s/batch]Batch 9500/20010 Done, mean position loss: 24.448505101203917\n",
      "Training growing_up:  48%|████▊     | 9603/20010 [7:39:30<7:26:03,  2.57s/batch]Batch 9700/20010 Done, mean position loss: 23.79160523176193\n",
      "Training growing_up:  47%|████▋     | 9420/20010 [7:39:39<7:06:34,  2.42s/batch]Batch 9600/20010 Done, mean position loss: 24.50489554166794\n",
      "Training growing_up:  47%|████▋     | 9428/20010 [7:39:55<6:48:05,  2.31s/batch]Batch 9600/20010 Done, mean position loss: 24.126313540935516\n",
      "Training growing_up:  49%|████▉     | 9755/20010 [7:40:00<7:11:43,  2.53s/batch]Batch 9600/20010 Done, mean position loss: 23.74684941530228\n",
      "Training growing_up:  47%|████▋     | 9432/20010 [7:40:06<7:45:56,  2.64s/batch]Batch 9400/20010 Done, mean position loss: 23.397102978229523\n",
      "Training growing_up:  48%|████▊     | 9617/20010 [7:40:15<7:11:07,  2.49s/batch]Batch 9700/20010 Done, mean position loss: 23.607844412326813\n",
      "Training growing_up:  48%|████▊     | 9612/20010 [7:40:22<7:19:04,  2.53s/batch]Batch 9600/20010 Done, mean position loss: 24.022832336425783\n",
      "Training growing_up:  48%|████▊     | 9586/20010 [7:40:29<8:07:11,  2.80s/batch]Batch 9600/20010 Done, mean position loss: 24.67753963947296\n",
      "Training growing_up:  47%|████▋     | 9436/20010 [7:40:29<6:59:40,  2.38s/batch]Batch 9400/20010 Done, mean position loss: 23.977362127304076\n",
      "Training growing_up:  48%|████▊     | 9681/20010 [7:40:37<6:21:06,  2.21s/batch]Batch 9600/20010 Done, mean position loss: 23.11504908323288\n",
      "Training growing_up:  48%|████▊     | 9652/20010 [7:40:40<6:48:04,  2.36s/batch]Batch 9700/20010 Done, mean position loss: 24.270975239276886\n",
      "Training growing_up:  48%|████▊     | 9627/20010 [7:40:58<7:06:45,  2.47s/batch]Batch 9700/20010 Done, mean position loss: 23.659310448169705\n",
      "Training growing_up:  47%|████▋     | 9451/20010 [7:41:09<7:34:05,  2.58s/batch]Batch 9600/20010 Done, mean position loss: 24.189181711673736\n",
      "Training growing_up:  48%|████▊     | 9552/20010 [7:41:11<7:53:59,  2.72s/batch]Batch 9600/20010 Done, mean position loss: 23.858695287704467\n",
      "Training growing_up:  48%|████▊     | 9660/20010 [7:41:19<7:50:33,  2.73s/batch]Batch 9500/20010 Done, mean position loss: 24.106491725444794\n",
      "Training growing_up:  48%|████▊     | 9583/20010 [7:41:27<6:25:49,  2.22s/batch]Batch 9700/20010 Done, mean position loss: 24.0481338262558\n",
      "Training growing_up:  47%|████▋     | 9431/20010 [7:41:44<8:04:52,  2.75s/batch]Batch 9600/20010 Done, mean position loss: 24.239934377670288\n",
      "Training growing_up:  48%|████▊     | 9667/20010 [7:41:47<7:07:29,  2.48s/batch]Batch 9800/20010 Done, mean position loss: 23.363394482135774\n",
      "Training growing_up:  48%|████▊     | 9576/20010 [7:41:51<7:07:05,  2.46s/batch]Batch 9600/20010 Done, mean position loss: 24.89441751241684\n",
      "Training growing_up:  47%|████▋     | 9482/20010 [7:42:07<6:27:28,  2.21s/batch]Batch 9500/20010 Done, mean position loss: 25.048098204135893\n",
      "Training growing_up:  49%|████▊     | 9748/20010 [7:42:11<7:58:46,  2.80s/batch]Batch 9600/20010 Done, mean position loss: 24.253716468811035\n",
      "Training growing_up:  48%|████▊     | 9684/20010 [7:42:12<7:32:23,  2.63s/batch]Batch 9600/20010 Done, mean position loss: 24.47162621974945\n",
      "Training growing_up:  47%|████▋     | 9477/20010 [7:42:16<8:08:07,  2.78s/batch]Batch 9700/20010 Done, mean position loss: 23.32826709270477\n",
      "Training growing_up:  48%|████▊     | 9676/20010 [7:42:26<6:44:23,  2.35s/batch]Batch 9600/20010 Done, mean position loss: 24.653168468475343\n",
      "Training growing_up:  48%|████▊     | 9649/20010 [7:42:35<7:08:09,  2.48s/batch]Batch 9700/20010 Done, mean position loss: 24.766939759254456\n",
      "Training growing_up:  49%|████▊     | 9742/20010 [7:42:39<6:20:24,  2.22s/batch]Batch 9700/20010 Done, mean position loss: 23.396185734272002\n",
      "Training growing_up:  48%|████▊     | 9540/20010 [7:42:50<5:38:57,  1.94s/batch]Batch 9700/20010 Done, mean position loss: 23.425805535316467\n",
      "Training growing_up:  49%|████▊     | 9716/20010 [7:42:52<6:30:58,  2.28s/batch]Batch 9600/20010 Done, mean position loss: 23.94322418451309\n",
      "Training growing_up:  49%|████▉     | 9757/20010 [7:42:55<6:16:49,  2.21s/batch]Batch 9500/20010 Done, mean position loss: 24.429763367176058\n",
      "Training growing_up:  48%|████▊     | 9687/20010 [7:42:56<6:15:58,  2.19s/batch]Batch 9500/20010 Done, mean position loss: 23.651554696559906\n",
      "Training growing_up:  48%|████▊     | 9621/20010 [7:43:00<6:48:47,  2.36s/batch]Batch 9700/20010 Done, mean position loss: 23.484014353752137\n",
      "Training growing_up:  48%|████▊     | 9694/20010 [7:43:06<6:56:59,  2.43s/batch]Batch 9700/20010 Done, mean position loss: 24.535480799674986\n",
      "Training growing_up:  48%|████▊     | 9678/20010 [7:43:08<6:11:48,  2.16s/batch]Batch 9600/20010 Done, mean position loss: 24.01258608341217\n",
      "Training growing_up:  47%|████▋     | 9465/20010 [7:43:11<7:27:54,  2.55s/batch]Batch 9600/20010 Done, mean position loss: 25.31402372121811\n",
      "Training growing_up:  49%|████▉     | 9775/20010 [7:43:15<6:15:27,  2.20s/batch]Batch 9500/20010 Done, mean position loss: 22.921267721652985\n",
      "Training growing_up:  48%|████▊     | 9641/20010 [7:43:20<7:18:48,  2.54s/batch]Batch 9600/20010 Done, mean position loss: 23.879409840106966\n",
      "Training growing_up:  48%|████▊     | 9669/20010 [7:43:25<8:35:32,  2.99s/batch]Batch 9700/20010 Done, mean position loss: 23.948872816562655\n",
      "Training growing_up:  48%|████▊     | 9604/20010 [7:43:29<8:16:45,  2.86s/batch]Batch 9700/20010 Done, mean position loss: 24.482638213634488\n",
      "Training growing_up:  48%|████▊     | 9506/20010 [7:43:29<7:39:18,  2.62s/batch]Batch 9700/20010 Done, mean position loss: 23.96671275138855\n",
      "Training growing_up:  48%|████▊     | 9606/20010 [7:43:34<7:58:44,  2.76s/batch]Batch 9800/20010 Done, mean position loss: 23.8082374215126\n",
      "Training growing_up:  49%|████▉     | 9765/20010 [7:43:36<7:26:51,  2.62s/batch]Batch 9600/20010 Done, mean position loss: 23.960156404972075\n",
      "Training growing_up:  49%|████▊     | 9722/20010 [7:43:41<6:48:51,  2.38s/batch]Batch 9700/20010 Done, mean position loss: 24.289083888530733\n",
      "Training growing_up:  48%|████▊     | 9608/20010 [7:43:55<8:12:57,  2.84s/batch]Batch 9700/20010 Done, mean position loss: 24.287259786129\n",
      "Training growing_up:  48%|████▊     | 9521/20010 [7:44:06<7:05:53,  2.44s/batch]Batch 9700/20010 Done, mean position loss: 24.183001849651337\n",
      "Training growing_up:  49%|████▊     | 9729/20010 [7:44:09<7:08:16,  2.50s/batch]Batch 9500/20010 Done, mean position loss: 23.88328278303146\n",
      "Training growing_up:  48%|████▊     | 9695/20010 [7:44:16<6:49:37,  2.38s/batch]Batch 9800/20010 Done, mean position loss: 23.571153986454007\n",
      "Training growing_up:  48%|████▊     | 9632/20010 [7:44:26<7:05:38,  2.46s/batch]Batch 9700/20010 Done, mean position loss: 24.003818757534027\n",
      "Training growing_up:  49%|████▊     | 9736/20010 [7:44:30<6:33:02,  2.30s/batch]Batch 9700/20010 Done, mean position loss: 24.846246364116666\n",
      "Training growing_up:  49%|████▊     | 9732/20010 [7:44:41<7:00:44,  2.46s/batch]Batch 9800/20010 Done, mean position loss: 24.675289902687073\n",
      "Training growing_up:  49%|████▊     | 9731/20010 [7:44:42<6:57:46,  2.44s/batch]Batch 9500/20010 Done, mean position loss: 23.424054386615754\n",
      "Training growing_up:  49%|████▊     | 9733/20010 [7:44:44<7:21:46,  2.58s/batch]Batch 9700/20010 Done, mean position loss: 23.08963300228119\n",
      "Training growing_up:  49%|████▊     | 9752/20010 [7:45:03<6:53:18,  2.42s/batch]Batch 9800/20010 Done, mean position loss: 23.633197329044343\n",
      "Training growing_up:  49%|████▊     | 9720/20010 [7:45:09<6:35:19,  2.31s/batch]Batch 9700/20010 Done, mean position loss: 24.01260854959488\n",
      "Training growing_up:  49%|████▉     | 9888/20010 [7:45:11<6:04:13,  2.16s/batch]Batch 9700/20010 Done, mean position loss: 24.18177524805069\n",
      "Training growing_up:  48%|████▊     | 9529/20010 [7:45:17<7:10:46,  2.47s/batch]Batch 9600/20010 Done, mean position loss: 23.36681457042694\n",
      "Training growing_up:  48%|████▊     | 9553/20010 [7:45:27<7:26:41,  2.56s/batch]Batch 9800/20010 Done, mean position loss: 23.112671422958375\n",
      "Training growing_up:  49%|████▊     | 9731/20010 [7:45:43<6:31:44,  2.29s/batch]Batch 9900/20010 Done, mean position loss: 23.10541202545166\n",
      "Training growing_up:  49%|████▉     | 9903/20010 [7:45:47<6:29:46,  2.31s/batch]Batch 9700/20010 Done, mean position loss: 24.478088915348053\n",
      "Training growing_up:  49%|████▉     | 9785/20010 [7:45:49<6:22:34,  2.24s/batch]Batch 9700/20010 Done, mean position loss: 24.24041589021683\n",
      "Training growing_up:  48%|████▊     | 9553/20010 [7:46:16<6:37:32,  2.28s/batch]Batch 9600/20010 Done, mean position loss: 24.734431509971618\n",
      "Training growing_up:  48%|████▊     | 9676/20010 [7:46:16<6:59:02,  2.43s/batch]Batch 9700/20010 Done, mean position loss: 24.33322303056717\n",
      "Training growing_up:  48%|████▊     | 9602/20010 [7:46:18<6:19:15,  2.19s/batch]Batch 9700/20010 Done, mean position loss: 24.12706224441528\n",
      "Training growing_up:  49%|████▊     | 9749/20010 [7:46:25<6:19:38,  2.22s/batch]Batch 9800/20010 Done, mean position loss: 23.469477038383484\n",
      "Training growing_up:  49%|████▉     | 9800/20010 [7:46:28<6:54:44,  2.44s/batch]Batch 9700/20010 Done, mean position loss: 24.430160741806027\n",
      "Training growing_up:  48%|████▊     | 9678/20010 [7:46:31<6:50:27,  2.38s/batch]Batch 9800/20010 Done, mean position loss: 24.01660809993744\n",
      "Training growing_up:  49%|████▉     | 9770/20010 [7:46:37<7:07:12,  2.50s/batch]Batch 9800/20010 Done, mean position loss: 23.690383217334748\n",
      "Training growing_up:  49%|████▊     | 9717/20010 [7:46:54<6:35:16,  2.30s/batch]Batch 9800/20010 Done, mean position loss: 23.282366826534272\n",
      "Training growing_up:  49%|████▊     | 9730/20010 [7:46:59<7:25:16,  2.60s/batch]Batch 9600/20010 Done, mean position loss: 24.482875514030457\n",
      "Training growing_up:  49%|████▊     | 9718/20010 [7:47:02<7:41:49,  2.69s/batch]Batch 9700/20010 Done, mean position loss: 23.748910024166108\n",
      "Training growing_up:  48%|████▊     | 9702/20010 [7:47:04<7:46:51,  2.72s/batch]Batch 9800/20010 Done, mean position loss: 23.47854446411133\n",
      "Training growing_up:  49%|████▉     | 9775/20010 [7:47:11<7:28:29,  2.63s/batch]Batch 9600/20010 Done, mean position loss: 23.247036352157593\n",
      "Training growing_up:  49%|████▉     | 9818/20010 [7:47:11<6:59:13,  2.47s/batch]Batch 9800/20010 Done, mean position loss: 24.34207598209381\n",
      "Training growing_up:  49%|████▊     | 9709/20010 [7:47:21<7:07:15,  2.49s/batch]Batch 9700/20010 Done, mean position loss: 23.86635010957718\n",
      "Training growing_up:  48%|████▊     | 9606/20010 [7:47:23<7:02:51,  2.44s/batch]Batch 9700/20010 Done, mean position loss: 25.111657185554506\n",
      "Training growing_up:  49%|████▉     | 9826/20010 [7:47:27<7:33:11,  2.67s/batch]Batch 9600/20010 Done, mean position loss: 23.00505394935608\n",
      "Training growing_up:  49%|████▊     | 9730/20010 [7:47:31<7:22:47,  2.58s/batch]Batch 9700/20010 Done, mean position loss: 23.49632459402084\n",
      "Training growing_up:  49%|████▉     | 9758/20010 [7:47:34<7:37:48,  2.68s/batch]Batch 9800/20010 Done, mean position loss: 24.968064620494843\n",
      "Training growing_up:  49%|████▉     | 9777/20010 [7:47:35<7:44:04,  2.72s/batch]Batch 9800/20010 Done, mean position loss: 23.888181591033934\n",
      "Training growing_up:  48%|████▊     | 9698/20010 [7:47:37<7:13:27,  2.52s/batch]Batch 9900/20010 Done, mean position loss: 23.786232404708862\n",
      "Training growing_up:  49%|████▉     | 9876/20010 [7:47:41<6:12:59,  2.21s/batch]Batch 9800/20010 Done, mean position loss: 23.080344955921174\n",
      "Training growing_up:  49%|████▊     | 9731/20010 [7:47:45<7:18:47,  2.56s/batch]Batch 9700/20010 Done, mean position loss: 24.150218851566315\n",
      "Training growing_up:  49%|████▉     | 9820/20010 [7:47:51<6:34:10,  2.32s/batch]Batch 9800/20010 Done, mean position loss: 24.935420105457307\n",
      "Training growing_up:  49%|████▉     | 9872/20010 [7:47:57<7:31:14,  2.67s/batch]Batch 9800/20010 Done, mean position loss: 23.904361448287965\n",
      "Training growing_up:  49%|████▉     | 9882/20010 [7:48:20<6:20:08,  2.25s/batch]Batch 9800/20010 Done, mean position loss: 23.727885699272157\n",
      "Training growing_up:  48%|████▊     | 9587/20010 [7:48:24<8:00:41,  2.77s/batch]Batch 9600/20010 Done, mean position loss: 23.32944265127182\n",
      "Training growing_up:  49%|████▉     | 9779/20010 [7:48:26<7:03:01,  2.48s/batch]Batch 9900/20010 Done, mean position loss: 23.378507726192474\n",
      "Training growing_up:  49%|████▉     | 9852/20010 [7:48:34<6:30:40,  2.31s/batch]Batch 9800/20010 Done, mean position loss: 24.744227554798126\n",
      "Training growing_up:  49%|████▉     | 9822/20010 [7:48:36<7:01:10,  2.48s/batch]Batch 9800/20010 Done, mean position loss: 24.040750069618227\n",
      "Training growing_up:  48%|████▊     | 9642/20010 [7:48:46<7:12:07,  2.50s/batch]Batch 9900/20010 Done, mean position loss: 24.788898265361787\n",
      "Training growing_up:  50%|████▉     | 9933/20010 [7:48:59<6:52:24,  2.46s/batch]Batch 9800/20010 Done, mean position loss: 23.127833197116853\n",
      "Training growing_up:  48%|████▊     | 9664/20010 [7:48:59<6:57:50,  2.42s/batch]Batch 9600/20010 Done, mean position loss: 24.055619583129882\n",
      "Training growing_up:  49%|████▊     | 9738/20010 [7:49:10<7:46:26,  2.72s/batch]Batch 9900/20010 Done, mean position loss: 23.291729989051817\n",
      "Training growing_up:  49%|████▉     | 9773/20010 [7:49:21<8:00:02,  2.81s/batch]Batch 9800/20010 Done, mean position loss: 23.896876826286316\n",
      "Training growing_up:  49%|████▉     | 9826/20010 [7:49:28<7:34:55,  2.68s/batch]Batch 9700/20010 Done, mean position loss: 23.217933790683745\n",
      "Training growing_up:  49%|████▉     | 9776/20010 [7:49:29<7:21:34,  2.59s/batch]Batch 9800/20010 Done, mean position loss: 24.110253994464877\n",
      "Training growing_up:  49%|████▉     | 9876/20010 [7:49:36<7:27:55,  2.65s/batch]Batch 9900/20010 Done, mean position loss: 22.908930358886717\n",
      "Training growing_up:  49%|████▉     | 9757/20010 [7:49:57<7:00:52,  2.46s/batch]Batch 10000/20010 Done, mean position loss: 23.20720907449722\n",
      "Training growing_up:  49%|████▉     | 9855/20010 [7:50:08<7:10:28,  2.54s/batch]Batch 9800/20010 Done, mean position loss: 24.11371358394623\n",
      "Training growing_up:  48%|████▊     | 9691/20010 [7:50:10<7:06:31,  2.48s/batch]Batch 9800/20010 Done, mean position loss: 24.49226172685623\n",
      "Training growing_up:  49%|████▉     | 9796/20010 [7:50:34<7:08:55,  2.52s/batch]Batch 9800/20010 Done, mean position loss: 24.352840051651\n",
      "Training growing_up:  49%|████▉     | 9887/20010 [7:50:37<7:20:04,  2.61s/batch]Batch 9700/20010 Done, mean position loss: 25.19580963611603\n",
      "Training growing_up:  49%|████▉     | 9813/20010 [7:50:38<7:33:04,  2.67s/batch]Batch 9800/20010 Done, mean position loss: 24.48648728609085\n",
      "Training growing_up:  49%|████▉     | 9848/20010 [7:50:39<6:35:30,  2.34s/batch]Batch 9900/20010 Done, mean position loss: 23.821678891181946\n",
      "Training growing_up:  49%|████▉     | 9868/20010 [7:50:40<7:33:26,  2.68s/batch]Batch 9900/20010 Done, mean position loss: 24.336410143375396\n",
      "Training growing_up:  50%|████▉     | 9957/20010 [7:50:46<6:39:49,  2.39s/batch]Batch 9800/20010 Done, mean position loss: 24.171005570888518\n",
      "Training growing_up:  48%|████▊     | 9689/20010 [7:50:51<8:01:33,  2.80s/batch]Batch 9900/20010 Done, mean position loss: 23.99643385410309\n",
      "Training growing_up:  49%|████▉     | 9816/20010 [7:51:12<7:28:42,  2.64s/batch]Batch 9900/20010 Done, mean position loss: 23.61711116313934\n",
      "Training growing_up:  49%|████▉     | 9865/20010 [7:51:17<7:27:41,  2.65s/batch]Batch 9900/20010 Done, mean position loss: 23.65690980672836\n",
      "Training growing_up:  49%|████▉     | 9818/20010 [7:51:18<7:43:15,  2.73s/batch]Batch 9800/20010 Done, mean position loss: 23.43941915273666\n",
      "Training growing_up:  50%|████▉     | 9954/20010 [7:51:23<7:59:23,  2.86s/batch]Batch 9900/20010 Done, mean position loss: 24.128607199192047\n",
      "Training growing_up:  49%|████▉     | 9858/20010 [7:51:23<7:03:27,  2.50s/batch]Batch 9700/20010 Done, mean position loss: 24.275883045196533\n",
      "Training growing_up:  49%|████▉     | 9849/20010 [7:51:30<6:49:12,  2.42s/batch]Batch 9700/20010 Done, mean position loss: 23.509693405628205\n",
      "Training growing_up:  50%|████▉     | 9961/20010 [7:51:40<6:55:45,  2.48s/batch]Batch 9800/20010 Done, mean position loss: 25.225330889225006\n",
      "Training growing_up:  49%|████▉     | 9900/20010 [7:51:41<6:09:35,  2.19s/batch]Batch 9900/20010 Done, mean position loss: 24.25228546142578\n",
      "Training growing_up:  49%|████▉     | 9889/20010 [7:51:44<6:30:02,  2.31s/batch]Batch 9900/20010 Done, mean position loss: 24.694772150516506\n",
      "Training growing_up:  49%|████▉     | 9793/20010 [7:51:44<7:03:23,  2.49s/batch]Batch 9800/20010 Done, mean position loss: 23.429377875328065\n",
      "Training growing_up:  49%|████▉     | 9841/20010 [7:51:49<7:05:26,  2.51s/batch]Batch 10000/20010 Done, mean position loss: 23.589580688476563\n",
      "Training growing_up:  50%|████▍    | 10001/20010 [7:51:49<7:12:37,  2.59s/batch]Batch 9700/20010 Done, mean position loss: 22.824286530017854\n",
      "Training growing_up:  49%|████▉     | 9869/20010 [7:51:50<7:12:15,  2.56s/batch]Batch 9800/20010 Done, mean position loss: 23.90290117263794\n",
      "Training growing_up:  49%|████▉     | 9830/20010 [7:51:58<6:47:54,  2.40s/batch]Batch 9900/20010 Done, mean position loss: 24.040922894477845\n",
      "Training growing_up:  50%|████▉     | 9959/20010 [7:52:04<7:10:52,  2.57s/batch]Batch 9900/20010 Done, mean position loss: 24.854242701530456\n",
      "Batch 9800/20010 Done, mean position loss: 24.62559297800064\n",
      "Training growing_up:  49%|████▉     | 9838/20010 [7:52:11<7:46:52,  2.75s/batch]Batch 9900/20010 Done, mean position loss: 24.12587856769562\n",
      "Training growing_up:  50%|████▉     | 9982/20010 [7:52:34<7:05:44,  2.55s/batch]Batch 9900/20010 Done, mean position loss: 24.130067994594576\n",
      "Training growing_up:  49%|████▉     | 9858/20010 [7:52:36<7:43:20,  2.74s/batch]Batch 10000/20010 Done, mean position loss: 23.39580193042755\n",
      "Training growing_up:  49%|████▉     | 9825/20010 [7:52:44<7:24:10,  2.62s/batch]Batch 9900/20010 Done, mean position loss: 24.32841553926468\n",
      "Training growing_up:  49%|████▉     | 9778/20010 [7:52:46<7:16:26,  2.56s/batch]Batch 9700/20010 Done, mean position loss: 23.937153527736662\n",
      "Training growing_up:  50%|████▌    | 10026/20010 [7:52:49<6:35:51,  2.38s/batch]Batch 9900/20010 Done, mean position loss: 23.80138182401657\n",
      "Training growing_up:  49%|████▉     | 9866/20010 [7:52:56<8:14:26,  2.92s/batch]Batch 10000/20010 Done, mean position loss: 24.63448432683945\n",
      "Training growing_up:  50%|████▉     | 9925/20010 [7:53:08<7:41:28,  2.75s/batch]Batch 9900/20010 Done, mean position loss: 23.795508232116703\n",
      "Training growing_up:  50%|████▉     | 9934/20010 [7:53:24<7:21:04,  2.63s/batch]Batch 10000/20010 Done, mean position loss: 23.884481678009035\n",
      "Training growing_up:  50%|████▉     | 9941/20010 [7:53:25<6:40:13,  2.38s/batch]Batch 9700/20010 Done, mean position loss: 23.704587302207948\n",
      "Training growing_up:  50%|████▉     | 9948/20010 [7:53:38<7:56:24,  2.84s/batch]Batch 9900/20010 Done, mean position loss: 24.098499715328217\n",
      "Training growing_up:  50%|████▉     | 9973/20010 [7:53:41<6:22:42,  2.29s/batch]Batch 9900/20010 Done, mean position loss: 24.29115315914154\n",
      "Training growing_up:  50%|████▉     | 9957/20010 [7:53:43<6:44:36,  2.41s/batch]Batch 9800/20010 Done, mean position loss: 24.151630387306213\n",
      "Training growing_up:  49%|████▉     | 9849/20010 [7:53:50<6:30:33,  2.31s/batch]Batch 10000/20010 Done, mean position loss: 22.67951558828354\n",
      "Training growing_up:  49%|████▉     | 9853/20010 [7:54:01<8:14:01,  2.92s/batch]Batch 10100/20010 Done, mean position loss: 23.325726554393768\n",
      "Training growing_up:  49%|████▊     | 9739/20010 [7:54:21<7:38:56,  2.68s/batch]Batch 9900/20010 Done, mean position loss: 24.724649131298065\n",
      "Training growing_up:  49%|████▉     | 9770/20010 [7:54:24<7:30:57,  2.64s/batch]Batch 9900/20010 Done, mean position loss: 24.745608389377594\n",
      "Training growing_up:  49%|████▉     | 9876/20010 [7:54:46<6:46:53,  2.41s/batch]Batch 9900/20010 Done, mean position loss: 24.110735538005827\n",
      "Training growing_up:  50%|████▌    | 10052/20010 [7:54:46<6:38:19,  2.40s/batch]Batch 10000/20010 Done, mean position loss: 22.890624194145204\n",
      "Training growing_up:  49%|████▊     | 9751/20010 [7:54:49<5:28:53,  1.92s/batch]Batch 10000/20010 Done, mean position loss: 23.863324065208435\n",
      "Training growing_up:  50%|████▉     | 9951/20010 [7:54:51<7:11:44,  2.58s/batch]Batch 9800/20010 Done, mean position loss: 24.77787078142166\n",
      "Training growing_up:  50%|████▉     | 9915/20010 [7:54:53<6:21:34,  2.27s/batch]Batch 10000/20010 Done, mean position loss: 23.502157802581788\n",
      "Training growing_up:  49%|████▉     | 9774/20010 [7:54:53<7:07:17,  2.50s/batch]Batch 9900/20010 Done, mean position loss: 24.37406803369522\n",
      "Training growing_up:  51%|████▌    | 10125/20010 [7:54:59<6:48:10,  2.48s/batch]Batch 9900/20010 Done, mean position loss: 24.605323724746704\n",
      "Training growing_up:  49%|████▉     | 9885/20010 [7:55:19<7:50:08,  2.79s/batch]Batch 10000/20010 Done, mean position loss: 23.458101465702057\n",
      "Training growing_up:  49%|████▉     | 9815/20010 [7:55:24<6:00:12,  2.12s/batch]Batch 10000/20010 Done, mean position loss: 23.71623775959015\n",
      "Training growing_up:  50%|████▉     | 9991/20010 [7:55:26<6:48:24,  2.45s/batch]Batch 9900/20010 Done, mean position loss: 23.343663914203646\n",
      "Training growing_up:  50%|████▌    | 10069/20010 [7:55:37<6:01:42,  2.18s/batch]Batch 10000/20010 Done, mean position loss: 23.456903424263\n",
      "Training growing_up:  50%|████▌    | 10056/20010 [7:55:40<5:49:29,  2.11s/batch]Batch 9800/20010 Done, mean position loss: 23.46894320964813\n",
      "Training growing_up:  50%|████▉     | 9971/20010 [7:55:41<6:08:37,  2.20s/batch]Batch 9800/20010 Done, mean position loss: 24.456460316181182\n",
      "Training growing_up:  49%|████▉     | 9803/20010 [7:55:45<6:42:07,  2.36s/batch]Batch 9900/20010 Done, mean position loss: 24.811450345516203\n",
      "Training growing_up:  49%|████▉     | 9798/20010 [7:55:52<7:20:21,  2.59s/batch]Batch 10100/20010 Done, mean position loss: 24.13584761142731\n",
      "Training growing_up:  49%|████▉     | 9899/20010 [7:55:53<7:02:47,  2.51s/batch]Batch 10000/20010 Done, mean position loss: 24.26757355213165\n",
      "Training growing_up:  50%|████▌    | 10016/20010 [7:55:56<6:25:59,  2.32s/batch]Batch 10000/20010 Done, mean position loss: 24.654159681797026\n",
      "Training growing_up:  50%|████▉     | 9939/20010 [7:55:58<8:08:27,  2.91s/batch]Batch 9900/20010 Done, mean position loss: 23.632698483467102\n",
      "Training growing_up:  50%|████▌    | 10063/20010 [7:55:58<6:48:45,  2.47s/batch]Batch 9900/20010 Done, mean position loss: 23.22806573390961\n",
      "Training growing_up:  49%|████▉     | 9896/20010 [7:56:00<7:08:52,  2.54s/batch]Batch 10000/20010 Done, mean position loss: 23.600428664684294\n",
      "Training growing_up:  50%|████▍    | 10004/20010 [7:56:00<7:21:21,  2.65s/batch]Batch 9800/20010 Done, mean position loss: 23.0736994767189\n",
      "Training growing_up:  50%|████▉     | 9921/20010 [7:56:14<6:59:45,  2.50s/batch]Batch 9900/20010 Done, mean position loss: 24.599862778186797\n",
      "Training growing_up:  51%|████▌    | 10157/20010 [7:56:18<7:06:48,  2.60s/batch]Batch 10000/20010 Done, mean position loss: 24.590288200378417\n",
      "Training growing_up:  50%|████▌    | 10062/20010 [7:56:26<6:14:42,  2.26s/batch]Batch 10000/20010 Done, mean position loss: 23.854611501693725\n",
      "Training growing_up:  50%|████▉     | 9946/20010 [7:56:41<7:09:37,  2.56s/batch]Batch 10000/20010 Done, mean position loss: 23.91268836259842\n",
      "Training growing_up:  51%|████▌    | 10166/20010 [7:56:41<6:25:54,  2.35s/batch]Batch 10000/20010 Done, mean position loss: 23.84447818517685\n",
      "Training growing_up:  50%|████▉     | 9977/20010 [7:56:47<7:21:01,  2.64s/batch]Batch 10100/20010 Done, mean position loss: 23.51545203924179\n",
      "Training growing_up:  50%|████▉     | 9924/20010 [7:56:52<6:41:37,  2.39s/batch]Batch 10000/20010 Done, mean position loss: 24.370122997760774\n",
      "Training growing_up:  50%|████▉     | 9922/20010 [7:56:52<7:00:19,  2.50s/batch]Batch 10100/20010 Done, mean position loss: 24.878789904117582\n",
      "Training growing_up:  50%|████▌    | 10104/20010 [7:56:54<6:12:25,  2.26s/batch]Batch 9800/20010 Done, mean position loss: 23.70441943645477\n",
      "Training growing_up:  50%|████▉     | 9932/20010 [7:57:17<7:38:36,  2.73s/batch]Batch 10000/20010 Done, mean position loss: 23.18838093996048\n",
      "Training growing_up:  50%|████▌    | 10054/20010 [7:57:27<6:39:02,  2.40s/batch]Batch 10100/20010 Done, mean position loss: 23.40067319393158\n",
      "Training growing_up:  49%|████▉     | 9895/20010 [7:57:33<5:27:22,  1.94s/batch]Batch 9800/20010 Done, mean position loss: 23.049881982803345\n",
      "Training growing_up:  50%|████▌    | 10028/20010 [7:57:45<6:53:13,  2.48s/batch]Batch 10000/20010 Done, mean position loss: 23.934699149131774\n",
      "Training growing_up:  50%|████▌    | 10048/20010 [7:57:46<6:10:38,  2.23s/batch]Batch 10000/20010 Done, mean position loss: 23.937049238681794\n",
      "Training growing_up:  50%|████▌    | 10026/20010 [7:57:47<6:05:53,  2.20s/batch]Batch 9900/20010 Done, mean position loss: 23.977038657665254\n",
      "Training growing_up:  50%|████▌    | 10008/20010 [7:58:02<7:01:24,  2.53s/batch]Batch 10100/20010 Done, mean position loss: 23.426886291503905\n",
      "Training growing_up:  50%|████▌    | 10043/20010 [7:58:04<6:36:06,  2.38s/batch]Batch 10200/20010 Done, mean position loss: 23.109390206336975\n",
      "Training growing_up:  50%|████▌    | 10087/20010 [7:58:15<5:36:20,  2.03s/batch]Batch 10000/20010 Done, mean position loss: 24.140115098953245\n",
      "Training growing_up:  50%|████▌    | 10046/20010 [7:58:29<5:36:12,  2.02s/batch]Batch 10000/20010 Done, mean position loss: 24.183437983989716\n",
      "Training growing_up:  50%|████▌    | 10024/20010 [7:58:39<6:01:44,  2.17s/batch]Batch 10100/20010 Done, mean position loss: 22.76993479251862\n",
      "Training growing_up:  50%|████▌    | 10077/20010 [7:58:42<6:42:58,  2.43s/batch]Batch 10000/20010 Done, mean position loss: 24.011083126068115\n",
      "Training growing_up:  50%|████▉     | 9996/20010 [7:58:47<6:52:19,  2.47s/batch]Batch 10100/20010 Done, mean position loss: 23.631037118434907\n",
      "Training growing_up:  50%|████▌    | 10063/20010 [7:58:49<6:20:19,  2.29s/batch]Batch 10100/20010 Done, mean position loss: 23.72160768032074\n",
      "Training growing_up:  50%|████▉     | 9997/20010 [7:58:49<6:42:14,  2.41s/batch]Batch 10000/20010 Done, mean position loss: 24.152877736091614\n",
      "Training growing_up:  50%|████▌    | 10103/20010 [7:58:54<6:15:52,  2.28s/batch]Batch 9900/20010 Done, mean position loss: 24.548509459495545\n",
      "Training growing_up:  50%|████▌    | 10059/20010 [7:58:59<6:40:28,  2.41s/batch]Batch 10000/20010 Done, mean position loss: 24.494551455974577\n",
      "Training growing_up:  51%|████▌    | 10112/20010 [7:59:12<6:45:00,  2.46s/batch]Batch 10100/20010 Done, mean position loss: 22.97488370895386\n",
      "Training growing_up:  50%|████▌    | 10028/20010 [7:59:18<7:23:31,  2.67s/batch]Batch 10100/20010 Done, mean position loss: 23.72152365922928\n",
      "Training growing_up:  51%|████▌    | 10164/20010 [7:59:20<7:14:13,  2.65s/batch]Batch 10000/20010 Done, mean position loss: 23.29971967458725\n",
      "Training growing_up:  51%|████▌    | 10138/20010 [7:59:30<6:29:56,  2.37s/batch]Batch 9900/20010 Done, mean position loss: 23.686612055301666\n",
      "Training growing_up:  50%|████▌    | 10095/20010 [7:59:34<6:46:56,  2.46s/batch]Batch 10000/20010 Done, mean position loss: 24.433588130474092\n",
      "Training growing_up:  49%|████▉     | 9856/20010 [7:59:41<5:49:52,  2.07s/batch]Batch 10100/20010 Done, mean position loss: 23.51130164861679\n",
      "Training growing_up:  51%|████▌    | 10124/20010 [7:59:42<6:23:15,  2.33s/batch]Batch 9900/20010 Done, mean position loss: 24.588103759288792\n",
      "Training growing_up:  50%|████▌    | 10088/20010 [7:59:48<6:15:52,  2.27s/batch]Batch 10100/20010 Done, mean position loss: 24.208997464179994\n",
      "Training growing_up:  51%|████▌    | 10246/20010 [7:59:50<6:18:02,  2.32s/batch]Batch 10200/20010 Done, mean position loss: 24.088142352104185\n",
      "Training growing_up:  51%|████▌    | 10129/20010 [7:59:52<5:50:17,  2.13s/batch]Batch 10100/20010 Done, mean position loss: 24.511527161598206\n",
      "Training growing_up:  51%|████▌    | 10128/20010 [7:59:57<6:35:50,  2.40s/batch]Batch 10100/20010 Done, mean position loss: 23.63932338953018\n",
      "Batch 10000/20010 Done, mean position loss: 23.27627283334732\n",
      "Training growing_up:  51%|████▌    | 10131/20010 [8:00:04<6:51:41,  2.50s/batch]Batch 9900/20010 Done, mean position loss: 22.919420733451844\n",
      "Training growing_up:  51%|████▌    | 10106/20010 [8:00:08<6:37:17,  2.41s/batch]Batch 10000/20010 Done, mean position loss: 24.756274485588072\n",
      "Training growing_up:  51%|████▌    | 10125/20010 [8:00:14<6:18:36,  2.30s/batch]Batch 10000/20010 Done, mean position loss: 23.909490060806274\n",
      "Training growing_up:  51%|████▌    | 10140/20010 [8:00:18<6:03:09,  2.21s/batch]Batch 10100/20010 Done, mean position loss: 25.06525369644165\n",
      "Training growing_up:  50%|████▉     | 9917/20010 [8:00:22<6:38:28,  2.37s/batch]Batch 10100/20010 Done, mean position loss: 23.799090824127198\n",
      "Training growing_up:  51%|████▌    | 10134/20010 [8:00:35<6:35:59,  2.41s/batch]Batch 10100/20010 Done, mean position loss: 24.076576128005982\n",
      "Training growing_up:  51%|████▌    | 10195/20010 [8:00:38<6:55:07,  2.54s/batch]Batch 10100/20010 Done, mean position loss: 24.385400393009185\n",
      "Training growing_up:  50%|████▌    | 10057/20010 [8:00:40<6:14:42,  2.26s/batch]Batch 10100/20010 Done, mean position loss: 23.65238799571991\n",
      "Training growing_up:  51%|████▌    | 10110/20010 [8:00:44<7:01:50,  2.56s/batch]Batch 10200/20010 Done, mean position loss: 24.26670085906982\n",
      "Training growing_up:  50%|████▌    | 10020/20010 [8:00:53<6:26:02,  2.32s/batch]Batch 10200/20010 Done, mean position loss: 23.445522096157074\n",
      "Training growing_up:  50%|████▌    | 10092/20010 [8:01:00<8:00:40,  2.91s/batch]Batch 9900/20010 Done, mean position loss: 23.443722360134124\n",
      "Training growing_up:  51%|████▌    | 10155/20010 [8:01:19<7:19:24,  2.68s/batch]Batch 10200/20010 Done, mean position loss: 23.550636916160585\n",
      "Training growing_up:  50%|████▉     | 9942/20010 [8:01:22<6:27:49,  2.31s/batch]Batch 10100/20010 Done, mean position loss: 23.056025164127348\n",
      "Training growing_up:  50%|████▉     | 9936/20010 [8:01:30<7:01:45,  2.51s/batch]Batch 9900/20010 Done, mean position loss: 23.43909457921982\n",
      "Training growing_up:  50%|████▌    | 10069/20010 [8:01:47<5:42:49,  2.07s/batch]Batch 10100/20010 Done, mean position loss: 23.734007766246798\n",
      "Training growing_up:  50%|████▉     | 9945/20010 [8:01:51<6:06:06,  2.18s/batch]Batch 10100/20010 Done, mean position loss: 23.95721050977707\n",
      "Batch 10000/20010 Done, mean position loss: 24.682123370170594\n",
      "Training growing_up:  51%|████▌    | 10159/20010 [8:02:04<6:52:02,  2.51s/batch]Batch 10300/20010 Done, mean position loss: 23.38119168281555\n",
      "Training growing_up:  51%|████▌    | 10221/20010 [8:02:06<5:58:52,  2.20s/batch]Batch 10200/20010 Done, mean position loss: 23.118627762794496\n",
      "Training growing_up:  50%|████▉     | 9986/20010 [8:02:18<6:39:47,  2.39s/batch]Batch 10100/20010 Done, mean position loss: 24.272028353214264\n",
      "Training growing_up:  51%|████▌    | 10144/20010 [8:02:25<6:42:21,  2.45s/batch]Batch 10100/20010 Done, mean position loss: 24.395790805816652\n",
      "Training growing_up:  51%|████▌    | 10192/20010 [8:02:44<7:53:13,  2.89s/batch]Batch 10200/20010 Done, mean position loss: 22.721379585266114\n",
      "Training growing_up:  51%|████▌    | 10134/20010 [8:02:44<6:22:01,  2.32s/batch]Batch 10200/20010 Done, mean position loss: 23.73234811067581\n",
      "Training growing_up:  50%|████▌    | 10080/20010 [8:02:47<6:44:43,  2.45s/batch]Batch 10100/20010 Done, mean position loss: 24.327315859794616\n",
      "Training growing_up:  50%|████▌    | 10027/20010 [8:02:53<6:17:55,  2.27s/batch]Batch 10100/20010 Done, mean position loss: 24.189398803710937\n",
      "Training growing_up:  50%|████▉     | 9936/20010 [8:02:57<7:12:52,  2.58s/batch]Batch 10000/20010 Done, mean position loss: 24.40420597076416\n",
      "Training growing_up:  51%|████▌    | 10263/20010 [8:03:06<6:17:49,  2.33s/batch]Batch 10200/20010 Done, mean position loss: 23.91609750986099\n",
      "Training growing_up:  51%|████▌    | 10225/20010 [8:03:09<6:58:48,  2.57s/batch]Batch 10100/20010 Done, mean position loss: 24.410629050731657\n",
      "Training growing_up:  50%|████▌    | 10103/20010 [8:03:12<6:08:29,  2.23s/batch]Batch 10200/20010 Done, mean position loss: 23.30546036720276\n",
      "Training growing_up:  51%|████▌    | 10169/20010 [8:03:13<6:34:14,  2.40s/batch]Batch 10200/20010 Done, mean position loss: 23.95548424243927\n",
      "Training growing_up:  52%|████▋    | 10331/20010 [8:03:20<6:25:46,  2.39s/batch]Batch 10100/20010 Done, mean position loss: 23.488438155651092\n",
      "Training growing_up:  50%|████▉     | 9950/20010 [8:03:32<6:05:54,  2.18s/batch]Batch 10000/20010 Done, mean position loss: 23.381509239673615\n",
      "Training growing_up:  51%|████▌    | 10144/20010 [8:03:34<6:12:42,  2.27s/batch]Batch 10100/20010 Done, mean position loss: 24.602814395427707\n",
      "Training growing_up:  51%|████▌    | 10215/20010 [8:03:46<6:35:40,  2.42s/batch]Batch 10200/20010 Done, mean position loss: 23.608121337890623\n",
      "Training growing_up:  50%|████▌    | 10087/20010 [8:03:47<6:25:45,  2.33s/batch]Batch 10200/20010 Done, mean position loss: 24.198814299106598\n",
      "Training growing_up:  51%|████▌    | 10201/20010 [8:03:48<7:45:39,  2.85s/batch]Batch 10000/20010 Done, mean position loss: 24.555225095748902\n",
      "Training growing_up:  51%|████▌    | 10138/20010 [8:03:50<6:09:27,  2.25s/batch]Batch 10200/20010 Done, mean position loss: 23.913437960147856\n",
      "Training growing_up:  51%|████▌    | 10151/20010 [8:03:51<6:11:07,  2.26s/batch]Batch 10300/20010 Done, mean position loss: 23.50573174715042\n",
      "Training growing_up:  50%|████▌    | 10096/20010 [8:03:57<7:04:13,  2.57s/batch]Batch 10200/20010 Done, mean position loss: 23.577889025211334\n",
      "Training growing_up:  51%|████▌    | 10170/20010 [8:04:07<6:46:35,  2.48s/batch]Batch 10100/20010 Done, mean position loss: 24.00806612968445\n",
      "Batch 10000/20010 Done, mean position loss: 22.720917439460756\n",
      "Training growing_up:  50%|████▌    | 10010/20010 [8:04:10<7:16:36,  2.62s/batch]Batch 10100/20010 Done, mean position loss: 23.45737107515335\n",
      "Training growing_up:  51%|████▌    | 10230/20010 [8:04:21<6:43:15,  2.47s/batch]Batch 10100/20010 Done, mean position loss: 23.9540070605278\n",
      "Training growing_up:  50%|████▌    | 10018/20010 [8:04:32<7:15:06,  2.61s/batch]Batch 10200/20010 Done, mean position loss: 24.623137929439544\n",
      "Training growing_up:  51%|████▌    | 10282/20010 [8:04:34<6:39:32,  2.46s/batch]Batch 10200/20010 Done, mean position loss: 24.330076658725737\n",
      "Training growing_up:  50%|████▌    | 10041/20010 [8:04:37<7:23:59,  2.67s/batch]Batch 10200/20010 Done, mean position loss: 24.08476701259613\n",
      "Training growing_up:  50%|████▌    | 10014/20010 [8:04:40<7:10:19,  2.58s/batch]Batch 10200/20010 Done, mean position loss: 23.995129618644714\n",
      "Training growing_up:  51%|████▌    | 10201/20010 [8:04:40<7:22:54,  2.71s/batch]Batch 10300/20010 Done, mean position loss: 24.778353238105773\n",
      "Training growing_up:  51%|████▌    | 10245/20010 [8:04:50<6:10:39,  2.28s/batch]Batch 10200/20010 Done, mean position loss: 23.909091844558716\n",
      "Training growing_up:  51%|████▌    | 10142/20010 [8:04:58<6:28:53,  2.36s/batch]Batch 10300/20010 Done, mean position loss: 23.24852741241455\n",
      "Training growing_up:  51%|████▌    | 10124/20010 [8:05:12<7:16:41,  2.65s/batch]Batch 10000/20010 Done, mean position loss: 23.744019310474393\n",
      "Training growing_up:  51%|████▌    | 10144/20010 [8:05:21<6:20:52,  2.32s/batch]Batch 10300/20010 Done, mean position loss: 24.433066780567167\n",
      "Training growing_up:  51%|████▌    | 10145/20010 [8:05:23<6:32:22,  2.39s/batch]Batch 10200/20010 Done, mean position loss: 23.393071417808535\n",
      "Training growing_up:  51%|████▌    | 10195/20010 [8:05:41<6:45:47,  2.48s/batch]Batch 10000/20010 Done, mean position loss: 23.72504209756851\n",
      "Training growing_up:  51%|████▌    | 10187/20010 [8:05:54<6:31:04,  2.39s/batch]Batch 10100/20010 Done, mean position loss: 23.554840667247774\n",
      "Training growing_up:  51%|████▌    | 10233/20010 [8:05:55<6:26:30,  2.37s/batch]Batch 10200/20010 Done, mean position loss: 24.054490418434142\n",
      "Training growing_up:  51%|████▌    | 10234/20010 [8:05:58<5:56:54,  2.19s/batch]Batch 10200/20010 Done, mean position loss: 23.83151190519333\n",
      "Training growing_up:  52%|████▋    | 10332/20010 [8:06:09<5:54:02,  2.19s/batch]Batch 10400/20010 Done, mean position loss: 23.165242307186126\n",
      "Training growing_up:  50%|████▌    | 10057/20010 [8:06:12<6:48:30,  2.46s/batch]Batch 10300/20010 Done, mean position loss: 23.092243375778196\n",
      "Training growing_up:  50%|████▌    | 10086/20010 [8:06:27<6:47:21,  2.46s/batch]Batch 10200/20010 Done, mean position loss: 24.42628742456436\n",
      "Batch 10200/20010 Done, mean position loss: 24.307296447753906\n",
      "Training growing_up:  51%|████▋    | 10292/20010 [8:06:44<6:29:12,  2.40s/batch]Batch 10300/20010 Done, mean position loss: 23.15638882637024\n",
      "Training growing_up:  51%|████▌    | 10269/20010 [8:06:46<6:17:37,  2.33s/batch]Batch 10300/20010 Done, mean position loss: 23.908408501148223\n",
      "Training growing_up:  51%|████▌    | 10274/20010 [8:06:49<6:55:07,  2.56s/batch]Batch 10200/20010 Done, mean position loss: 24.012709007263183\n",
      "Training growing_up:  52%|████▋    | 10422/20010 [8:06:57<5:57:33,  2.24s/batch]Batch 10200/20010 Done, mean position loss: 24.593822209835054\n",
      "Training growing_up:  50%|████▌    | 10078/20010 [8:07:05<6:54:20,  2.50s/batch]Batch 10300/20010 Done, mean position loss: 23.92578714847565\n",
      "Batch 10100/20010 Done, mean position loss: 24.738882222175597\n",
      "Training growing_up:  50%|████▌    | 10091/20010 [8:07:14<6:52:13,  2.49s/batch]Batch 10200/20010 Done, mean position loss: 24.135032324790956\n",
      "Training growing_up:  51%|████▌    | 10281/20010 [8:07:14<5:59:22,  2.22s/batch]Batch 10300/20010 Done, mean position loss: 23.548791432380675\n",
      "Training growing_up:  51%|████▋    | 10288/20010 [8:07:16<6:36:39,  2.45s/batch]Batch 10300/20010 Done, mean position loss: 23.17389909744263\n",
      "Training growing_up:  52%|████▋    | 10363/20010 [8:07:27<6:27:10,  2.41s/batch]Batch 10200/20010 Done, mean position loss: 23.08821367263794\n",
      "Training growing_up:  52%|████▋    | 10357/20010 [8:07:38<6:50:15,  2.55s/batch]Batch 10100/20010 Done, mean position loss: 23.59108288526535\n",
      "Training growing_up:  51%|████▌    | 10231/20010 [8:07:42<6:56:17,  2.55s/batch]Batch 10200/20010 Done, mean position loss: 24.93400862455368\n",
      "Training growing_up:  51%|████▌    | 10282/20010 [8:07:50<7:13:27,  2.67s/batch]Batch 10300/20010 Done, mean position loss: 24.59102162361145\n",
      "Training growing_up:  51%|████▌    | 10224/20010 [8:07:52<6:28:51,  2.38s/batch]Batch 10300/20010 Done, mean position loss: 23.317002692222594\n",
      "Training growing_up:  52%|████▋    | 10373/20010 [8:07:55<6:51:52,  2.56s/batch]Batch 10300/20010 Done, mean position loss: 24.38052112340927\n",
      "Training growing_up:  51%|████▋    | 10304/20010 [8:08:00<6:39:25,  2.47s/batch]Batch 10400/20010 Done, mean position loss: 23.624296061992645\n",
      "Training growing_up:  51%|████▌    | 10209/20010 [8:08:03<6:54:45,  2.54s/batch]Batch 10100/20010 Done, mean position loss: 23.76324569225311\n",
      "Training growing_up:  50%|████▌    | 10060/20010 [8:08:09<8:44:25,  3.16s/batch]Batch 10300/20010 Done, mean position loss: 23.44817310094833\n",
      "Training growing_up:  51%|████▋    | 10289/20010 [8:08:15<8:14:25,  3.05s/batch]Batch 10200/20010 Done, mean position loss: 24.143907182216644\n",
      "Training growing_up:  51%|████▌    | 10199/20010 [8:08:26<6:42:18,  2.46s/batch]Batch 10100/20010 Done, mean position loss: 23.321303055286407\n",
      "Training growing_up:  51%|████▌    | 10165/20010 [8:08:32<7:08:46,  2.61s/batch]Batch 10200/20010 Done, mean position loss: 23.717750902175904\n",
      "Training growing_up:  51%|████▌    | 10113/20010 [8:08:36<6:53:02,  2.50s/batch]Batch 10200/20010 Done, mean position loss: 23.442325682640075\n",
      "Training growing_up:  50%|████▌    | 10071/20010 [8:08:38<6:27:48,  2.34s/batch]Batch 10300/20010 Done, mean position loss: 23.952237203121186\n",
      "Training growing_up:  51%|████▌    | 10169/20010 [8:08:40<6:00:07,  2.20s/batch]Batch 10300/20010 Done, mean position loss: 24.10040168046951\n",
      "Training growing_up:  51%|████▌    | 10108/20010 [8:08:43<6:56:17,  2.52s/batch]Batch 10300/20010 Done, mean position loss: 24.03659843921661\n",
      "Training growing_up:  51%|████▌    | 10255/20010 [8:08:46<7:53:31,  2.91s/batch]Batch 10400/20010 Done, mean position loss: 24.399561624526978\n",
      "Training growing_up:  50%|████▌    | 10074/20010 [8:08:47<7:17:35,  2.64s/batch]Batch 10300/20010 Done, mean position loss: 23.70986138343811\n",
      "Training growing_up:  52%|████▋    | 10395/20010 [8:08:51<6:34:54,  2.46s/batch]Batch 10300/20010 Done, mean position loss: 23.73858889102936\n",
      "Training growing_up:  51%|████▌    | 10264/20010 [8:09:08<6:30:17,  2.40s/batch]Batch 10400/20010 Done, mean position loss: 23.207673552036287\n",
      "Training growing_up:  52%|████▋    | 10363/20010 [8:09:21<6:12:07,  2.31s/batch]Batch 10100/20010 Done, mean position loss: 23.77923658847809\n",
      "Training growing_up:  52%|████▋    | 10333/20010 [8:09:31<7:12:36,  2.68s/batch]Batch 10400/20010 Done, mean position loss: 23.75225970029831\n",
      "Training growing_up:  51%|████▌    | 10227/20010 [8:09:38<6:34:44,  2.42s/batch]Batch 10300/20010 Done, mean position loss: 23.05862459897995\n",
      "Training growing_up:  51%|████▌    | 10243/20010 [8:10:03<7:28:03,  2.75s/batch]Batch 10100/20010 Done, mean position loss: 23.17425488471985\n",
      "Training growing_up:  50%|████▌    | 10102/20010 [8:10:04<6:42:23,  2.44s/batch]Batch 10200/20010 Done, mean position loss: 23.538769574165343\n",
      "Training growing_up:  51%|████▌    | 10204/20010 [8:10:11<6:06:19,  2.24s/batch]Batch 10300/20010 Done, mean position loss: 24.095494511127473\n",
      "Training growing_up:  52%|████▋    | 10384/20010 [8:10:18<6:34:48,  2.46s/batch]Batch 10500/20010 Done, mean position loss: 23.25633682012558\n",
      "Training growing_up:  51%|████▋    | 10290/20010 [8:10:19<7:40:33,  2.84s/batch]Batch 10300/20010 Done, mean position loss: 23.926074843406678\n",
      "Training growing_up:  52%|████▋    | 10422/20010 [8:10:22<6:27:00,  2.42s/batch]Batch 10400/20010 Done, mean position loss: 22.78875174999237\n",
      "Training growing_up:  52%|████▋    | 10469/20010 [8:10:47<6:20:41,  2.39s/batch]Batch 10300/20010 Done, mean position loss: 24.25616238117218\n",
      "Training growing_up:  51%|████▌    | 10119/20010 [8:10:50<7:29:17,  2.73s/batch]Batch 10300/20010 Done, mean position loss: 24.411617622375488\n",
      "Training growing_up:  52%|████▋    | 10445/20010 [8:10:58<6:50:17,  2.57s/batch]Batch 10400/20010 Done, mean position loss: 22.852107548713683\n",
      "Training growing_up:  52%|████▋    | 10333/20010 [8:11:01<6:20:35,  2.36s/batch]Batch 10400/20010 Done, mean position loss: 23.36245896100998\n",
      "Training growing_up:  52%|████▋    | 10357/20010 [8:11:05<6:35:04,  2.46s/batch]Batch 10300/20010 Done, mean position loss: 23.85082165956497\n",
      "Training growing_up:  51%|████▌    | 10232/20010 [8:11:19<6:11:38,  2.28s/batch]Batch 10300/20010 Done, mean position loss: 23.7365994477272\n",
      "Training growing_up:  52%|████▋    | 10329/20010 [8:11:21<6:50:56,  2.55s/batch]Batch 10400/20010 Done, mean position loss: 23.915431191921236\n",
      "Training growing_up:  52%|████▋    | 10402/20010 [8:11:24<6:23:28,  2.39s/batch]Batch 10400/20010 Done, mean position loss: 23.67802052497864\n",
      "Training growing_up:  52%|████▋    | 10343/20010 [8:11:27<7:18:26,  2.72s/batch]Batch 10200/20010 Done, mean position loss: 24.872792193889616\n",
      "Training growing_up:  51%|████▋    | 10291/20010 [8:11:28<7:12:33,  2.67s/batch]Batch 10300/20010 Done, mean position loss: 24.45251522779465\n",
      "Training growing_up:  51%|████▌    | 10237/20010 [8:11:32<6:56:38,  2.56s/batch]Batch 10400/20010 Done, mean position loss: 23.13392210960388\n",
      "Training growing_up:  51%|████▋    | 10299/20010 [8:11:47<6:45:11,  2.50s/batch]Batch 10300/20010 Done, mean position loss: 23.140579912662506\n",
      "Training growing_up:  52%|████▋    | 10328/20010 [8:11:52<6:35:03,  2.45s/batch]Batch 10300/20010 Done, mean position loss: 25.32787566423416\n",
      "Training growing_up:  51%|████▌    | 10199/20010 [8:12:03<7:55:45,  2.91s/batch]Batch 10500/20010 Done, mean position loss: 23.771321969032286\n",
      "Training growing_up:  52%|████▋    | 10331/20010 [8:12:07<6:30:28,  2.42s/batch]Batch 10400/20010 Done, mean position loss: 24.191990365982058\n",
      "Training growing_up:  53%|████▋    | 10546/20010 [8:12:07<6:55:33,  2.63s/batch]Batch 10200/20010 Done, mean position loss: 23.448786206245423\n",
      "Training growing_up:  52%|████▋    | 10360/20010 [8:12:08<6:20:22,  2.36s/batch]Batch 10400/20010 Done, mean position loss: 23.216205050945284\n",
      "Training growing_up:  53%|████▋    | 10547/20010 [8:12:09<6:35:05,  2.51s/batch]Batch 10400/20010 Done, mean position loss: 24.530300722122192\n",
      "Training growing_up:  52%|████▋    | 10353/20010 [8:12:24<7:49:05,  2.91s/batch]Batch 10400/20010 Done, mean position loss: 23.17049400806427\n",
      "Training growing_up:  52%|████▋    | 10436/20010 [8:12:25<6:35:24,  2.48s/batch]Batch 10300/20010 Done, mean position loss: 23.759878630638124\n",
      "Training growing_up:  51%|████▋    | 10293/20010 [8:12:26<6:49:47,  2.53s/batch]Batch 10200/20010 Done, mean position loss: 23.89007873773575\n",
      "Training growing_up:  52%|████▋    | 10375/20010 [8:12:45<6:57:10,  2.60s/batch]Batch 10200/20010 Done, mean position loss: 22.76643792629242\n",
      "Training growing_up:  53%|████▊    | 10562/20010 [8:12:45<6:05:27,  2.32s/batch]Batch 10300/20010 Done, mean position loss: 24.405779316425324\n",
      "Training growing_up:  52%|████▋    | 10446/20010 [8:12:51<6:57:59,  2.62s/batch]Batch 10400/20010 Done, mean position loss: 24.2619106054306\n",
      "Training growing_up:  52%|████▋    | 10363/20010 [8:12:53<6:18:18,  2.35s/batch]Batch 10300/20010 Done, mean position loss: 23.190180690288543\n",
      "Training growing_up:  52%|████▋    | 10354/20010 [8:12:56<6:30:48,  2.43s/batch]Batch 10400/20010 Done, mean position loss: 24.74693262577057\n",
      "Training growing_up:  52%|████▋    | 10449/20010 [8:12:58<6:52:50,  2.59s/batch]Batch 10400/20010 Done, mean position loss: 24.73725550413132\n",
      "Training growing_up:  51%|████▌    | 10206/20010 [8:13:00<7:32:29,  2.77s/batch]Batch 10500/20010 Done, mean position loss: 24.374506764411926\n",
      "Training growing_up:  52%|████▋    | 10342/20010 [8:13:05<6:55:44,  2.58s/batch]Batch 10400/20010 Done, mean position loss: 23.6890726518631\n",
      "Training growing_up:  52%|████▋    | 10465/20010 [8:13:06<7:19:11,  2.76s/batch]Batch 10400/20010 Done, mean position loss: 23.64820427417755\n",
      "Training growing_up:  52%|████▋    | 10315/20010 [8:13:19<6:10:26,  2.29s/batch]Batch 10500/20010 Done, mean position loss: 23.020700757503512\n",
      "Training growing_up:  52%|████▋    | 10437/20010 [8:13:41<7:00:11,  2.63s/batch]Batch 10200/20010 Done, mean position loss: 23.985731992721558\n",
      "Training growing_up:  52%|████▋    | 10355/20010 [8:13:43<6:47:17,  2.53s/batch]Batch 10500/20010 Done, mean position loss: 23.579860446453097\n",
      "Training growing_up:  52%|████▋    | 10439/20010 [8:13:53<6:49:01,  2.56s/batch]Batch 10400/20010 Done, mean position loss: 23.26310407161713\n",
      "Training growing_up:  52%|████▋    | 10495/20010 [8:14:17<6:50:12,  2.59s/batch]Batch 10300/20010 Done, mean position loss: 24.20589897632599\n",
      "Training growing_up:  52%|████▋    | 10349/20010 [8:14:27<6:40:53,  2.49s/batch]Batch 10600/20010 Done, mean position loss: 23.442394325733183\n",
      "Training growing_up:  51%|████▌    | 10240/20010 [8:14:28<7:18:14,  2.69s/batch]Batch 10400/20010 Done, mean position loss: 23.914213209152223\n",
      "Training growing_up:  52%|████▋    | 10452/20010 [8:14:28<7:15:44,  2.74s/batch]Batch 10200/20010 Done, mean position loss: 23.254472444057466\n",
      "Training growing_up:  52%|████▋    | 10343/20010 [8:14:30<7:19:48,  2.73s/batch]Batch 10400/20010 Done, mean position loss: 23.21270422935486\n",
      "Training growing_up:  52%|████▋    | 10434/20010 [8:14:32<7:00:04,  2.63s/batch]Batch 10500/20010 Done, mean position loss: 22.92500035762787\n",
      "Training growing_up:  52%|████▋    | 10461/20010 [8:14:57<6:33:44,  2.47s/batch]Batch 10400/20010 Done, mean position loss: 23.767244145870208\n",
      "Training growing_up:  52%|████▋    | 10465/20010 [8:15:00<6:09:57,  2.33s/batch]Batch 10400/20010 Done, mean position loss: 24.362145943641664\n",
      "Training growing_up:  52%|████▋    | 10381/20010 [8:15:09<6:34:34,  2.46s/batch]Batch 10500/20010 Done, mean position loss: 22.809695653915405\n",
      "Training growing_up:  53%|████▊    | 10621/20010 [8:15:14<6:25:57,  2.47s/batch]Batch 10500/20010 Done, mean position loss: 23.44675515651703\n",
      "Training growing_up:  52%|████▋    | 10496/20010 [8:15:19<5:27:03,  2.06s/batch]Batch 10400/20010 Done, mean position loss: 24.318548183441163\n",
      "Training growing_up:  52%|████▋    | 10475/20010 [8:15:29<5:14:45,  1.98s/batch]Batch 10400/20010 Done, mean position loss: 24.147795584201813\n",
      "Training growing_up:  52%|████▋    | 10401/20010 [8:15:30<5:24:11,  2.02s/batch]Batch 10500/20010 Done, mean position loss: 23.700745186805726\n",
      "Training growing_up:  52%|████▋    | 10428/20010 [8:15:30<6:16:59,  2.36s/batch]Batch 10400/20010 Done, mean position loss: 24.357876760959627\n",
      "Training growing_up:  51%|████▌    | 10277/20010 [8:15:33<6:03:06,  2.24s/batch]Batch 10500/20010 Done, mean position loss: 23.87623382806778\n",
      "Training growing_up:  53%|████▋    | 10513/20010 [8:15:43<6:47:14,  2.57s/batch]Batch 10500/20010 Done, mean position loss: 22.930057859420778\n",
      "Training growing_up:  51%|████▌    | 10251/20010 [8:15:45<6:41:29,  2.47s/batch]Batch 10300/20010 Done, mean position loss: 24.21202198982239\n",
      "Training growing_up:  52%|████▋    | 10468/20010 [8:15:53<6:23:57,  2.41s/batch]Batch 10400/20010 Done, mean position loss: 25.077938718795778\n",
      "Training growing_up:  51%|████▋    | 10296/20010 [8:16:00<6:27:59,  2.40s/batch]Batch 10400/20010 Done, mean position loss: 23.351083211898803\n",
      "Training growing_up:  51%|████▌    | 10240/20010 [8:16:07<7:55:06,  2.92s/batch]Batch 10600/20010 Done, mean position loss: 23.624891917705536\n",
      "Training growing_up:  53%|████▋    | 10517/20010 [8:16:11<7:13:34,  2.74s/batch]Batch 10300/20010 Done, mean position loss: 23.90385807514191\n",
      "Training growing_up:  52%|████▋    | 10425/20010 [8:16:15<7:00:39,  2.63s/batch]Batch 10500/20010 Done, mean position loss: 23.68196798563004\n",
      "Training growing_up:  52%|████▋    | 10382/20010 [8:16:17<7:45:28,  2.90s/batch]Batch 10500/20010 Done, mean position loss: 23.235828447341916\n",
      "Training growing_up:  51%|████▋    | 10286/20010 [8:16:27<6:49:14,  2.53s/batch]Batch 10500/20010 Done, mean position loss: 23.563083429336547\n",
      "Training growing_up:  52%|████▋    | 10412/20010 [8:16:29<7:12:15,  2.70s/batch]Batch 10300/20010 Done, mean position loss: 23.64302013158798\n",
      "Training growing_up:  52%|████▋    | 10414/20010 [8:16:34<6:33:46,  2.46s/batch]Batch 10400/20010 Done, mean position loss: 23.418933255672457\n",
      "Training growing_up:  53%|████▋    | 10527/20010 [8:16:34<6:41:27,  2.54s/batch]Batch 10500/20010 Done, mean position loss: 23.815429377555848\n",
      "Training growing_up:  52%|████▋    | 10329/20010 [8:16:56<6:39:24,  2.48s/batch]Batch 10500/20010 Done, mean position loss: 23.556102385520937\n",
      "Training growing_up:  52%|████▋    | 10494/20010 [8:16:56<7:54:11,  2.99s/batch]Batch 10400/20010 Done, mean position loss: 23.6786269903183\n",
      "Training growing_up:  52%|████▋    | 10330/20010 [8:16:58<6:21:04,  2.36s/batch]Batch 10500/20010 Done, mean position loss: 24.54918937444687\n",
      "Training growing_up:  53%|████▋    | 10541/20010 [8:17:01<5:18:49,  2.02s/batch]Batch 10600/20010 Done, mean position loss: 24.27401378631592\n",
      "Training growing_up:  53%|████▊    | 10583/20010 [8:17:02<5:51:26,  2.24s/batch]Batch 10300/20010 Done, mean position loss: 22.59312784433365\n",
      "Training growing_up:  51%|████▋    | 10303/20010 [8:17:06<6:20:29,  2.35s/batch]Batch 10400/20010 Done, mean position loss: 23.32598701238632\n",
      "Training growing_up:  53%|████▊    | 10566/20010 [8:17:08<6:32:57,  2.50s/batch]Batch 10500/20010 Done, mean position loss: 23.544073755741117\n",
      "Training growing_up:  52%|████▋    | 10328/20010 [8:17:12<6:29:23,  2.41s/batch]Batch 10500/20010 Done, mean position loss: 24.307977457046512\n",
      "Training growing_up:  52%|████▋    | 10501/20010 [8:17:13<7:09:08,  2.71s/batch]Batch 10500/20010 Done, mean position loss: 23.781753396987913\n",
      "Training growing_up:  52%|████▋    | 10377/20010 [8:17:20<6:24:06,  2.39s/batch]Batch 10600/20010 Done, mean position loss: 23.121456885337828\n",
      "Training growing_up:  52%|████▋    | 10471/20010 [8:17:43<6:26:00,  2.43s/batch]Batch 10300/20010 Done, mean position loss: 23.407356116771695\n",
      "Training growing_up:  53%|████▊    | 10619/20010 [8:17:46<7:02:36,  2.70s/batch]Batch 10600/20010 Done, mean position loss: 23.256242280006408\n",
      "Training growing_up:  52%|████▋    | 10447/20010 [8:17:56<6:10:50,  2.33s/batch]Batch 10500/20010 Done, mean position loss: 22.713093323707582\n",
      "Training growing_up:  53%|████▋    | 10546/20010 [8:18:16<5:58:17,  2.27s/batch]Batch 10400/20010 Done, mean position loss: 23.359775803089143\n",
      "Training growing_up:  53%|████▋    | 10535/20010 [8:18:32<6:57:32,  2.64s/batch]Batch 10500/20010 Done, mean position loss: 23.67512620449066\n",
      "Batch 10600/20010 Done, mean position loss: 22.831474328041075\n",
      "Training growing_up:  53%|████▊    | 10576/20010 [8:18:33<7:24:22,  2.83s/batch]Batch 10700/20010 Done, mean position loss: 23.118171861171724\n",
      "Training growing_up:  53%|████▋    | 10540/20010 [8:18:37<6:44:01,  2.56s/batch]Batch 10500/20010 Done, mean position loss: 23.694750690460204\n",
      "Training growing_up:  52%|████▋    | 10487/20010 [8:18:43<7:18:35,  2.76s/batch]Batch 10300/20010 Done, mean position loss: 23.428617753982543\n",
      "Training growing_up:  52%|████▋    | 10494/20010 [8:18:59<6:16:49,  2.38s/batch]Batch 10500/20010 Done, mean position loss: 23.82830471992493\n",
      "Training growing_up:  53%|████▋    | 10528/20010 [8:19:00<6:10:07,  2.34s/batch]Batch 10500/20010 Done, mean position loss: 24.067746634483335\n",
      "Training growing_up:  52%|████▋    | 10368/20010 [8:19:10<6:19:09,  2.36s/batch]Batch 10600/20010 Done, mean position loss: 22.729859507083894\n",
      "Training growing_up:  52%|████▋    | 10354/20010 [8:19:11<5:30:34,  2.05s/batch]Batch 10600/20010 Done, mean position loss: 23.593143849372865\n",
      "Training growing_up:  53%|████▋    | 10553/20010 [8:19:17<6:47:20,  2.58s/batch]Batch 10500/20010 Done, mean position loss: 24.177631053924564\n",
      "Training growing_up:  52%|████▋    | 10499/20010 [8:19:24<6:35:15,  2.49s/batch]Batch 10500/20010 Done, mean position loss: 24.705456101894377\n",
      "Training growing_up:  52%|████▋    | 10475/20010 [8:19:28<6:39:34,  2.51s/batch]Batch 10600/20010 Done, mean position loss: 23.69958012104034\n",
      "Training growing_up:  53%|████▋    | 10513/20010 [8:19:29<6:07:54,  2.32s/batch]Batch 10500/20010 Done, mean position loss: 23.84158152580261\n",
      "Training growing_up:  53%|████▊    | 10576/20010 [8:19:31<5:28:50,  2.09s/batch]Batch 10600/20010 Done, mean position loss: 23.78622082710266\n",
      "Training growing_up:  53%|████▊    | 10631/20010 [8:19:44<6:40:57,  2.57s/batch]Batch 10600/20010 Done, mean position loss: 22.770633165836337\n",
      "Training growing_up:  52%|████▋    | 10492/20010 [8:19:50<6:56:16,  2.62s/batch]Batch 10500/20010 Done, mean position loss: 25.44893604040146\n",
      "Training growing_up:  53%|████▊    | 10619/20010 [8:19:54<6:04:23,  2.33s/batch]Batch 10400/20010 Done, mean position loss: 24.576648852825166\n",
      "Training growing_up:  53%|████▋    | 10524/20010 [8:19:56<6:04:15,  2.30s/batch]Batch 10700/20010 Done, mean position loss: 23.570703575611113\n",
      "Training growing_up:  53%|████▊    | 10593/20010 [8:20:11<6:07:55,  2.34s/batch]Batch 10500/20010 Done, mean position loss: 23.312127647399905\n",
      "Training growing_up:  52%|████▋    | 10503/20010 [8:20:15<5:22:50,  2.04s/batch]Batch 10400/20010 Done, mean position loss: 22.8414325094223\n",
      "Training growing_up:  52%|████▋    | 10364/20010 [8:20:16<5:40:53,  2.12s/batch]Batch 10600/20010 Done, mean position loss: 24.409608187675477\n",
      "Training growing_up:  53%|████▋    | 10526/20010 [8:20:18<6:06:45,  2.32s/batch]Batch 10600/20010 Done, mean position loss: 23.51780195713043\n",
      "Training growing_up:  53%|████▊    | 10688/20010 [8:20:29<5:46:00,  2.23s/batch]Batch 10600/20010 Done, mean position loss: 24.179175863265993\n",
      "Training growing_up:  52%|████▋    | 10485/20010 [8:20:29<6:38:59,  2.51s/batch]Batch 10400/20010 Done, mean position loss: 24.21741068601608\n",
      "Training growing_up:  53%|████▊    | 10589/20010 [8:20:32<7:02:37,  2.69s/batch]Batch 10500/20010 Done, mean position loss: 23.6091313791275\n",
      "Training growing_up:  52%|████▋    | 10457/20010 [8:20:34<6:33:10,  2.47s/batch]Batch 10600/20010 Done, mean position loss: 23.585749731063842\n",
      "Training growing_up:  52%|████▋    | 10410/20010 [8:20:49<5:28:08,  2.05s/batch]Batch 10500/20010 Done, mean position loss: 24.00411649465561\n",
      "Training growing_up:  53%|████▋    | 10521/20010 [8:20:59<6:43:59,  2.55s/batch]Batch 10700/20010 Done, mean position loss: 24.34894567012787\n",
      "Training growing_up:  53%|████▊    | 10647/20010 [8:21:01<6:09:24,  2.37s/batch]Batch 10600/20010 Done, mean position loss: 23.647663152217866\n",
      "Training growing_up:  53%|████▋    | 10543/20010 [8:21:01<6:35:03,  2.50s/batch]Batch 10400/20010 Done, mean position loss: 22.6746844124794\n",
      "Training growing_up:  53%|████▊    | 10621/20010 [8:21:09<6:47:11,  2.60s/batch]Batch 10600/20010 Done, mean position loss: 24.729292342662813\n",
      "Training growing_up:  53%|████▊    | 10650/20010 [8:21:09<6:36:37,  2.54s/batch]Batch 10600/20010 Done, mean position loss: 23.264733152389525\n",
      "Training growing_up:  53%|████▊    | 10605/20010 [8:21:10<6:09:51,  2.36s/batch]Batch 10500/20010 Done, mean position loss: 22.79400869846344\n",
      "Training growing_up:  52%|████▋    | 10387/20010 [8:21:11<6:10:07,  2.31s/batch]Batch 10600/20010 Done, mean position loss: 24.001705660820008\n",
      "Training growing_up:  53%|████▋    | 10556/20010 [8:21:16<6:37:51,  2.53s/batch]Batch 10600/20010 Done, mean position loss: 24.415961883068086\n",
      "Training growing_up:  53%|████▊    | 10570/20010 [8:21:16<6:13:46,  2.38s/batch]Batch 10700/20010 Done, mean position loss: 23.085370683670043\n",
      "Training growing_up:  52%|████▋    | 10485/20010 [8:21:42<6:32:55,  2.48s/batch]Batch 10700/20010 Done, mean position loss: 23.214855313301086\n",
      "Training growing_up:  53%|████▊    | 10593/20010 [8:21:43<7:14:52,  2.77s/batch]Batch 10400/20010 Done, mean position loss: 23.79080288887024\n",
      "Training growing_up:  53%|████▋    | 10539/20010 [8:22:01<6:07:52,  2.33s/batch]Batch 10600/20010 Done, mean position loss: 23.196344168186187\n",
      "Training growing_up:  52%|████▋    | 10418/20010 [8:22:22<5:56:05,  2.23s/batch]Batch 10500/20010 Done, mean position loss: 23.28621761083603\n",
      "Training growing_up:  54%|████▊    | 10766/20010 [8:22:31<6:10:29,  2.40s/batch]Batch 10800/20010 Done, mean position loss: 22.941335968971252\n",
      "Training growing_up:  53%|████▊    | 10635/20010 [8:22:32<6:56:50,  2.67s/batch]Batch 10600/20010 Done, mean position loss: 23.83260375022888\n",
      "Training growing_up:  54%|████▊    | 10804/20010 [8:22:37<5:30:30,  2.15s/batch]Batch 10600/20010 Done, mean position loss: 23.5557098698616\n",
      "Batch 10700/20010 Done, mean position loss: 22.982889177799223\n",
      "Training growing_up:  53%|████▊    | 10561/20010 [8:22:54<7:26:38,  2.84s/batch]Batch 10400/20010 Done, mean position loss: 23.31639738798141\n",
      "Training growing_up:  54%|████▊    | 10777/20010 [8:22:59<6:29:04,  2.53s/batch]Batch 10600/20010 Done, mean position loss: 23.991610782146456\n",
      "Training growing_up:  53%|████▊    | 10583/20010 [8:23:01<5:46:20,  2.20s/batch]Batch 10600/20010 Done, mean position loss: 23.936183714866637\n",
      "Training growing_up:  52%|████▋    | 10454/20010 [8:23:14<6:54:33,  2.60s/batch]Batch 10700/20010 Done, mean position loss: 23.477579281330108\n",
      "Training growing_up:  52%|████▋    | 10438/20010 [8:23:14<6:51:38,  2.58s/batch]Batch 10700/20010 Done, mean position loss: 22.78329431056976\n",
      "Training growing_up:  53%|████▊    | 10624/20010 [8:23:24<5:42:41,  2.19s/batch]Batch 10600/20010 Done, mean position loss: 24.05457473278046\n",
      "Training growing_up:  54%|████▊    | 10721/20010 [8:23:25<6:07:35,  2.37s/batch]Batch 10600/20010 Done, mean position loss: 23.695386040210725\n",
      "Training growing_up:  53%|████▊    | 10700/20010 [8:23:31<6:21:08,  2.46s/batch]Batch 10600/20010 Done, mean position loss: 24.338464846611025\n",
      "Training growing_up:  53%|████▊    | 10602/20010 [8:23:33<6:03:42,  2.32s/batch]Batch 10700/20010 Done, mean position loss: 23.6614103102684\n",
      "Training growing_up:  53%|████▊    | 10583/20010 [8:23:34<6:42:32,  2.56s/batch]Batch 10700/20010 Done, mean position loss: 23.669059398174284\n",
      "Training growing_up:  53%|████▊    | 10681/20010 [8:23:44<6:27:28,  2.49s/batch]Batch 10600/20010 Done, mean position loss: 24.587916321754456\n",
      "Training growing_up:  54%|████▊    | 10714/20010 [8:23:45<5:56:57,  2.30s/batch]Batch 10700/20010 Done, mean position loss: 22.915910477638242\n",
      "Training growing_up:  53%|████▊    | 10688/20010 [8:23:52<5:35:32,  2.16s/batch]Batch 10500/20010 Done, mean position loss: 24.628233325481418\n",
      "Training growing_up:  54%|████▊    | 10773/20010 [8:23:56<6:19:27,  2.46s/batch]Batch 10800/20010 Done, mean position loss: 23.466858332157138\n",
      "Training growing_up:  53%|████▊    | 10645/20010 [8:24:19<6:55:54,  2.66s/batch]Batch 10600/20010 Done, mean position loss: 22.800009813308716\n",
      "Training growing_up:  54%|████▊    | 10813/20010 [8:24:24<5:56:44,  2.33s/batch]Batch 10700/20010 Done, mean position loss: 23.207331624031067\n",
      "Batch 10700/20010 Done, mean position loss: 23.66376015663147\n",
      "Training growing_up:  53%|████▊    | 10646/20010 [8:24:28<6:14:38,  2.40s/batch]Batch 10500/20010 Done, mean position loss: 23.286665608882906\n",
      "Training growing_up:  52%|████▋    | 10499/20010 [8:24:33<6:16:58,  2.38s/batch]Batch 10700/20010 Done, mean position loss: 24.26809559345245\n",
      "Training growing_up:  53%|████▋    | 10518/20010 [8:24:34<6:12:25,  2.35s/batch]Batch 10700/20010 Done, mean position loss: 23.312235279083254\n",
      "Training growing_up:  53%|████▊    | 10608/20010 [8:24:35<6:00:45,  2.30s/batch]Batch 10600/20010 Done, mean position loss: 23.590013585090638\n",
      "Training growing_up:  53%|████▊    | 10609/20010 [8:24:38<6:08:20,  2.35s/batch]Batch 10500/20010 Done, mean position loss: 23.70057356119156\n",
      "Training growing_up:  54%|████▊    | 10740/20010 [8:24:50<6:36:14,  2.56s/batch]Batch 10600/20010 Done, mean position loss: 24.152971277236936\n",
      "Training growing_up:  53%|████▊    | 10698/20010 [8:25:02<6:11:26,  2.39s/batch]Batch 10700/20010 Done, mean position loss: 23.645055203437806\n",
      "Training growing_up:  53%|████▊    | 10679/20010 [8:25:05<5:54:03,  2.28s/batch]Batch 10800/20010 Done, mean position loss: 24.30590500116348\n",
      "Training growing_up:  54%|████▊    | 10749/20010 [8:25:09<6:08:30,  2.39s/batch]Batch 10800/20010 Done, mean position loss: 23.314868779182433\n",
      "Training growing_up:  53%|████▋    | 10515/20010 [8:25:10<6:08:46,  2.33s/batch]Batch 10700/20010 Done, mean position loss: 23.700790023803712\n",
      "Training growing_up:  53%|████▊    | 10662/20010 [8:25:10<7:03:48,  2.72s/batch]Batch 10500/20010 Done, mean position loss: 22.779673154354093\n",
      "Training growing_up:  53%|████▋    | 10520/20010 [8:25:14<6:44:24,  2.56s/batch]Batch 10700/20010 Done, mean position loss: 24.271411428451536\n",
      "Training growing_up:  54%|████▉    | 10872/20010 [8:25:14<5:44:03,  2.26s/batch]Batch 10700/20010 Done, mean position loss: 23.835855038166045\n",
      "Training growing_up:  54%|████▊    | 10792/20010 [8:25:20<6:16:55,  2.45s/batch]Batch 10600/20010 Done, mean position loss: 23.398846757411956\n",
      "Training growing_up:  54%|████▊    | 10770/20010 [8:25:24<6:50:39,  2.67s/batch]Batch 10700/20010 Done, mean position loss: 24.0989014673233\n",
      "Training growing_up:  53%|████▊    | 10667/20010 [8:25:41<6:47:59,  2.62s/batch]Batch 10800/20010 Done, mean position loss: 23.150749893188475\n",
      "Training growing_up:  52%|████▋    | 10472/20010 [8:25:55<6:45:38,  2.55s/batch]Batch 10500/20010 Done, mean position loss: 23.63221086263657\n",
      "Training growing_up:  53%|████▊    | 10628/20010 [8:25:59<6:51:48,  2.63s/batch]Batch 10700/20010 Done, mean position loss: 23.151304361820223\n",
      "Training growing_up:  52%|████▋    | 10485/20010 [8:26:27<6:26:44,  2.44s/batch]Batch 10900/20010 Done, mean position loss: 22.923020792007446\n",
      "Training growing_up:  54%|████▊    | 10729/20010 [8:26:33<6:34:26,  2.55s/batch]Batch 10600/20010 Done, mean position loss: 23.11879286289215\n",
      "Training growing_up:  54%|████▊    | 10732/20010 [8:26:40<6:07:19,  2.38s/batch]Batch 10700/20010 Done, mean position loss: 23.654450397491452\n",
      "Training growing_up:  54%|████▊    | 10785/20010 [8:26:42<6:34:34,  2.57s/batch]Batch 10800/20010 Done, mean position loss: 22.73696403980255\n",
      "Training growing_up:  53%|████▋    | 10557/20010 [8:26:47<6:11:05,  2.36s/batch]Batch 10700/20010 Done, mean position loss: 23.606427605152128\n",
      "Training growing_up:  54%|████▊    | 10764/20010 [8:27:10<7:32:23,  2.94s/batch]Batch 10700/20010 Done, mean position loss: 24.028874738216402\n",
      "Training growing_up:  53%|████▊    | 10693/20010 [8:27:14<7:14:35,  2.80s/batch]Batch 10500/20010 Done, mean position loss: 23.48989884376526\n",
      "Training growing_up:  53%|████▊    | 10646/20010 [8:27:16<7:11:45,  2.77s/batch]Batch 10800/20010 Done, mean position loss: 22.92900936603546\n",
      "Training growing_up:  53%|████▊    | 10694/20010 [8:27:17<7:36:35,  2.94s/batch]Batch 10700/20010 Done, mean position loss: 24.484134094715117\n",
      "Training growing_up:  53%|████▊    | 10673/20010 [8:27:24<7:04:48,  2.73s/batch]Batch 10800/20010 Done, mean position loss: 22.477236423492432\n",
      "Training growing_up:  53%|████▊    | 10625/20010 [8:27:39<8:59:08,  3.45s/batch]Batch 10700/20010 Done, mean position loss: 24.23674341917038\n",
      "Training growing_up:  54%|████▊    | 10711/20010 [8:27:41<9:08:47,  3.54s/batch]Batch 10700/20010 Done, mean position loss: 23.508062584400175\n",
      "Training growing_up:  54%|████▉    | 10849/20010 [8:27:46<8:25:50,  3.31s/batch]Batch 10800/20010 Done, mean position loss: 23.43427977800369\n",
      "Training growing_up:  54%|████▊    | 10812/20010 [8:27:50<7:32:18,  2.95s/batch]Batch 10700/20010 Done, mean position loss: 24.69959129333496\n",
      "Training growing_up:  54%|████▊    | 10803/20010 [8:27:52<8:02:03,  3.14s/batch]Batch 10800/20010 Done, mean position loss: 23.742386317253114\n",
      "Training growing_up:  53%|████▊    | 10578/20010 [8:28:00<8:16:27,  3.16s/batch]Batch 10700/20010 Done, mean position loss: 24.5000741481781\n",
      "Training growing_up:  53%|████▊    | 10579/20010 [8:28:02<7:43:06,  2.95s/batch]Batch 10800/20010 Done, mean position loss: 23.14468550443649\n",
      "Training growing_up:  53%|████▊    | 10691/20010 [8:28:15<6:55:16,  2.67s/batch]Batch 10900/20010 Done, mean position loss: 23.621941623687743\n",
      "Training growing_up:  54%|████▊    | 10787/20010 [8:28:22<8:20:51,  3.26s/batch]Batch 10600/20010 Done, mean position loss: 25.14062625646591\n",
      "Training growing_up:  54%|████▊    | 10821/20010 [8:28:46<7:01:02,  2.75s/batch]Batch 10700/20010 Done, mean position loss: 23.523663630485533\n",
      "Training growing_up:  55%|████▉    | 10913/20010 [8:28:52<8:04:09,  3.19s/batch]Batch 10800/20010 Done, mean position loss: 23.77872833967209\n",
      "Training growing_up:  54%|████▊    | 10762/20010 [8:28:53<8:17:03,  3.22s/batch]Batch 10800/20010 Done, mean position loss: 23.19064513683319\n",
      "Training growing_up:  54%|████▊    | 10724/20010 [8:28:58<7:43:28,  2.99s/batch]Batch 10600/20010 Done, mean position loss: 22.856923205852507\n",
      "Training growing_up:  55%|████▉    | 10916/20010 [8:29:01<7:02:51,  2.79s/batch]Batch 10800/20010 Done, mean position loss: 23.6325376534462\n",
      "Training growing_up:  54%|████▊    | 10801/20010 [8:29:01<7:57:33,  3.11s/batch]Batch 10800/20010 Done, mean position loss: 24.28498803138733\n",
      "Training growing_up:  54%|████▊    | 10833/20010 [8:29:06<8:00:07,  3.14s/batch]Batch 10700/20010 Done, mean position loss: 23.68976770401001\n",
      "Training growing_up:  54%|████▊    | 10807/20010 [8:29:09<7:19:46,  2.87s/batch]Batch 10600/20010 Done, mean position loss: 23.731504511833194\n",
      "Training growing_up:  55%|████▉    | 10923/20010 [8:29:22<7:38:45,  3.03s/batch]Batch 10700/20010 Done, mean position loss: 23.72407193660736\n",
      "Training growing_up:  54%|████▊    | 10719/20010 [8:29:39<7:58:53,  3.09s/batch]Batch 10800/20010 Done, mean position loss: 23.507769019603728\n",
      "Training growing_up:  54%|████▊    | 10816/20010 [8:29:40<7:39:07,  3.00s/batch]Batch 10800/20010 Done, mean position loss: 23.23603696346283\n",
      "Training growing_up:  54%|████▉    | 10840/20010 [8:29:43<7:25:13,  2.91s/batch]Batch 10900/20010 Done, mean position loss: 22.822858986854555\n",
      "Training growing_up:  54%|████▊    | 10736/20010 [8:29:47<8:40:33,  3.37s/batch]Batch 10900/20010 Done, mean position loss: 24.451458654403687\n",
      "Training growing_up:  55%|████▉    | 10932/20010 [8:29:48<7:34:37,  3.00s/batch]Batch 10800/20010 Done, mean position loss: 24.371940536499025\n",
      "Training growing_up:  53%|████▊    | 10622/20010 [8:30:00<7:37:53,  2.93s/batch]Batch 10600/20010 Done, mean position loss: 22.44933182001114\n",
      "Training growing_up:  54%|████▊    | 10749/20010 [8:30:06<7:02:54,  2.74s/batch]Batch 10800/20010 Done, mean position loss: 23.866211223602296\n",
      "Training growing_up:  54%|████▊    | 10801/20010 [8:30:06<7:24:27,  2.90s/batch]Batch 10700/20010 Done, mean position loss: 23.0525257062912\n",
      "Training growing_up:  53%|████▋    | 10555/20010 [8:30:09<7:47:21,  2.97s/batch]Batch 10800/20010 Done, mean position loss: 24.215143177509308\n",
      "Training growing_up:  54%|████▊    | 10815/20010 [8:30:22<7:48:18,  3.06s/batch]Batch 10900/20010 Done, mean position loss: 23.347500987052918\n",
      "Training growing_up:  54%|████▉    | 10857/20010 [8:30:52<7:45:10,  3.05s/batch]Batch 10600/20010 Done, mean position loss: 23.359112136363983\n",
      "Training growing_up:  53%|████▊    | 10602/20010 [8:30:55<8:20:51,  3.19s/batch]Batch 10800/20010 Done, mean position loss: 23.13589073419571\n",
      "Training growing_up:  54%|████▉    | 10848/20010 [8:31:23<8:28:00,  3.33s/batch]Batch 11000/20010 Done, mean position loss: 23.26098683357239\n",
      "Training growing_up:  54%|████▉    | 10875/20010 [8:31:29<8:28:50,  3.34s/batch]Batch 10800/20010 Done, mean position loss: 23.447216298580173\n",
      "Training growing_up:  54%|████▊    | 10788/20010 [8:31:35<7:43:32,  3.02s/batch]Batch 10700/20010 Done, mean position loss: 23.330482432842253\n",
      "Training growing_up:  54%|████▊    | 10831/20010 [8:31:40<7:16:05,  2.85s/batch]Batch 10900/20010 Done, mean position loss: 22.87786856889725\n",
      "Training growing_up:  53%|████▊    | 10653/20010 [8:31:46<7:42:15,  2.96s/batch]Batch 10800/20010 Done, mean position loss: 23.424050266742704\n",
      "Training growing_up:  54%|████▉    | 10844/20010 [8:32:12<7:00:44,  2.75s/batch]Batch 10800/20010 Done, mean position loss: 24.195083096027375\n",
      "Training growing_up:  54%|████▉    | 10854/20010 [8:32:16<7:50:02,  3.08s/batch]Batch 10900/20010 Done, mean position loss: 22.999080822467803\n",
      "Training growing_up:  55%|████▉    | 10956/20010 [8:32:24<7:19:26,  2.91s/batch]Batch 10600/20010 Done, mean position loss: 22.97725971698761\n",
      "Training growing_up:  54%|████▉    | 10887/20010 [8:32:26<8:07:24,  3.21s/batch]Batch 10800/20010 Done, mean position loss: 24.23342172384262\n",
      "Training growing_up:  55%|████▉    | 10958/20010 [8:32:31<8:02:14,  3.20s/batch]Batch 10900/20010 Done, mean position loss: 22.435251576900484\n",
      "Training growing_up:  54%|████▉    | 10873/20010 [8:32:40<7:34:59,  2.99s/batch]Batch 10800/20010 Done, mean position loss: 23.669382565021515\n",
      "Training growing_up:  54%|████▊    | 10723/20010 [8:32:40<7:44:12,  3.00s/batch]Batch 10800/20010 Done, mean position loss: 24.245834612846377\n",
      "Training growing_up:  54%|████▊    | 10772/20010 [8:32:46<7:49:28,  3.05s/batch]Batch 10900/20010 Done, mean position loss: 23.579825031757352\n",
      "Training growing_up:  54%|████▉    | 10867/20010 [8:32:53<7:14:48,  2.85s/batch]Batch 10800/20010 Done, mean position loss: 24.920930166244503\n",
      "Training growing_up:  55%|████▉    | 10967/20010 [8:32:57<7:25:47,  2.96s/batch]Batch 10900/20010 Done, mean position loss: 23.786670050621034\n",
      "Training growing_up:  54%|████▊    | 10731/20010 [8:33:04<7:51:04,  3.05s/batch]Batch 10800/20010 Done, mean position loss: 24.569032695293426\n",
      "Training growing_up:  54%|████▊    | 10784/20010 [8:33:07<7:45:59,  3.03s/batch]Batch 10900/20010 Done, mean position loss: 23.00586795091629\n",
      "Training growing_up:  54%|████▊    | 10814/20010 [8:33:15<7:26:11,  2.91s/batch]Batch 11000/20010 Done, mean position loss: 23.576992194652554\n",
      "Training growing_up:  54%|████▊    | 10812/20010 [8:33:26<7:55:28,  3.10s/batch]Batch 10700/20010 Done, mean position loss: 23.99304037094116\n",
      "Training growing_up:  55%|████▉    | 10923/20010 [8:33:52<6:50:04,  2.71s/batch]Batch 10900/20010 Done, mean position loss: 22.989752674102782\n",
      "Training growing_up:  53%|████▊    | 10679/20010 [8:33:54<8:16:20,  3.19s/batch]Batch 10900/20010 Done, mean position loss: 23.75593514204025\n",
      "Training growing_up:  53%|████▊    | 10680/20010 [8:33:57<8:14:59,  3.18s/batch]Batch 10900/20010 Done, mean position loss: 23.932582831382753\n",
      "Training growing_up:  53%|████▊    | 10632/20010 [8:33:58<7:54:08,  3.03s/batch]Batch 10800/20010 Done, mean position loss: 22.938673224449158\n",
      "Training growing_up:  55%|████▉    | 10936/20010 [8:34:00<6:30:38,  2.58s/batch]Batch 10900/20010 Done, mean position loss: 23.53410882949829\n",
      "Training growing_up:  54%|████▊    | 10802/20010 [8:34:01<7:47:20,  3.05s/batch]Batch 10700/20010 Done, mean position loss: 22.791875658035277\n",
      "Training growing_up:  55%|████▉    | 11059/20010 [8:34:12<6:40:00,  2.68s/batch]Batch 10800/20010 Done, mean position loss: 23.64871782064438\n",
      "Training growing_up:  54%|████▊    | 10801/20010 [8:34:13<8:16:39,  3.24s/batch]Batch 10700/20010 Done, mean position loss: 23.615197036266327\n",
      "Training growing_up:  54%|████▉    | 10855/20010 [8:34:14<7:41:59,  3.03s/batch]Batch 10800/20010 Done, mean position loss: 23.776553528308867\n",
      "Training growing_up:  55%|████▉    | 10995/20010 [8:34:30<7:20:46,  2.93s/batch]Batch 10900/20010 Done, mean position loss: 24.27764357805252\n",
      "Training growing_up:  54%|████▊    | 10807/20010 [8:34:32<8:34:15,  3.35s/batch]Batch 11000/20010 Done, mean position loss: 22.676504559516907\n",
      "Training growing_up:  54%|████▊    | 10713/20010 [8:34:37<8:04:05,  3.12s/batch]Batch 10900/20010 Done, mean position loss: 23.895815761089324\n",
      "Training growing_up:  54%|████▊    | 10709/20010 [8:34:39<8:37:23,  3.34s/batch]Batch 10900/20010 Done, mean position loss: 23.943851606845854\n",
      "Training growing_up:  55%|████▉    | 10989/20010 [8:34:48<8:33:53,  3.42s/batch]Batch 11000/20010 Done, mean position loss: 24.4078466963768\n",
      "Training growing_up:  55%|████▉    | 10943/20010 [8:35:01<6:58:34,  2.77s/batch]Batch 10700/20010 Done, mean position loss: 22.86421424627304\n",
      "Training growing_up:  55%|████▉    | 11012/20010 [8:35:02<6:11:41,  2.48s/batch]Batch 10900/20010 Done, mean position loss: 23.631647968292235\n",
      "Training growing_up:  53%|████▊    | 10654/20010 [8:35:03<7:18:10,  2.81s/batch]Batch 10900/20010 Done, mean position loss: 24.735210351943973\n",
      "Training growing_up:  55%|████▉    | 10959/20010 [8:35:10<7:22:42,  2.93s/batch]Batch 10800/20010 Done, mean position loss: 23.220411608219145\n",
      "Training growing_up:  54%|████▊    | 10724/20010 [8:35:22<6:12:20,  2.41s/batch]Batch 11000/20010 Done, mean position loss: 23.08017501831055\n",
      "Training growing_up:  55%|████▉    | 10986/20010 [8:35:45<6:54:04,  2.75s/batch]Batch 10900/20010 Done, mean position loss: 22.832430431842806\n",
      "Training growing_up:  54%|████▊    | 10752/20010 [8:35:56<8:22:38,  3.26s/batch]Batch 10700/20010 Done, mean position loss: 23.64106593608856\n",
      "Training growing_up:  55%|████▉    | 10948/20010 [8:36:11<6:58:11,  2.77s/batch]Batch 11100/20010 Done, mean position loss: 23.026656725406646\n",
      "Training growing_up:  54%|████▉    | 10878/20010 [8:36:26<7:57:06,  3.13s/batch]Batch 10900/20010 Done, mean position loss: 23.43536267518997\n",
      "Training growing_up:  54%|████▊    | 10712/20010 [8:36:29<8:46:19,  3.40s/batch]Batch 11000/20010 Done, mean position loss: 22.987005908489227\n",
      "Training growing_up:  54%|████▉    | 10891/20010 [8:36:30<7:30:52,  2.97s/batch]Batch 10800/20010 Done, mean position loss: 23.616245748996732\n",
      "Training growing_up:  53%|████▊    | 10685/20010 [8:36:34<7:39:07,  2.95s/batch]Batch 10900/20010 Done, mean position loss: 23.57691390275955\n",
      "Training growing_up:  55%|████▉    | 10998/20010 [8:37:01<7:01:51,  2.81s/batch]Batch 10900/20010 Done, mean position loss: 23.64453817129135\n",
      "Training growing_up:  55%|████▉    | 10913/20010 [8:37:09<7:24:53,  2.93s/batch]Batch 11000/20010 Done, mean position loss: 22.959508748054503\n",
      "Training growing_up:  54%|████▊    | 10765/20010 [8:37:13<8:27:48,  3.30s/batch]Batch 11000/20010 Done, mean position loss: 22.68337149143219\n",
      "Training growing_up:  55%|████▉    | 10920/20010 [8:37:20<7:06:19,  2.81s/batch]Batch 10700/20010 Done, mean position loss: 23.348665957450866\n",
      "Training growing_up:  54%|████▉    | 10866/20010 [8:37:25<7:40:37,  3.02s/batch]Batch 10900/20010 Done, mean position loss: 24.064881207942964\n",
      "Training growing_up:  54%|████▉    | 10852/20010 [8:37:35<6:43:38,  2.64s/batch]Batch 10900/20010 Done, mean position loss: 23.405621218681336\n",
      "Training growing_up:  54%|████▊    | 10754/20010 [8:37:37<7:19:49,  2.85s/batch]Batch 11000/20010 Done, mean position loss: 24.06116983175278\n",
      "Training growing_up:  55%|████▉    | 10963/20010 [8:37:39<7:34:54,  3.02s/batch]Batch 10900/20010 Done, mean position loss: 24.133117964267733\n",
      "Training growing_up:  54%|████▉    | 10876/20010 [8:37:45<7:51:16,  3.10s/batch]Batch 10900/20010 Done, mean position loss: 23.88425526380539\n",
      "Training growing_up:  54%|████▊    | 10829/20010 [8:37:48<6:38:40,  2.61s/batch]Batch 11000/20010 Done, mean position loss: 23.428551614284515\n",
      "Training growing_up:  54%|████▉    | 10860/20010 [8:37:58<7:31:40,  2.96s/batch]Batch 10900/20010 Done, mean position loss: 24.822739093303678\n",
      "Training growing_up:  54%|████▉    | 10901/20010 [8:37:58<7:26:29,  2.94s/batch]Batch 11000/20010 Done, mean position loss: 23.03820909023285\n",
      "Training growing_up:  55%|████▉    | 11019/20010 [8:38:08<7:57:10,  3.18s/batch]Batch 11100/20010 Done, mean position loss: 23.402723593711855\n",
      "Training growing_up:  54%|████▊    | 10721/20010 [8:38:18<6:35:57,  2.56s/batch]Batch 10800/20010 Done, mean position loss: 24.44271770954132\n",
      "Training growing_up:  55%|████▉    | 11017/20010 [8:38:39<7:00:01,  2.80s/batch]Batch 11000/20010 Done, mean position loss: 23.10194678068161\n",
      "Training growing_up:  55%|████▉    | 10947/20010 [8:38:44<7:09:19,  2.84s/batch]Batch 11000/20010 Done, mean position loss: 23.009487562179565\n",
      "Training growing_up:  55%|████▉    | 10930/20010 [8:38:49<7:02:06,  2.79s/batch]Batch 11000/20010 Done, mean position loss: 23.814639818668365\n",
      "Training growing_up:  55%|████▉    | 11005/20010 [8:38:49<6:30:05,  2.60s/batch]Batch 11000/20010 Done, mean position loss: 23.741511340141294\n",
      "Training growing_up:  55%|████▉    | 10951/20010 [8:38:56<7:13:18,  2.87s/batch]Batch 10900/20010 Done, mean position loss: 22.69059295415878\n",
      "Training growing_up:  54%|████▊    | 10815/20010 [8:38:57<7:15:49,  2.84s/batch]Batch 10800/20010 Done, mean position loss: 23.014273192882538\n",
      "Training growing_up:  55%|████▉    | 10984/20010 [8:39:04<6:58:07,  2.78s/batch]Batch 10900/20010 Done, mean position loss: 23.19835748910904\n",
      "Batch 10900/20010 Done, mean position loss: 23.639409046173093\n",
      "Training growing_up:  55%|████▉    | 11012/20010 [8:39:09<6:57:52,  2.79s/batch]Batch 11100/20010 Done, mean position loss: 22.744643383026123\n",
      "Training growing_up:  54%|████▊    | 10786/20010 [8:39:11<7:26:02,  2.90s/batch]Batch 10800/20010 Done, mean position loss: 23.327109925746917\n",
      "Training growing_up:  55%|████▉    | 11043/20010 [8:39:14<7:08:25,  2.87s/batch]Batch 11000/20010 Done, mean position loss: 23.730072281360627\n",
      "Training growing_up:  55%|████▉    | 11014/20010 [8:39:21<6:49:19,  2.73s/batch]Batch 11000/20010 Done, mean position loss: 23.303484315872193\n",
      "Training growing_up:  54%|████▊    | 10791/20010 [8:39:25<6:39:11,  2.60s/batch]Batch 11000/20010 Done, mean position loss: 24.278624997138976\n",
      "Training growing_up:  55%|████▉    | 11037/20010 [8:39:33<8:02:05,  3.22s/batch]Batch 11100/20010 Done, mean position loss: 24.206009867191316\n",
      "Training growing_up:  55%|████▉    | 11047/20010 [8:39:46<6:30:45,  2.62s/batch]Batch 11000/20010 Done, mean position loss: 23.668814804553985\n",
      "Training growing_up:  55%|████▉    | 10988/20010 [8:39:53<7:04:39,  2.82s/batch]Batch 10800/20010 Done, mean position loss: 22.742234444618227\n",
      "Training growing_up:  54%|████▊    | 10820/20010 [8:39:55<8:25:47,  3.30s/batch]Batch 11000/20010 Done, mean position loss: 24.569809286594392\n",
      "Training growing_up:  54%|████▊    | 10817/20010 [8:40:00<7:39:08,  3.00s/batch]Batch 10900/20010 Done, mean position loss: 22.948944070339202\n",
      "Training growing_up:  55%|████▉    | 11015/20010 [8:40:06<7:18:09,  2.92s/batch]Batch 11100/20010 Done, mean position loss: 23.573379492759706\n",
      "Training growing_up:  54%|████▊    | 10765/20010 [8:40:30<8:32:49,  3.33s/batch]Batch 11000/20010 Done, mean position loss: 22.580708994865418\n",
      "Training growing_up:  55%|████▉    | 11024/20010 [8:40:51<7:24:15,  2.97s/batch]Batch 11200/20010 Done, mean position loss: 22.98995268344879\n",
      "Training growing_up:  55%|████▉    | 11064/20010 [8:40:54<7:07:17,  2.87s/batch]Batch 10800/20010 Done, mean position loss: 23.58515876531601\n",
      "Training growing_up:  55%|████▉    | 11053/20010 [8:41:17<7:32:55,  3.03s/batch]Batch 11000/20010 Done, mean position loss: 23.682064130306244\n",
      "Training growing_up:  55%|████▉    | 10990/20010 [8:41:21<7:35:49,  3.03s/batch]Batch 11100/20010 Done, mean position loss: 22.687255783081056\n",
      "Training growing_up:  55%|████▉    | 11055/20010 [8:41:27<7:56:03,  3.19s/batch]Batch 11000/20010 Done, mean position loss: 23.1813774561882\n",
      "Training growing_up:  55%|████▉    | 10949/20010 [8:41:28<6:41:16,  2.66s/batch]Batch 10900/20010 Done, mean position loss: 23.50302276134491\n",
      "Training growing_up:  54%|████▉    | 10873/20010 [8:41:53<8:07:29,  3.20s/batch]Batch 11000/20010 Done, mean position loss: 23.61447756767273\n",
      "Training growing_up:  55%|████▉    | 10961/20010 [8:42:02<6:50:25,  2.72s/batch]Batch 11100/20010 Done, mean position loss: 23.220791444778442\n",
      "Training growing_up:  55%|████▉    | 10992/20010 [8:42:06<7:28:14,  2.98s/batch]Batch 11100/20010 Done, mean position loss: 22.363832881450655\n",
      "Training growing_up:  55%|████▉    | 10966/20010 [8:42:17<7:17:13,  2.90s/batch]Batch 10800/20010 Done, mean position loss: 23.316027925014495\n",
      "Training growing_up:  54%|████▊    | 10804/20010 [8:42:26<7:30:42,  2.94s/batch]Batch 11000/20010 Done, mean position loss: 23.654595243930817\n",
      "Training growing_up:  54%|████▊    | 10831/20010 [8:42:28<8:00:15,  3.14s/batch]Batch 11100/20010 Done, mean position loss: 23.606670010089875\n",
      "Training growing_up:  54%|████▉    | 10866/20010 [8:42:33<8:28:15,  3.34s/batch]Batch 11000/20010 Done, mean position loss: 24.10148708343506\n",
      "Training growing_up:  55%|████▉    | 11103/20010 [8:42:34<7:36:44,  3.08s/batch]Batch 11000/20010 Done, mean position loss: 23.66064002275467\n",
      "Training growing_up:  56%|████▉    | 11116/20010 [8:42:44<6:50:22,  2.77s/batch]Batch 11100/20010 Done, mean position loss: 23.557080199718477\n",
      "Training growing_up:  54%|████▉    | 10877/20010 [8:42:47<8:02:40,  3.17s/batch]Batch 11000/20010 Done, mean position loss: 23.758424274921417\n",
      "Training growing_up:  54%|████▉    | 10839/20010 [8:42:52<7:30:37,  2.95s/batch]Batch 11100/20010 Done, mean position loss: 22.847734167575837\n",
      "Training growing_up:  56%|████▉    | 11111/20010 [8:42:57<7:31:58,  3.05s/batch]Batch 11000/20010 Done, mean position loss: 24.37683182001114\n",
      "Training growing_up:  55%|████▉    | 11051/20010 [8:43:03<7:20:23,  2.95s/batch]Batch 11200/20010 Done, mean position loss: 23.40846690893173\n",
      "Training growing_up:  56%|█████    | 11183/20010 [8:43:14<6:51:40,  2.80s/batch]Batch 10900/20010 Done, mean position loss: 24.378513205051423\n",
      "Training growing_up:  56%|█████    | 11191/20010 [8:43:36<6:45:23,  2.76s/batch]Batch 11100/20010 Done, mean position loss: 23.378789236545565\n",
      "Training growing_up:  56%|█████    | 11119/20010 [8:43:44<7:40:12,  3.11s/batch]Batch 11100/20010 Done, mean position loss: 23.11310788869858\n",
      "Training growing_up:  56%|████▉    | 11106/20010 [8:43:49<6:19:35,  2.56s/batch]Batch 11100/20010 Done, mean position loss: 24.122672035694123\n",
      "Training growing_up:  56%|█████    | 11128/20010 [8:43:49<6:52:18,  2.79s/batch]Batch 11100/20010 Done, mean position loss: 23.636253929138185\n",
      "Training growing_up:  55%|████▉    | 11102/20010 [8:43:52<8:17:04,  3.35s/batch]Batch 11000/20010 Done, mean position loss: 23.2353933763504\n",
      "Training growing_up:  56%|█████    | 11140/20010 [8:43:58<6:59:49,  2.84s/batch]Batch 11000/20010 Done, mean position loss: 23.746548342704774\n",
      "Training growing_up:  55%|████▉    | 11024/20010 [8:43:59<8:33:57,  3.43s/batch]Batch 10900/20010 Done, mean position loss: 23.197855427265168\n",
      "Training growing_up:  56%|█████    | 11183/20010 [8:44:05<6:59:14,  2.85s/batch]Batch 11200/20010 Done, mean position loss: 23.0799741768837\n",
      "Training growing_up:  55%|████▉    | 11045/20010 [8:44:09<8:07:40,  3.26s/batch]Batch 11000/20010 Done, mean position loss: 23.76992788553238\n",
      "Training growing_up:  56%|█████    | 11160/20010 [8:44:15<7:21:45,  2.99s/batch]Batch 11100/20010 Done, mean position loss: 24.148095548152924\n",
      "Training growing_up:  56%|████▉    | 11111/20010 [8:44:18<7:29:05,  3.03s/batch]Batch 11100/20010 Done, mean position loss: 23.64804443836212\n",
      "Training growing_up:  56%|████▉    | 11111/20010 [8:44:22<7:50:25,  3.17s/batch]Batch 11200/20010 Done, mean position loss: 24.065875120162964\n",
      "Training growing_up:  55%|████▉    | 11091/20010 [8:44:24<6:44:00,  2.72s/batch]Batch 10900/20010 Done, mean position loss: 22.950097796916964\n",
      "Training growing_up:  55%|████▉    | 11053/20010 [8:44:31<6:56:21,  2.79s/batch]Batch 11100/20010 Done, mean position loss: 24.204829068183898\n",
      "Training growing_up:  55%|████▉    | 11096/20010 [8:44:40<8:22:29,  3.38s/batch]Batch 11100/20010 Done, mean position loss: 23.396295833587644\n",
      "Training growing_up:  56%|████▉    | 11109/20010 [8:44:55<7:54:43,  3.20s/batch]Batch 11100/20010 Done, mean position loss: 23.71793666124344\n",
      "Training growing_up:  56%|████▉    | 11114/20010 [8:44:56<7:30:57,  3.04s/batch]Batch 10900/20010 Done, mean position loss: 22.467331449985505\n",
      "Training growing_up:  56%|█████    | 11125/20010 [8:45:00<7:24:51,  3.00s/batch]Batch 11200/20010 Done, mean position loss: 23.513549876213077\n",
      "Training growing_up:  56%|████▉    | 11112/20010 [8:45:04<7:34:35,  3.07s/batch]Batch 11000/20010 Done, mean position loss: 23.152617585659026\n",
      "Training growing_up:  55%|████▉    | 10934/20010 [8:45:41<7:42:25,  3.06s/batch]Batch 11100/20010 Done, mean position loss: 22.9524072098732\n",
      "Training growing_up:  55%|████▉    | 10917/20010 [8:45:47<8:00:41,  3.17s/batch]Batch 11300/20010 Done, mean position loss: 22.735532457828523\n",
      "Training growing_up:  56%|█████    | 11143/20010 [8:46:01<7:50:03,  3.18s/batch]Batch 10900/20010 Done, mean position loss: 23.265890996456143\n",
      "Training growing_up:  56%|█████    | 11226/20010 [8:46:15<6:40:43,  2.74s/batch]Batch 11100/20010 Done, mean position loss: 23.399053165912626\n",
      "Training growing_up:  56%|█████    | 11188/20010 [8:46:21<6:52:46,  2.81s/batch]Batch 11100/20010 Done, mean position loss: 23.417381992340086\n",
      "Batch 11200/20010 Done, mean position loss: 23.00472462415695\n",
      "Training growing_up:  56%|█████    | 11173/20010 [8:46:29<7:11:40,  2.93s/batch]Batch 11000/20010 Done, mean position loss: 23.80599643945694\n",
      "Training growing_up:  56%|█████    | 11160/20010 [8:46:51<6:37:16,  2.69s/batch]Batch 11200/20010 Done, mean position loss: 23.123754959106446\n",
      "Training growing_up:  55%|████▉    | 11091/20010 [8:46:57<7:36:08,  3.07s/batch]Batch 11100/20010 Done, mean position loss: 23.721797008514404\n",
      "Training growing_up:  54%|████▉    | 10893/20010 [8:47:02<7:55:49,  3.13s/batch]Batch 11200/20010 Done, mean position loss: 22.489486515522003\n",
      "Training growing_up:  56%|█████    | 11177/20010 [8:47:24<7:33:00,  3.08s/batch]Batch 10900/20010 Done, mean position loss: 22.780385835170744\n",
      "Training growing_up:  55%|████▉    | 11048/20010 [8:47:24<7:23:40,  2.97s/batch]Batch 11200/20010 Done, mean position loss: 23.58513699531555\n",
      "Training growing_up:  56%|█████    | 11209/20010 [8:47:26<8:13:53,  3.37s/batch]Batch 11100/20010 Done, mean position loss: 23.659037423133853\n",
      "Training growing_up:  55%|████▉    | 11074/20010 [8:47:35<7:38:06,  3.08s/batch]Batch 11100/20010 Done, mean position loss: 23.64072182416916\n",
      "Training growing_up:  55%|████▉    | 10988/20010 [8:47:37<7:30:48,  3.00s/batch]Batch 11200/20010 Done, mean position loss: 23.666584420204163\n",
      "Training growing_up:  56%|█████    | 11205/20010 [8:47:38<8:07:58,  3.33s/batch]Batch 11100/20010 Done, mean position loss: 24.054445555210116\n",
      "Training growing_up:  56%|████▉    | 11106/20010 [8:47:48<7:15:00,  2.93s/batch]Batch 11100/20010 Done, mean position loss: 23.670539944171907\n",
      "Training growing_up:  56%|█████    | 11159/20010 [8:47:50<6:31:11,  2.65s/batch]Batch 11300/20010 Done, mean position loss: 23.614769952297213\n",
      "Training growing_up:  56%|█████    | 11280/20010 [8:47:55<7:00:06,  2.89s/batch]Batch 11200/20010 Done, mean position loss: 23.235504186153413\n",
      "Training growing_up:  55%|████▉    | 10974/20010 [8:48:09<7:29:00,  2.98s/batch]Batch 11100/20010 Done, mean position loss: 24.52432361125946\n",
      "Training growing_up:  55%|████▉    | 11082/20010 [8:48:15<7:24:04,  2.98s/batch]Batch 11000/20010 Done, mean position loss: 24.65296165943146\n",
      "Training growing_up:  56%|█████    | 11178/20010 [8:48:28<6:38:53,  2.71s/batch]Batch 11200/20010 Done, mean position loss: 23.12938900947571\n",
      "Training growing_up:  56%|█████    | 11151/20010 [8:48:37<6:12:35,  2.52s/batch]Batch 11200/20010 Done, mean position loss: 23.573877017498013\n",
      "Training growing_up:  56%|█████    | 11137/20010 [8:48:40<7:17:27,  2.96s/batch]Batch 11200/20010 Done, mean position loss: 22.984341604709627\n",
      "Training growing_up:  56%|█████    | 11204/20010 [8:48:47<7:06:05,  2.90s/batch]Batch 11200/20010 Done, mean position loss: 23.265254418849945\n",
      "Training growing_up:  55%|████▉    | 11099/20010 [8:48:52<7:59:06,  3.23s/batch]Batch 11100/20010 Done, mean position loss: 22.67611362218857\n",
      "Training growing_up:  56%|█████    | 11141/20010 [8:48:53<7:31:55,  3.06s/batch]Batch 11300/20010 Done, mean position loss: 22.877449131011964\n",
      "Training growing_up:  56%|█████    | 11126/20010 [8:48:55<6:22:40,  2.58s/batch]Batch 11000/20010 Done, mean position loss: 23.674501156806947\n",
      "Training growing_up:  56%|█████    | 11118/20010 [8:48:56<6:36:12,  2.67s/batch]Batch 11100/20010 Done, mean position loss: 23.225982029438022\n",
      "Training growing_up:  55%|████▉    | 11084/20010 [8:49:08<7:23:44,  2.98s/batch]Batch 11200/20010 Done, mean position loss: 23.578650262355804\n",
      "Training growing_up:  57%|█████    | 11331/20010 [8:49:10<5:32:52,  2.30s/batch]Batch 11100/20010 Done, mean position loss: 23.265265333652497\n",
      "Training growing_up:  56%|█████    | 11235/20010 [8:49:12<6:15:49,  2.57s/batch]Batch 11200/20010 Done, mean position loss: 23.41132857322693\n",
      "Training growing_up:  56%|█████    | 11213/20010 [8:49:16<6:34:27,  2.69s/batch]Batch 11300/20010 Done, mean position loss: 23.785244398117065\n",
      "Training growing_up:  56%|█████    | 11165/20010 [8:49:23<6:08:44,  2.50s/batch]Batch 11000/20010 Done, mean position loss: 23.258177309036256\n",
      "Training growing_up:  56%|████▉    | 11107/20010 [8:49:25<6:36:39,  2.67s/batch]Batch 11200/20010 Done, mean position loss: 23.77434630393982\n",
      "Training growing_up:  56%|█████    | 11244/20010 [8:49:27<6:34:55,  2.70s/batch]Batch 11200/20010 Done, mean position loss: 23.623511834144594\n",
      "Training growing_up:  55%|████▉    | 11010/20010 [8:49:48<6:47:54,  2.72s/batch]Batch 11000/20010 Done, mean position loss: 22.53485608816147\n",
      "Training growing_up:  56%|█████    | 11273/20010 [8:49:48<6:54:53,  2.85s/batch]Batch 11300/20010 Done, mean position loss: 22.98677939414978\n",
      "Training growing_up:  56%|█████    | 11227/20010 [8:49:48<6:20:52,  2.60s/batch]Batch 11200/20010 Done, mean position loss: 23.7544496679306\n",
      "Training growing_up:  56%|█████    | 11151/20010 [8:49:53<7:26:29,  3.02s/batch]Batch 11100/20010 Done, mean position loss: 22.766861352920532\n",
      "Training growing_up:  56%|█████    | 11134/20010 [8:50:22<6:47:23,  2.75s/batch]Batch 11200/20010 Done, mean position loss: 23.119939835071563\n",
      "Training growing_up:  56%|█████    | 11221/20010 [8:50:23<7:10:52,  2.94s/batch]Batch 11400/20010 Done, mean position loss: 23.169425344467165\n",
      "Training growing_up:  56%|█████    | 11228/20010 [8:50:41<8:06:32,  3.32s/batch]Batch 11000/20010 Done, mean position loss: 23.339511077404023\n",
      "Training growing_up:  56%|█████    | 11160/20010 [8:50:51<7:10:46,  2.92s/batch]Batch 11200/20010 Done, mean position loss: 23.96306586265564\n",
      "Training growing_up:  56%|█████    | 11175/20010 [8:51:01<7:40:25,  3.13s/batch]Batch 11200/20010 Done, mean position loss: 23.324359319210053\n",
      "Training growing_up:  56%|█████    | 11228/20010 [8:51:03<7:35:23,  3.11s/batch]Batch 11300/20010 Done, mean position loss: 22.9408752989769\n",
      "Training growing_up:  55%|████▉    | 11051/20010 [8:51:17<6:47:21,  2.73s/batch]Batch 11100/20010 Done, mean position loss: 23.34186458826065\n",
      "Training growing_up:  57%|█████    | 11338/20010 [8:51:29<6:34:34,  2.73s/batch]Batch 11300/20010 Done, mean position loss: 22.63671248435974\n",
      "Training growing_up:  56%|█████    | 11188/20010 [8:51:40<7:24:00,  3.02s/batch]Batch 11300/20010 Done, mean position loss: 22.775955183506014\n",
      "Training growing_up:  56%|█████    | 11242/20010 [8:51:44<7:14:49,  2.98s/batch]Batch 11200/20010 Done, mean position loss: 23.84266926527023\n",
      "Training growing_up:  57%|█████    | 11310/20010 [8:52:03<6:03:33,  2.51s/batch]Batch 11300/20010 Done, mean position loss: 23.649012806415556\n",
      "Training growing_up:  55%|████▉    | 11068/20010 [8:52:09<7:43:41,  3.11s/batch]Batch 11200/20010 Done, mean position loss: 23.6738334608078\n",
      "Training growing_up:  57%|█████    | 11375/20010 [8:52:16<7:02:29,  2.94s/batch]Batch 11000/20010 Done, mean position loss: 22.78790592432022\n",
      "Training growing_up:  55%|████▉    | 11087/20010 [8:52:17<8:11:55,  3.31s/batch]Batch 11200/20010 Done, mean position loss: 24.420057919025417\n",
      "Training growing_up:  56%|█████    | 11262/20010 [8:52:18<6:58:50,  2.87s/batch]Batch 11300/20010 Done, mean position loss: 23.55669739484787\n",
      "Training growing_up:  57%|█████    | 11307/20010 [8:52:19<6:54:45,  2.86s/batch]Batch 11200/20010 Done, mean position loss: 23.746813418865205\n",
      "Training growing_up:  55%|████▉    | 11072/20010 [8:52:22<7:44:51,  3.12s/batch]Batch 11400/20010 Done, mean position loss: 23.159319036006927\n",
      "Training growing_up:  56%|█████    | 11264/20010 [8:52:24<7:26:29,  3.06s/batch]Batch 11200/20010 Done, mean position loss: 23.751575391292572\n",
      "Training growing_up:  57%|█████▏   | 11450/20010 [8:52:40<7:20:47,  3.09s/batch]Batch 11300/20010 Done, mean position loss: 22.97719329357147\n",
      "Training growing_up:  55%|████▉    | 11074/20010 [8:52:51<7:51:25,  3.17s/batch]Batch 11200/20010 Done, mean position loss: 24.451281476020814\n",
      "Training growing_up:  57%|█████▏   | 11456/20010 [8:52:56<6:33:09,  2.76s/batch]Batch 11100/20010 Done, mean position loss: 24.135231466293334\n",
      "Training growing_up:  56%|█████    | 11271/20010 [8:53:05<7:07:18,  2.93s/batch]Batch 11300/20010 Done, mean position loss: 23.01423041820526\n",
      "Training growing_up:  56%|█████    | 11142/20010 [8:53:17<6:35:01,  2.67s/batch]Batch 11300/20010 Done, mean position loss: 23.63212876558304\n",
      "Training growing_up:  57%|█████    | 11341/20010 [8:53:26<6:21:23,  2.64s/batch]Batch 11300/20010 Done, mean position loss: 23.928584501743316\n",
      "Training growing_up:  56%|█████    | 11197/20010 [8:53:30<7:02:41,  2.88s/batch]Batch 11300/20010 Done, mean position loss: 22.865983905792234\n",
      "Training growing_up:  57%|█████▏   | 11469/20010 [8:53:32<6:29:27,  2.74s/batch]Batch 11400/20010 Done, mean position loss: 23.271641387939454\n",
      "Training growing_up:  56%|█████    | 11227/20010 [8:53:34<7:06:37,  2.91s/batch]Batch 11200/20010 Done, mean position loss: 23.235466549396516\n",
      "Training growing_up:  57%|█████    | 11393/20010 [8:53:40<6:42:22,  2.80s/batch]Batch 11200/20010 Done, mean position loss: 23.321132066249845\n",
      "Training growing_up:  57%|█████    | 11316/20010 [8:53:46<6:38:55,  2.75s/batch]Batch 11300/20010 Done, mean position loss: 23.97724720239639\n",
      "Training growing_up:  56%|█████    | 11292/20010 [8:53:46<7:11:49,  2.97s/batch]Batch 11100/20010 Done, mean position loss: 23.04308172941208\n",
      "Training growing_up:  56%|█████    | 11272/20010 [8:53:49<7:06:08,  2.93s/batch]Batch 11300/20010 Done, mean position loss: 23.304830408096315\n",
      "Training growing_up:  57%|█████    | 11315/20010 [8:53:57<6:52:31,  2.85s/batch]Batch 11200/20010 Done, mean position loss: 23.420539507865904\n",
      "Training growing_up:  57%|█████    | 11322/20010 [8:54:04<7:11:15,  2.98s/batch]Batch 11400/20010 Done, mean position loss: 23.823038651943207\n",
      "Training growing_up:  57%|█████▏   | 11403/20010 [8:54:09<7:01:47,  2.94s/batch]Batch 11100/20010 Done, mean position loss: 23.42000684261322\n",
      "Training growing_up:  56%|█████    | 11243/20010 [8:54:10<7:04:00,  2.90s/batch]Batch 11300/20010 Done, mean position loss: 24.015642659664152\n",
      "Training growing_up:  57%|█████    | 11310/20010 [8:54:11<7:08:25,  2.95s/batch]Batch 11300/20010 Done, mean position loss: 23.391483125686648\n",
      "Training growing_up:  57%|█████    | 11353/20010 [8:54:27<6:31:41,  2.71s/batch]Batch 11400/20010 Done, mean position loss: 22.838250436782836\n",
      "Training growing_up:  56%|█████    | 11277/20010 [8:54:32<6:48:43,  2.81s/batch]Batch 11300/20010 Done, mean position loss: 23.76410233736038\n",
      "Training growing_up:  55%|████▉    | 11083/20010 [8:54:40<6:48:53,  2.75s/batch]Batch 11100/20010 Done, mean position loss: 22.817429389953613\n",
      "Training growing_up:  57%|█████    | 11367/20010 [8:54:44<7:47:51,  3.25s/batch]Batch 11200/20010 Done, mean position loss: 23.106931898593903\n",
      "Training growing_up:  57%|█████    | 11351/20010 [8:55:09<7:49:19,  3.25s/batch]Batch 11500/20010 Done, mean position loss: 23.002745192050934\n",
      "Training growing_up:  56%|█████    | 11261/20010 [8:55:16<7:36:10,  3.13s/batch]Batch 11300/20010 Done, mean position loss: 22.52704159975052\n",
      "Training growing_up:  56%|█████    | 11218/20010 [8:55:34<7:16:05,  2.98s/batch]Batch 11100/20010 Done, mean position loss: 23.56801218032837\n",
      "Training growing_up:  57%|█████    | 11364/20010 [8:55:46<6:43:54,  2.80s/batch]Batch 11400/20010 Done, mean position loss: 22.754687292575838\n",
      "Training growing_up:  56%|█████    | 11190/20010 [8:55:47<7:53:26,  3.22s/batch]Batch 11300/20010 Done, mean position loss: 24.20606925487518\n",
      "Training growing_up:  57%|█████    | 11335/20010 [8:55:56<7:48:05,  3.24s/batch]Batch 11300/20010 Done, mean position loss: 23.400607922077178\n",
      "Training growing_up:  56%|█████    | 11117/20010 [8:56:21<7:17:41,  2.95s/batch]Batch 11200/20010 Done, mean position loss: 23.425095844268796\n",
      "Training growing_up:  57%|█████    | 11345/20010 [8:56:26<6:59:55,  2.91s/batch]Batch 11400/20010 Done, mean position loss: 22.961593697071077\n",
      "Training growing_up:  57%|█████▏   | 11460/20010 [8:56:29<7:01:52,  2.96s/batch]Batch 11400/20010 Done, mean position loss: 22.755303835868837\n",
      "Training growing_up:  57%|█████▏   | 11487/20010 [8:56:43<7:54:32,  3.34s/batch]Batch 11300/20010 Done, mean position loss: 23.737203414440156\n",
      "Training growing_up:  57%|█████    | 11332/20010 [8:56:51<6:59:50,  2.90s/batch]Batch 11400/20010 Done, mean position loss: 23.281561655998228\n",
      "Training growing_up:  56%|█████    | 11297/20010 [8:57:07<7:38:15,  3.16s/batch]Batch 11300/20010 Done, mean position loss: 23.762180395126343\n",
      "Training growing_up:  57%|█████    | 11381/20010 [8:57:17<7:21:43,  3.07s/batch]Batch 11400/20010 Done, mean position loss: 23.46306615591049\n",
      "Training growing_up:  57%|█████    | 11384/20010 [8:57:17<7:18:14,  3.05s/batch]Batch 11300/20010 Done, mean position loss: 23.72171857357025\n",
      "Training growing_up:  56%|█████    | 11154/20010 [8:57:18<6:55:41,  2.82s/batch]Batch 11300/20010 Done, mean position loss: 23.820769963264468\n",
      "Training growing_up:  56%|█████    | 11171/20010 [8:57:19<7:16:15,  2.96s/batch]Batch 11300/20010 Done, mean position loss: 23.935632293224337\n",
      "Training growing_up:  57%|█████    | 11364/20010 [8:57:20<6:33:34,  2.73s/batch]Batch 11100/20010 Done, mean position loss: 22.987423734664915\n",
      "Training growing_up:  57%|█████    | 11385/20010 [8:57:29<6:06:38,  2.55s/batch]Batch 11500/20010 Done, mean position loss: 23.205048260688784\n",
      "Training growing_up:  57%|█████▏   | 11407/20010 [8:57:36<7:34:00,  3.17s/batch]Batch 11400/20010 Done, mean position loss: 23.19196187019348\n",
      "Training growing_up:  56%|████▉    | 11110/20010 [8:57:49<7:32:58,  3.05s/batch]Batch 11300/20010 Done, mean position loss: 24.37005854845047\n",
      "Training growing_up:  56%|█████    | 11177/20010 [8:58:06<8:14:51,  3.36s/batch]Batch 11200/20010 Done, mean position loss: 24.217780313491822\n",
      "Training growing_up:  57%|█████    | 11379/20010 [8:58:08<7:12:00,  3.00s/batch]Batch 11400/20010 Done, mean position loss: 23.31450195789337\n",
      "Training growing_up:  57%|█████▏   | 11398/20010 [8:58:17<6:35:25,  2.75s/batch]Batch 11400/20010 Done, mean position loss: 23.813173627853395\n",
      "Training growing_up:  57%|█████    | 11384/20010 [8:58:19<6:26:16,  2.69s/batch]Batch 11400/20010 Done, mean position loss: 23.609288735389708\n",
      "Training growing_up:  57%|█████    | 11325/20010 [8:58:26<6:45:06,  2.80s/batch]Batch 11400/20010 Done, mean position loss: 22.917309520244597\n",
      "Training growing_up:  57%|█████▏   | 11403/20010 [8:58:32<6:42:49,  2.81s/batch]Batch 11300/20010 Done, mean position loss: 23.32054024219513\n",
      "Training growing_up:  57%|█████    | 11387/20010 [8:58:33<7:22:40,  3.08s/batch]Batch 11500/20010 Done, mean position loss: 23.06056572198868\n",
      "Training growing_up:  57%|█████▏   | 11495/20010 [8:58:42<7:19:25,  3.10s/batch]Batch 11400/20010 Done, mean position loss: 23.514311554431913\n",
      "Training growing_up:  57%|█████    | 11359/20010 [8:58:46<5:59:37,  2.49s/batch]Batch 11400/20010 Done, mean position loss: 23.635263719558715\n",
      "Training growing_up:  57%|█████    | 11306/20010 [8:58:47<7:09:10,  2.96s/batch]Batch 11300/20010 Done, mean position loss: 23.364271345138548\n",
      "Training growing_up:  57%|█████▏   | 11404/20010 [8:58:50<6:54:02,  2.89s/batch]Batch 11200/20010 Done, mean position loss: 23.11342498064041\n",
      "Training growing_up:  57%|█████    | 11333/20010 [8:58:54<6:34:10,  2.73s/batch]Batch 11300/20010 Done, mean position loss: 23.928286776542663\n",
      "Training growing_up:  57%|█████    | 11346/20010 [8:59:01<7:35:25,  3.15s/batch]Batch 11500/20010 Done, mean position loss: 23.84261357307434\n",
      "Training growing_up:  57%|█████▏   | 11418/20010 [8:59:12<7:15:56,  3.04s/batch]Batch 11400/20010 Done, mean position loss: 23.80897679567337\n",
      "Training growing_up:  58%|█████▏   | 11506/20010 [8:59:17<7:14:09,  3.06s/batch]Batch 11400/20010 Done, mean position loss: 23.929828124046324\n",
      "Training growing_up:  57%|█████▏   | 11462/20010 [8:59:24<6:35:55,  2.78s/batch]Batch 11200/20010 Done, mean position loss: 23.589684939384462\n",
      "Training growing_up:  58%|█████▏   | 11509/20010 [8:59:25<6:31:07,  2.76s/batch]Batch 11500/20010 Done, mean position loss: 22.801380321979522\n",
      "Training growing_up:  57%|█████    | 11344/20010 [8:59:30<7:28:04,  3.10s/batch]Batch 11400/20010 Done, mean position loss: 23.58700938463211\n",
      "Training growing_up:  57%|█████    | 11348/20010 [8:59:41<7:30:14,  3.12s/batch]Batch 11200/20010 Done, mean position loss: 22.809278936386107\n",
      "Training growing_up:  57%|█████▏   | 11433/20010 [8:59:46<7:25:37,  3.12s/batch]Batch 11300/20010 Done, mean position loss: 22.77824115037918\n",
      "Training growing_up:  57%|█████▏   | 11437/20010 [9:00:06<7:01:39,  2.95s/batch]Batch 11600/20010 Done, mean position loss: 22.670237073898313\n",
      "Training growing_up:  57%|█████    | 11392/20010 [9:00:15<6:42:08,  2.80s/batch]Batch 11400/20010 Done, mean position loss: 22.43676312446594\n",
      "Training growing_up:  56%|█████    | 11225/20010 [9:00:41<7:04:48,  2.90s/batch]Batch 11500/20010 Done, mean position loss: 22.510586469173433\n",
      "Training growing_up:  58%|█████▏   | 11613/20010 [9:00:41<7:07:52,  3.06s/batch]Batch 11200/20010 Done, mean position loss: 23.41121429681778\n",
      "Training growing_up:  57%|█████▏   | 11502/20010 [9:00:44<6:38:16,  2.81s/batch]Batch 11400/20010 Done, mean position loss: 23.532421319484712\n",
      "Training growing_up:  57%|█████▏   | 11449/20010 [9:00:53<6:47:15,  2.85s/batch]Batch 11400/20010 Done, mean position loss: 23.492517812252043\n",
      "Training growing_up:  57%|█████▏   | 11445/20010 [9:01:22<7:08:16,  3.00s/batch]Batch 11500/20010 Done, mean position loss: 22.47233310222626\n",
      "Training growing_up:  57%|█████    | 11382/20010 [9:01:28<6:44:56,  2.82s/batch]Batch 11300/20010 Done, mean position loss: 23.10529009103775\n",
      "Training growing_up:  58%|█████▏   | 11631/20010 [9:01:35<7:37:02,  3.27s/batch]Batch 11500/20010 Done, mean position loss: 22.934488859176636\n",
      "Training growing_up:  57%|█████▏   | 11487/20010 [9:01:49<6:54:33,  2.92s/batch]Batch 11400/20010 Done, mean position loss: 23.721021392345428\n",
      "Training growing_up:  57%|█████    | 11386/20010 [9:02:01<7:01:25,  2.93s/batch]Batch 11500/20010 Done, mean position loss: 23.542149498462678\n",
      "Training growing_up:  58%|█████▏   | 11555/20010 [9:02:05<6:49:23,  2.91s/batch]Batch 11400/20010 Done, mean position loss: 24.031274333000184\n",
      "Training growing_up:  56%|█████    | 11256/20010 [9:02:12<7:06:40,  2.92s/batch]Batch 11500/20010 Done, mean position loss: 23.42288310289383\n",
      "Training growing_up:  57%|█████▏   | 11497/20010 [9:02:19<7:08:26,  3.02s/batch]Batch 11400/20010 Done, mean position loss: 23.657722008228305\n",
      "Training growing_up:  57%|█████▏   | 11401/20010 [9:02:19<7:12:30,  3.01s/batch]Batch 11400/20010 Done, mean position loss: 24.085099925994875\n",
      "Training growing_up:  57%|█████    | 11374/20010 [9:02:27<6:28:26,  2.70s/batch]Batch 11400/20010 Done, mean position loss: 23.485803563594818\n",
      "Training growing_up:  57%|█████▏   | 11476/20010 [9:02:27<7:25:27,  3.13s/batch]Batch 11200/20010 Done, mean position loss: 22.730188982486723\n",
      "Training growing_up:  58%|█████▏   | 11511/20010 [9:02:31<7:26:35,  3.15s/batch]Batch 11500/20010 Done, mean position loss: 23.13675461292267\n",
      "Training growing_up:  57%|█████▏   | 11449/20010 [9:02:39<7:16:02,  3.06s/batch]Batch 11600/20010 Done, mean position loss: 23.42023989200592\n",
      "Training growing_up:  58%|█████▏   | 11529/20010 [9:02:44<7:46:22,  3.30s/batch]Batch 11400/20010 Done, mean position loss: 24.86740951061249\n",
      "Training growing_up:  56%|█████    | 11213/20010 [9:03:04<7:22:31,  3.02s/batch]Batch 11500/20010 Done, mean position loss: 22.990871460437774\n",
      "Training growing_up:  56%|█████    | 11270/20010 [9:03:10<7:46:12,  3.20s/batch]Batch 11500/20010 Done, mean position loss: 23.620023074150083\n",
      "Training growing_up:  58%|█████▏   | 11578/20010 [9:03:14<6:56:03,  2.96s/batch]Batch 11300/20010 Done, mean position loss: 23.854080340862275\n",
      "Training growing_up:  56%|█████    | 11218/20010 [9:03:20<7:12:31,  2.95s/batch]Batch 11500/20010 Done, mean position loss: 23.360718982219698\n",
      "Training growing_up:  57%|█████▏   | 11487/20010 [9:03:26<6:34:15,  2.78s/batch]Batch 11400/20010 Done, mean position loss: 22.902930920124057\n",
      "Training growing_up:  57%|█████▏   | 11397/20010 [9:03:32<6:23:34,  2.67s/batch]Batch 11500/20010 Done, mean position loss: 22.826594662666324\n",
      "Training growing_up:  57%|█████▏   | 11491/20010 [9:03:36<6:19:08,  2.67s/batch]Batch 11600/20010 Done, mean position loss: 22.77977391719818\n",
      "Training growing_up:  57%|█████    | 11381/20010 [9:03:40<5:51:55,  2.45s/batch]Batch 11500/20010 Done, mean position loss: 23.2246932721138\n",
      "Training growing_up:  57%|█████▏   | 11505/20010 [9:03:41<5:37:12,  2.38s/batch]Batch 11400/20010 Done, mean position loss: 23.231196608543396\n",
      "Training growing_up:  58%|█████▏   | 11589/20010 [9:03:44<6:00:12,  2.57s/batch]Batch 11400/20010 Done, mean position loss: 23.76117589235306\n",
      "Training growing_up:  58%|█████▏   | 11535/20010 [9:03:46<5:55:28,  2.52s/batch]Batch 11300/20010 Done, mean position loss: 22.567724924087525\n",
      "Training growing_up:  57%|█████▏   | 11443/20010 [9:03:50<6:23:47,  2.69s/batch]Batch 11500/20010 Done, mean position loss: 23.50405678510666\n",
      "Training growing_up:  58%|█████▎   | 11678/20010 [9:03:50<5:54:22,  2.55s/batch]Batch 11600/20010 Done, mean position loss: 24.06426390647888\n",
      "Training growing_up:  57%|█████▏   | 11470/20010 [9:04:04<7:09:16,  3.02s/batch]Batch 11500/20010 Done, mean position loss: 23.703643550872805\n",
      "Training growing_up:  58%|█████▏   | 11524/20010 [9:04:09<6:11:06,  2.62s/batch]Batch 11500/20010 Done, mean position loss: 23.895673332214354\n",
      "Training growing_up:  57%|█████▏   | 11432/20010 [9:04:13<6:50:02,  2.87s/batch]Batch 11600/20010 Done, mean position loss: 23.10161312818527\n",
      "Training growing_up:  56%|█████    | 11293/20010 [9:04:16<6:33:48,  2.71s/batch]Batch 11300/20010 Done, mean position loss: 23.422570893764494\n",
      "Training growing_up:  58%|█████▏   | 11551/20010 [9:04:28<6:35:50,  2.81s/batch]Batch 11500/20010 Done, mean position loss: 23.782238624095918\n",
      "Training growing_up:  57%|█████    | 11329/20010 [9:04:35<7:33:43,  3.14s/batch]Batch 11400/20010 Done, mean position loss: 22.982921597957613\n",
      "Training growing_up:  58%|█████▏   | 11585/20010 [9:04:39<6:34:33,  2.81s/batch]Batch 11300/20010 Done, mean position loss: 22.712280478477478\n",
      "Training growing_up:  57%|█████    | 11325/20010 [9:04:52<7:35:59,  3.15s/batch]Batch 11700/20010 Done, mean position loss: 22.665929987430573\n",
      "Training growing_up:  57%|█████▏   | 11464/20010 [9:05:09<7:39:01,  3.22s/batch]Batch 11500/20010 Done, mean position loss: 22.531708319187164\n",
      "Training growing_up:  57%|█████▏   | 11437/20010 [9:05:22<6:25:18,  2.70s/batch]Batch 11600/20010 Done, mean position loss: 22.294138536453247\n",
      "Training growing_up:  58%|█████▏   | 11639/20010 [9:05:35<7:43:30,  3.32s/batch]Batch 11500/20010 Done, mean position loss: 23.472280263900757\n",
      "Training growing_up:  57%|█████▏   | 11468/20010 [9:05:42<7:16:42,  3.07s/batch]Batch 11300/20010 Done, mean position loss: 23.30945872783661\n",
      "Training growing_up:  58%|█████▏   | 11547/20010 [9:05:45<6:15:53,  2.66s/batch]Batch 11500/20010 Done, mean position loss: 23.342350747585296\n",
      "Training growing_up:  58%|█████▏   | 11536/20010 [9:06:12<6:45:31,  2.87s/batch]Batch 11600/20010 Done, mean position loss: 22.50700599193573\n",
      "Training growing_up:  58%|█████▏   | 11645/20010 [9:06:19<6:17:19,  2.71s/batch]Batch 11400/20010 Done, mean position loss: 22.97969005346298\n",
      "Training growing_up:  58%|█████▏   | 11513/20010 [9:06:21<6:21:59,  2.70s/batch]Batch 11600/20010 Done, mean position loss: 22.943445262908938\n",
      "Training growing_up:  58%|█████▏   | 11651/20010 [9:06:38<6:44:11,  2.90s/batch]Batch 11500/20010 Done, mean position loss: 23.685563504695892\n",
      "Training growing_up:  57%|█████    | 11376/20010 [9:06:55<7:18:47,  3.05s/batch]Batch 11600/20010 Done, mean position loss: 23.702864944934845\n",
      "Training growing_up:  58%|█████▏   | 11567/20010 [9:06:55<6:51:06,  2.92s/batch]Batch 11600/20010 Done, mean position loss: 23.45090781211853\n",
      "Training growing_up:  57%|█████▏   | 11498/20010 [9:06:58<7:07:02,  3.01s/batch]Batch 11500/20010 Done, mean position loss: 23.732944712638854\n",
      "Training growing_up:  58%|█████▏   | 11638/20010 [9:07:08<6:32:27,  2.81s/batch]Batch 11500/20010 Done, mean position loss: 23.832112007141113\n",
      "Training growing_up:  58%|█████▏   | 11577/20010 [9:07:09<6:41:00,  2.85s/batch]Batch 11500/20010 Done, mean position loss: 23.747224371433255\n",
      "Training growing_up:  56%|█████    | 11298/20010 [9:07:14<6:28:57,  2.68s/batch]Batch 11600/20010 Done, mean position loss: 22.825270144939424\n",
      "Training growing_up:  58%|█████▏   | 11622/20010 [9:07:22<6:54:34,  2.97s/batch]Batch 11500/20010 Done, mean position loss: 23.902000319957736\n",
      "Training growing_up:  58%|█████▎   | 11700/20010 [9:07:23<6:41:25,  2.90s/batch]Batch 11300/20010 Done, mean position loss: 23.005311846733093\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:07:25<6:31:09,  2.82s/batch]Batch 11700/20010 Done, mean position loss: 23.063255772590637\n",
      "Training growing_up:  57%|█████    | 11359/20010 [9:07:35<7:32:47,  3.14s/batch]Batch 11500/20010 Done, mean position loss: 24.313446578979494\n",
      "Training growing_up:  58%|█████▏   | 11598/20010 [9:07:48<7:25:03,  3.17s/batch]Batch 11600/20010 Done, mean position loss: 23.721460568904874\n",
      "Training growing_up:  58%|█████▏   | 11550/20010 [9:07:56<8:14:52,  3.51s/batch]Batch 11600/20010 Done, mean position loss: 22.494750735759734\n",
      "Training growing_up:  59%|█████▎   | 11766/20010 [9:08:04<6:17:51,  2.75s/batch]Batch 11600/20010 Done, mean position loss: 23.897036128044128\n",
      "Training growing_up:  58%|█████▏   | 11521/20010 [9:08:06<7:16:21,  3.08s/batch]Batch 11500/20010 Done, mean position loss: 22.952800292968753\n",
      "Training growing_up:  59%|█████▎   | 11768/20010 [9:08:10<6:48:29,  2.97s/batch]Batch 11400/20010 Done, mean position loss: 24.062635560035705\n",
      "Training growing_up:  57%|█████▏   | 11476/20010 [9:08:15<6:24:22,  2.70s/batch]Batch 11600/20010 Done, mean position loss: 22.947051203250886\n",
      "Training growing_up:  58%|█████▎   | 11698/20010 [9:08:17<6:13:21,  2.70s/batch]Batch 11600/20010 Done, mean position loss: 23.53972115755081\n",
      "Training growing_up:  57%|█████    | 11354/20010 [9:08:20<6:33:45,  2.73s/batch]Batch 11700/20010 Done, mean position loss: 22.94202535867691\n",
      "Training growing_up:  58%|█████▏   | 11583/20010 [9:08:25<6:25:40,  2.75s/batch]Batch 11700/20010 Done, mean position loss: 23.969291472434996\n",
      "Training growing_up:  57%|█████    | 11359/20010 [9:08:35<7:37:16,  3.17s/batch]Batch 11500/20010 Done, mean position loss: 22.84378332853317\n",
      "Training growing_up:  59%|█████▎   | 11726/20010 [9:08:36<6:31:42,  2.84s/batch]Batch 11600/20010 Done, mean position loss: 23.173596930503845\n",
      "Training growing_up:  58%|█████▏   | 11532/20010 [9:08:39<7:13:32,  3.07s/batch]Batch 11500/20010 Done, mean position loss: 23.58854638814926\n",
      "Training growing_up:  58%|█████▏   | 11631/20010 [9:08:41<7:37:16,  3.27s/batch]Batch 11400/20010 Done, mean position loss: 22.886170914173128\n",
      "Training growing_up:  58%|█████▏   | 11612/20010 [9:08:48<7:21:02,  3.15s/batch]Batch 11600/20010 Done, mean position loss: 23.644789595603946\n",
      "Training growing_up:  58%|█████▎   | 11675/20010 [9:08:54<6:27:53,  2.79s/batch]Batch 11600/20010 Done, mean position loss: 23.91208334207535\n",
      "Training growing_up:  58%|█████▏   | 11621/20010 [9:09:04<6:54:23,  2.96s/batch]Batch 11700/20010 Done, mean position loss: 23.195223302841185\n",
      "Training growing_up:  58%|█████▏   | 11622/20010 [9:09:18<7:40:58,  3.30s/batch]Batch 11400/20010 Done, mean position loss: 23.45072417497635\n",
      "Training growing_up:  59%|█████▎   | 11706/20010 [9:09:19<7:16:44,  3.16s/batch]Batch 11600/20010 Done, mean position loss: 23.602992992401127\n",
      "Training growing_up:  58%|█████▏   | 11648/20010 [9:09:32<7:08:04,  3.07s/batch]Batch 11500/20010 Done, mean position loss: 22.919783339500427\n",
      "Training growing_up:  59%|█████▎   | 11748/20010 [9:09:39<6:42:29,  2.92s/batch]Batch 11400/20010 Done, mean position loss: 22.180701241493225\n",
      "Training growing_up:  58%|█████▏   | 11564/20010 [9:09:43<6:25:16,  2.74s/batch]Batch 11800/20010 Done, mean position loss: 22.968864164352418\n",
      "Training growing_up:  58%|█████▏   | 11659/20010 [9:10:04<6:43:17,  2.90s/batch]Batch 11600/20010 Done, mean position loss: 22.95264070749283\n",
      "Training growing_up:  58%|█████▏   | 11638/20010 [9:10:08<6:54:31,  2.97s/batch]Batch 11700/20010 Done, mean position loss: 22.37111798763275\n",
      "Training growing_up:  58%|█████▎   | 11688/20010 [9:10:27<6:49:53,  2.96s/batch]Batch 11600/20010 Done, mean position loss: 23.93666981458664\n",
      "Training growing_up:  58%|█████▏   | 11659/20010 [9:10:39<6:24:30,  2.76s/batch]Batch 11400/20010 Done, mean position loss: 23.884859535694122\n",
      "Training growing_up:  58%|█████▏   | 11648/20010 [9:10:41<7:29:39,  3.23s/batch]Batch 11600/20010 Done, mean position loss: 23.192258102893827\n",
      "Training growing_up:  57%|█████▏   | 11460/20010 [9:11:05<6:57:53,  2.93s/batch]Batch 11700/20010 Done, mean position loss: 22.352540764808655\n",
      "Training growing_up:  58%|█████▏   | 11671/20010 [9:11:16<7:01:22,  3.03s/batch]Batch 11700/20010 Done, mean position loss: 22.59133815050125\n",
      "Training growing_up:  58%|█████▎   | 11685/20010 [9:11:20<6:52:42,  2.97s/batch]Batch 11500/20010 Done, mean position loss: 23.48277175426483\n",
      "Training growing_up:  59%|█████▎   | 11841/20010 [9:11:41<6:28:35,  2.85s/batch]Batch 11600/20010 Done, mean position loss: 23.54200604915619\n",
      "Training growing_up:  57%|█████▏   | 11465/20010 [9:11:53<7:01:02,  2.96s/batch]Batch 11700/20010 Done, mean position loss: 23.206465067863466\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:11:56<7:01:54,  3.04s/batch]Batch 11700/20010 Done, mean position loss: 24.00362683773041\n",
      "Training growing_up:  57%|█████▏   | 11469/20010 [9:12:05<7:03:42,  2.98s/batch]Batch 11700/20010 Done, mean position loss: 22.945184059143067\n",
      "Training growing_up:  58%|█████▎   | 11701/20010 [9:12:06<6:22:09,  2.76s/batch]Batch 11600/20010 Done, mean position loss: 23.698587117195128\n",
      "Training growing_up:  59%|█████▎   | 11718/20010 [9:12:07<7:03:54,  3.07s/batch]Batch 11600/20010 Done, mean position loss: 23.85205909013748\n",
      "Training growing_up:  58%|█████▏   | 11657/20010 [9:12:08<6:17:42,  2.71s/batch]Batch 11600/20010 Done, mean position loss: 24.014353415966035\n",
      "Training growing_up:  57%|█████▏   | 11473/20010 [9:12:17<6:44:38,  2.84s/batch]Batch 11800/20010 Done, mean position loss: 23.702658863067626\n",
      "Training growing_up:  59%|█████▎   | 11722/20010 [9:12:21<7:45:05,  3.37s/batch]Batch 11600/20010 Done, mean position loss: 23.597654538154604\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:12:33<6:49:15,  2.95s/batch]Batch 11400/20010 Done, mean position loss: 22.819367175102233\n",
      "Training growing_up:  58%|█████▏   | 11669/20010 [9:12:43<6:25:34,  2.77s/batch]Batch 11700/20010 Done, mean position loss: 23.650059890747073\n",
      "Training growing_up:  57%|█████▏   | 11405/20010 [9:12:47<7:54:24,  3.31s/batch]Batch 11600/20010 Done, mean position loss: 23.988274230957032\n",
      "Training growing_up:  58%|█████▏   | 11658/20010 [9:12:56<7:08:17,  3.08s/batch]Batch 11700/20010 Done, mean position loss: 23.026851727962494\n",
      "Training growing_up:  58%|█████▏   | 11616/20010 [9:13:04<6:44:32,  2.89s/batch]Batch 11600/20010 Done, mean position loss: 22.868246645927428\n",
      "Training growing_up:  59%|█████▎   | 11711/20010 [9:13:09<6:00:00,  2.60s/batch]Batch 11700/20010 Done, mean position loss: 23.367119567394255\n",
      "Training growing_up:  59%|█████▎   | 11798/20010 [9:13:15<6:46:56,  2.97s/batch]Batch 11500/20010 Done, mean position loss: 23.92186236143112\n",
      "Training growing_up:  58%|█████▏   | 11624/20010 [9:13:16<7:26:24,  3.19s/batch]Batch 11700/20010 Done, mean position loss: 23.420287766456603\n",
      "Training growing_up:  59%|█████▎   | 11725/20010 [9:13:17<6:39:22,  2.89s/batch]Batch 11700/20010 Done, mean position loss: 22.688787095546722\n",
      "Training growing_up:  59%|█████▎   | 11821/20010 [9:13:18<6:55:35,  3.04s/batch]Batch 11800/20010 Done, mean position loss: 22.777460844516753\n",
      "Training growing_up:  58%|█████▏   | 11626/20010 [9:13:23<7:39:10,  3.29s/batch]Batch 11800/20010 Done, mean position loss: 24.4366583609581\n",
      "Training growing_up:  59%|█████▎   | 11734/20010 [9:13:28<6:43:55,  2.93s/batch]Batch 11600/20010 Done, mean position loss: 23.102057688236236\n",
      "Training growing_up:  58%|█████▏   | 11577/20010 [9:13:31<7:08:08,  3.05s/batch]Batch 11700/20010 Done, mean position loss: 23.343422486782075\n",
      "Training growing_up:  58%|█████▏   | 11632/20010 [9:13:40<7:44:02,  3.32s/batch]Batch 11500/20010 Done, mean position loss: 23.181079518795016\n",
      "Training growing_up:  58%|█████▏   | 11616/20010 [9:13:48<6:43:40,  2.89s/batch]Batch 11600/20010 Done, mean position loss: 23.752198612689973\n",
      "Training growing_up:  59%|█████▎   | 11711/20010 [9:13:48<7:27:36,  3.24s/batch]Batch 11700/20010 Done, mean position loss: 23.66372462272644\n",
      "Training growing_up:  58%|█████▏   | 11585/20010 [9:13:56<7:14:27,  3.09s/batch]Batch 11700/20010 Done, mean position loss: 23.928245742321014\n",
      "Training growing_up:  58%|█████▏   | 11646/20010 [9:13:59<7:50:33,  3.38s/batch]Batch 11800/20010 Done, mean position loss: 22.550676963329316\n",
      "Training growing_up:  59%|█████▎   | 11719/20010 [9:14:20<6:21:24,  2.76s/batch]Batch 11700/20010 Done, mean position loss: 23.980000538825987\n",
      "Training growing_up:  59%|█████▎   | 11768/20010 [9:14:30<6:58:30,  3.05s/batch]Batch 11500/20010 Done, mean position loss: 23.888304736614227\n",
      "Training growing_up:  57%|█████▏   | 11499/20010 [9:14:43<7:42:57,  3.26s/batch]Batch 11900/20010 Done, mean position loss: 22.85067476272583\n",
      "Training growing_up:  58%|█████▏   | 11523/20010 [9:14:45<6:51:20,  2.91s/batch]Batch 11600/20010 Done, mean position loss: 22.77691620826721\n",
      "Training growing_up:  59%|█████▎   | 11850/20010 [9:14:50<7:46:19,  3.43s/batch]Batch 11500/20010 Done, mean position loss: 22.30150163412094\n",
      "Training growing_up:  59%|█████▎   | 11780/20010 [9:15:06<6:45:36,  2.96s/batch]Batch 11800/20010 Done, mean position loss: 22.265540761947634\n",
      "Training growing_up:  58%|█████▎   | 11693/20010 [9:15:07<7:07:22,  3.08s/batch]Batch 11700/20010 Done, mean position loss: 22.523432390689848\n",
      "Training growing_up:  58%|█████▏   | 11668/20010 [9:15:30<7:00:36,  3.03s/batch]Batch 11700/20010 Done, mean position loss: 23.88715993642807\n",
      "Training growing_up:  59%|█████▎   | 11793/20010 [9:15:44<6:33:43,  2.87s/batch]Batch 11700/20010 Done, mean position loss: 22.971376957893373\n",
      "Training growing_up:  58%|█████▎   | 11676/20010 [9:15:54<6:46:35,  2.93s/batch]Batch 11500/20010 Done, mean position loss: 23.173244314193724\n",
      "Training growing_up:  58%|█████▏   | 11527/20010 [9:16:08<6:48:56,  2.89s/batch]Batch 11800/20010 Done, mean position loss: 22.40956649541855\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:16:10<6:52:26,  2.97s/batch]Batch 11800/20010 Done, mean position loss: 22.67625460624695\n",
      "Training growing_up:  59%|█████▎   | 11859/20010 [9:16:18<7:01:17,  3.10s/batch]Batch 11600/20010 Done, mean position loss: 23.054284501075745\n",
      "Training growing_up:  59%|█████▎   | 11889/20010 [9:16:44<6:06:00,  2.70s/batch]Batch 11700/20010 Done, mean position loss: 23.659238917827608\n",
      "Training growing_up:  59%|█████▎   | 11723/20010 [9:16:50<7:12:33,  3.13s/batch]Batch 11800/20010 Done, mean position loss: 23.37578106880188\n",
      "Training growing_up:  60%|█████▎   | 11946/20010 [9:16:55<5:53:05,  2.63s/batch]Batch 11800/20010 Done, mean position loss: 23.867526655197146\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:17:03<6:59:04,  3.02s/batch]Batch 11800/20010 Done, mean position loss: 23.241050491333006\n",
      "Training growing_up:  58%|█████▎   | 11686/20010 [9:17:06<7:25:28,  3.21s/batch]Batch 11700/20010 Done, mean position loss: 24.309034280776977\n",
      "Training growing_up:  58%|█████▎   | 11687/20010 [9:17:08<6:51:40,  2.97s/batch]Batch 11700/20010 Done, mean position loss: 24.240061192512513\n",
      "Training growing_up:  59%|█████▎   | 11776/20010 [9:17:10<6:39:30,  2.91s/batch]Batch 11700/20010 Done, mean position loss: 23.901831178665162\n",
      "Training growing_up:  59%|█████▎   | 11780/20010 [9:17:13<6:50:14,  2.99s/batch]Batch 11700/20010 Done, mean position loss: 23.43862873315811\n",
      "Training growing_up:  58%|█████▎   | 11703/20010 [9:17:18<6:44:34,  2.92s/batch]Batch 11900/20010 Done, mean position loss: 23.73684031248093\n",
      "Training growing_up:  59%|█████▎   | 11710/20010 [9:17:38<7:19:47,  3.18s/batch]Batch 11800/20010 Done, mean position loss: 23.593901889324187\n",
      "Training growing_up:  58%|█████▏   | 11561/20010 [9:17:48<7:00:33,  2.99s/batch]Batch 11500/20010 Done, mean position loss: 22.80564714431763\n",
      "Training growing_up:  60%|█████▎   | 11912/20010 [9:17:51<6:12:26,  2.76s/batch]Batch 11700/20010 Done, mean position loss: 24.15815758943558\n",
      "Training growing_up:  59%|█████▎   | 11719/20010 [9:17:59<6:22:04,  2.76s/batch]Batch 11700/20010 Done, mean position loss: 22.764280686378477\n",
      "Training growing_up:  59%|█████▎   | 11793/20010 [9:17:59<6:30:38,  2.85s/batch]Batch 11800/20010 Done, mean position loss: 22.95912153482437\n",
      "Training growing_up:  59%|█████▎   | 11789/20010 [9:18:03<5:42:57,  2.50s/batch]Batch 11800/20010 Done, mean position loss: 23.315520257949828\n",
      "Training growing_up:  59%|█████▎   | 11864/20010 [9:18:12<7:09:24,  3.16s/batch]Batch 11600/20010 Done, mean position loss: 24.458085322380068\n",
      "Training growing_up:  58%|█████▏   | 11570/20010 [9:18:12<7:06:02,  3.03s/batch]Batch 11800/20010 Done, mean position loss: 23.017245326042175\n",
      "Training growing_up:  59%|█████▎   | 11798/20010 [9:18:12<6:48:46,  2.99s/batch]Batch 11800/20010 Done, mean position loss: 23.330487382411956\n",
      "Training growing_up:  59%|█████▎   | 11831/20010 [9:18:15<6:02:07,  2.66s/batch]Batch 11900/20010 Done, mean position loss: 22.83181625127792\n",
      "Training growing_up:  59%|█████▎   | 11867/20010 [9:18:20<6:24:36,  2.83s/batch]Batch 11700/20010 Done, mean position loss: 23.158754534721375\n",
      "Training growing_up:  60%|█████▍   | 11979/20010 [9:18:21<5:41:36,  2.55s/batch]Batch 11800/20010 Done, mean position loss: 23.19721312046051\n",
      "Training growing_up:  59%|█████▎   | 11791/20010 [9:18:28<6:46:10,  2.97s/batch]Batch 11900/20010 Done, mean position loss: 23.820945336818696\n",
      "Training growing_up:  58%|█████▏   | 11577/20010 [9:18:30<6:22:38,  2.72s/batch]Batch 11600/20010 Done, mean position loss: 23.115259006023408\n",
      "Training growing_up:  59%|█████▎   | 11839/20010 [9:18:35<5:06:52,  2.25s/batch]Batch 11800/20010 Done, mean position loss: 23.571628968715668\n",
      "Training growing_up:  59%|█████▎   | 11813/20010 [9:18:46<6:34:06,  2.88s/batch]Batch 11700/20010 Done, mean position loss: 23.495465042591096\n",
      "Training growing_up:  59%|█████▎   | 11769/20010 [9:18:51<5:14:44,  2.29s/batch]Batch 11800/20010 Done, mean position loss: 23.852582030296325\n",
      "Training growing_up:  59%|█████▎   | 11847/20010 [9:18:57<6:06:53,  2.70s/batch]Batch 11900/20010 Done, mean position loss: 23.118940436840056\n",
      "Training growing_up:  58%|█████▎   | 11693/20010 [9:19:07<5:35:26,  2.42s/batch]Batch 11800/20010 Done, mean position loss: 24.03077514410019\n",
      "Training growing_up:  59%|█████▎   | 11831/20010 [9:19:23<6:05:44,  2.68s/batch]Batch 12000/20010 Done, mean position loss: 22.809039406776428\n",
      "Training growing_up:  59%|█████▎   | 11714/20010 [9:19:22<6:37:04,  2.87s/batch]Batch 11600/20010 Done, mean position loss: 23.404690418243412\n",
      "Training growing_up:  59%|█████▎   | 11752/20010 [9:19:26<6:20:30,  2.76s/batch]Batch 11700/20010 Done, mean position loss: 22.75612207651138\n",
      "Training growing_up:  59%|█████▎   | 11831/20010 [9:19:33<6:10:22,  2.72s/batch]Batch 11600/20010 Done, mean position loss: 22.67248880147934\n",
      "Training growing_up:  59%|█████▎   | 11828/20010 [9:19:46<5:36:10,  2.47s/batch]Batch 11900/20010 Done, mean position loss: 22.511895051002504\n",
      "Training growing_up:  60%|█████▍   | 12013/20010 [9:19:54<5:54:19,  2.66s/batch]Batch 11800/20010 Done, mean position loss: 22.398921718597414\n",
      "Training growing_up:  58%|█████▏   | 11647/20010 [9:20:13<6:14:38,  2.69s/batch]Batch 11800/20010 Done, mean position loss: 23.68043642997742\n",
      "Training growing_up:  60%|█████▎   | 11932/20010 [9:20:21<6:14:34,  2.78s/batch]Batch 11800/20010 Done, mean position loss: 23.17450819730759\n",
      "Training growing_up:  60%|█████▎   | 11950/20010 [9:20:39<6:14:13,  2.79s/batch]Batch 11600/20010 Done, mean position loss: 23.386062653064727\n",
      "Training growing_up:  58%|█████▏   | 11632/20010 [9:20:44<6:10:52,  2.66s/batch]Batch 11900/20010 Done, mean position loss: 22.46970236301422\n",
      "Training growing_up:  59%|█████▎   | 11889/20010 [9:20:46<5:20:10,  2.37s/batch]Batch 11900/20010 Done, mean position loss: 22.365846819877625\n",
      "Training growing_up:  59%|█████▎   | 11863/20010 [9:20:50<5:36:44,  2.48s/batch]Batch 11700/20010 Done, mean position loss: 22.942367975711825\n",
      "Training growing_up:  59%|█████▎   | 11743/20010 [9:21:16<6:03:45,  2.64s/batch]Batch 11800/20010 Done, mean position loss: 23.559866528511048\n",
      "Training growing_up:  59%|█████▎   | 11712/20010 [9:21:18<5:55:29,  2.57s/batch]Batch 11900/20010 Done, mean position loss: 23.038314549922944\n",
      "Training growing_up:  59%|█████▎   | 11824/20010 [9:21:23<5:44:47,  2.53s/batch]Batch 11900/20010 Done, mean position loss: 23.40125350952148\n",
      "Training growing_up:  59%|█████▎   | 11894/20010 [9:21:35<6:31:55,  2.90s/batch]Batch 11900/20010 Done, mean position loss: 22.7031809091568\n",
      "Training growing_up:  59%|█████▎   | 11880/20010 [9:21:37<6:09:06,  2.72s/batch]Batch 11800/20010 Done, mean position loss: 23.51303310394287\n",
      "Training growing_up:  60%|█████▎   | 11906/20010 [9:21:38<6:39:27,  2.96s/batch]Batch 11800/20010 Done, mean position loss: 24.090639884471894\n",
      "Training growing_up:  60%|█████▍   | 11975/20010 [9:21:42<6:36:47,  2.96s/batch]Batch 11800/20010 Done, mean position loss: 23.592034339904785\n",
      "Training growing_up:  59%|█████▎   | 11898/20010 [9:21:46<6:00:30,  2.67s/batch]Batch 12000/20010 Done, mean position loss: 23.277874150276183\n",
      "Training growing_up:  59%|█████▎   | 11723/20010 [9:21:52<7:12:02,  3.13s/batch]Batch 11800/20010 Done, mean position loss: 23.539764630794526\n",
      "Training growing_up:  59%|█████▎   | 11838/20010 [9:21:55<7:07:18,  3.14s/batch]Batch 11900/20010 Done, mean position loss: 23.84495876789093\n",
      "Training growing_up:  60%|█████▍   | 11989/20010 [9:22:23<6:35:29,  2.96s/batch]Batch 11800/20010 Done, mean position loss: 24.48486575603485\n",
      "Training growing_up:  59%|█████▎   | 11900/20010 [9:22:27<5:43:03,  2.54s/batch]Batch 11800/20010 Done, mean position loss: 23.151064784526827\n",
      "Training growing_up:  59%|█████▎   | 11850/20010 [9:22:29<6:32:17,  2.88s/batch]Batch 11900/20010 Done, mean position loss: 23.274141919612884\n",
      "Training growing_up:  59%|█████▎   | 11903/20010 [9:22:35<6:10:35,  2.74s/batch]Batch 11600/20010 Done, mean position loss: 23.127664461135865\n",
      "Training growing_up:  60%|█████▍   | 11982/20010 [9:22:36<6:08:52,  2.76s/batch]Batch 11900/20010 Done, mean position loss: 23.12374471664429\n",
      "Training growing_up:  60%|█████▎   | 11942/20010 [9:22:42<6:30:06,  2.90s/batch]Batch 11900/20010 Done, mean position loss: 23.20223811626434\n",
      "Training growing_up:  60%|█████▎   | 11920/20010 [9:22:49<6:01:12,  2.68s/batch]Batch 11900/20010 Done, mean position loss: 23.53716391324997\n",
      "Training growing_up:  59%|█████▎   | 11864/20010 [9:22:49<6:16:31,  2.77s/batch]Batch 11700/20010 Done, mean position loss: 23.911275436878206\n",
      "Batch 11800/20010 Done, mean position loss: 23.399106743335725\n",
      "Training growing_up:  59%|█████▎   | 11896/20010 [9:22:51<6:08:16,  2.72s/batch]Batch 12000/20010 Done, mean position loss: 22.84652671813965\n",
      "Training growing_up:  59%|█████▎   | 11836/20010 [9:22:58<7:03:26,  3.11s/batch]Batch 12000/20010 Done, mean position loss: 23.79955497980118\n",
      "Training growing_up:  60%|█████▎   | 11931/20010 [9:23:01<5:51:26,  2.61s/batch]Batch 11900/20010 Done, mean position loss: 23.21359692335129\n",
      "Training growing_up:  59%|█████▎   | 11859/20010 [9:23:05<6:37:09,  2.92s/batch]Batch 11900/20010 Done, mean position loss: 23.441710932254793\n",
      "Training growing_up:  59%|█████▎   | 11748/20010 [9:23:06<6:51:27,  2.99s/batch]Batch 11700/20010 Done, mean position loss: 22.742156064510347\n",
      "Training growing_up:  60%|█████▍   | 12033/20010 [9:23:18<6:19:57,  2.86s/batch]Batch 11800/20010 Done, mean position loss: 23.665541536808014\n",
      "Training growing_up:  60%|█████▍   | 11960/20010 [9:23:31<6:55:57,  3.10s/batch]Batch 11900/20010 Done, mean position loss: 23.580288410186768\n",
      "Training growing_up:  59%|█████▎   | 11880/20010 [9:23:33<6:11:19,  2.74s/batch]Batch 12000/20010 Done, mean position loss: 22.851842346191408\n",
      "Training growing_up:  58%|█████▎   | 11691/20010 [9:23:41<6:19:16,  2.74s/batch]Batch 11900/20010 Done, mean position loss: 23.800013771057127\n",
      "Training growing_up:  59%|█████▎   | 11876/20010 [9:23:52<6:04:52,  2.69s/batch]Batch 12100/20010 Done, mean position loss: 22.76027773857117\n",
      "Training growing_up:  59%|█████▎   | 11721/20010 [9:24:04<7:25:03,  3.22s/batch]Batch 11700/20010 Done, mean position loss: 23.436877522468567\n",
      "Batch 11800/20010 Done, mean position loss: 22.574989297389983\n",
      "Training growing_up:  59%|█████▎   | 11854/20010 [9:24:12<5:47:05,  2.55s/batch]Batch 11700/20010 Done, mean position loss: 22.27650511741638\n",
      "Training growing_up:  59%|█████▎   | 11858/20010 [9:24:24<6:27:18,  2.85s/batch]Batch 12000/20010 Done, mean position loss: 22.684277033805845\n",
      "Training growing_up:  60%|█████▍   | 11965/20010 [9:24:35<6:07:59,  2.74s/batch]Batch 11900/20010 Done, mean position loss: 22.750760226249696\n",
      "Training growing_up:  60%|█████▍   | 11973/20010 [9:24:58<7:02:24,  3.15s/batch]Batch 11900/20010 Done, mean position loss: 23.623578820228577\n",
      "Training growing_up:  60%|█████▍   | 11952/20010 [9:25:06<6:33:03,  2.93s/batch]Batch 11900/20010 Done, mean position loss: 22.95149160861969\n",
      "Training growing_up:  60%|█████▍   | 12078/20010 [9:25:26<6:04:30,  2.76s/batch]Batch 11700/20010 Done, mean position loss: 23.140140073299406\n",
      "Training growing_up:  60%|█████▍   | 12054/20010 [9:25:31<6:30:20,  2.94s/batch]Batch 12000/20010 Done, mean position loss: 22.485342285633088\n",
      "Training growing_up:  59%|█████▎   | 11879/20010 [9:25:33<6:59:13,  3.09s/batch]Batch 12000/20010 Done, mean position loss: 22.451962659358976\n",
      "Training growing_up:  59%|█████▎   | 11870/20010 [9:25:46<6:34:42,  2.91s/batch]Batch 11800/20010 Done, mean position loss: 23.29074458360672\n",
      "Training growing_up:  60%|█████▍   | 11996/20010 [9:26:06<6:44:33,  3.03s/batch]Batch 12000/20010 Done, mean position loss: 23.59665882587433\n",
      "Training growing_up:  60%|█████▍   | 12069/20010 [9:26:15<6:19:39,  2.87s/batch]Batch 11900/20010 Done, mean position loss: 23.6439585518837\n",
      "Training growing_up:  60%|█████▎   | 11925/20010 [9:26:20<6:33:54,  2.92s/batch]Batch 12000/20010 Done, mean position loss: 22.874097974300383\n",
      "Training growing_up:  59%|█████▎   | 11897/20010 [9:26:22<6:49:20,  3.03s/batch]Batch 12000/20010 Done, mean position loss: 23.61573978185654\n",
      "Training growing_up:  59%|█████▎   | 11769/20010 [9:26:32<8:00:56,  3.50s/batch]Batch 12100/20010 Done, mean position loss: 23.06895503282547\n",
      "Training growing_up:  59%|█████▎   | 11899/20010 [9:26:34<6:10:52,  2.74s/batch]Batch 11900/20010 Done, mean position loss: 23.94199998140335\n",
      "Training growing_up:  60%|█████▍   | 11972/20010 [9:26:34<7:13:36,  3.24s/batch]Batch 11900/20010 Done, mean position loss: 23.40222359895706\n",
      "Training growing_up:  61%|█████▍   | 12160/20010 [9:26:40<6:37:21,  3.04s/batch]Batch 11900/20010 Done, mean position loss: 23.567812752723693\n",
      "Training growing_up:  58%|█████▎   | 11682/20010 [9:26:44<6:13:55,  2.69s/batch]Batch 12000/20010 Done, mean position loss: 23.585488770008087\n",
      "Training growing_up:  60%|█████▍   | 12003/20010 [9:26:50<6:31:32,  2.93s/batch]Batch 11900/20010 Done, mean position loss: 23.35276186943054\n",
      "Training growing_up:  59%|█████▎   | 11763/20010 [9:27:17<7:39:41,  3.34s/batch]Batch 11900/20010 Done, mean position loss: 24.21402413606644\n",
      "Training growing_up:  60%|█████▍   | 11988/20010 [9:27:23<6:51:26,  3.08s/batch]Batch 11900/20010 Done, mean position loss: 22.9513591337204\n",
      "Training growing_up:  59%|█████▎   | 11788/20010 [9:27:27<6:37:16,  2.90s/batch]Batch 12000/20010 Done, mean position loss: 23.23554365634918\n",
      "Training growing_up:  60%|█████▍   | 12040/20010 [9:27:32<7:28:40,  3.38s/batch]Batch 12000/20010 Done, mean position loss: 22.815878884792326\n",
      "Training growing_up:  60%|█████▎   | 11919/20010 [9:27:35<7:09:58,  3.19s/batch]Batch 12000/20010 Done, mean position loss: 23.228741240501407\n",
      "Training growing_up:  60%|█████▍   | 11954/20010 [9:27:39<6:52:26,  3.07s/batch]Batch 11700/20010 Done, mean position loss: 23.42284764289856\n",
      "Training growing_up:  60%|█████▎   | 11921/20010 [9:27:41<7:04:46,  3.15s/batch]Batch 12100/20010 Done, mean position loss: 22.872734985351563\n",
      "Training growing_up:  59%|█████▎   | 11772/20010 [9:27:45<6:43:43,  2.94s/batch]Batch 12100/20010 Done, mean position loss: 23.751592106819153\n",
      "Training growing_up:  60%|█████▍   | 12046/20010 [9:27:47<5:38:31,  2.55s/batch]Batch 12000/20010 Done, mean position loss: 23.088179409503937\n",
      "Training growing_up:  60%|█████▍   | 12006/20010 [9:27:50<6:51:03,  3.08s/batch]Batch 11900/20010 Done, mean position loss: 23.14725792646408\n",
      "Training growing_up:  59%|█████▎   | 11797/20010 [9:27:55<7:21:52,  3.23s/batch]Batch 11800/20010 Done, mean position loss: 23.691308381557462\n",
      "Training growing_up:  60%|█████▎   | 11929/20010 [9:27:58<6:32:05,  2.91s/batch]Batch 12000/20010 Done, mean position loss: 23.304034020900723\n",
      "Training growing_up:  61%|█████▍   | 12190/20010 [9:28:03<5:40:21,  2.61s/batch]Batch 12000/20010 Done, mean position loss: 23.242764549255373\n",
      "Training growing_up:  59%|█████▎   | 11779/20010 [9:28:07<6:29:56,  2.84s/batch]Batch 11800/20010 Done, mean position loss: 22.457323336601256\n",
      "Training growing_up:  61%|█████▍   | 12194/20010 [9:28:16<6:28:15,  2.98s/batch]Batch 11900/20010 Done, mean position loss: 23.36567502737045\n",
      "Training growing_up:  60%|█████▍   | 12044/20010 [9:28:25<6:37:04,  2.99s/batch]Batch 12000/20010 Done, mean position loss: 23.890228798389437\n",
      "Training growing_up:  60%|█████▍   | 12011/20010 [9:28:26<6:17:10,  2.83s/batch]Batch 12100/20010 Done, mean position loss: 23.074076833724973\n",
      "Training growing_up:  60%|█████▎   | 11933/20010 [9:28:33<7:31:39,  3.36s/batch]Batch 12000/20010 Done, mean position loss: 23.734985065460208\n",
      "Training growing_up:  60%|█████▎   | 11926/20010 [9:28:36<6:53:44,  3.07s/batch]Batch 12200/20010 Done, mean position loss: 22.853182384967802\n",
      "Training growing_up:  60%|█████▍   | 12095/20010 [9:29:08<6:11:25,  2.82s/batch]Batch 11900/20010 Done, mean position loss: 22.413060774803164\n",
      "Training growing_up:  59%|█████▎   | 11902/20010 [9:29:11<7:10:22,  3.18s/batch]Batch 11800/20010 Done, mean position loss: 22.214519901275636\n",
      "Training growing_up:  60%|█████▍   | 12025/20010 [9:29:12<7:57:30,  3.59s/batch]Batch 11800/20010 Done, mean position loss: 23.1141472196579\n",
      "Training growing_up:  60%|█████▍   | 12069/20010 [9:29:26<6:15:16,  2.84s/batch]Batch 12100/20010 Done, mean position loss: 22.610082156658173\n",
      "Training growing_up:  59%|█████▎   | 11808/20010 [9:29:31<6:21:10,  2.79s/batch]Batch 12000/20010 Done, mean position loss: 22.651279735565186\n",
      "Training growing_up:  61%|█████▍   | 12145/20010 [9:29:56<6:35:44,  3.02s/batch]Batch 12000/20010 Done, mean position loss: 23.34084794998169\n",
      "Training growing_up:  60%|█████▍   | 12094/20010 [9:30:04<6:08:01,  2.79s/batch]Batch 12000/20010 Done, mean position loss: 23.13985207080841\n",
      "Training growing_up:  60%|█████▍   | 12062/20010 [9:30:27<6:38:27,  3.01s/batch]Batch 12100/20010 Done, mean position loss: 22.565768859386445\n",
      "Training growing_up:  59%|█████▎   | 11798/20010 [9:30:32<7:26:44,  3.26s/batch]Batch 12100/20010 Done, mean position loss: 22.49857340335846\n",
      "Training growing_up:  60%|█████▍   | 12015/20010 [9:30:40<6:47:43,  3.06s/batch]Batch 11800/20010 Done, mean position loss: 22.875469093322753\n",
      "Training growing_up:  60%|█████▍   | 11989/20010 [9:30:58<6:40:49,  3.00s/batch]Batch 11900/20010 Done, mean position loss: 23.109615437984466\n",
      "Training growing_up:  60%|█████▍   | 12032/20010 [9:30:59<5:50:14,  2.63s/batch]Batch 12100/20010 Done, mean position loss: 23.43706619977951\n",
      "Training growing_up:  59%|█████▎   | 11811/20010 [9:31:10<6:46:25,  2.97s/batch]Batch 12100/20010 Done, mean position loss: 22.588330934047697\n",
      "Training growing_up:  60%|█████▍   | 12102/20010 [9:31:13<5:53:30,  2.68s/batch]Batch 12000/20010 Done, mean position loss: 23.652759811878205\n",
      "Training growing_up:  59%|█████▎   | 11842/20010 [9:31:15<7:25:04,  3.27s/batch]Batch 12100/20010 Done, mean position loss: 23.342195842266083\n",
      "Training growing_up:  61%|█████▌   | 12261/20010 [9:31:29<6:09:51,  2.86s/batch]Batch 12200/20010 Done, mean position loss: 23.081987705230713\n",
      "Training growing_up:  60%|█████▍   | 11975/20010 [9:31:32<6:29:48,  2.91s/batch]Batch 12000/20010 Done, mean position loss: 23.697223432064057\n",
      "Training growing_up:  60%|█████▍   | 12085/20010 [9:31:34<6:37:45,  3.01s/batch]Batch 12000/20010 Done, mean position loss: 23.649183118343352\n",
      "Training growing_up:  61%|█████▍   | 12125/20010 [9:31:41<6:31:24,  2.98s/batch]Batch 12000/20010 Done, mean position loss: 23.173733379840854\n",
      "Training growing_up:  59%|█████▎   | 11850/20010 [9:31:41<7:08:40,  3.15s/batch]Batch 12100/20010 Done, mean position loss: 23.480510375499726\n",
      "Training growing_up:  60%|█████▍   | 12093/20010 [9:31:57<6:08:00,  2.79s/batch]Batch 12000/20010 Done, mean position loss: 23.350259673595428\n",
      "Training growing_up:  61%|█████▌   | 12276/20010 [9:32:15<6:09:25,  2.87s/batch]Batch 12000/20010 Done, mean position loss: 24.154906907081603\n",
      "Training growing_up:  60%|█████▎   | 11928/20010 [9:32:22<7:03:46,  3.15s/batch]Batch 12100/20010 Done, mean position loss: 23.070039556026458\n",
      "Training growing_up:  61%|█████▍   | 12195/20010 [9:32:25<6:05:28,  2.81s/batch]Batch 12000/20010 Done, mean position loss: 22.90378103494644\n",
      "Training growing_up:  60%|█████▍   | 12081/20010 [9:32:29<5:56:44,  2.70s/batch]Batch 12100/20010 Done, mean position loss: 22.80103788614273\n",
      "Training growing_up:  60%|█████▍   | 12082/20010 [9:32:33<6:18:01,  2.86s/batch]Batch 12100/20010 Done, mean position loss: 22.898641912937165\n",
      "Training growing_up:  60%|█████▍   | 11995/20010 [9:32:33<6:50:07,  3.07s/batch]Batch 12200/20010 Done, mean position loss: 22.806319837570193\n",
      "Training growing_up:  61%|█████▍   | 12149/20010 [9:32:42<6:25:51,  2.95s/batch]Batch 11800/20010 Done, mean position loss: 23.327093498706816\n",
      "Training growing_up:  60%|█████▍   | 12057/20010 [9:32:43<6:40:28,  3.02s/batch]Batch 12200/20010 Done, mean position loss: 23.939621024131775\n",
      "Training growing_up:  60%|█████▍   | 12097/20010 [9:32:49<6:37:56,  3.02s/batch]Batch 12100/20010 Done, mean position loss: 22.56141456127167\n",
      "Training growing_up:  61%|█████▍   | 12192/20010 [9:32:49<6:15:25,  2.88s/batch]Batch 12000/20010 Done, mean position loss: 23.405709519386292\n",
      "Training growing_up:  60%|█████▍   | 11992/20010 [9:32:53<6:12:26,  2.79s/batch]Batch 12100/20010 Done, mean position loss: 22.859944775104523\n",
      "Training growing_up:  60%|█████▍   | 11994/20010 [9:32:58<5:54:26,  2.65s/batch]Batch 11900/20010 Done, mean position loss: 23.86305686712265\n",
      "Training growing_up:  60%|█████▍   | 12027/20010 [9:33:00<6:31:31,  2.94s/batch]Batch 12100/20010 Done, mean position loss: 23.14553882598877\n",
      "Training growing_up:  60%|█████▍   | 12066/20010 [9:33:10<6:17:53,  2.85s/batch]Batch 11900/20010 Done, mean position loss: 23.135158219337463\n",
      "Training growing_up:  59%|█████▎   | 11853/20010 [9:33:16<6:39:06,  2.94s/batch]Batch 12200/20010 Done, mean position loss: 22.920114269256594\n",
      "Training growing_up:  60%|█████▍   | 12021/20010 [9:33:22<6:11:27,  2.79s/batch]Batch 12000/20010 Done, mean position loss: 23.27863628149033\n",
      "Training growing_up:  60%|█████▍   | 12040/20010 [9:33:26<6:12:11,  2.80s/batch]Batch 12100/20010 Done, mean position loss: 24.272280569076536\n",
      "Training growing_up:  60%|█████▍   | 12101/20010 [9:33:26<6:06:54,  2.78s/batch]Batch 12300/20010 Done, mean position loss: 22.550323705673218\n",
      "Training growing_up:  60%|█████▍   | 12102/20010 [9:33:28<5:58:36,  2.72s/batch]Batch 12100/20010 Done, mean position loss: 24.125327551364897\n",
      "Training growing_up:  61%|█████▍   | 12127/20010 [9:34:01<6:03:06,  2.76s/batch]Batch 12000/20010 Done, mean position loss: 22.43392122268677\n",
      "Training growing_up:  61%|█████▍   | 12221/20010 [9:34:05<5:33:51,  2.57s/batch]Batch 11900/20010 Done, mean position loss: 22.976564159393313\n",
      "Training growing_up:  59%|█████▎   | 11834/20010 [9:34:14<6:00:03,  2.64s/batch]Batch 11900/20010 Done, mean position loss: 22.0734899520874\n",
      "Training growing_up:  59%|█████▎   | 11837/20010 [9:34:22<5:43:34,  2.52s/batch]Batch 12200/20010 Done, mean position loss: 22.64134256839752\n",
      "Training growing_up:  61%|█████▍   | 12228/20010 [9:34:22<5:28:45,  2.53s/batch]Batch 12100/20010 Done, mean position loss: 22.417948307991026\n",
      "Training growing_up:  60%|█████▍   | 12098/20010 [9:34:43<6:13:09,  2.83s/batch]Batch 12100/20010 Done, mean position loss: 23.423671295642855\n",
      "Training growing_up:  60%|█████▍   | 11979/20010 [9:34:50<6:23:35,  2.87s/batch]Batch 12100/20010 Done, mean position loss: 23.025983881950378\n",
      "Training growing_up:  61%|█████▍   | 12148/20010 [9:35:03<5:00:18,  2.29s/batch]Batch 12200/20010 Done, mean position loss: 22.265828990936278\n",
      "Training growing_up:  60%|█████▎   | 11945/20010 [9:35:05<6:08:48,  2.74s/batch]Batch 12200/20010 Done, mean position loss: 22.58185401439667\n",
      "Training growing_up:  61%|█████▍   | 12181/20010 [9:35:26<6:01:12,  2.77s/batch]Batch 11900/20010 Done, mean position loss: 22.9407107591629\n",
      "Training growing_up:  59%|█████▎   | 11866/20010 [9:35:43<7:15:46,  3.21s/batch]Batch 12200/20010 Done, mean position loss: 23.214845118522646\n",
      "Training growing_up:  60%|█████▍   | 12097/20010 [9:35:52<6:34:06,  2.99s/batch]Batch 12000/20010 Done, mean position loss: 23.288975069522856\n",
      "Training growing_up:  61%|█████▍   | 12155/20010 [9:35:53<6:19:15,  2.90s/batch]Batch 12200/20010 Done, mean position loss: 23.58953878879547\n",
      "Training growing_up:  60%|█████▎   | 11943/20010 [9:35:59<6:35:28,  2.94s/batch]Batch 12200/20010 Done, mean position loss: 22.848458452224733\n",
      "Training growing_up:  61%|█████▍   | 12173/20010 [9:36:05<6:24:26,  2.94s/batch]Batch 12100/20010 Done, mean position loss: 23.782346420288086\n",
      "Training growing_up:  61%|█████▍   | 12207/20010 [9:36:09<5:59:43,  2.77s/batch]Batch 12100/20010 Done, mean position loss: 23.787660048007965\n",
      "Training growing_up:  61%|█████▍   | 12162/20010 [9:36:15<6:40:46,  3.06s/batch]Batch 12100/20010 Done, mean position loss: 23.731046614646914\n",
      "Training growing_up:  60%|█████▍   | 12104/20010 [9:36:16<5:41:31,  2.59s/batch]Batch 12300/20010 Done, mean position loss: 22.984889373779296\n",
      "Training growing_up:  59%|█████▎   | 11881/20010 [9:36:25<6:14:18,  2.76s/batch]Batch 12100/20010 Done, mean position loss: 23.552024648189544\n",
      "Batch 12200/20010 Done, mean position loss: 23.836461930274965\n",
      "Training growing_up:  61%|█████▍   | 12168/20010 [9:36:32<6:20:07,  2.91s/batch]Batch 12100/20010 Done, mean position loss: 23.76436332941055\n",
      "Training growing_up:  61%|█████▍   | 12224/20010 [9:36:57<6:11:29,  2.86s/batch]Batch 12200/20010 Done, mean position loss: 23.492607369422913\n",
      "Training growing_up:  61%|█████▍   | 12222/20010 [9:36:57<6:07:20,  2.83s/batch]Batch 12100/20010 Done, mean position loss: 24.1563734793663\n",
      "Training growing_up:  61%|█████▍   | 12112/20010 [9:37:04<6:36:03,  3.01s/batch]Batch 12100/20010 Done, mean position loss: 22.69731657743454\n",
      "Training growing_up:  61%|█████▌   | 12297/20010 [9:37:09<6:10:11,  2.88s/batch]Batch 12300/20010 Done, mean position loss: 22.84907821178436\n",
      "Training growing_up:  61%|█████▍   | 12159/20010 [9:37:09<6:09:34,  2.82s/batch]Batch 12200/20010 Done, mean position loss: 22.686953659057618\n",
      "Training growing_up:  60%|█████▍   | 11990/20010 [9:37:15<6:14:56,  2.81s/batch]Batch 12200/20010 Done, mean position loss: 22.864667551517485\n",
      "Training growing_up:  62%|█████▌   | 12323/20010 [9:37:20<6:18:01,  2.95s/batch]Batch 12300/20010 Done, mean position loss: 23.62376909255981\n",
      "Training growing_up:  62%|█████▌   | 12389/20010 [9:37:21<5:51:39,  2.77s/batch]Batch 12200/20010 Done, mean position loss: 23.003019099235534\n",
      "Training growing_up:  61%|█████▍   | 12204/20010 [9:37:24<6:15:03,  2.88s/batch]Batch 11900/20010 Done, mean position loss: 22.882021605968475\n",
      "Training growing_up:  61%|█████▍   | 12211/20010 [9:37:26<6:22:15,  2.94s/batch]Batch 12200/20010 Done, mean position loss: 22.56876948595047\n",
      "Training growing_up:  61%|█████▍   | 12208/20010 [9:37:36<6:32:54,  3.02s/batch]Batch 12200/20010 Done, mean position loss: 23.486579756736752\n",
      "Training growing_up:  61%|█████▍   | 12125/20010 [9:37:37<6:34:19,  3.00s/batch]Batch 12100/20010 Done, mean position loss: 23.045792982578277\n",
      "Training growing_up:  61%|█████▍   | 12115/20010 [9:37:45<6:22:12,  2.90s/batch]Batch 12000/20010 Done, mean position loss: 23.852542259693145\n",
      "Training growing_up:  60%|█████▍   | 12081/20010 [9:37:47<6:34:12,  2.98s/batch]Batch 12000/20010 Done, mean position loss: 23.119169631004333\n",
      "Training growing_up:  61%|█████▍   | 12209/20010 [9:37:49<6:12:53,  2.87s/batch]Batch 12300/20010 Done, mean position loss: 22.86166162252426\n",
      "Training growing_up:  61%|█████▍   | 12208/20010 [9:37:56<6:39:00,  3.07s/batch]Batch 12400/20010 Done, mean position loss: 22.128761734962463\n",
      "Training growing_up:  61%|█████▍   | 12139/20010 [9:38:00<6:16:30,  2.87s/batch]Batch 12100/20010 Done, mean position loss: 23.481463305950165\n",
      "Training growing_up:  61%|█████▍   | 12173/20010 [9:38:06<6:09:28,  2.83s/batch]Batch 12200/20010 Done, mean position loss: 23.323706531524657\n",
      "Training growing_up:  61%|█████▍   | 12135/20010 [9:38:08<6:26:37,  2.95s/batch]Batch 12200/20010 Done, mean position loss: 23.736986620426176\n",
      "Training growing_up:  61%|█████▍   | 12214/20010 [9:38:44<6:07:51,  2.83s/batch]Batch 12100/20010 Done, mean position loss: 22.695103561878202\n",
      "Training growing_up:  61%|█████▍   | 12186/20010 [9:38:44<6:04:16,  2.79s/batch]Batch 12000/20010 Done, mean position loss: 22.938239867687223\n",
      "Training growing_up:  61%|█████▌   | 12254/20010 [9:39:01<5:49:48,  2.71s/batch]Batch 12300/20010 Done, mean position loss: 22.67748224258423\n",
      "Training growing_up:  61%|█████▍   | 12147/20010 [9:39:06<5:36:39,  2.57s/batch]Batch 12000/20010 Done, mean position loss: 22.234289097785947\n",
      "Training growing_up:  61%|█████▍   | 12132/20010 [9:39:10<5:44:28,  2.62s/batch]Batch 12200/20010 Done, mean position loss: 22.819627077579497\n",
      "Training growing_up:  60%|█████▍   | 12075/20010 [9:39:28<6:31:58,  2.96s/batch]Batch 12200/20010 Done, mean position loss: 23.186270916461943\n",
      "Training growing_up:  62%|█████▌   | 12340/20010 [9:39:40<5:52:19,  2.76s/batch]Batch 12200/20010 Done, mean position loss: 23.091387124061583\n",
      "Training growing_up:  61%|█████▍   | 12216/20010 [9:39:54<6:41:31,  3.09s/batch]Batch 12300/20010 Done, mean position loss: 22.953979103565217\n",
      "Training growing_up:  61%|█████▌   | 12248/20010 [9:39:56<7:06:01,  3.29s/batch]Batch 12300/20010 Done, mean position loss: 22.19804899215698\n",
      "Training growing_up:  61%|█████▌   | 12248/20010 [9:40:29<7:37:01,  3.53s/batch]Batch 12000/20010 Done, mean position loss: 22.870732946395876\n",
      "Training growing_up:  61%|█████▍   | 12194/20010 [9:40:40<7:22:02,  3.39s/batch]Batch 12300/20010 Done, mean position loss: 23.06403179168701\n",
      "Training growing_up:  62%|█████▌   | 12318/20010 [9:40:50<6:48:29,  3.19s/batch]Batch 12100/20010 Done, mean position loss: 23.12025486946106\n",
      "Training growing_up:  62%|█████▌   | 12317/20010 [9:40:50<7:19:12,  3.43s/batch]Batch 12300/20010 Done, mean position loss: 23.282421507835387\n",
      "Training growing_up:  62%|█████▌   | 12338/20010 [9:40:55<7:14:08,  3.40s/batch]Batch 12300/20010 Done, mean position loss: 22.770080075263976\n",
      "Training growing_up:  61%|█████▍   | 12145/20010 [9:40:59<6:36:32,  3.03s/batch]Batch 12200/20010 Done, mean position loss: 23.757660241127013\n",
      "Training growing_up:  60%|█████▍   | 12067/20010 [9:41:05<6:18:38,  2.86s/batch]Batch 12200/20010 Done, mean position loss: 23.836414976119993\n",
      "Training growing_up:  61%|█████▍   | 12165/20010 [9:41:15<6:22:40,  2.93s/batch]Batch 12200/20010 Done, mean position loss: 23.677714774608614\n",
      "Training growing_up:  60%|█████▍   | 12043/20010 [9:41:17<6:18:08,  2.85s/batch]Batch 12400/20010 Done, mean position loss: 23.209674322605135\n",
      "Training growing_up:  60%|█████▍   | 11978/20010 [9:41:20<6:48:30,  3.05s/batch]Batch 12300/20010 Done, mean position loss: 23.6769147849083\n",
      "Training growing_up:  60%|█████▍   | 11981/20010 [9:41:31<7:52:36,  3.53s/batch]Batch 12200/20010 Done, mean position loss: 23.633571069240567\n",
      "Training growing_up:  62%|█████▌   | 12387/20010 [9:41:34<6:33:04,  3.09s/batch]Batch 12200/20010 Done, mean position loss: 23.65705042362213\n",
      "Training growing_up:  61%|█████▌   | 12285/20010 [9:41:54<6:19:36,  2.95s/batch]Batch 12200/20010 Done, mean position loss: 24.44246662139893\n",
      "Training growing_up:  60%|█████▍   | 12029/20010 [9:41:57<6:58:51,  3.15s/batch]Batch 12300/20010 Done, mean position loss: 23.277651174068453\n",
      "Training growing_up:  60%|█████▍   | 12087/20010 [9:42:09<6:24:21,  2.91s/batch]Batch 12200/20010 Done, mean position loss: 22.90752729415894\n",
      "Training growing_up:  61%|█████▍   | 12129/20010 [9:42:15<7:41:56,  3.52s/batch]Batch 12400/20010 Done, mean position loss: 24.240499641895298\n",
      "Training growing_up:  60%|█████▍   | 12089/20010 [9:42:15<6:41:23,  3.04s/batch]Batch 12400/20010 Done, mean position loss: 22.741886732578276\n",
      "Training growing_up:  61%|█████▌   | 12252/20010 [9:42:15<6:55:24,  3.21s/batch]Batch 12300/20010 Done, mean position loss: 22.95655692577362\n",
      "Training growing_up:  60%|█████▍   | 12064/20010 [9:42:21<6:29:07,  2.94s/batch]Batch 12300/20010 Done, mean position loss: 23.256724853515625\n",
      "Training growing_up:  61%|█████▌   | 12285/20010 [9:42:26<6:30:57,  3.04s/batch]Batch 12300/20010 Done, mean position loss: 22.96577022790909\n",
      "Training growing_up:  61%|█████▌   | 12302/20010 [9:42:29<6:31:19,  3.05s/batch]Batch 12300/20010 Done, mean position loss: 22.652781422138215\n",
      "Training growing_up:  62%|█████▌   | 12495/20010 [9:42:35<6:03:01,  2.90s/batch]Batch 12000/20010 Done, mean position loss: 22.5094776558876\n",
      "Training growing_up:  60%|█████▍   | 12071/20010 [9:42:43<6:45:51,  3.07s/batch]Batch 12300/20010 Done, mean position loss: 23.216679055690765\n",
      "Training growing_up:  61%|█████▌   | 12235/20010 [9:42:47<7:15:20,  3.36s/batch]Batch 12200/20010 Done, mean position loss: 22.943478145599364\n",
      "Training growing_up:  62%|█████▌   | 12339/20010 [9:42:49<6:15:47,  2.94s/batch]Batch 12100/20010 Done, mean position loss: 23.0249821138382\n",
      "Training growing_up:  61%|█████▌   | 12304/20010 [9:42:51<5:51:17,  2.74s/batch]Batch 12500/20010 Done, mean position loss: 22.546572029590607\n",
      "Batch 12100/20010 Done, mean position loss: 23.98667282104492\n",
      "Training growing_up:  60%|█████▍   | 12081/20010 [9:42:54<6:20:39,  2.88s/batch]Batch 12400/20010 Done, mean position loss: 22.83742352962494\n",
      "Training growing_up:  62%|█████▌   | 12318/20010 [9:43:06<6:20:26,  2.97s/batch]Batch 12200/20010 Done, mean position loss: 23.590770440101622\n",
      "Training growing_up:  61%|█████▌   | 12272/20010 [9:43:10<6:18:42,  2.94s/batch]Batch 12300/20010 Done, mean position loss: 23.9731215596199\n",
      "Training growing_up:  62%|█████▌   | 12383/20010 [9:43:12<6:12:22,  2.93s/batch]Batch 12300/20010 Done, mean position loss: 23.809235739707944\n",
      "Training growing_up:  63%|█████▋   | 12522/20010 [9:43:54<5:50:25,  2.81s/batch]Batch 12100/20010 Done, mean position loss: 22.94188637971878\n",
      "Training growing_up:  60%|█████▍   | 12029/20010 [9:43:58<6:38:54,  3.00s/batch]Batch 12200/20010 Done, mean position loss: 22.293806829452514\n",
      "Training growing_up:  62%|█████▌   | 12343/20010 [9:44:04<7:05:22,  3.33s/batch]Batch 12400/20010 Done, mean position loss: 22.107904353141784\n",
      "Training growing_up:  62%|█████▌   | 12331/20010 [9:44:14<6:17:24,  2.95s/batch]Batch 12100/20010 Done, mean position loss: 22.361302049160003\n",
      "Training growing_up:  62%|█████▌   | 12366/20010 [9:44:14<6:02:30,  2.85s/batch]Batch 12300/20010 Done, mean position loss: 22.41378273010254\n",
      "Training growing_up:  62%|█████▌   | 12330/20010 [9:44:37<5:39:02,  2.65s/batch]Batch 12300/20010 Done, mean position loss: 23.057905128002165\n",
      "Training growing_up:  62%|█████▌   | 12395/20010 [9:44:43<6:33:35,  3.10s/batch]Batch 12300/20010 Done, mean position loss: 23.090750176906585\n",
      "Training growing_up:  62%|█████▌   | 12352/20010 [9:45:01<6:14:13,  2.93s/batch]Batch 12400/20010 Done, mean position loss: 22.26358093500137\n",
      "Training growing_up:  62%|█████▌   | 12363/20010 [9:45:03<6:26:44,  3.03s/batch]Batch 12400/20010 Done, mean position loss: 22.624764089584353\n",
      "Training growing_up:  62%|█████▌   | 12321/20010 [9:45:35<6:13:56,  2.92s/batch]Batch 12100/20010 Done, mean position loss: 22.9019495010376\n",
      "Training growing_up:  61%|█████▍   | 12158/20010 [9:45:48<6:34:28,  3.01s/batch]Batch 12400/20010 Done, mean position loss: 23.197145183086395\n",
      "Training growing_up:  62%|█████▌   | 12390/20010 [9:45:48<7:19:48,  3.46s/batch]Batch 12200/20010 Done, mean position loss: 23.06948150873184\n",
      "Training growing_up:  62%|█████▌   | 12367/20010 [9:45:57<6:05:25,  2.87s/batch]Batch 12300/20010 Done, mean position loss: 23.851131899356844\n",
      "Training growing_up:  62%|█████▌   | 12356/20010 [9:45:57<6:32:45,  3.08s/batch]Batch 12400/20010 Done, mean position loss: 23.209892942905427\n",
      "Training growing_up:  62%|█████▌   | 12369/20010 [9:46:03<6:25:56,  3.03s/batch]Batch 12400/20010 Done, mean position loss: 22.651173532009125\n",
      "Training growing_up:  62%|█████▌   | 12358/20010 [9:46:04<6:29:14,  3.05s/batch]Batch 12300/20010 Done, mean position loss: 23.729131007194518\n",
      "Training growing_up:  62%|█████▌   | 12375/20010 [9:46:10<5:56:53,  2.80s/batch]Batch 12500/20010 Done, mean position loss: 23.504800555706026\n",
      "Training growing_up:  62%|█████▌   | 12363/20010 [9:46:20<6:32:28,  3.08s/batch]Batch 12300/20010 Done, mean position loss: 23.393015007972718\n",
      "Batch 12400/20010 Done, mean position loss: 23.533756754398347\n",
      "Training growing_up:  62%|█████▌   | 12415/20010 [9:46:33<6:49:48,  3.24s/batch]Batch 12300/20010 Done, mean position loss: 23.56464136123657\n",
      "Training growing_up:  61%|█████▌   | 12305/20010 [9:46:33<7:08:49,  3.34s/batch]Batch 12300/20010 Done, mean position loss: 23.400045964717865\n",
      "Training growing_up:  61%|█████▍   | 12182/20010 [9:46:55<6:43:52,  3.10s/batch]Batch 12400/20010 Done, mean position loss: 23.072381930351256\n",
      "Training growing_up:  60%|█████▍   | 12088/20010 [9:46:57<6:20:38,  2.88s/batch]Batch 12300/20010 Done, mean position loss: 24.136840772628783\n",
      "Training growing_up:  62%|█████▌   | 12348/20010 [9:47:04<6:04:53,  2.86s/batch]Batch 12500/20010 Done, mean position loss: 22.774215445518493\n",
      "Training growing_up:  61%|█████▌   | 12282/20010 [9:47:10<6:27:44,  3.01s/batch]Batch 12300/20010 Done, mean position loss: 22.656674087047577\n",
      "Training growing_up:  61%|█████▍   | 12159/20010 [9:47:13<6:41:27,  3.07s/batch]Batch 12500/20010 Done, mean position loss: 23.902549533843995\n",
      "Training growing_up:  62%|█████▌   | 12429/20010 [9:47:17<7:40:58,  3.65s/batch]Batch 12400/20010 Done, mean position loss: 23.27084409236908\n",
      "Training growing_up:  62%|█████▌   | 12451/20010 [9:47:25<5:43:15,  2.72s/batch]Batch 12400/20010 Done, mean position loss: 23.22795431137085\n",
      "Training growing_up:  62%|█████▌   | 12396/20010 [9:47:26<7:09:03,  3.38s/batch]Batch 12400/20010 Done, mean position loss: 22.60246877193451\n",
      "Training growing_up:  62%|█████▌   | 12359/20010 [9:47:32<6:48:10,  3.20s/batch]Batch 12400/20010 Done, mean position loss: 22.853097929954526\n",
      "Training growing_up:  60%|█████▍   | 12100/20010 [9:47:34<6:25:13,  2.92s/batch]Batch 12600/20010 Done, mean position loss: 22.5229212641716\n",
      "Training growing_up:  63%|█████▋   | 12512/20010 [9:47:37<6:02:37,  2.90s/batch]Batch 12100/20010 Done, mean position loss: 22.33500568151474\n",
      "Training growing_up:  61%|█████▌   | 12300/20010 [9:47:42<6:29:12,  3.03s/batch]Batch 12400/20010 Done, mean position loss: 23.27878813028336\n",
      "Training growing_up:  62%|█████▌   | 12402/20010 [9:47:44<5:58:19,  2.83s/batch]Batch 12300/20010 Done, mean position loss: 23.066716463565825\n",
      "Training growing_up:  62%|█████▌   | 12410/20010 [9:47:52<6:04:39,  2.88s/batch]Batch 12200/20010 Done, mean position loss: 22.792895970344546\n",
      "Training growing_up:  62%|█████▌   | 12442/20010 [9:47:55<5:26:05,  2.59s/batch]Batch 12500/20010 Done, mean position loss: 22.84799887657166\n",
      "Training growing_up:  62%|█████▌   | 12425/20010 [9:48:00<5:17:37,  2.51s/batch]Batch 12200/20010 Done, mean position loss: 23.509184165000917\n",
      "Training growing_up:  62%|█████▌   | 12442/20010 [9:48:02<5:53:59,  2.81s/batch]Batch 12300/20010 Done, mean position loss: 23.325135180950163\n",
      "Training growing_up:  62%|█████▌   | 12383/20010 [9:48:09<5:30:59,  2.60s/batch]Batch 12400/20010 Done, mean position loss: 23.234908409118653\n",
      "Training growing_up:  62%|█████▌   | 12445/20010 [9:48:11<6:02:29,  2.88s/batch]Batch 12400/20010 Done, mean position loss: 24.211195676326753\n",
      "Training growing_up:  62%|█████▌   | 12351/20010 [9:48:51<5:44:57,  2.70s/batch]Batch 12200/20010 Done, mean position loss: 23.40216734409332\n",
      "Training growing_up:  62%|█████▌   | 12418/20010 [9:48:52<5:09:28,  2.45s/batch]Batch 12300/20010 Done, mean position loss: 22.82239207983017\n",
      "Training growing_up:  63%|█████▋   | 12561/20010 [9:48:58<5:39:02,  2.73s/batch]Batch 12400/20010 Done, mean position loss: 22.57421408891678\n",
      "Training growing_up:  62%|█████▌   | 12421/20010 [9:48:59<4:49:48,  2.29s/batch]Batch 12500/20010 Done, mean position loss: 21.875450799465177\n",
      "Training growing_up:  63%|█████▋   | 12547/20010 [9:49:11<4:59:46,  2.41s/batch]Batch 12200/20010 Done, mean position loss: 22.389523644447323\n",
      "Training growing_up:  62%|█████▌   | 12496/20010 [9:49:22<5:08:51,  2.47s/batch]Batch 12400/20010 Done, mean position loss: 23.3049551320076\n",
      "Training growing_up:  62%|█████▌   | 12450/20010 [9:49:35<5:22:57,  2.56s/batch]Batch 12400/20010 Done, mean position loss: 22.987624726295472\n",
      "Training growing_up:  62%|█████▌   | 12343/20010 [9:49:35<5:37:40,  2.64s/batch]Batch 12500/20010 Done, mean position loss: 22.53203186273575\n",
      "Training growing_up:  61%|█████▍   | 12211/20010 [9:49:36<5:10:01,  2.39s/batch]Batch 12500/20010 Done, mean position loss: 21.80456754684448\n",
      "Training growing_up:  62%|█████▌   | 12424/20010 [9:50:21<5:23:00,  2.55s/batch]Batch 12200/20010 Done, mean position loss: 22.726312398910522\n",
      "Training growing_up:  62%|█████▌   | 12471/20010 [9:50:28<5:29:22,  2.62s/batch]Batch 12300/20010 Done, mean position loss: 22.90332623720169\n",
      "Training growing_up:  61%|█████▌   | 12301/20010 [9:50:28<5:21:08,  2.50s/batch]Batch 12400/20010 Done, mean position loss: 23.670500125885013\n",
      "Training growing_up:  62%|█████▌   | 12472/20010 [9:50:31<5:40:47,  2.71s/batch]Batch 12500/20010 Done, mean position loss: 23.082945671081546\n",
      "Training growing_up:  62%|█████▌   | 12473/20010 [9:50:34<5:57:15,  2.84s/batch]Batch 12500/20010 Done, mean position loss: 23.25622448682785\n",
      "Training growing_up:  63%|█████▋   | 12523/20010 [9:50:35<5:43:34,  2.75s/batch]Batch 12400/20010 Done, mean position loss: 23.85532522916794\n",
      "Training growing_up:  62%|█████▌   | 12460/20010 [9:50:41<5:42:08,  2.72s/batch]Batch 12500/20010 Done, mean position loss: 22.70319238901138\n",
      "Training growing_up:  62%|█████▌   | 12380/20010 [9:50:42<5:53:50,  2.78s/batch]Batch 12600/20010 Done, mean position loss: 22.90098804950714\n",
      "Training growing_up:  62%|█████▌   | 12396/20010 [9:50:49<5:46:25,  2.73s/batch]Batch 12500/20010 Done, mean position loss: 23.26497588157654\n",
      "Training growing_up:  62%|█████▌   | 12505/20010 [9:51:01<5:47:54,  2.78s/batch]Batch 12400/20010 Done, mean position loss: 23.539522800445553\n",
      "Training growing_up:  63%|█████▋   | 12681/20010 [9:51:03<5:43:38,  2.81s/batch]Batch 12400/20010 Done, mean position loss: 23.609502527713776\n",
      "Training growing_up:  62%|█████▌   | 12351/20010 [9:51:07<6:41:59,  3.15s/batch]Batch 12400/20010 Done, mean position loss: 23.21209260463715\n",
      "Training growing_up:  62%|█████▌   | 12469/20010 [9:51:22<5:48:43,  2.77s/batch]Batch 12500/20010 Done, mean position loss: 23.45394783735275\n",
      "Training growing_up:  61%|█████▌   | 12282/20010 [9:51:38<6:27:24,  3.01s/batch]Batch 12400/20010 Done, mean position loss: 24.158994381427764\n",
      "Training growing_up:  63%|█████▋   | 12507/20010 [9:51:39<6:02:25,  2.90s/batch]Batch 12600/20010 Done, mean position loss: 22.42689510822296\n",
      "Training growing_up:  62%|█████▌   | 12427/20010 [9:51:42<6:19:25,  3.00s/batch]Batch 12500/20010 Done, mean position loss: 22.874576959609985\n",
      "Training growing_up:  63%|█████▋   | 12600/20010 [9:51:45<6:20:00,  3.08s/batch]Batch 12400/20010 Done, mean position loss: 22.886724047660827\n",
      "Training growing_up:  62%|█████▌   | 12478/20010 [9:51:48<5:43:43,  2.74s/batch]Batch 12600/20010 Done, mean position loss: 23.171753189563752\n",
      "Training growing_up:  62%|█████▌   | 12427/20010 [9:51:51<5:38:46,  2.68s/batch]Batch 12500/20010 Done, mean position loss: 22.648895082473757\n",
      "Training growing_up:  62%|█████▌   | 12367/20010 [9:51:52<6:20:32,  2.99s/batch]Batch 12500/20010 Done, mean position loss: 22.889502694606783\n",
      "Training growing_up:  62%|█████▌   | 12503/20010 [9:51:58<5:31:39,  2.65s/batch]Batch 12500/20010 Done, mean position loss: 23.313651258945463\n",
      "Training growing_up:  62%|█████▌   | 12397/20010 [9:52:00<5:02:14,  2.38s/batch]Batch 12700/20010 Done, mean position loss: 22.548160293102264\n",
      "Training growing_up:  62%|█████▌   | 12409/20010 [9:52:07<5:44:43,  2.72s/batch]Batch 12500/20010 Done, mean position loss: 23.330899374485018\n",
      "Training growing_up:  63%|█████▋   | 12518/20010 [9:52:10<6:20:49,  3.05s/batch]Batch 12400/20010 Done, mean position loss: 23.251716957092288\n",
      "Training growing_up:  62%|█████▌   | 12411/20010 [9:52:13<6:03:29,  2.87s/batch]Batch 12200/20010 Done, mean position loss: 22.545433664321898\n",
      "Training growing_up:  62%|█████▌   | 12404/20010 [9:52:18<5:16:57,  2.50s/batch]Batch 12600/20010 Done, mean position loss: 22.569542167186736\n",
      "Training growing_up:  63%|█████▋   | 12522/20010 [9:52:22<6:05:00,  2.92s/batch]Batch 12300/20010 Done, mean position loss: 23.17832468032837\n",
      "Training growing_up:  62%|█████▌   | 12445/20010 [9:52:31<5:51:39,  2.79s/batch]Batch 12300/20010 Done, mean position loss: 23.51049732208252\n",
      "Training growing_up:  63%|█████▋   | 12567/20010 [9:52:33<5:41:50,  2.76s/batch]Batch 12500/20010 Done, mean position loss: 23.983065176010133\n",
      "Batch 12400/20010 Done, mean position loss: 23.564334638118744\n",
      "Training growing_up:  62%|█████▌   | 12470/20010 [9:52:54<6:25:18,  3.07s/batch]Batch 12500/20010 Done, mean position loss: 23.784582767486576\n",
      "Training growing_up:  63%|█████▋   | 12545/20010 [9:53:27<5:47:03,  2.79s/batch]Batch 12600/20010 Done, mean position loss: 22.16523078918457\n",
      "Training growing_up:  62%|█████▌   | 12321/20010 [9:53:29<6:28:20,  3.03s/batch]Batch 12300/20010 Done, mean position loss: 23.180634922981262\n",
      "Training growing_up:  61%|█████▌   | 12301/20010 [9:53:29<6:41:05,  3.12s/batch]Batch 12400/20010 Done, mean position loss: 22.290420811176297\n",
      "Training growing_up:  63%|█████▋   | 12565/20010 [9:53:33<5:23:28,  2.61s/batch]Batch 12500/20010 Done, mean position loss: 22.798888704776765\n",
      "Training growing_up:  63%|█████▋   | 12518/20010 [9:53:45<6:20:19,  3.05s/batch]Batch 12300/20010 Done, mean position loss: 22.114479641914365\n",
      "Training growing_up:  62%|█████▌   | 12473/20010 [9:53:56<6:06:19,  2.92s/batch]Batch 12500/20010 Done, mean position loss: 23.14532586097717\n",
      "Training growing_up:  62%|█████▌   | 12340/20010 [9:54:10<6:06:48,  2.87s/batch]Batch 12600/20010 Done, mean position loss: 22.975549619197846\n",
      "Training growing_up:  63%|█████▋   | 12552/20010 [9:54:18<6:08:57,  2.97s/batch]Batch 12600/20010 Done, mean position loss: 22.24077555179596\n",
      "Training growing_up:  63%|█████▋   | 12658/20010 [9:54:22<6:45:24,  3.31s/batch]Batch 12500/20010 Done, mean position loss: 23.000525803565978\n",
      "Training growing_up:  64%|█████▋   | 12771/20010 [9:55:11<5:34:25,  2.77s/batch]Batch 12500/20010 Done, mean position loss: 23.567588796615603\n",
      "Training growing_up:  62%|█████▌   | 12488/20010 [9:55:12<6:18:58,  3.02s/batch]Batch 12400/20010 Done, mean position loss: 23.130605306625366\n",
      "Training growing_up:  62%|█████▌   | 12466/20010 [9:55:15<6:37:51,  3.16s/batch]Batch 12300/20010 Done, mean position loss: 22.971887912750244\n",
      "Training growing_up:  63%|█████▋   | 12677/20010 [9:55:16<5:54:45,  2.90s/batch]Batch 12600/20010 Done, mean position loss: 23.479376461505893\n",
      "Training growing_up:  62%|█████▌   | 12480/20010 [9:55:19<5:33:52,  2.66s/batch]Batch 12600/20010 Done, mean position loss: 23.160715913772584\n",
      "Training growing_up:  63%|█████▋   | 12603/20010 [9:55:24<5:30:47,  2.68s/batch]Batch 12500/20010 Done, mean position loss: 23.791600761413576\n",
      "Training growing_up:  61%|█████▌   | 12266/20010 [9:55:27<6:13:48,  2.90s/batch]Batch 12600/20010 Done, mean position loss: 22.97277323246002\n",
      "Training growing_up:  62%|█████▌   | 12443/20010 [9:55:31<6:20:45,  3.02s/batch]Batch 12700/20010 Done, mean position loss: 23.153514020442962\n",
      "Training growing_up:  63%|█████▋   | 12579/20010 [9:55:39<6:26:09,  3.12s/batch]Batch 12600/20010 Done, mean position loss: 23.30511169910431\n",
      "Training growing_up:  63%|█████▋   | 12539/20010 [9:55:49<6:07:54,  2.95s/batch]Batch 12500/20010 Done, mean position loss: 23.384171354770658\n",
      "Training growing_up:  63%|█████▋   | 12650/20010 [9:55:50<6:14:58,  3.06s/batch]Batch 12500/20010 Done, mean position loss: 23.41397828578949\n",
      "Training growing_up:  63%|█████▋   | 12595/20010 [9:55:54<5:53:15,  2.86s/batch]Batch 12500/20010 Done, mean position loss: 23.51467722415924\n",
      "Training growing_up:  63%|█████▋   | 12591/20010 [9:56:14<6:11:26,  3.00s/batch]Batch 12600/20010 Done, mean position loss: 23.143829357624053\n",
      "Training growing_up:  63%|█████▋   | 12623/20010 [9:56:27<6:53:24,  3.36s/batch]Batch 12700/20010 Done, mean position loss: 23.060989263057706\n",
      "Training growing_up:  63%|█████▋   | 12559/20010 [9:56:27<6:09:51,  2.98s/batch]Batch 12500/20010 Done, mean position loss: 24.19882246732712\n",
      "Training growing_up:  63%|█████▋   | 12574/20010 [9:56:31<6:48:14,  3.29s/batch]Batch 12600/20010 Done, mean position loss: 23.38972276210785\n",
      "Training growing_up:  63%|█████▋   | 12596/20010 [9:56:37<5:49:03,  2.82s/batch]Batch 12500/20010 Done, mean position loss: 22.69413261651993\n",
      "Training growing_up:  63%|█████▋   | 12516/20010 [9:56:37<5:59:57,  2.88s/batch]Batch 12800/20010 Done, mean position loss: 22.17288156032562\n",
      "Training growing_up:  64%|█████▊   | 12801/20010 [9:56:38<6:23:15,  3.19s/batch]Batch 12600/20010 Done, mean position loss: 22.62328407764435\n",
      "Training growing_up:  63%|█████▋   | 12600/20010 [9:56:43<5:53:23,  2.86s/batch]Batch 12700/20010 Done, mean position loss: 23.541752462387084\n",
      "Training growing_up:  63%|█████▋   | 12519/20010 [9:56:46<6:12:25,  2.98s/batch]Batch 12600/20010 Done, mean position loss: 22.908315572738648\n",
      "Training growing_up:  63%|█████▋   | 12629/20010 [9:56:46<6:39:53,  3.25s/batch]Batch 12600/20010 Done, mean position loss: 23.13866416454315\n",
      "Training growing_up:  61%|█████▌   | 12295/20010 [9:56:53<6:27:47,  3.02s/batch]Batch 12600/20010 Done, mean position loss: 23.311828372478487\n",
      "Training growing_up:  63%|█████▋   | 12672/20010 [9:56:59<5:47:42,  2.84s/batch]Batch 12500/20010 Done, mean position loss: 23.132264442443848\n",
      "Training growing_up:  63%|█████▋   | 12564/20010 [9:57:05<6:09:09,  2.97s/batch]Batch 12700/20010 Done, mean position loss: 22.722428941726683\n",
      "Training growing_up:  63%|█████▋   | 12512/20010 [9:57:10<6:28:14,  3.11s/batch]Batch 12300/20010 Done, mean position loss: 22.44209262132645\n",
      "Training growing_up:  63%|█████▋   | 12633/20010 [9:57:15<6:04:05,  2.96s/batch]Batch 12400/20010 Done, mean position loss: 22.667172484397888\n",
      "Training growing_up:  63%|█████▋   | 12679/20010 [9:57:22<6:24:34,  3.15s/batch]Batch 12400/20010 Done, mean position loss: 23.5808225274086\n",
      "Training growing_up:  63%|█████▋   | 12617/20010 [9:57:24<6:11:12,  3.01s/batch]Batch 12600/20010 Done, mean position loss: 23.459309129714967\n",
      "Training growing_up:  63%|█████▋   | 12532/20010 [9:57:26<6:24:56,  3.09s/batch]Batch 12500/20010 Done, mean position loss: 23.520686428546906\n",
      "Training growing_up:  63%|█████▋   | 12623/20010 [9:57:50<5:40:33,  2.77s/batch]Batch 12600/20010 Done, mean position loss: 23.74973372936249\n",
      "Training growing_up:  63%|█████▋   | 12541/20010 [9:58:29<6:17:05,  3.03s/batch]Batch 12500/20010 Done, mean position loss: 22.408735313415526\n",
      "Training growing_up:  64%|█████▋   | 12741/20010 [9:58:30<5:59:27,  2.97s/batch]Batch 12700/20010 Done, mean position loss: 22.254580965042116\n",
      "Training growing_up:  63%|█████▋   | 12540/20010 [9:58:32<5:58:47,  2.88s/batch]Batch 12400/20010 Done, mean position loss: 23.17602155447006\n",
      "Training growing_up:  62%|█████▌   | 12329/20010 [9:58:37<6:47:30,  3.18s/batch]Batch 12600/20010 Done, mean position loss: 22.663151597976686\n",
      "Batch 12400/20010 Done, mean position loss: 22.29595395088196\n",
      "Training growing_up:  63%|█████▋   | 12632/20010 [9:58:56<6:19:54,  3.09s/batch]Batch 12600/20010 Done, mean position loss: 23.493607664108275\n",
      "Training growing_up:  63%|█████▋   | 12557/20010 [9:59:14<5:40:49,  2.74s/batch]Batch 12700/20010 Done, mean position loss: 22.667511923313143\n",
      "Training growing_up:  63%|█████▋   | 12517/20010 [9:59:16<5:22:48,  2.58s/batch]Batch 12600/20010 Done, mean position loss: 23.28384387254715\n",
      "Training growing_up:  63%|█████▋   | 12601/20010 [9:59:16<6:02:50,  2.94s/batch]Batch 12700/20010 Done, mean position loss: 22.057359428405764\n",
      "Training growing_up:  62%|████▉   | 12455/20010 [10:00:09<6:09:58,  2.94s/batch]Batch 12500/20010 Done, mean position loss: 23.3479195356369\n",
      "Training growing_up:  63%|█████   | 12649/20010 [10:00:13<5:52:48,  2.88s/batch]Batch 12600/20010 Done, mean position loss: 23.377267322540284\n",
      "Training growing_up:  62%|████▉   | 12400/20010 [10:00:19<5:55:36,  2.80s/batch]Batch 12700/20010 Done, mean position loss: 23.06271596431732\n",
      "Batch 12700/20010 Done, mean position loss: 23.263311035633087\n",
      "Training growing_up:  63%|█████   | 12631/20010 [10:00:21<5:53:26,  2.87s/batch]Batch 12400/20010 Done, mean position loss: 23.103173434734344\n",
      "Training growing_up:  63%|█████   | 12699/20010 [10:00:27<5:57:38,  2.94s/batch]Batch 12800/20010 Done, mean position loss: 23.45041519880295\n",
      "Training growing_up:  63%|█████   | 12593/20010 [10:00:33<6:17:35,  3.05s/batch]Batch 12700/20010 Done, mean position loss: 22.963529839515687\n",
      "Training growing_up:  64%|█████▏  | 12881/20010 [10:00:34<5:17:20,  2.67s/batch]Batch 12600/20010 Done, mean position loss: 23.68818531036377\n",
      "Training growing_up:  63%|█████   | 12603/20010 [10:00:41<6:24:29,  3.11s/batch]Batch 12700/20010 Done, mean position loss: 23.84493703365326\n",
      "Training growing_up:  64%|█████   | 12774/20010 [10:00:48<6:47:53,  3.38s/batch]Batch 12600/20010 Done, mean position loss: 23.569764490127564\n",
      "Training growing_up:  63%|█████   | 12695/20010 [10:00:58<6:09:34,  3.03s/batch]Batch 12600/20010 Done, mean position loss: 23.335338046550753\n",
      "Training growing_up:  63%|█████   | 12664/20010 [10:00:59<6:16:55,  3.08s/batch]Batch 12600/20010 Done, mean position loss: 23.634689223766326\n",
      "Training growing_up:  64%|█████   | 12743/20010 [10:01:17<6:11:59,  3.07s/batch]Batch 12700/20010 Done, mean position loss: 23.153888063430784\n",
      "Training growing_up:  62%|████▉   | 12422/20010 [10:01:26<6:38:11,  3.15s/batch]Batch 12600/20010 Done, mean position loss: 24.059384467601777\n",
      "Training growing_up:  63%|█████   | 12612/20010 [10:01:29<6:02:21,  2.94s/batch]Batch 12700/20010 Done, mean position loss: 23.232662432193756\n",
      "Training growing_up:  63%|█████   | 12658/20010 [10:01:29<5:35:38,  2.74s/batch]Batch 12800/20010 Done, mean position loss: 22.87409274101257\n",
      "Training growing_up:  63%|█████   | 12700/20010 [10:01:35<6:16:23,  3.09s/batch]Batch 12900/20010 Done, mean position loss: 22.242304699420927\n",
      "Training growing_up:  63%|█████   | 12629/20010 [10:01:36<5:59:14,  2.92s/batch]Batch 12600/20010 Done, mean position loss: 22.510462217330932\n",
      "Training growing_up:  63%|█████   | 12697/20010 [10:01:37<5:44:06,  2.82s/batch]Batch 12700/20010 Done, mean position loss: 22.503553600311278\n",
      "Training growing_up:  63%|█████   | 12602/20010 [10:01:39<5:37:54,  2.74s/batch]Batch 12700/20010 Done, mean position loss: 22.909671359062195\n",
      "Training growing_up:  63%|█████   | 12690/20010 [10:01:46<5:38:15,  2.77s/batch]Batch 12800/20010 Done, mean position loss: 23.507232744693756\n",
      "Training growing_up:  62%|████▉   | 12490/20010 [10:01:46<6:41:02,  3.20s/batch]Batch 12700/20010 Done, mean position loss: 22.933492975234984\n",
      "Training growing_up:  64%|█████▏  | 12830/20010 [10:01:52<5:22:20,  2.69s/batch]Batch 12700/20010 Done, mean position loss: 23.110536956787108\n",
      "Training growing_up:  63%|█████   | 12682/20010 [10:01:54<5:54:31,  2.90s/batch]Batch 12600/20010 Done, mean position loss: 23.170438106060026\n",
      "Training growing_up:  64%|█████▏  | 12834/20010 [10:02:05<6:08:36,  3.08s/batch]Batch 12800/20010 Done, mean position loss: 22.594995081424713\n",
      "Training growing_up:  63%|█████   | 12640/20010 [10:02:11<6:35:32,  3.22s/batch]Batch 12400/20010 Done, mean position loss: 22.288002541065215\n",
      "Training growing_up:  62%|████▉   | 12404/20010 [10:02:19<6:00:25,  2.84s/batch]Batch 12700/20010 Done, mean position loss: 23.52595650434494\n",
      "Training growing_up:  63%|█████   | 12629/20010 [10:02:20<5:39:05,  2.76s/batch]Batch 12500/20010 Done, mean position loss: 22.71059433698654\n",
      "Training growing_up:  62%|████▉   | 12479/20010 [10:02:31<6:27:16,  3.09s/batch]Batch 12600/20010 Done, mean position loss: 23.222484872341155\n",
      "Training growing_up:  65%|█████▏  | 12923/20010 [10:02:36<5:17:06,  2.68s/batch]Batch 12500/20010 Done, mean position loss: 24.110909264087677\n",
      "Training growing_up:  64%|█████▏  | 12824/20010 [10:02:53<6:18:02,  3.16s/batch]Batch 12700/20010 Done, mean position loss: 24.144575865268706\n",
      "Training growing_up:  63%|█████   | 12686/20010 [10:03:24<4:43:22,  2.32s/batch]Batch 12800/20010 Done, mean position loss: 22.392768268585208\n",
      "Training growing_up:  63%|█████   | 12659/20010 [10:03:28<5:12:47,  2.55s/batch]Batch 12600/20010 Done, mean position loss: 22.485736780166626\n",
      "Training growing_up:  64%|█████   | 12726/20010 [10:03:33<6:09:49,  3.05s/batch]Batch 12500/20010 Done, mean position loss: 23.17265592336655\n",
      "Training growing_up:  63%|█████   | 12522/20010 [10:03:33<5:38:17,  2.71s/batch]Batch 12500/20010 Done, mean position loss: 22.141534929275515\n",
      "Training growing_up:  64%|█████▏  | 12842/20010 [10:03:39<5:12:41,  2.62s/batch]Batch 12700/20010 Done, mean position loss: 22.67897786140442\n",
      "Training growing_up:  64%|█████   | 12742/20010 [10:03:45<4:53:53,  2.43s/batch]Batch 12700/20010 Done, mean position loss: 23.250685851573945\n",
      "Training growing_up:  64%|█████   | 12707/20010 [10:03:59<4:47:39,  2.36s/batch]Batch 12800/20010 Done, mean position loss: 22.489160146713257\n",
      "Training growing_up:  63%|█████   | 12682/20010 [10:04:04<4:54:52,  2.41s/batch]Batch 12700/20010 Done, mean position loss: 22.659725224971773\n",
      "Training growing_up:  63%|█████   | 12701/20010 [10:04:05<6:19:40,  3.12s/batch]Batch 12800/20010 Done, mean position loss: 22.424789955616\n",
      "Training growing_up:  64%|█████▏  | 12819/20010 [10:04:54<5:29:38,  2.75s/batch]Batch 12600/20010 Done, mean position loss: 22.907832117080687\n",
      "Training growing_up:  63%|█████   | 12667/20010 [10:04:54<5:23:53,  2.65s/batch]Batch 12700/20010 Done, mean position loss: 23.36880176067352\n",
      "Training growing_up:  63%|█████   | 12675/20010 [10:04:58<5:29:21,  2.69s/batch]Batch 12800/20010 Done, mean position loss: 23.523607449531553\n",
      "Training growing_up:  63%|█████   | 12686/20010 [10:04:59<5:24:39,  2.66s/batch]Batch 12800/20010 Done, mean position loss: 23.13695111274719\n",
      "Training growing_up:  64%|█████▏  | 12874/20010 [10:05:01<5:42:01,  2.88s/batch]Batch 12500/20010 Done, mean position loss: 22.894452874660495\n",
      "Training growing_up:  63%|█████   | 12639/20010 [10:05:04<5:50:09,  2.85s/batch]Batch 12900/20010 Done, mean position loss: 23.320733251571653\n",
      "Training growing_up:  64%|█████   | 12786/20010 [10:05:14<5:58:50,  2.98s/batch]Batch 12800/20010 Done, mean position loss: 22.874054555892947\n",
      "Training growing_up:  63%|█████   | 12560/20010 [10:05:15<6:01:26,  2.91s/batch]Batch 12700/20010 Done, mean position loss: 23.50549998283386\n",
      "Training growing_up:  63%|█████   | 12542/20010 [10:05:18<5:07:37,  2.47s/batch]Batch 12800/20010 Done, mean position loss: 23.47384618997574\n",
      "Training growing_up:  64%|█████   | 12805/20010 [10:05:26<5:54:42,  2.95s/batch]Batch 12700/20010 Done, mean position loss: 23.39466108083725\n",
      "Training growing_up:  64%|█████   | 12808/20010 [10:05:36<5:08:59,  2.57s/batch]Batch 12700/20010 Done, mean position loss: 23.170815098285676\n",
      "Training growing_up:  64%|█████   | 12738/20010 [10:05:37<4:45:34,  2.36s/batch]Batch 12700/20010 Done, mean position loss: 23.370917017459867\n",
      "Training growing_up:  63%|█████   | 12520/20010 [10:05:57<6:13:53,  3.00s/batch]Batch 12800/20010 Done, mean position loss: 22.959123752117158\n",
      "Training growing_up:  64%|█████▏  | 12895/20010 [10:06:02<5:41:13,  2.88s/batch]Batch 12700/20010 Done, mean position loss: 24.077143058776855\n",
      "Training growing_up:  64%|█████   | 12792/20010 [10:06:04<5:57:17,  2.97s/batch]Batch 12900/20010 Done, mean position loss: 22.702349107265473\n",
      "Training growing_up:  64%|█████▏  | 12845/20010 [10:06:05<5:48:35,  2.92s/batch]Batch 13000/20010 Done, mean position loss: 22.230546598434447\n",
      "Training growing_up:  63%|█████   | 12679/20010 [10:06:06<6:07:52,  3.01s/batch]Batch 12800/20010 Done, mean position loss: 23.170020492076873\n",
      "Training growing_up:  64%|█████   | 12717/20010 [10:06:11<5:44:15,  2.83s/batch]Batch 12800/20010 Done, mean position loss: 22.52827972650528\n",
      "Training growing_up:  63%|█████   | 12681/20010 [10:06:11<5:56:41,  2.92s/batch]Batch 12700/20010 Done, mean position loss: 22.60005432367325\n",
      "Training growing_up:  63%|█████   | 12580/20010 [10:06:16<6:15:16,  3.03s/batch]Batch 12800/20010 Done, mean position loss: 22.865619032382966\n",
      "Training growing_up:  65%|█████▏  | 13006/20010 [10:06:20<6:01:57,  3.10s/batch]Batch 12900/20010 Done, mean position loss: 23.428320901393892\n",
      "Training growing_up:  64%|█████   | 12762/20010 [10:06:27<6:12:34,  3.08s/batch]Batch 12800/20010 Done, mean position loss: 22.8580859208107\n",
      "Training growing_up:  64%|█████   | 12792/20010 [10:06:32<6:30:27,  3.25s/batch]Batch 12700/20010 Done, mean position loss: 22.964948103427886\n",
      "Training growing_up:  63%|█████   | 12589/20010 [10:06:33<6:57:44,  3.38s/batch]Batch 12800/20010 Done, mean position loss: 23.031515374183655\n",
      "Training growing_up:  64%|█████   | 12805/20010 [10:06:45<6:03:50,  3.03s/batch]Batch 12900/20010 Done, mean position loss: 23.05571694135666\n",
      "Training growing_up:  64%|█████   | 12814/20010 [10:06:52<6:07:04,  3.06s/batch]Batch 12500/20010 Done, mean position loss: 22.572149369716644\n",
      "Training growing_up:  64%|█████▏  | 12906/20010 [10:06:59<5:57:36,  3.02s/batch]Batch 12800/20010 Done, mean position loss: 23.545947959423064\n",
      "Training growing_up:  63%|█████   | 12507/20010 [10:07:09<6:17:13,  3.02s/batch]Batch 12600/20010 Done, mean position loss: 22.329563636779785\n",
      "Training growing_up:  64%|█████   | 12775/20010 [10:07:13<6:29:23,  3.23s/batch]Batch 12700/20010 Done, mean position loss: 22.836794750690462\n",
      "Training growing_up:  64%|█████   | 12717/20010 [10:07:22<6:36:15,  3.26s/batch]Batch 12600/20010 Done, mean position loss: 23.74568831205368\n",
      "Training growing_up:  65%|█████▏  | 12929/20010 [10:07:27<6:11:14,  3.15s/batch]Batch 12800/20010 Done, mean position loss: 23.65643840551376\n",
      "Training growing_up:  64%|█████▏  | 12865/20010 [10:08:04<5:10:27,  2.61s/batch]Batch 12700/20010 Done, mean position loss: 22.728752610683443\n",
      "Training growing_up:  64%|█████▏  | 12864/20010 [10:08:05<6:14:14,  3.14s/batch]Batch 12900/20010 Done, mean position loss: 22.531335072517393\n",
      "Training growing_up:  64%|█████▏  | 12869/20010 [10:08:14<5:19:44,  2.69s/batch]Batch 12600/20010 Done, mean position loss: 22.292359790802003\n",
      "Training growing_up:  63%|█████   | 12623/20010 [10:08:15<6:13:45,  3.04s/batch]Batch 12600/20010 Done, mean position loss: 23.505861763954165\n",
      "Training growing_up:  65%|█████▏  | 12950/20010 [10:08:26<5:41:45,  2.90s/batch]Batch 12800/20010 Done, mean position loss: 22.70445879220963\n",
      "Batch 12800/20010 Done, mean position loss: 23.23890726327896\n",
      "Training growing_up:  63%|█████   | 12610/20010 [10:08:42<6:48:45,  3.31s/batch]Batch 12800/20010 Done, mean position loss: 22.87776286840439\n",
      "Training growing_up:  64%|█████▏  | 12828/20010 [10:08:45<6:18:51,  3.17s/batch]Batch 12900/20010 Done, mean position loss: 22.448474485874176\n",
      "Training growing_up:  64%|█████   | 12765/20010 [10:08:50<6:29:37,  3.23s/batch]Batch 12900/20010 Done, mean position loss: 22.01393565416336\n",
      "Training growing_up:  64%|█████▏  | 12866/20010 [10:09:47<6:46:35,  3.41s/batch]Batch 12800/20010 Done, mean position loss: 23.77105886697769\n",
      "Training growing_up:  65%|█████▏  | 12977/20010 [10:09:47<5:55:16,  3.03s/batch]Batch 13000/20010 Done, mean position loss: 22.865886905193328\n",
      "Training growing_up:  64%|█████▏  | 12871/20010 [10:09:49<6:04:10,  3.06s/batch]Batch 12700/20010 Done, mean position loss: 22.998482410907748\n",
      "Training growing_up:  64%|█████▏  | 12860/20010 [10:09:53<6:02:19,  3.04s/batch]Batch 12900/20010 Done, mean position loss: 22.91763214111328\n",
      "Training growing_up:  64%|█████▏  | 12882/20010 [10:09:57<5:39:35,  2.86s/batch]Batch 12900/20010 Done, mean position loss: 23.287386798858645\n",
      "Training growing_up:  64%|█████▏  | 12828/20010 [10:10:00<5:45:31,  2.89s/batch]Batch 12600/20010 Done, mean position loss: 22.767699639797208\n",
      "Training growing_up:  63%|█████   | 12639/20010 [10:10:07<6:01:12,  2.94s/batch]Batch 12900/20010 Done, mean position loss: 22.691677658557893\n",
      "Training growing_up:  64%|█████   | 12761/20010 [10:10:09<6:05:57,  3.03s/batch]Batch 12800/20010 Done, mean position loss: 23.51783259153366\n",
      "Training growing_up:  64%|█████   | 12785/20010 [10:10:12<5:27:55,  2.72s/batch]Batch 12900/20010 Done, mean position loss: 23.64756607055664\n",
      "Training growing_up:  64%|█████▏  | 12868/20010 [10:10:16<5:58:33,  3.01s/batch]Batch 12800/20010 Done, mean position loss: 23.351237962245943\n",
      "Training growing_up:  64%|█████▏  | 12845/20010 [10:10:34<5:19:54,  2.68s/batch]Batch 12800/20010 Done, mean position loss: 23.288240208625794\n",
      "Training growing_up:  64%|█████   | 12783/20010 [10:10:40<6:48:43,  3.39s/batch]Batch 12800/20010 Done, mean position loss: 23.293156883716584\n",
      "Training growing_up:  65%|█████▏  | 12957/20010 [10:10:52<5:47:47,  2.96s/batch]Batch 12900/20010 Done, mean position loss: 23.418691406249998\n",
      "Training growing_up:  63%|█████   | 12676/20010 [10:10:54<5:43:45,  2.81s/batch]Batch 13000/20010 Done, mean position loss: 22.467700085639954\n",
      "Training growing_up:  64%|█████▏  | 12852/20010 [10:10:55<6:02:54,  3.04s/batch]Batch 13100/20010 Done, mean position loss: 22.87343585252762\n",
      "Training growing_up:  64%|█████▏  | 12853/20010 [10:10:57<5:35:42,  2.81s/batch]Batch 12900/20010 Done, mean position loss: 22.708901739120485\n",
      "Training growing_up:  65%|█████▏  | 13026/20010 [10:10:59<5:28:51,  2.83s/batch]Batch 12800/20010 Done, mean position loss: 22.669633636474607\n",
      "Training growing_up:  64%|█████▏  | 12903/20010 [10:11:04<5:59:08,  3.03s/batch]Batch 12800/20010 Done, mean position loss: 23.945688302516935\n",
      "Training growing_up:  64%|█████   | 12760/20010 [10:11:05<5:55:02,  2.94s/batch]Batch 12900/20010 Done, mean position loss: 22.738445994853972\n",
      "Training growing_up:  65%|█████▏  | 12921/20010 [10:11:11<6:01:22,  3.06s/batch]Batch 12900/20010 Done, mean position loss: 22.441042904853823\n",
      "Training growing_up:  63%|█████   | 12661/20010 [10:11:16<6:02:48,  2.96s/batch]Batch 13000/20010 Done, mean position loss: 23.436124250888824\n",
      "Training growing_up:  64%|█████▏  | 12896/20010 [10:11:16<5:57:13,  3.01s/batch]Batch 12900/20010 Done, mean position loss: 22.929886913299562\n",
      "Training growing_up:  64%|█████▏  | 12827/20010 [10:11:30<6:09:38,  3.09s/batch]Batch 12900/20010 Done, mean position loss: 23.126163890361788\n",
      "Training growing_up:  63%|█████   | 12595/20010 [10:11:32<6:37:35,  3.22s/batch]Batch 13000/20010 Done, mean position loss: 22.639743835926055\n",
      "Training growing_up:  64%|█████   | 12789/20010 [10:11:32<6:33:21,  3.27s/batch]Batch 12800/20010 Done, mean position loss: 23.097937471866608\n",
      "Training growing_up:  65%|█████▏  | 12916/20010 [10:11:50<5:56:13,  3.01s/batch]Batch 12600/20010 Done, mean position loss: 22.38094403028488\n",
      "Training growing_up:  63%|█████   | 12638/20010 [10:11:52<6:36:54,  3.23s/batch]Batch 12900/20010 Done, mean position loss: 23.712217977046965\n",
      "Training growing_up:  64%|█████▏  | 12870/20010 [10:12:11<6:24:27,  3.23s/batch]Batch 12800/20010 Done, mean position loss: 23.380171101093293\n",
      "Training growing_up:  64%|█████▏  | 12834/20010 [10:12:12<5:59:22,  3.00s/batch]Batch 12700/20010 Done, mean position loss: 22.651676647663116\n",
      "Training growing_up:  65%|█████▏  | 12926/20010 [10:12:19<5:29:01,  2.79s/batch]Batch 12900/20010 Done, mean position loss: 23.849883365631104\n",
      "Training growing_up:  65%|█████▏  | 13017/20010 [10:12:19<6:04:16,  3.13s/batch]Batch 12700/20010 Done, mean position loss: 23.553221423625946\n",
      "Training growing_up:  64%|█████▏  | 12857/20010 [10:13:03<5:24:22,  2.72s/batch]Batch 13000/20010 Done, mean position loss: 21.98936707019806\n",
      "Training growing_up:  64%|█████▏  | 12846/20010 [10:13:16<6:15:42,  3.15s/batch]Batch 12800/20010 Done, mean position loss: 22.444033439159394\n",
      "Training growing_up:  64%|█████▏  | 12893/20010 [10:13:18<5:43:09,  2.89s/batch]Batch 12700/20010 Done, mean position loss: 23.243419995307924\n",
      "Training growing_up:  65%|█████▏  | 12921/20010 [10:13:18<6:03:27,  3.08s/batch]Batch 12700/20010 Done, mean position loss: 21.95281877040863\n",
      "Training growing_up:  64%|█████   | 12722/20010 [10:13:24<6:20:09,  3.13s/batch]Batch 12900/20010 Done, mean position loss: 23.709735848903655\n",
      "Training growing_up:  65%|█████▏  | 12943/20010 [10:13:33<5:46:14,  2.94s/batch]Batch 12900/20010 Done, mean position loss: 22.49212733030319\n",
      "Training growing_up:  64%|█████   | 12708/20010 [10:13:38<5:41:29,  2.81s/batch]Batch 13000/20010 Done, mean position loss: 22.42084058523178\n",
      "Training growing_up:  64%|█████▏  | 12846/20010 [10:13:45<6:09:13,  3.09s/batch]Batch 12900/20010 Done, mean position loss: 23.14142128944397\n",
      "Training growing_up:  65%|█████▏  | 12974/20010 [10:13:52<5:47:09,  2.96s/batch]Batch 13000/20010 Done, mean position loss: 22.5968253493309\n",
      "Training growing_up:  65%|█████▏  | 12989/20010 [10:14:38<6:16:00,  3.21s/batch]Batch 13100/20010 Done, mean position loss: 23.117495377063754\n",
      "Training growing_up:  64%|█████▏  | 12900/20010 [10:14:46<5:47:09,  2.93s/batch]Batch 13000/20010 Done, mean position loss: 23.19724898815155\n",
      "Training growing_up:  65%|█████▏  | 12978/20010 [10:14:49<5:45:19,  2.95s/batch]Batch 12900/20010 Done, mean position loss: 23.556076729297637\n",
      "Training growing_up:  65%|█████▏  | 12953/20010 [10:14:52<5:43:54,  2.92s/batch]Batch 13000/20010 Done, mean position loss: 23.485192744731904\n",
      "Training growing_up:  65%|█████▏  | 12977/20010 [10:15:00<5:55:46,  3.04s/batch]Batch 12800/20010 Done, mean position loss: 23.18172872543335\n",
      "Training growing_up:  64%|█████   | 12756/20010 [10:15:07<5:23:04,  2.67s/batch]Batch 12700/20010 Done, mean position loss: 22.87133760213852\n",
      "Training growing_up:  65%|█████▏  | 13010/20010 [10:15:11<5:42:25,  2.94s/batch]Batch 13000/20010 Done, mean position loss: 22.42890504837036\n",
      "Training growing_up:  65%|█████▏  | 12982/20010 [10:15:13<5:24:50,  2.77s/batch]Batch 12900/20010 Done, mean position loss: 23.21914011478424\n",
      "Training growing_up:  64%|█████   | 12742/20010 [10:15:18<5:25:49,  2.69s/batch]Batch 13000/20010 Done, mean position loss: 23.21519757270813\n",
      "Training growing_up:  65%|█████▏  | 12984/20010 [10:15:19<5:17:18,  2.71s/batch]Batch 12900/20010 Done, mean position loss: 23.587663857936857\n",
      "Training growing_up:  65%|█████▏  | 12937/20010 [10:15:33<5:39:31,  2.88s/batch]Batch 12900/20010 Done, mean position loss: 23.383899121284486\n",
      "Training growing_up:  64%|█████   | 12772/20010 [10:15:47<6:24:53,  3.19s/batch]Batch 13000/20010 Done, mean position loss: 23.44077249765396\n",
      "Training growing_up:  65%|█████▏  | 13012/20010 [10:15:48<5:07:58,  2.64s/batch]Batch 12900/20010 Done, mean position loss: 23.400052983760833\n",
      "Training growing_up:  65%|█████▏  | 12971/20010 [10:15:48<5:57:49,  3.05s/batch]Batch 13200/20010 Done, mean position loss: 22.463512861728667\n",
      "Training growing_up:  64%|█████   | 12771/20010 [10:15:51<5:43:36,  2.85s/batch]Batch 13100/20010 Done, mean position loss: 22.386631603240964\n",
      "Training growing_up:  64%|█████▏  | 12877/20010 [10:15:57<6:25:42,  3.24s/batch]Batch 13000/20010 Done, mean position loss: 22.628731093406678\n",
      "Training growing_up:  65%|█████▏  | 13005/20010 [10:15:58<5:54:23,  3.04s/batch]Batch 13000/20010 Done, mean position loss: 22.778071320056917\n",
      "Training growing_up:  65%|█████▏  | 12945/20010 [10:15:58<6:05:18,  3.10s/batch]Batch 12900/20010 Done, mean position loss: 22.655240478515623\n",
      "Training growing_up:  64%|█████   | 12755/20010 [10:16:00<5:49:48,  2.89s/batch]Batch 12900/20010 Done, mean position loss: 23.737981057167055\n",
      "Training growing_up:  65%|█████▏  | 13047/20010 [10:16:07<6:04:52,  3.14s/batch]Batch 13000/20010 Done, mean position loss: 22.76443935394287\n",
      "Training growing_up:  65%|█████▏  | 12978/20010 [10:16:07<5:21:10,  2.74s/batch]Batch 13000/20010 Done, mean position loss: 23.103139426708225\n",
      "Training growing_up:  65%|█████▏  | 12920/20010 [10:16:09<6:20:42,  3.22s/batch]Batch 13100/20010 Done, mean position loss: 23.320740642547605\n",
      "Training growing_up:  65%|█████▏  | 13099/20010 [10:16:23<5:39:32,  2.95s/batch]Batch 12900/20010 Done, mean position loss: 23.043989403247835\n",
      "Training growing_up:  65%|█████▏  | 12957/20010 [10:16:23<5:28:30,  2.79s/batch]Batch 13000/20010 Done, mean position loss: 23.11899838209152\n",
      "Training growing_up:  64%|█████▏  | 12830/20010 [10:16:29<5:33:37,  2.79s/batch]Batch 13100/20010 Done, mean position loss: 22.678183207511903\n",
      "Training growing_up:  65%|█████▏  | 12916/20010 [10:16:44<6:03:38,  3.08s/batch]Batch 13000/20010 Done, mean position loss: 23.543470280170443\n",
      "Training growing_up:  65%|█████▏  | 12993/20010 [10:16:51<5:57:36,  3.06s/batch]Batch 12700/20010 Done, mean position loss: 22.480837864875795\n",
      "Training growing_up:  65%|█████▏  | 12924/20010 [10:17:08<6:07:29,  3.11s/batch]Batch 12900/20010 Done, mean position loss: 23.295055685043334\n",
      "Training growing_up:  65%|█████▏  | 12926/20010 [10:17:10<5:37:55,  2.86s/batch]Batch 12800/20010 Done, mean position loss: 22.886396648883817\n",
      "Training growing_up:  65%|█████▏  | 12938/20010 [10:17:14<6:26:44,  3.28s/batch]Batch 13000/20010 Done, mean position loss: 24.129446625709534\n",
      "Training growing_up:  65%|█████▏  | 12954/20010 [10:17:22<5:50:29,  2.98s/batch]Batch 12800/20010 Done, mean position loss: 23.706503238677975\n",
      "Training growing_up:  65%|█████▏  | 12952/20010 [10:17:59<5:40:45,  2.90s/batch]Batch 13100/20010 Done, mean position loss: 22.593079307079314\n",
      "Training growing_up:  64%|█████   | 12817/20010 [10:18:09<5:43:12,  2.86s/batch]Batch 12900/20010 Done, mean position loss: 22.56188798904419\n",
      "Training growing_up:  65%|█████▏  | 12947/20010 [10:18:13<4:54:01,  2.50s/batch]Batch 13000/20010 Done, mean position loss: 23.13933569431305\n",
      "Training growing_up:  65%|█████▏  | 12924/20010 [10:18:16<5:30:11,  2.80s/batch]Batch 12800/20010 Done, mean position loss: 21.925500092506407\n",
      "Training growing_up:  65%|█████▏  | 12990/20010 [10:18:17<5:49:52,  2.99s/batch]Batch 12800/20010 Done, mean position loss: 22.987863588333127\n",
      "Training growing_up:  65%|█████▏  | 12954/20010 [10:18:28<4:27:30,  2.27s/batch]Batch 13000/20010 Done, mean position loss: 22.71047719240189\n",
      "Training growing_up:  65%|█████▏  | 13052/20010 [10:18:30<5:37:28,  2.91s/batch]Batch 13100/20010 Done, mean position loss: 22.2581006526947\n",
      "Training growing_up:  66%|█████▎  | 13188/20010 [10:18:43<4:37:27,  2.44s/batch]Batch 13100/20010 Done, mean position loss: 22.153662848472592\n",
      "Training growing_up:  65%|█████▏  | 13059/20010 [10:18:47<5:15:05,  2.72s/batch]Batch 13000/20010 Done, mean position loss: 23.08255136489868\n",
      "Training growing_up:  65%|█████▏  | 12928/20010 [10:19:17<4:33:50,  2.32s/batch]Batch 13200/20010 Done, mean position loss: 23.096372528076174\n",
      "Training growing_up:  66%|█████▏  | 13123/20010 [10:19:28<4:50:01,  2.53s/batch]Batch 13000/20010 Done, mean position loss: 23.50454291343689\n",
      "Training growing_up:  65%|█████▏  | 12985/20010 [10:19:31<5:08:26,  2.63s/batch]Batch 13100/20010 Done, mean position loss: 22.695602357387543\n",
      "Training growing_up:  64%|█████▏  | 12853/20010 [10:19:35<5:25:19,  2.73s/batch]Batch 13100/20010 Done, mean position loss: 23.392453880310057\n",
      "Training growing_up:  66%|█████▎  | 13188/20010 [10:19:52<5:46:23,  3.05s/batch]Batch 13100/20010 Done, mean position loss: 22.503123123645786\n",
      "Training growing_up:  65%|█████▏  | 12992/20010 [10:19:54<6:19:59,  3.25s/batch]Batch 12900/20010 Done, mean position loss: 22.823097732067108\n",
      "Training growing_up:  66%|█████▏  | 13129/20010 [10:19:56<6:00:43,  3.15s/batch]Batch 12800/20010 Done, mean position loss: 22.60192229747772\n",
      "Training growing_up:  65%|█████▏  | 13083/20010 [10:20:03<5:30:15,  2.86s/batch]Batch 13100/20010 Done, mean position loss: 23.20337472677231\n",
      "Batch 13000/20010 Done, mean position loss: 23.27847845554352\n",
      "Training growing_up:  64%|█████▏  | 12862/20010 [10:20:16<5:54:55,  2.98s/batch]Batch 13000/20010 Done, mean position loss: 23.50342291355133\n",
      "Training growing_up:  64%|█████   | 12777/20010 [10:20:20<5:30:11,  2.74s/batch]Batch 13000/20010 Done, mean position loss: 23.263849096298216\n",
      "Training growing_up:  65%|█████▏  | 12952/20010 [10:20:22<5:20:24,  2.72s/batch]Batch 13300/20010 Done, mean position loss: 22.637684314250944\n",
      "Training growing_up:  66%|█████▏  | 13109/20010 [10:20:25<5:10:09,  2.70s/batch]Batch 13100/20010 Done, mean position loss: 23.27949761390686\n",
      "Training growing_up:  65%|█████▏  | 12913/20010 [10:20:27<5:20:17,  2.71s/batch]Batch 13200/20010 Done, mean position loss: 22.601739485263828\n",
      "Training growing_up:  66%|█████▏  | 13116/20010 [10:20:35<5:33:06,  2.90s/batch]Batch 13000/20010 Done, mean position loss: 22.689790513515472\n",
      "Batch 13000/20010 Done, mean position loss: 23.20830995321274\n",
      "Training growing_up:  66%|█████▏  | 13124/20010 [10:20:38<5:10:01,  2.70s/batch]Batch 13000/20010 Done, mean position loss: 24.119872624874112\n",
      "Training growing_up:  64%|█████▏  | 12871/20010 [10:20:40<5:11:29,  2.62s/batch]Batch 13100/20010 Done, mean position loss: 22.78141374349594\n",
      "Training growing_up:  65%|█████▏  | 13028/20010 [10:20:42<5:12:58,  2.69s/batch]Batch 13100/20010 Done, mean position loss: 23.173343489170072\n",
      "Training growing_up:  65%|█████▏  | 13078/20010 [10:20:46<5:17:59,  2.75s/batch]Batch 13100/20010 Done, mean position loss: 22.82290115594864\n",
      "Training growing_up:  67%|█████▎  | 13313/20010 [10:20:52<4:49:47,  2.60s/batch]Batch 13200/20010 Done, mean position loss: 23.439341242313382\n",
      "Training growing_up:  65%|█████▏  | 13095/20010 [10:20:56<5:02:45,  2.63s/batch]Batch 13100/20010 Done, mean position loss: 22.546678833961487\n",
      "Training growing_up:  65%|█████▏  | 13058/20010 [10:21:11<5:26:34,  2.82s/batch]Batch 13200/20010 Done, mean position loss: 22.5603781414032\n",
      "Training growing_up:  64%|█████▏  | 12862/20010 [10:21:11<6:28:55,  3.26s/batch]Batch 13000/20010 Done, mean position loss: 22.95752622127533\n",
      "Training growing_up:  65%|█████▏  | 13013/20010 [10:21:14<6:08:03,  3.16s/batch]Batch 13100/20010 Done, mean position loss: 23.460112235546113\n",
      "Training growing_up:  65%|█████▏  | 13025/20010 [10:21:26<5:15:03,  2.71s/batch]Batch 13100/20010 Done, mean position loss: 23.41495418548584\n",
      "Training growing_up:  66%|█████▏  | 13107/20010 [10:21:30<5:11:39,  2.71s/batch]Batch 12800/20010 Done, mean position loss: 22.156766221523284\n",
      "Training growing_up:  66%|█████▎  | 13186/20010 [10:21:51<5:05:04,  2.68s/batch]Batch 13000/20010 Done, mean position loss: 23.09302572488785\n",
      "Training growing_up:  65%|█████▏  | 13027/20010 [10:21:53<5:44:21,  2.96s/batch]Batch 13100/20010 Done, mean position loss: 23.656468148231504\n",
      "Training growing_up:  65%|█████▏  | 13079/20010 [10:21:55<5:34:09,  2.89s/batch]Batch 12900/20010 Done, mean position loss: 22.245238933563236\n",
      "Training growing_up:  65%|█████▏  | 13078/20010 [10:22:09<5:44:32,  2.98s/batch]Batch 12900/20010 Done, mean position loss: 23.561624760627748\n",
      "Training growing_up:  66%|█████▎  | 13165/20010 [10:22:32<5:16:05,  2.77s/batch]Batch 13200/20010 Done, mean position loss: 22.324874131679536\n",
      "Training growing_up:  65%|█████▏  | 13054/20010 [10:22:48<5:28:10,  2.83s/batch]Batch 13000/20010 Done, mean position loss: 22.518766345977784\n",
      "Training growing_up:  65%|█████▏  | 12924/20010 [10:23:00<5:03:44,  2.57s/batch]Batch 13100/20010 Done, mean position loss: 22.97657529115677\n",
      "Training growing_up:  66%|█████▎  | 13199/20010 [10:23:01<5:32:41,  2.93s/batch]Batch 12900/20010 Done, mean position loss: 21.780568323135377\n",
      "Training growing_up:  66%|█████▎  | 13151/20010 [10:23:07<5:47:03,  3.04s/batch]Batch 13200/20010 Done, mean position loss: 22.09505171060562\n",
      "Training growing_up:  65%|█████▏  | 13065/20010 [10:23:08<5:34:05,  2.89s/batch]Batch 12900/20010 Done, mean position loss: 23.17826416015625\n",
      "Training growing_up:  64%|█████▏  | 12870/20010 [10:23:16<5:50:08,  2.94s/batch]Batch 13100/20010 Done, mean position loss: 22.555598108768464\n",
      "Training growing_up:  65%|█████▏  | 13045/20010 [10:23:21<5:42:43,  2.95s/batch]Batch 13200/20010 Done, mean position loss: 22.057711522579194\n",
      "Training growing_up:  65%|█████▏  | 13067/20010 [10:23:32<5:31:01,  2.86s/batch]Batch 13100/20010 Done, mean position loss: 22.775773956775666\n",
      "Training growing_up:  65%|█████▏  | 13070/20010 [10:24:01<6:22:58,  3.31s/batch]Batch 13300/20010 Done, mean position loss: 22.783271083831785\n",
      "Training growing_up:  66%|█████▎  | 13236/20010 [10:24:14<5:41:18,  3.02s/batch]Batch 13200/20010 Done, mean position loss: 23.47320471048355\n",
      "Training growing_up:  66%|█████▎  | 13220/20010 [10:24:14<5:25:29,  2.88s/batch]Batch 13100/20010 Done, mean position loss: 23.265096356868742\n",
      "Training growing_up:  67%|█████▎  | 13385/20010 [10:24:15<5:03:33,  2.75s/batch]Batch 13200/20010 Done, mean position loss: 22.759130008220673\n",
      "Training growing_up:  66%|█████▎  | 13210/20010 [10:24:38<5:12:39,  2.76s/batch]Batch 13200/20010 Done, mean position loss: 22.911048250198363\n",
      "Training growing_up:  66%|█████▎  | 13248/20010 [10:24:50<6:04:17,  3.23s/batch]Batch 13100/20010 Done, mean position loss: 22.909150004386902\n",
      "Training growing_up:  66%|█████▎  | 13183/20010 [10:24:56<5:44:16,  3.03s/batch]Batch 12900/20010 Done, mean position loss: 23.231234998703002\n",
      "Training growing_up:  65%|█████▏  | 13103/20010 [10:24:55<5:53:32,  3.07s/batch]Batch 13200/20010 Done, mean position loss: 23.04017617702484\n",
      "Training growing_up:  66%|█████▎  | 13195/20010 [10:25:00<6:01:31,  3.18s/batch]Batch 13000/20010 Done, mean position loss: 22.921280102729796\n",
      "Training growing_up:  65%|█████▏  | 13067/20010 [10:25:04<5:41:06,  2.95s/batch]Batch 13400/20010 Done, mean position loss: 22.295307681560516\n",
      "Training growing_up:  66%|█████▎  | 13219/20010 [10:25:07<5:43:19,  3.03s/batch]Batch 13100/20010 Done, mean position loss: 23.449240953922274\n",
      "Training growing_up:  65%|█████▏  | 13082/20010 [10:25:11<4:54:19,  2.55s/batch]Batch 13300/20010 Done, mean position loss: 22.494926884174347\n",
      "Training growing_up:  66%|█████▎  | 13188/20010 [10:25:13<6:09:39,  3.25s/batch]Batch 13100/20010 Done, mean position loss: 23.11621954917908\n",
      "Training growing_up:  66%|█████▎  | 13303/20010 [10:25:17<5:31:48,  2.97s/batch]Batch 13200/20010 Done, mean position loss: 22.93114266395569\n",
      "Training growing_up:  65%|█████▏  | 13054/20010 [10:25:31<6:27:46,  3.34s/batch]Batch 13100/20010 Done, mean position loss: 22.769784741401672\n",
      "Training growing_up:  65%|█████▏  | 12913/20010 [10:25:31<6:04:38,  3.08s/batch]Batch 13200/20010 Done, mean position loss: 22.822915575504304\n",
      "Training growing_up:  65%|█████▏  | 12915/20010 [10:25:37<5:53:47,  2.99s/batch]Batch 13200/20010 Done, mean position loss: 22.613407306671142\n",
      "Training growing_up:  66%|█████▎  | 13250/20010 [10:25:38<5:38:01,  3.00s/batch]Batch 13100/20010 Done, mean position loss: 23.219207479953766\n",
      "Training growing_up:  65%|█████▏  | 13104/20010 [10:25:40<5:36:42,  2.93s/batch]Batch 13300/20010 Done, mean position loss: 23.354366321563724\n",
      "Training growing_up:  66%|█████▏  | 13110/20010 [10:25:41<6:30:49,  3.40s/batch]Batch 13200/20010 Done, mean position loss: 22.695552506446838\n",
      "Training growing_up:  66%|█████▎  | 13144/20010 [10:25:41<6:00:04,  3.15s/batch]Batch 13100/20010 Done, mean position loss: 24.11094457387924\n",
      "Training growing_up:  67%|█████▎  | 13415/20010 [10:25:52<6:14:21,  3.41s/batch]Batch 13200/20010 Done, mean position loss: 22.601210672855377\n",
      "Training growing_up:  66%|█████▎  | 13163/20010 [10:26:08<5:45:11,  3.02s/batch]Batch 13100/20010 Done, mean position loss: 23.408519818782807\n",
      "Training growing_up:  65%|█████▏  | 12984/20010 [10:26:10<7:02:45,  3.61s/batch]Batch 13300/20010 Done, mean position loss: 22.447007710933686\n",
      "Training growing_up:  66%|█████▎  | 13302/20010 [10:26:13<5:33:56,  2.99s/batch]Batch 13200/20010 Done, mean position loss: 23.05218367099762\n",
      "Training growing_up:  66%|█████▎  | 13278/20010 [10:26:18<5:20:53,  2.86s/batch]Batch 13200/20010 Done, mean position loss: 23.33394870996475\n",
      "Training growing_up:  65%|█████▏  | 13072/20010 [10:26:32<6:44:21,  3.50s/batch]Batch 12900/20010 Done, mean position loss: 22.14182482481003\n",
      "Training growing_up:  66%|█████▏  | 13112/20010 [10:26:46<6:40:32,  3.48s/batch]Batch 13100/20010 Done, mean position loss: 23.363855955600737\n",
      "Training growing_up:  65%|█████▏  | 13103/20010 [10:26:51<5:23:56,  2.81s/batch]Batch 13200/20010 Done, mean position loss: 23.924978053569795\n",
      "Training growing_up:  65%|█████▏  | 13083/20010 [10:27:05<6:03:06,  3.15s/batch]Batch 13000/20010 Done, mean position loss: 23.58260118484497\n",
      "Batch 13000/20010 Done, mean position loss: 22.522200038433077\n",
      "Training growing_up:  65%|█████▏  | 12987/20010 [10:27:30<5:51:20,  3.00s/batch]Batch 13300/20010 Done, mean position loss: 22.05464693069458\n",
      "Training growing_up:  66%|█████▎  | 13297/20010 [10:28:01<5:06:16,  2.74s/batch]Batch 13100/20010 Done, mean position loss: 22.30311871290207\n",
      "Training growing_up:  66%|█████▎  | 13255/20010 [10:28:01<5:07:37,  2.73s/batch]Batch 13200/20010 Done, mean position loss: 23.042334127426148\n",
      "Training growing_up:  65%|█████▏  | 13022/20010 [10:28:08<5:34:17,  2.87s/batch]Batch 13000/20010 Done, mean position loss: 21.984286539554596\n",
      "Training growing_up:  66%|█████▎  | 13273/20010 [10:28:14<6:07:30,  3.27s/batch]Batch 13300/20010 Done, mean position loss: 22.24700408220291\n",
      "Training growing_up:  66%|█████▎  | 13282/20010 [10:28:17<6:19:26,  3.38s/batch]Batch 13300/20010 Done, mean position loss: 21.82483951807022\n",
      "Training growing_up:  66%|█████▎  | 13230/20010 [10:28:19<6:10:58,  3.28s/batch]Batch 13200/20010 Done, mean position loss: 22.215464375019074\n",
      "Training growing_up:  67%|█████▎  | 13316/20010 [10:28:20<5:51:19,  3.15s/batch]Batch 13000/20010 Done, mean position loss: 23.400703389644622\n",
      "Training growing_up:  66%|█████▎  | 13215/20010 [10:28:40<5:17:40,  2.81s/batch]Batch 13200/20010 Done, mean position loss: 22.939775731563568\n",
      "Training growing_up:  66%|█████▎  | 13222/20010 [10:29:03<6:14:50,  3.31s/batch]Batch 13400/20010 Done, mean position loss: 23.324416704177857\n",
      "Training growing_up:  66%|█████▎  | 13291/20010 [10:29:14<6:51:02,  3.67s/batch]Batch 13300/20010 Done, mean position loss: 22.82493902683258\n",
      "Training growing_up:  66%|█████▎  | 13279/20010 [10:29:15<5:35:30,  2.99s/batch]Batch 13300/20010 Done, mean position loss: 23.643852901458743\n",
      "Training growing_up:  66%|█████▎  | 13270/20010 [10:29:21<5:45:53,  3.08s/batch]Batch 13200/20010 Done, mean position loss: 23.180014123916624\n",
      "Training growing_up:  65%|█████▏  | 12997/20010 [10:29:45<5:39:26,  2.90s/batch]Batch 13300/20010 Done, mean position loss: 22.480720496177675\n",
      "Training growing_up:  67%|█████▎  | 13420/20010 [10:29:56<5:04:38,  2.77s/batch]Batch 13200/20010 Done, mean position loss: 23.137167525291442\n",
      "Training growing_up:  66%|█████▎  | 13282/20010 [10:29:57<5:24:48,  2.90s/batch]Batch 13000/20010 Done, mean position loss: 22.895058498382568\n",
      "Training growing_up:  67%|█████▍  | 13499/20010 [10:29:57<5:43:28,  3.17s/batch]Batch 13300/20010 Done, mean position loss: 23.084438097476962\n",
      "Training growing_up:  67%|█████▎  | 13307/20010 [10:30:03<5:34:22,  2.99s/batch]Batch 13500/20010 Done, mean position loss: 22.436193528175355\n",
      "Training growing_up:  67%|█████▎  | 13320/20010 [10:30:08<5:13:56,  2.82s/batch]Batch 13100/20010 Done, mean position loss: 22.80298921585083\n",
      "Training growing_up:  66%|█████▎  | 13269/20010 [10:30:13<4:47:44,  2.56s/batch]Batch 13400/20010 Done, mean position loss: 22.350120208263398\n",
      "Training growing_up:  66%|█████▎  | 13180/20010 [10:30:15<6:02:56,  3.19s/batch]Batch 13200/20010 Done, mean position loss: 23.641459605693818\n",
      "Training growing_up:  66%|█████▎  | 13239/20010 [10:30:17<6:07:01,  3.25s/batch]Batch 13300/20010 Done, mean position loss: 23.008879938125613\n",
      "Training growing_up:  65%|█████▏  | 13041/20010 [10:30:21<5:54:05,  3.05s/batch]Batch 13200/20010 Done, mean position loss: 23.32284692764282\n",
      "Training growing_up:  66%|█████▎  | 13203/20010 [10:30:28<6:00:25,  3.18s/batch]Batch 13300/20010 Done, mean position loss: 22.915096788406373\n",
      "Training growing_up:  65%|█████▏  | 13068/20010 [10:30:29<5:42:05,  2.96s/batch]Batch 13200/20010 Done, mean position loss: 22.32960646867752\n",
      "Training growing_up:  66%|█████▎  | 13286/20010 [10:30:40<5:16:57,  2.83s/batch]Batch 13300/20010 Done, mean position loss: 22.938823463916776\n",
      "Training growing_up:  66%|█████▎  | 13248/20010 [10:30:43<6:04:02,  3.23s/batch]Batch 13400/20010 Done, mean position loss: 23.227008244991303\n",
      "Training growing_up:  65%|█████▏  | 13017/20010 [10:30:45<5:48:55,  2.99s/batch]Batch 13300/20010 Done, mean position loss: 22.40304532766342\n",
      "Training growing_up:  67%|█████▎  | 13333/20010 [10:30:48<5:02:37,  2.72s/batch]Batch 13200/20010 Done, mean position loss: 23.328498752117156\n",
      "Training growing_up:  66%|█████▎  | 13215/20010 [10:30:54<5:25:18,  2.87s/batch]Batch 13300/20010 Done, mean position loss: 22.3772462105751\n",
      "Training growing_up:  66%|█████▎  | 13210/20010 [10:30:55<5:45:55,  3.05s/batch]Batch 13200/20010 Done, mean position loss: 24.392490177154542\n",
      "Training growing_up:  66%|█████▎  | 13161/20010 [10:31:10<6:22:34,  3.35s/batch]Batch 13400/20010 Done, mean position loss: 23.00314587831497\n",
      "Training growing_up:  67%|█████▎  | 13422/20010 [10:31:16<5:26:26,  2.97s/batch]Batch 13300/20010 Done, mean position loss: 23.003043320178985\n",
      "Training growing_up:  66%|█████▎  | 13290/20010 [10:31:19<6:12:51,  3.33s/batch]Batch 13200/20010 Done, mean position loss: 23.046829915046693\n",
      "Training growing_up:  65%|█████▏  | 13086/20010 [10:31:23<5:37:13,  2.92s/batch]Batch 13300/20010 Done, mean position loss: 23.213315179347994\n",
      "Training growing_up:  67%|█████▎  | 13334/20010 [10:31:35<5:37:35,  3.03s/batch]Batch 13000/20010 Done, mean position loss: 21.98286871671677\n",
      "Training growing_up:  65%|█████▏  | 13094/20010 [10:31:49<6:13:16,  3.24s/batch]Batch 13200/20010 Done, mean position loss: 22.786815276145937\n",
      "Training growing_up:  66%|█████▎  | 13250/20010 [10:31:53<5:38:14,  3.00s/batch]Batch 13300/20010 Done, mean position loss: 23.595039324760435\n",
      "Training growing_up:  66%|█████▎  | 13208/20010 [10:32:09<5:11:47,  2.75s/batch]Batch 13100/20010 Done, mean position loss: 22.81059961080551\n",
      "Training growing_up:  67%|█████▎  | 13346/20010 [10:32:12<5:34:20,  3.01s/batch]Batch 13100/20010 Done, mean position loss: 23.923138990402222\n",
      "Training growing_up:  68%|█████▍  | 13551/20010 [10:32:31<5:39:40,  3.16s/batch]Batch 13400/20010 Done, mean position loss: 22.45619630098343\n",
      "Training growing_up:  66%|█████▎  | 13225/20010 [10:32:57<5:04:28,  2.69s/batch]Batch 13300/20010 Done, mean position loss: 22.876022491455075\n",
      "Training growing_up:  66%|█████▎  | 13295/20010 [10:33:02<4:41:41,  2.52s/batch]Batch 13200/20010 Done, mean position loss: 22.41460196018219\n",
      "Training growing_up:  66%|█████▎  | 13304/20010 [10:33:05<5:11:11,  2.78s/batch]Batch 13400/20010 Done, mean position loss: 22.37157072305679\n",
      "Training growing_up:  68%|█████▍  | 13564/20010 [10:33:07<5:01:03,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 22.126585097312926\n",
      "Training growing_up:  65%|█████▏  | 13066/20010 [10:33:10<5:20:47,  2.77s/batch]Batch 13100/20010 Done, mean position loss: 21.737506906986237\n",
      "Training growing_up:  67%|█████▎  | 13364/20010 [10:33:17<4:20:39,  2.35s/batch]Batch 13100/20010 Done, mean position loss: 23.11632567405701\n",
      "Training growing_up:  65%|█████▏  | 13101/20010 [10:33:18<5:22:04,  2.80s/batch]Batch 13300/20010 Done, mean position loss: 22.28217744350433\n",
      "Training growing_up:  67%|█████▎  | 13414/20010 [10:33:41<5:06:13,  2.79s/batch]Batch 13300/20010 Done, mean position loss: 22.70503885984421\n",
      "Training growing_up:  67%|█████▎  | 13362/20010 [10:33:46<4:53:21,  2.65s/batch]Batch 13500/20010 Done, mean position loss: 23.198586289882662\n",
      "Training growing_up:  66%|█████▎  | 13265/20010 [10:33:57<5:43:25,  3.05s/batch]Batch 13400/20010 Done, mean position loss: 23.587409567832946\n",
      "Training growing_up:  67%|█████▎  | 13310/20010 [10:34:05<5:05:35,  2.74s/batch]Batch 13400/20010 Done, mean position loss: 23.407708332538604\n",
      "Training growing_up:  66%|█████▎  | 13184/20010 [10:34:10<5:41:05,  3.00s/batch]Batch 13300/20010 Done, mean position loss: 23.21218627691269\n",
      "Training growing_up:  67%|█████▎  | 13325/20010 [10:34:26<5:42:11,  3.07s/batch]Batch 13400/20010 Done, mean position loss: 23.15489809513092\n",
      "Training growing_up:  67%|█████▎  | 13338/20010 [10:34:43<5:06:19,  2.75s/batch]Batch 13300/20010 Done, mean position loss: 23.29872493982315\n",
      "Training growing_up:  66%|█████▎  | 13135/20010 [10:34:47<5:20:51,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 23.04717536211014\n",
      "Training growing_up:  66%|█████▎  | 13157/20010 [10:34:47<4:56:42,  2.60s/batch]Batch 13600/20010 Done, mean position loss: 22.415087847709657\n",
      "Training growing_up:  66%|█████▎  | 13159/20010 [10:34:52<5:47:01,  3.04s/batch]Batch 13100/20010 Done, mean position loss: 22.678935158252717\n",
      "Training growing_up:  66%|█████▎  | 13161/20010 [10:34:57<5:16:21,  2.77s/batch]Batch 13300/20010 Done, mean position loss: 23.59941607475281\n",
      "Training growing_up:  65%|█████▏  | 13072/20010 [10:35:00<5:55:21,  3.07s/batch]Batch 13200/20010 Done, mean position loss: 22.70095486164093\n",
      "Training growing_up:  66%|█████▎  | 13140/20010 [10:35:01<5:33:13,  2.91s/batch]Batch 13500/20010 Done, mean position loss: 22.597605464458464\n",
      "Training growing_up:  66%|█████▎  | 13162/20010 [10:35:02<5:19:07,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 22.84845661878586\n",
      "Training growing_up:  66%|█████▏  | 13107/20010 [10:35:09<5:32:14,  2.89s/batch]Batch 13300/20010 Done, mean position loss: 22.832590024471283\n",
      "Training growing_up:  67%|█████▎  | 13332/20010 [10:35:10<5:04:29,  2.74s/batch]Batch 13300/20010 Done, mean position loss: 23.23985833644867\n",
      "Training growing_up:  66%|█████▎  | 13207/20010 [10:35:16<4:56:55,  2.62s/batch]Batch 13400/20010 Done, mean position loss: 22.685313696861265\n",
      "Training growing_up:  67%|█████▎  | 13346/20010 [10:35:25<5:07:47,  2.77s/batch]Batch 13400/20010 Done, mean position loss: 22.981388297080994\n",
      "Training growing_up:  67%|█████▎  | 13424/20010 [10:35:33<5:02:27,  2.76s/batch]Batch 13400/20010 Done, mean position loss: 22.020741200447084\n",
      "Batch 13300/20010 Done, mean position loss: 23.22223321914673\n",
      "Training growing_up:  67%|█████▎  | 13316/20010 [10:35:38<4:59:04,  2.68s/batch]Batch 13500/20010 Done, mean position loss: 23.367660152912137\n",
      "Training growing_up:  67%|█████▎  | 13317/20010 [10:35:40<4:41:24,  2.52s/batch]Batch 13400/20010 Done, mean position loss: 22.742658627033233\n",
      "Training growing_up:  67%|█████▎  | 13385/20010 [10:35:45<5:29:22,  2.98s/batch]Batch 13300/20010 Done, mean position loss: 24.184565091133116\n",
      "Training growing_up:  67%|█████▍  | 13462/20010 [10:35:57<5:22:43,  2.96s/batch]Batch 13500/20010 Done, mean position loss: 22.824927823543547\n",
      "Training growing_up:  67%|█████▎  | 13391/20010 [10:36:01<5:09:18,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 23.064382138252256\n",
      "Training growing_up:  68%|█████▍  | 13510/20010 [10:36:03<5:01:16,  2.78s/batch]Batch 13400/20010 Done, mean position loss: 23.204783096313477\n",
      "Training growing_up:  66%|█████▎  | 13185/20010 [10:36:10<6:10:12,  3.25s/batch]Batch 13300/20010 Done, mean position loss: 22.66319313287735\n",
      "Training growing_up:  67%|█████▎  | 13418/20010 [10:36:27<4:59:17,  2.72s/batch]Batch 13100/20010 Done, mean position loss: 22.123954162597656\n",
      "Training growing_up:  67%|█████▎  | 13329/20010 [10:36:31<5:28:39,  2.95s/batch]Batch 13400/20010 Done, mean position loss: 23.445042107105255\n",
      "Training growing_up:  68%|█████▍  | 13514/20010 [10:36:35<5:00:04,  2.77s/batch]Batch 13300/20010 Done, mean position loss: 23.094010038375853\n",
      "Training growing_up:  67%|█████▍  | 13453/20010 [10:36:57<5:09:57,  2.84s/batch]Batch 13200/20010 Done, mean position loss: 22.516593024730682\n",
      "Training growing_up:  66%|█████▎  | 13285/20010 [10:36:57<4:57:32,  2.65s/batch]Batch 13200/20010 Done, mean position loss: 23.41900255203247\n",
      "Training growing_up:  67%|█████▍  | 13452/20010 [10:37:12<5:45:15,  3.16s/batch]Batch 13500/20010 Done, mean position loss: 22.17871912240982\n",
      "Training growing_up:  66%|█████▎  | 13256/20010 [10:37:41<5:42:43,  3.04s/batch]Batch 13300/20010 Done, mean position loss: 22.1995125246048\n",
      "Training growing_up:  67%|█████▍  | 13448/20010 [10:37:45<4:52:20,  2.67s/batch]Batch 13500/20010 Done, mean position loss: 22.267042038440707\n",
      "Training growing_up:  67%|█████▎  | 13438/20010 [10:37:47<5:22:30,  2.94s/batch]Batch 13400/20010 Done, mean position loss: 23.24118385076523\n",
      "Training growing_up:  67%|█████▎  | 13387/20010 [10:37:48<5:21:34,  2.91s/batch]Batch 13500/20010 Done, mean position loss: 22.235299522876737\n",
      "Training growing_up:  68%|█████▍  | 13518/20010 [10:38:00<5:34:01,  3.09s/batch]Batch 13200/20010 Done, mean position loss: 22.02334337472916\n",
      "Training growing_up:  68%|█████▍  | 13554/20010 [10:38:08<5:03:58,  2.83s/batch]Batch 13400/20010 Done, mean position loss: 22.141405522823334\n",
      "Training growing_up:  67%|█████▍  | 13487/20010 [10:38:08<5:39:25,  3.12s/batch]Batch 13200/20010 Done, mean position loss: 23.070093400478363\n",
      "Training growing_up:  67%|█████▎  | 13349/20010 [10:38:29<5:40:23,  3.07s/batch]Batch 13600/20010 Done, mean position loss: 22.748099999427797\n",
      "Training growing_up:  67%|█████▍  | 13463/20010 [10:38:30<5:32:48,  3.05s/batch]Batch 13400/20010 Done, mean position loss: 22.794073259830476\n",
      "Training growing_up:  67%|█████▎  | 13382/20010 [10:38:48<5:13:11,  2.84s/batch]Batch 13500/20010 Done, mean position loss: 22.877255473136902\n",
      "Training growing_up:  66%|█████▎  | 13213/20010 [10:38:49<6:41:12,  3.54s/batch]Batch 13500/20010 Done, mean position loss: 22.99363599061966\n",
      "Training growing_up:  67%|█████▍  | 13461/20010 [10:38:59<5:43:20,  3.15s/batch]Batch 13400/20010 Done, mean position loss: 23.93136130809784\n",
      "Training growing_up:  67%|█████▎  | 13386/20010 [10:39:18<5:20:37,  2.90s/batch]Batch 13500/20010 Done, mean position loss: 22.785566771030428\n",
      "Training growing_up:  67%|█████▎  | 13360/20010 [10:39:38<6:00:22,  3.25s/batch]Batch 13700/20010 Done, mean position loss: 22.230482902526855\n",
      "Training growing_up:  68%|█████▍  | 13537/20010 [10:39:41<5:54:40,  3.29s/batch]Batch 13500/20010 Done, mean position loss: 22.71007191181183\n",
      "Training growing_up:  66%|█████▎  | 13200/20010 [10:39:48<5:15:06,  2.78s/batch]Batch 13400/20010 Done, mean position loss: 23.468218898773195\n",
      "Training growing_up:  67%|█████▎  | 13383/20010 [10:39:48<5:44:44,  3.12s/batch]Batch 13400/20010 Done, mean position loss: 23.277543275356294\n",
      "Training growing_up:  67%|█████▎  | 13364/20010 [10:39:49<5:11:36,  2.81s/batch]Batch 13200/20010 Done, mean position loss: 22.938385167121886\n",
      "Training growing_up:  66%|█████▎  | 13260/20010 [10:39:54<5:47:47,  3.09s/batch]Batch 13500/20010 Done, mean position loss: 23.222787137031553\n",
      "Training growing_up:  68%|█████▍  | 13584/20010 [10:40:01<5:12:48,  2.92s/batch]Batch 13300/20010 Done, mean position loss: 22.637646508216857\n",
      "Training growing_up:  67%|█████▎  | 13346/20010 [10:40:01<6:26:41,  3.48s/batch]Batch 13600/20010 Done, mean position loss: 22.4845015335083\n",
      "Training growing_up:  67%|█████▎  | 13381/20010 [10:40:03<5:40:10,  3.08s/batch]Batch 13400/20010 Done, mean position loss: 23.1469686126709\n",
      "Training growing_up:  67%|█████▍  | 13482/20010 [10:40:03<5:05:07,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 22.391160094738005\n",
      "Training growing_up:  66%|█████▎  | 13267/20010 [10:40:17<6:09:19,  3.29s/batch]Batch 13500/20010 Done, mean position loss: 22.981706981658935\n",
      "Training growing_up:  68%|█████▍  | 13591/20010 [10:40:22<5:29:48,  3.08s/batch]Batch 13500/20010 Done, mean position loss: 22.72053949832916\n",
      "Training growing_up:  68%|█████▍  | 13640/20010 [10:40:27<5:46:58,  3.27s/batch]Batch 13500/20010 Done, mean position loss: 22.27231912136078\n",
      "Training growing_up:  68%|█████▍  | 13512/20010 [10:40:28<5:59:05,  3.32s/batch]Batch 13400/20010 Done, mean position loss: 23.218304347991946\n",
      "Training growing_up:  67%|█████▎  | 13415/20010 [10:40:29<5:25:57,  2.97s/batch]Batch 13500/20010 Done, mean position loss: 22.2268657708168\n",
      "Training growing_up:  68%|█████▍  | 13535/20010 [10:40:31<5:14:41,  2.92s/batch]Batch 13600/20010 Done, mean position loss: 23.3248925113678\n",
      "Training growing_up:  68%|█████▍  | 13507/20010 [10:40:44<5:15:40,  2.91s/batch]Batch 13400/20010 Done, mean position loss: 23.901362500190736\n",
      "Training growing_up:  68%|█████▍  | 13520/20010 [10:40:51<5:11:35,  2.88s/batch]Batch 13600/20010 Done, mean position loss: 22.679682466983795\n",
      "Training growing_up:  68%|█████▍  | 13535/20010 [10:41:01<5:06:23,  2.84s/batch]Batch 13500/20010 Done, mean position loss: 22.942418706417087\n",
      "Training growing_up:  67%|█████▍  | 13452/20010 [10:41:01<5:12:04,  2.86s/batch]Batch 13500/20010 Done, mean position loss: 23.349540355205537\n",
      "Training growing_up:  68%|█████▍  | 13514/20010 [10:41:05<5:02:58,  2.80s/batch]Batch 13400/20010 Done, mean position loss: 22.725873618125913\n",
      "Training growing_up:  68%|█████▍  | 13521/20010 [10:41:26<5:12:16,  2.89s/batch]Batch 13200/20010 Done, mean position loss: 22.248116385936736\n",
      "Training growing_up:  68%|█████▍  | 13525/20010 [10:41:32<5:51:17,  3.25s/batch]Batch 13500/20010 Done, mean position loss: 23.504968330860137\n",
      "Training growing_up:  67%|█████▎  | 13421/20010 [10:41:43<5:18:59,  2.90s/batch]Batch 13400/20010 Done, mean position loss: 23.285327529907228\n",
      "Training growing_up:  67%|█████▎  | 13407/20010 [10:42:00<5:11:11,  2.83s/batch]Batch 13300/20010 Done, mean position loss: 22.57820901632309\n",
      "Training growing_up:  68%|█████▍  | 13671/20010 [10:42:02<5:16:07,  2.99s/batch]Batch 13300/20010 Done, mean position loss: 23.316929025650026\n",
      "Training growing_up:  68%|█████▍  | 13537/20010 [10:42:05<4:58:14,  2.76s/batch]Batch 13600/20010 Done, mean position loss: 22.326888353824614\n",
      "Training growing_up:  67%|█████▍  | 13487/20010 [10:42:48<5:11:52,  2.87s/batch]Batch 13400/20010 Done, mean position loss: 22.093558406829835\n",
      "Training growing_up:  68%|█████▍  | 13615/20010 [10:42:48<5:37:20,  3.16s/batch]Batch 13600/20010 Done, mean position loss: 22.25886374235153\n",
      "Training growing_up:  67%|█████▍  | 13457/20010 [10:42:56<5:44:59,  3.16s/batch]Batch 13600/20010 Done, mean position loss: 22.140345129966732\n",
      "Training growing_up:  67%|█████▍  | 13446/20010 [10:42:57<5:03:41,  2.78s/batch]Batch 13500/20010 Done, mean position loss: 23.194343638420108\n",
      "Training growing_up:  67%|█████▍  | 13451/20010 [10:43:12<5:16:50,  2.90s/batch]Batch 13500/20010 Done, mean position loss: 22.30884873151779\n",
      "Training growing_up:  67%|█████▍  | 13486/20010 [10:43:15<5:22:20,  2.96s/batch]Batch 13300/20010 Done, mean position loss: 23.11932578086853\n",
      "Batch 13300/20010 Done, mean position loss: 21.675860586166383\n",
      "Training growing_up:  68%|█████▍  | 13630/20010 [10:43:31<4:56:40,  2.79s/batch]Batch 13700/20010 Done, mean position loss: 23.218326711654665\n",
      "Training growing_up:  67%|█████▍  | 13463/20010 [10:43:32<5:23:50,  2.97s/batch]Batch 13500/20010 Done, mean position loss: 22.761670718193052\n",
      "Training growing_up:  68%|█████▍  | 13569/20010 [10:43:45<5:16:18,  2.95s/batch]Batch 13600/20010 Done, mean position loss: 23.261827876567843\n",
      "Training growing_up:  67%|█████▎  | 13423/20010 [10:43:55<5:54:44,  3.23s/batch]Batch 13600/20010 Done, mean position loss: 23.00369253873825\n",
      "Training growing_up:  68%|█████▍  | 13511/20010 [10:43:59<4:41:37,  2.60s/batch]Batch 13500/20010 Done, mean position loss: 23.687931411266327\n",
      "Training growing_up:  67%|█████▎  | 13348/20010 [10:44:23<5:23:20,  2.91s/batch]Batch 13600/20010 Done, mean position loss: 22.721852462291718\n",
      "Training growing_up:  68%|█████▍  | 13527/20010 [10:44:34<5:08:26,  2.85s/batch]Batch 13800/20010 Done, mean position loss: 22.117427303791047\n",
      "Training growing_up:  68%|█████▍  | 13563/20010 [10:44:39<5:17:58,  2.96s/batch]Batch 13600/20010 Done, mean position loss: 23.086396715641023\n",
      "Training growing_up:  68%|█████▍  | 13687/20010 [10:44:42<4:49:21,  2.75s/batch]Batch 13500/20010 Done, mean position loss: 23.17025728225708\n",
      "Training growing_up:  66%|█████▎  | 13268/20010 [10:44:48<5:41:47,  3.04s/batch]Batch 13500/20010 Done, mean position loss: 24.022901916503905\n",
      "Training growing_up:  68%|█████▍  | 13591/20010 [10:44:53<5:18:57,  2.98s/batch]Batch 13300/20010 Done, mean position loss: 22.519409425258637\n",
      "Training growing_up:  68%|█████▍  | 13541/20010 [10:44:59<5:54:48,  3.29s/batch]Batch 13600/20010 Done, mean position loss: 23.07661880493164\n",
      "Training growing_up:  68%|█████▍  | 13693/20010 [10:45:00<5:12:08,  2.96s/batch]Batch 13700/20010 Done, mean position loss: 22.271400489807128\n",
      "Training growing_up:  68%|█████▍  | 13646/20010 [10:45:01<5:03:48,  2.86s/batch]Batch 13400/20010 Done, mean position loss: 22.710333456993105\n",
      "Training growing_up:  67%|█████▍  | 13494/20010 [10:45:05<5:35:12,  3.09s/batch]Batch 13500/20010 Done, mean position loss: 22.2375571846962\n",
      "Training growing_up:  68%|█████▍  | 13544/20010 [10:45:09<5:42:38,  3.18s/batch]Batch 13500/20010 Done, mean position loss: 22.952046747207643\n",
      "Training growing_up:  68%|█████▍  | 13621/20010 [10:45:23<5:24:17,  3.05s/batch]Batch 13600/20010 Done, mean position loss: 22.643927738666534\n",
      "Training growing_up:  68%|█████▍  | 13700/20010 [10:45:23<5:48:40,  3.32s/batch]Batch 13600/20010 Done, mean position loss: 22.081374733448026\n",
      "Training growing_up:  68%|█████▍  | 13513/20010 [10:45:25<5:41:18,  3.15s/batch]Batch 13600/20010 Done, mean position loss: 22.955823400020602\n",
      "Training growing_up:  68%|█████▍  | 13544/20010 [10:45:25<5:31:42,  3.08s/batch]Batch 13700/20010 Done, mean position loss: 23.234336459636687\n",
      "Training growing_up:  69%|█████▍  | 13709/20010 [10:45:26<5:22:26,  3.07s/batch]Batch 13600/20010 Done, mean position loss: 23.143761627674102\n",
      "Training growing_up:  67%|█████▎  | 13312/20010 [10:45:27<6:19:11,  3.40s/batch]Batch 13500/20010 Done, mean position loss: 22.908927536010744\n",
      "Training growing_up:  68%|█████▍  | 13673/20010 [10:45:42<5:25:50,  3.09s/batch]Batch 13500/20010 Done, mean position loss: 23.659656360149384\n",
      "Training growing_up:  67%|█████▎  | 13320/20010 [10:45:52<5:48:12,  3.12s/batch]Batch 13700/20010 Done, mean position loss: 22.43812155008316\n",
      "Training growing_up:  67%|█████▎  | 13356/20010 [10:46:03<5:09:49,  2.79s/batch]Batch 13600/20010 Done, mean position loss: 23.365746557712555\n",
      "Training growing_up:  68%|█████▍  | 13552/20010 [10:46:06<4:46:58,  2.67s/batch]Batch 13500/20010 Done, mean position loss: 22.973543367385865\n",
      "Training growing_up:  68%|█████▍  | 13618/20010 [10:46:14<5:37:24,  3.17s/batch]Batch 13600/20010 Done, mean position loss: 23.266527886390683\n",
      "Training growing_up:  68%|█████▍  | 13566/20010 [10:46:25<4:37:20,  2.58s/batch]Batch 13300/20010 Done, mean position loss: 22.24704565525055\n",
      "Training growing_up:  67%|█████▎  | 13367/20010 [10:46:34<5:09:29,  2.80s/batch]Batch 13600/20010 Done, mean position loss: 23.213206548690795\n",
      "Training growing_up:  69%|█████▍  | 13736/20010 [10:46:40<4:05:26,  2.35s/batch]Batch 13500/20010 Done, mean position loss: 23.17089287042618\n",
      "Training growing_up:  68%|█████▍  | 13577/20010 [10:46:54<4:37:28,  2.59s/batch]Batch 13700/20010 Done, mean position loss: 21.9053893327713\n",
      "Training growing_up:  68%|█████▍  | 13656/20010 [10:46:57<4:48:08,  2.72s/batch]Batch 13400/20010 Done, mean position loss: 23.553915710449218\n",
      "Training growing_up:  68%|█████▍  | 13638/20010 [10:47:06<4:47:27,  2.71s/batch]Batch 13400/20010 Done, mean position loss: 22.61325675725937\n",
      "Training growing_up:  68%|█████▍  | 13685/20010 [10:47:35<4:04:22,  2.32s/batch]Batch 13700/20010 Done, mean position loss: 22.036774904727935\n",
      "Training growing_up:  68%|█████▍  | 13581/20010 [10:47:43<4:15:25,  2.38s/batch]Batch 13700/20010 Done, mean position loss: 22.251519360542297\n",
      "Training growing_up:  68%|█████▍  | 13598/20010 [10:47:46<4:13:20,  2.37s/batch]Batch 13500/20010 Done, mean position loss: 22.3812890124321\n",
      "Training growing_up:  69%|█████▌  | 13876/20010 [10:47:52<3:58:39,  2.33s/batch]Batch 13600/20010 Done, mean position loss: 22.990980508327482\n",
      "Training growing_up:  69%|█████▌  | 13877/20010 [10:47:54<3:49:57,  2.25s/batch]Batch 13600/20010 Done, mean position loss: 22.287929496765138\n",
      "Training growing_up:  69%|█████▍  | 13709/20010 [10:48:03<4:12:30,  2.40s/batch]Batch 13400/20010 Done, mean position loss: 21.88219115972519\n",
      "Training growing_up:  67%|█████▎  | 13340/20010 [10:48:06<4:37:21,  2.49s/batch]Batch 13400/20010 Done, mean position loss: 23.35664432764053\n",
      "Training growing_up:  67%|█████▎  | 13369/20010 [10:48:07<4:52:12,  2.64s/batch]Batch 13800/20010 Done, mean position loss: 23.128502600193023\n",
      "Training growing_up:  69%|█████▍  | 13714/20010 [10:48:18<4:59:12,  2.85s/batch]Batch 13700/20010 Done, mean position loss: 22.952316744327547\n",
      "Training growing_up:  68%|█████▍  | 13688/20010 [10:48:19<4:13:01,  2.40s/batch]Batch 13600/20010 Done, mean position loss: 22.50805142879486\n",
      "Training growing_up:  69%|█████▌  | 13761/20010 [10:48:33<4:58:38,  2.87s/batch]Batch 13600/20010 Done, mean position loss: 23.423323888778686\n",
      "Training growing_up:  69%|█████▌  | 13812/20010 [10:48:39<4:58:33,  2.89s/batch]Batch 13700/20010 Done, mean position loss: 23.03055837869644\n",
      "Training growing_up:  68%|█████▍  | 13622/20010 [10:48:56<5:10:26,  2.92s/batch]Batch 13700/20010 Done, mean position loss: 22.636494305133816\n",
      "Training growing_up:  68%|█████▍  | 13695/20010 [10:49:00<5:10:50,  2.95s/batch]Batch 13900/20010 Done, mean position loss: 21.89053519010544\n",
      "Training growing_up:  68%|█████▍  | 13534/20010 [10:49:17<5:18:34,  2.95s/batch]Batch 13700/20010 Done, mean position loss: 23.306969299316407\n",
      "Training growing_up:  69%|█████▌  | 13826/20010 [10:49:20<5:00:46,  2.92s/batch]Batch 13600/20010 Done, mean position loss: 23.24173642873764\n",
      "Training growing_up:  68%|█████▍  | 13634/20010 [10:49:36<5:21:01,  3.02s/batch]Batch 13800/20010 Done, mean position loss: 22.509373719692228\n",
      "Training growing_up:  68%|█████▍  | 13622/20010 [10:49:38<5:48:53,  3.28s/batch]Batch 13600/20010 Done, mean position loss: 23.712026538848875\n",
      "Training growing_up:  69%|█████▌  | 13785/20010 [10:49:44<6:13:54,  3.60s/batch]Batch 13400/20010 Done, mean position loss: 22.750601756572724\n",
      "Training growing_up:  67%|█████▎  | 13401/20010 [10:49:44<7:33:33,  4.12s/batch]Batch 13500/20010 Done, mean position loss: 22.918186080455783\n",
      "Training growing_up:  67%|█████▎  | 13435/20010 [10:49:44<6:49:14,  3.73s/batch]Batch 13700/20010 Done, mean position loss: 22.761687054634095\n",
      "Training growing_up:  68%|█████▍  | 13610/20010 [10:49:50<6:32:14,  3.68s/batch]Batch 13600/20010 Done, mean position loss: 22.904252924919128\n",
      "Training growing_up:  68%|█████▍  | 13602/20010 [10:49:54<5:52:07,  3.30s/batch]Batch 13600/20010 Done, mean position loss: 22.275680062770846\n",
      "Training growing_up:  68%|█████▍  | 13699/20010 [10:50:05<6:25:36,  3.67s/batch]Batch 13800/20010 Done, mean position loss: 23.508958501815798\n",
      "Training growing_up:  68%|█████▍  | 13682/20010 [10:50:08<6:22:23,  3.63s/batch]Batch 13700/20010 Done, mean position loss: 22.480218245983124\n",
      "Training growing_up:  69%|█████▌  | 13803/20010 [10:50:12<6:10:05,  3.58s/batch]Batch 13700/20010 Done, mean position loss: 22.888017473220827\n",
      "Training growing_up:  68%|█████▍  | 13611/20010 [10:50:13<6:24:47,  3.61s/batch]Batch 13600/20010 Done, mean position loss: 23.0509482216835\n",
      "Training growing_up:  68%|█████▍  | 13612/20010 [10:50:17<6:33:23,  3.69s/batch]Batch 13700/20010 Done, mean position loss: 22.807553384304047\n",
      "Training growing_up:  68%|█████▍  | 13690/20010 [10:50:17<6:36:31,  3.76s/batch]Batch 13700/20010 Done, mean position loss: 22.486743829250337\n",
      "Training growing_up:  67%|█████▍  | 13448/20010 [10:50:30<6:53:15,  3.78s/batch]Batch 13600/20010 Done, mean position loss: 24.32266926765442\n",
      "Training growing_up:  69%|█████▍  | 13708/20010 [10:50:40<6:00:18,  3.43s/batch]Batch 13800/20010 Done, mean position loss: 22.554275727272035\n",
      "Training growing_up:  68%|█████▍  | 13627/20010 [10:50:55<6:47:52,  3.83s/batch]Batch 13700/20010 Done, mean position loss: 22.96696839570999\n",
      "Training growing_up:  69%|█████▍  | 13724/20010 [10:51:07<6:03:18,  3.47s/batch]Batch 13600/20010 Done, mean position loss: 22.673469967842102\n",
      "Training growing_up:  67%|█████▍  | 13461/20010 [10:51:16<5:28:27,  3.01s/batch]Batch 13700/20010 Done, mean position loss: 23.18797365427017\n",
      "Training growing_up:  67%|█████▍  | 13489/20010 [10:51:29<5:02:14,  2.78s/batch]Batch 13400/20010 Done, mean position loss: 22.178682084083558\n",
      "Training growing_up:  68%|█████▍  | 13629/20010 [10:51:31<5:26:53,  3.07s/batch]Batch 13700/20010 Done, mean position loss: 23.573314764499663\n",
      "Training growing_up:  68%|█████▍  | 13705/20010 [10:51:42<4:43:07,  2.69s/batch]Batch 13600/20010 Done, mean position loss: 23.215189471244813\n",
      "Training growing_up:  68%|█████▍  | 13642/20010 [10:51:54<5:02:37,  2.85s/batch]Batch 13800/20010 Done, mean position loss: 22.285173666477203\n",
      "Training growing_up:  68%|█████▍  | 13641/20010 [10:52:05<5:09:52,  2.92s/batch]Batch 13500/20010 Done, mean position loss: 23.799489545822144\n",
      "Training growing_up:  68%|█████▍  | 13545/20010 [10:52:13<5:12:20,  2.90s/batch]Batch 13500/20010 Done, mean position loss: 22.46804486989975\n",
      "Training growing_up:  69%|█████▌  | 13764/20010 [10:52:44<4:56:22,  2.85s/batch]Batch 13800/20010 Done, mean position loss: 21.928094820976256\n",
      "Training growing_up:  68%|█████▍  | 13668/20010 [10:52:55<5:14:45,  2.98s/batch]Batch 13800/20010 Done, mean position loss: 22.180680840015413\n",
      "Training growing_up:  69%|█████▌  | 13895/20010 [10:52:59<5:04:33,  2.99s/batch]Batch 13600/20010 Done, mean position loss: 22.023268678188323\n",
      "Training growing_up:  67%|█████▎  | 13433/20010 [10:53:07<5:22:08,  2.94s/batch]Batch 13700/20010 Done, mean position loss: 22.60064135789871\n",
      "Training growing_up:  69%|█████▌  | 13761/20010 [10:53:12<5:24:47,  3.12s/batch]Batch 13700/20010 Done, mean position loss: 22.95845896482468\n",
      "Training growing_up:  69%|█████▌  | 13761/20010 [10:53:15<5:02:07,  2.90s/batch]Batch 13500/20010 Done, mean position loss: 21.846304953098297\n",
      "Training growing_up:  69%|█████▌  | 13761/20010 [10:53:16<4:49:55,  2.78s/batch]Batch 13900/20010 Done, mean position loss: 23.096183478832245\n",
      "Training growing_up:  69%|█████▌  | 13857/20010 [10:53:25<4:48:24,  2.81s/batch]Batch 13800/20010 Done, mean position loss: 23.14808799982071\n",
      "Training growing_up:  69%|█████▌  | 13801/20010 [10:53:25<5:00:16,  2.90s/batch]Batch 13500/20010 Done, mean position loss: 22.97430356502533\n",
      "Training growing_up:  69%|█████▌  | 13767/20010 [10:53:32<4:48:09,  2.77s/batch]Batch 13700/20010 Done, mean position loss: 22.59927330493927\n",
      "Training growing_up:  69%|█████▌  | 13770/20010 [10:53:48<6:08:56,  3.55s/batch]Batch 13700/20010 Done, mean position loss: 23.205476603507993\n",
      "Training growing_up:  69%|█████▌  | 13775/20010 [10:53:52<4:50:22,  2.79s/batch]Batch 13800/20010 Done, mean position loss: 23.14188300848007\n",
      "Training growing_up:  68%|█████▍  | 13584/20010 [10:54:08<5:02:28,  2.82s/batch]Batch 14000/20010 Done, mean position loss: 22.174412810802462\n",
      "Training growing_up:  68%|█████▍  | 13651/20010 [10:54:10<6:30:07,  3.68s/batch]Batch 13800/20010 Done, mean position loss: 22.367945072650908\n",
      "Training growing_up:  69%|█████▍  | 13721/20010 [10:54:35<5:34:32,  3.19s/batch]Batch 13800/20010 Done, mean position loss: 23.010073223114013\n",
      "Training growing_up:  68%|█████▍  | 13659/20010 [10:54:35<5:54:41,  3.35s/batch]Batch 13700/20010 Done, mean position loss: 23.114506957530974\n",
      "Training growing_up:  69%|█████▌  | 13794/20010 [10:54:52<5:13:29,  3.03s/batch]Batch 13900/20010 Done, mean position loss: 22.076527359485624\n",
      "Training growing_up:  69%|█████▌  | 13830/20010 [10:54:54<4:25:49,  2.58s/batch]Batch 13800/20010 Done, mean position loss: 23.029403345584868\n",
      "Training growing_up:  69%|█████▍  | 13728/20010 [10:54:57<5:24:52,  3.10s/batch]Batch 13700/20010 Done, mean position loss: 23.291430950164795\n",
      "Training growing_up:  69%|█████▌  | 13888/20010 [10:55:01<5:19:37,  3.13s/batch]Batch 13600/20010 Done, mean position loss: 22.684820754528047\n",
      "Training growing_up:  68%|█████▍  | 13641/20010 [10:55:04<6:16:30,  3.55s/batch]Batch 13700/20010 Done, mean position loss: 22.973213298320772\n",
      "Training growing_up:  68%|█████▍  | 13556/20010 [10:55:05<5:20:32,  2.98s/batch]Batch 13500/20010 Done, mean position loss: 22.248327193260195\n",
      "Training growing_up:  69%|█████▍  | 13737/20010 [10:55:09<5:01:46,  2.89s/batch]Batch 13700/20010 Done, mean position loss: 22.766591143608093\n",
      "Training growing_up:  69%|█████▍  | 13728/20010 [10:55:12<5:13:12,  2.99s/batch]Batch 13800/20010 Done, mean position loss: 22.755798699855802\n",
      "Training growing_up:  70%|█████▌  | 13908/20010 [10:55:14<5:31:01,  3.25s/batch]Batch 13800/20010 Done, mean position loss: 22.198409020900726\n",
      "Training growing_up:  69%|█████▌  | 13822/20010 [10:55:15<5:20:38,  3.11s/batch]Batch 13900/20010 Done, mean position loss: 23.20543171167374\n",
      "Training growing_up:  69%|█████▌  | 13831/20010 [10:55:20<5:23:36,  3.14s/batch]Batch 13700/20010 Done, mean position loss: 22.94366189956665\n",
      "Training growing_up:  69%|█████▌  | 13810/20010 [10:55:22<5:20:37,  3.10s/batch]Batch 13800/20010 Done, mean position loss: 22.243256945610046\n",
      "Training growing_up:  68%|█████▍  | 13702/20010 [10:55:23<5:03:10,  2.88s/batch]Batch 13800/20010 Done, mean position loss: 22.632172496318816\n",
      "Training growing_up:  70%|█████▌  | 13916/20010 [10:55:39<5:13:36,  3.09s/batch]Batch 13900/20010 Done, mean position loss: 22.452865569591523\n",
      "Training growing_up:  68%|█████▍  | 13616/20010 [10:55:46<4:54:46,  2.77s/batch]Batch 13700/20010 Done, mean position loss: 24.113719475269317\n",
      "Training growing_up:  70%|█████▌  | 13907/20010 [10:55:56<4:47:15,  2.82s/batch]Batch 13800/20010 Done, mean position loss: 22.99194101333618\n",
      "Training growing_up:  69%|█████▌  | 13824/20010 [10:56:08<5:08:05,  2.99s/batch]Batch 13700/20010 Done, mean position loss: 22.890774908065794\n",
      "Training growing_up:  69%|█████▍  | 13728/20010 [10:56:20<5:29:51,  3.15s/batch]Batch 13800/20010 Done, mean position loss: 23.368755054473873\n",
      "Training growing_up:  68%|█████▍  | 13564/20010 [10:56:30<5:28:56,  3.06s/batch]Batch 13800/20010 Done, mean position loss: 23.606454498767853\n",
      "Training growing_up:  69%|█████▌  | 13829/20010 [10:56:36<5:03:01,  2.94s/batch]Batch 13500/20010 Done, mean position loss: 22.394599571228028\n",
      "Training growing_up:  69%|█████▍  | 13720/20010 [10:56:48<5:48:14,  3.32s/batch]Batch 13700/20010 Done, mean position loss: 22.978999435901642\n",
      "Training growing_up:  69%|█████▌  | 13835/20010 [10:56:53<4:51:31,  2.83s/batch]Batch 13900/20010 Done, mean position loss: 22.329132182598112\n",
      "Training growing_up:  70%|█████▌  | 14063/20010 [10:57:08<4:24:03,  2.66s/batch]Batch 13600/20010 Done, mean position loss: 23.17417636871338\n",
      "Training growing_up:  69%|█████▌  | 13842/20010 [10:57:16<5:21:31,  3.13s/batch]Batch 13600/20010 Done, mean position loss: 22.408228833675384\n",
      "Training growing_up:  69%|█████▌  | 13846/20010 [10:57:43<4:51:52,  2.84s/batch]Batch 13900/20010 Done, mean position loss: 22.039713923931124\n",
      "Training growing_up:  69%|█████▌  | 13759/20010 [10:58:05<5:18:55,  3.06s/batch]Batch 13900/20010 Done, mean position loss: 22.188564462661745\n",
      "Training growing_up:  69%|█████▌  | 13790/20010 [10:58:07<4:49:40,  2.79s/batch]Batch 13800/20010 Done, mean position loss: 22.37428605556488\n",
      "Training growing_up:  68%|█████▍  | 13620/20010 [10:58:07<5:38:03,  3.17s/batch]Batch 13700/20010 Done, mean position loss: 22.26012158870697\n",
      "Training growing_up:  70%|█████▋  | 14084/20010 [10:58:12<4:25:29,  2.69s/batch]Batch 14000/20010 Done, mean position loss: 23.155142481327054\n",
      "Training growing_up:  70%|█████▌  | 13913/20010 [10:58:17<5:12:16,  3.07s/batch]Batch 13800/20010 Done, mean position loss: 23.31351730585098\n",
      "Training growing_up:  70%|█████▋  | 14087/20010 [10:58:21<4:33:55,  2.77s/batch]Batch 13600/20010 Done, mean position loss: 21.985143275260928\n",
      "Training growing_up:  69%|█████▍  | 13749/20010 [10:58:32<5:26:41,  3.13s/batch]Batch 13600/20010 Done, mean position loss: 23.143272521495817\n",
      "Training growing_up:  69%|█████▌  | 13771/20010 [10:58:33<5:10:51,  2.99s/batch]Batch 13900/20010 Done, mean position loss: 23.090062613487245\n",
      "Training growing_up:  69%|█████▌  | 13766/20010 [10:58:38<4:49:55,  2.79s/batch]Batch 13800/20010 Done, mean position loss: 22.478577041625975\n",
      "Training growing_up:  68%|█████▍  | 13634/20010 [10:58:51<5:35:11,  3.15s/batch]Batch 13800/20010 Done, mean position loss: 23.677253680229185\n",
      "Training growing_up:  70%|█████▌  | 13909/20010 [10:58:55<4:43:38,  2.79s/batch]Batch 13900/20010 Done, mean position loss: 22.786140224933625\n",
      "Training growing_up:  69%|█████▌  | 13897/20010 [10:59:07<4:43:09,  2.78s/batch]Batch 14100/20010 Done, mean position loss: 22.023790841102603\n",
      "Training growing_up:  69%|█████▌  | 13780/20010 [10:59:18<5:05:15,  2.94s/batch]Batch 13900/20010 Done, mean position loss: 22.502787647247317\n",
      "Training growing_up:  69%|█████▌  | 13818/20010 [10:59:40<5:04:28,  2.95s/batch]Batch 13800/20010 Done, mean position loss: 23.176673724651337\n",
      "Training growing_up:  68%|█████▍  | 13694/20010 [10:59:40<6:22:05,  3.63s/batch]Batch 13900/20010 Done, mean position loss: 23.083315742015838\n",
      "Training growing_up:  69%|█████▌  | 13867/20010 [10:59:49<5:16:36,  3.09s/batch]Batch 14000/20010 Done, mean position loss: 22.431646440029148\n",
      "Training growing_up:  68%|█████▍  | 13633/20010 [11:00:01<5:21:54,  3.03s/batch]Batch 13900/20010 Done, mean position loss: 23.177306563854216\n",
      "Training growing_up:  69%|█████▌  | 13901/20010 [11:00:02<5:17:38,  3.12s/batch]Batch 13700/20010 Done, mean position loss: 22.85609125614166\n",
      "Training growing_up:  69%|█████▌  | 13827/20010 [11:00:03<5:12:28,  3.03s/batch]Batch 13800/20010 Done, mean position loss: 23.384782712459565\n",
      "Training growing_up:  69%|█████▌  | 13803/20010 [11:00:09<5:01:33,  2.92s/batch]Batch 13900/20010 Done, mean position loss: 22.12403006315231\n",
      "Training growing_up:  69%|█████▌  | 13874/20010 [11:00:11<5:11:08,  3.04s/batch]Batch 13800/20010 Done, mean position loss: 22.968444828987124\n",
      "Training growing_up:  69%|█████▌  | 13900/20010 [11:00:13<5:06:04,  3.01s/batch]Batch 14000/20010 Done, mean position loss: 23.640590932369232\n",
      "Training growing_up:  69%|█████▌  | 13813/20010 [11:00:13<4:47:35,  2.78s/batch]Batch 13800/20010 Done, mean position loss: 22.83396845817566\n",
      "Training growing_up:  69%|█████▌  | 13881/20010 [11:00:16<5:05:18,  2.99s/batch]Batch 13900/20010 Done, mean position loss: 22.80340958118439\n",
      "Training growing_up:  69%|█████▌  | 13803/20010 [11:00:19<5:28:15,  3.17s/batch]Batch 13600/20010 Done, mean position loss: 22.551301071643827\n",
      "Training growing_up:  69%|█████▌  | 13878/20010 [11:00:23<4:52:45,  2.86s/batch]Batch 13800/20010 Done, mean position loss: 22.6145552444458\n",
      "Training growing_up:  69%|█████▌  | 13807/20010 [11:00:29<4:57:09,  2.87s/batch]Batch 13900/20010 Done, mean position loss: 22.356388099193573\n",
      "Training growing_up:  70%|█████▌  | 14015/20010 [11:00:30<4:56:59,  2.97s/batch]Batch 13900/20010 Done, mean position loss: 22.492682652473448\n",
      "Training growing_up:  69%|█████▌  | 13813/20010 [11:00:39<4:38:55,  2.70s/batch]Batch 14000/20010 Done, mean position loss: 22.581124663352966\n",
      "Training growing_up:  70%|█████▌  | 13910/20010 [11:00:56<4:51:07,  2.86s/batch]Batch 13900/20010 Done, mean position loss: 22.71624477863312\n",
      "Training growing_up:  69%|█████▌  | 13902/20010 [11:00:59<4:47:24,  2.82s/batch]Batch 13800/20010 Done, mean position loss: 23.76039748430252\n",
      "Training growing_up:  69%|█████▌  | 13849/20010 [11:01:09<5:02:25,  2.95s/batch]Batch 13800/20010 Done, mean position loss: 22.650096945762634\n",
      "Training growing_up:  71%|█████▋  | 14145/20010 [11:01:16<4:21:59,  2.68s/batch]Batch 13900/20010 Done, mean position loss: 22.987078273296355\n",
      "Training growing_up:  70%|█████▌  | 13946/20010 [11:01:29<4:28:40,  2.66s/batch]Batch 13900/20010 Done, mean position loss: 23.757477691173555\n",
      "Training growing_up:  68%|█████▍  | 13689/20010 [11:01:35<4:26:32,  2.53s/batch]Batch 13600/20010 Done, mean position loss: 21.98206456184387\n",
      "Training growing_up:  69%|█████▌  | 13831/20010 [11:01:37<4:14:41,  2.47s/batch]Batch 13800/20010 Done, mean position loss: 22.87262097120285\n",
      "Training growing_up:  68%|█████▍  | 13671/20010 [11:01:52<4:33:03,  2.58s/batch]Batch 14000/20010 Done, mean position loss: 22.219867517948153\n",
      "Training growing_up:  69%|█████▌  | 13868/20010 [11:02:06<5:08:50,  3.02s/batch]Batch 13700/20010 Done, mean position loss: 23.38410087108612\n",
      "Training growing_up:  70%|█████▋  | 14086/20010 [11:02:13<4:25:33,  2.69s/batch]Batch 13700/20010 Done, mean position loss: 22.363747239112854\n",
      "Training growing_up:  70%|█████▌  | 13960/20010 [11:02:30<4:39:46,  2.77s/batch]Batch 14000/20010 Done, mean position loss: 21.942200615406037\n",
      "Training growing_up:  69%|█████▌  | 13828/20010 [11:02:45<4:16:32,  2.49s/batch]Batch 14000/20010 Done, mean position loss: 22.179814896583558\n",
      "Training growing_up:  70%|█████▌  | 14022/20010 [11:02:48<4:04:17,  2.45s/batch]Batch 14100/20010 Done, mean position loss: 23.245206162929534\n",
      "Training growing_up:  69%|█████▌  | 13857/20010 [11:02:56<4:03:26,  2.37s/batch]Batch 13800/20010 Done, mean position loss: 22.032159938812256\n",
      "Training growing_up:  71%|█████▋  | 14184/20010 [11:02:56<4:01:23,  2.49s/batch]Batch 13900/20010 Done, mean position loss: 22.44827874660492\n",
      "Training growing_up:  68%|█████▍  | 13660/20010 [11:03:09<4:50:38,  2.75s/batch]Batch 14000/20010 Done, mean position loss: 22.931324439048765\n",
      "Training growing_up:  69%|█████▍  | 13726/20010 [11:03:13<4:28:52,  2.57s/batch]Batch 13700/20010 Done, mean position loss: 22.906110548973086\n",
      "Training growing_up:  70%|█████▌  | 13969/20010 [11:03:15<4:03:52,  2.42s/batch]Batch 13900/20010 Done, mean position loss: 23.04581109046936\n",
      "Training growing_up:  69%|█████▌  | 13901/20010 [11:03:15<4:35:22,  2.70s/batch]Batch 13700/20010 Done, mean position loss: 21.777550156116483\n",
      "Training growing_up:  70%|█████▌  | 13947/20010 [11:03:28<4:22:52,  2.60s/batch]Batch 13900/20010 Done, mean position loss: 22.78999278783798\n",
      "Training growing_up:  69%|█████▌  | 13845/20010 [11:03:32<5:02:03,  2.94s/batch]Batch 14000/20010 Done, mean position loss: 22.990627677440642\n",
      "Training growing_up:  70%|█████▌  | 13962/20010 [11:03:33<3:39:02,  2.17s/batch]Batch 13900/20010 Done, mean position loss: 23.463878417015078\n",
      "Training growing_up:  70%|█████▋  | 14071/20010 [11:03:45<3:59:12,  2.42s/batch]Batch 14200/20010 Done, mean position loss: 22.210950028896335\n",
      "Training growing_up:  70%|█████▌  | 13916/20010 [11:03:54<4:33:18,  2.69s/batch]Batch 14000/20010 Done, mean position loss: 22.361724460124968\n",
      "Training growing_up:  70%|█████▋  | 14099/20010 [11:04:13<4:02:53,  2.47s/batch]Batch 13900/20010 Done, mean position loss: 23.19046685218811\n",
      "Training growing_up:  69%|█████▌  | 13891/20010 [11:04:20<5:11:25,  3.05s/batch]Batch 14100/20010 Done, mean position loss: 22.29513960599899\n",
      "Training growing_up:  69%|█████▌  | 13904/20010 [11:04:21<4:12:11,  2.48s/batch]Batch 14000/20010 Done, mean position loss: 22.970909061431886\n",
      "Training growing_up:  70%|█████▌  | 13921/20010 [11:04:27<4:49:54,  2.86s/batch]Batch 14000/20010 Done, mean position loss: 23.090814266204834\n",
      "Training growing_up:  70%|█████▌  | 14025/20010 [11:04:35<4:45:34,  2.86s/batch]Batch 13800/20010 Done, mean position loss: 22.905787184238434\n",
      "Training growing_up:  70%|█████▌  | 13994/20010 [11:04:37<4:23:10,  2.62s/batch]Batch 14000/20010 Done, mean position loss: 21.97659471273422\n",
      "Training growing_up:  70%|█████▌  | 13939/20010 [11:04:38<4:07:11,  2.44s/batch]Batch 13900/20010 Done, mean position loss: 23.335964183807377\n",
      "Training growing_up:  70%|█████▋  | 14093/20010 [11:04:41<3:56:35,  2.40s/batch]Batch 13900/20010 Done, mean position loss: 23.055147020816804\n",
      "Training growing_up:  70%|█████▋  | 14095/20010 [11:04:46<4:02:33,  2.46s/batch]Batch 13900/20010 Done, mean position loss: 22.815609123706817\n",
      "Training growing_up:  70%|█████▋  | 14100/20010 [11:04:49<3:58:57,  2.43s/batch]Batch 13900/20010 Done, mean position loss: 22.902590048313144\n",
      "Training growing_up:  70%|█████▌  | 13915/20010 [11:04:51<4:07:26,  2.44s/batch]Batch 14100/20010 Done, mean position loss: 23.369658715724945\n",
      "Training growing_up:  70%|█████▌  | 13907/20010 [11:04:58<4:43:33,  2.79s/batch]Batch 14000/20010 Done, mean position loss: 22.981943321228027\n",
      "Training growing_up:  68%|█████▍  | 13680/20010 [11:05:00<4:30:04,  2.56s/batch]Batch 14000/20010 Done, mean position loss: 22.882512407302855\n",
      "Training growing_up:  70%|█████▌  | 13909/20010 [11:05:00<4:40:42,  2.76s/batch]Batch 14000/20010 Done, mean position loss: 22.17929157972336\n",
      "Training growing_up:  70%|█████▌  | 14001/20010 [11:05:00<5:29:43,  3.29s/batch]Batch 13700/20010 Done, mean position loss: 22.493392121791842\n",
      "Training growing_up:  70%|█████▌  | 14003/20010 [11:05:04<3:54:51,  2.35s/batch]Batch 14100/20010 Done, mean position loss: 22.322017982006074\n",
      "Training growing_up:  71%|█████▋  | 14121/20010 [11:05:16<4:25:06,  2.70s/batch]Batch 14000/20010 Done, mean position loss: 22.84896324157715\n",
      "Training growing_up:  71%|█████▋  | 14114/20010 [11:05:24<4:14:44,  2.59s/batch]Batch 13900/20010 Done, mean position loss: 24.05815933227539\n",
      "Training growing_up:  69%|█████▌  | 13886/20010 [11:05:26<4:50:54,  2.85s/batch]Batch 13900/20010 Done, mean position loss: 22.458116250038145\n",
      "Training growing_up:  70%|█████▋  | 14088/20010 [11:05:41<4:36:50,  2.80s/batch]Batch 14000/20010 Done, mean position loss: 23.019516830444335\n",
      "Training growing_up:  69%|█████▍  | 13720/20010 [11:05:49<5:08:59,  2.95s/batch]Batch 14000/20010 Done, mean position loss: 23.67688920497894\n",
      "Training growing_up:  70%|█████▋  | 14073/20010 [11:06:00<4:25:07,  2.68s/batch]Batch 13700/20010 Done, mean position loss: 22.04959439992905\n",
      "Training growing_up:  69%|█████▌  | 13766/20010 [11:06:11<4:41:16,  2.70s/batch]Batch 13900/20010 Done, mean position loss: 23.01112962245941\n",
      "Training growing_up:  69%|█████▌  | 13874/20010 [11:06:19<4:20:10,  2.54s/batch]Batch 14100/20010 Done, mean position loss: 22.135264623165128\n",
      "Training growing_up:  70%|█████▋  | 14087/20010 [11:06:43<5:06:28,  3.10s/batch]Batch 13800/20010 Done, mean position loss: 23.388171334266662\n",
      "Training growing_up:  69%|█████▌  | 13801/20010 [11:06:43<5:15:18,  3.05s/batch]Batch 13800/20010 Done, mean position loss: 22.48908124923706\n",
      "Training growing_up:  70%|█████▋  | 14097/20010 [11:07:10<4:29:35,  2.74s/batch]Batch 14200/20010 Done, mean position loss: 22.77028302192688\n",
      "Training growing_up:  71%|█████▋  | 14148/20010 [11:07:12<4:21:47,  2.68s/batch]Batch 14100/20010 Done, mean position loss: 22.044819779396057\n",
      "Training growing_up:  71%|█████▋  | 14279/20010 [11:07:21<4:20:08,  2.72s/batch]Batch 14100/20010 Done, mean position loss: 22.23737774133682\n",
      "Training growing_up:  70%|█████▌  | 13987/20010 [11:07:31<4:17:40,  2.57s/batch]Batch 14000/20010 Done, mean position loss: 22.271997423171996\n",
      "Training growing_up:  70%|█████▌  | 14051/20010 [11:07:38<4:30:36,  2.72s/batch]Batch 13900/20010 Done, mean position loss: 22.08871351003647\n",
      "Training growing_up:  70%|█████▌  | 13965/20010 [11:07:40<5:06:23,  3.04s/batch]Batch 14100/20010 Done, mean position loss: 22.973902277946472\n",
      "Training growing_up:  70%|█████▌  | 13971/20010 [11:07:55<4:39:30,  2.78s/batch]Batch 13800/20010 Done, mean position loss: 21.75953080892563\n",
      "Training growing_up:  70%|█████▋  | 14107/20010 [11:07:56<4:27:16,  2.72s/batch]Batch 13800/20010 Done, mean position loss: 22.876148145198822\n",
      "Training growing_up:  70%|█████▋  | 14088/20010 [11:07:57<4:50:12,  2.94s/batch]Batch 14000/20010 Done, mean position loss: 23.172015273571013\n",
      "Training growing_up:  70%|█████▋  | 14070/20010 [11:08:07<4:38:06,  2.81s/batch]Batch 14000/20010 Done, mean position loss: 22.829470117092136\n",
      "Training growing_up:  71%|█████▋  | 14222/20010 [11:08:09<4:40:08,  2.90s/batch]Batch 14100/20010 Done, mean position loss: 22.896717262268066\n",
      "Training growing_up:  70%|█████▌  | 13985/20010 [11:08:09<4:49:22,  2.88s/batch]Batch 14000/20010 Done, mean position loss: 22.850192484855654\n",
      "Training growing_up:  70%|█████▌  | 14010/20010 [11:08:22<4:38:05,  2.78s/batch]Batch 14300/20010 Done, mean position loss: 22.18207020998001\n",
      "Training growing_up:  71%|█████▋  | 14177/20010 [11:08:34<4:41:47,  2.90s/batch]Batch 14100/20010 Done, mean position loss: 22.25082347393036\n",
      "Training growing_up:  70%|█████▋  | 14088/20010 [11:08:55<4:16:10,  2.60s/batch]Batch 14000/20010 Done, mean position loss: 22.76567529439926\n",
      "Training growing_up:  70%|█████▌  | 14022/20010 [11:08:56<4:49:43,  2.90s/batch]Batch 14100/20010 Done, mean position loss: 23.148113803863524\n",
      "Training growing_up:  69%|█████▌  | 13850/20010 [11:09:06<4:52:33,  2.85s/batch]Batch 14200/20010 Done, mean position loss: 22.732723445892336\n",
      "Training growing_up:  71%|█████▋  | 14241/20010 [11:09:09<5:01:49,  3.14s/batch]Batch 14100/20010 Done, mean position loss: 23.424345295429227\n",
      "Training growing_up:  70%|█████▋  | 14104/20010 [11:09:18<4:47:31,  2.92s/batch]Batch 14100/20010 Done, mean position loss: 22.14339453935623\n",
      "Training growing_up:  69%|█████▌  | 13830/20010 [11:09:21<5:36:16,  3.26s/batch]Batch 14200/20010 Done, mean position loss: 23.309446194171905\n",
      "Training growing_up:  70%|█████▌  | 13984/20010 [11:09:23<5:04:31,  3.03s/batch]Batch 14000/20010 Done, mean position loss: 23.60467879772186\n",
      "Training growing_up:  71%|█████▋  | 14203/20010 [11:09:27<4:47:12,  2.97s/batch]Batch 13900/20010 Done, mean position loss: 22.97528478384018\n",
      "Training growing_up:  69%|█████▌  | 13832/20010 [11:09:32<5:49:56,  3.40s/batch]Batch 14000/20010 Done, mean position loss: 22.969726312160493\n",
      "Training growing_up:  70%|█████▌  | 13988/20010 [11:09:34<4:46:12,  2.85s/batch]Batch 14000/20010 Done, mean position loss: 22.401950232982635\n",
      "Training growing_up:  71%|█████▋  | 14139/20010 [11:09:35<5:27:00,  3.34s/batch]Batch 14100/20010 Done, mean position loss: 22.745315346717835\n",
      "Training growing_up:  69%|█████▌  | 13774/20010 [11:09:40<5:43:52,  3.31s/batch]Batch 14000/20010 Done, mean position loss: 23.01154530286789\n",
      "Training growing_up:  69%|█████▌  | 13861/20010 [11:09:41<5:57:20,  3.49s/batch]Batch 14100/20010 Done, mean position loss: 22.86342406988144\n",
      "Training growing_up:  71%|█████▋  | 14112/20010 [11:09:42<5:03:28,  3.09s/batch]Batch 13800/20010 Done, mean position loss: 22.594552335739138\n",
      "Training growing_up:  71%|█████▋  | 14214/20010 [11:09:43<4:54:30,  3.05s/batch]Batch 14200/20010 Done, mean position loss: 22.383127057552336\n",
      "Training growing_up:  69%|█████▌  | 13865/20010 [11:09:53<5:21:59,  3.14s/batch]Batch 14100/20010 Done, mean position loss: 22.03078391313553\n",
      "Training growing_up:  70%|█████▌  | 14014/20010 [11:10:05<5:34:50,  3.35s/batch]Batch 14100/20010 Done, mean position loss: 22.67246220827103\n",
      "Training growing_up:  71%|█████▋  | 14180/20010 [11:10:11<5:00:35,  3.09s/batch]Batch 14000/20010 Done, mean position loss: 23.948397467136385\n",
      "Training growing_up:  70%|█████▌  | 14017/20010 [11:10:23<5:13:41,  3.14s/batch]Batch 14000/20010 Done, mean position loss: 22.677814736366273\n",
      "Training growing_up:  71%|█████▋  | 14267/20010 [11:10:27<4:25:56,  2.78s/batch]Batch 14100/20010 Done, mean position loss: 23.291578118801116\n",
      "Training growing_up:  71%|█████▋  | 14161/20010 [11:10:41<4:57:28,  3.05s/batch]Batch 14100/20010 Done, mean position loss: 23.700577168464662\n",
      "Training growing_up:  70%|█████▌  | 14014/20010 [11:11:03<5:20:35,  3.21s/batch]Batch 13800/20010 Done, mean position loss: 22.214639077186586\n",
      "Training growing_up:  70%|█████▌  | 14060/20010 [11:11:08<5:02:39,  3.05s/batch]Batch 14000/20010 Done, mean position loss: 22.917808043956757\n",
      "Training growing_up:  71%|█████▋  | 14283/20010 [11:11:16<4:58:44,  3.13s/batch]Batch 14200/20010 Done, mean position loss: 21.868698313236237\n",
      "Training growing_up:  70%|█████▌  | 14027/20010 [11:11:42<4:46:22,  2.87s/batch]Batch 13900/20010 Done, mean position loss: 22.38387464761734\n",
      "Training growing_up:  71%|█████▋  | 14151/20010 [11:11:49<4:47:05,  2.94s/batch]Batch 13900/20010 Done, mean position loss: 23.28405553817749\n",
      "Training growing_up:  71%|█████▋  | 14159/20010 [11:12:13<4:38:04,  2.85s/batch]Batch 14300/20010 Done, mean position loss: 22.886074285507203\n",
      "Training growing_up:  71%|█████▋  | 14302/20010 [11:12:15<4:36:25,  2.91s/batch]Batch 14200/20010 Done, mean position loss: 21.803183765411376\n",
      "Training growing_up:  71%|█████▋  | 14188/20010 [11:12:21<4:23:47,  2.72s/batch]Batch 14200/20010 Done, mean position loss: 22.11010087132454\n",
      "Training growing_up:  70%|█████▌  | 14043/20010 [11:12:23<5:57:25,  3.59s/batch]Batch 14100/20010 Done, mean position loss: 22.27223498821259\n",
      "Training growing_up:  70%|█████▌  | 14047/20010 [11:12:42<5:04:41,  3.07s/batch]Batch 14200/20010 Done, mean position loss: 23.081843938827515\n",
      "Training growing_up:  70%|█████▋  | 14078/20010 [11:12:44<5:10:21,  3.14s/batch]Batch 14000/20010 Done, mean position loss: 22.291934459209443\n",
      "Training growing_up:  71%|█████▋  | 14197/20010 [11:12:49<4:40:02,  2.89s/batch]Batch 14100/20010 Done, mean position loss: 22.787777326107026\n",
      "Training growing_up:  71%|█████▋  | 14189/20010 [11:13:01<5:00:21,  3.10s/batch]Batch 13900/20010 Done, mean position loss: 22.764307653903963\n",
      "Training growing_up:  70%|█████▌  | 14056/20010 [11:13:01<5:09:00,  3.11s/batch]Batch 13900/20010 Done, mean position loss: 22.08136159658432\n",
      "Training growing_up:  70%|█████▋  | 14071/20010 [11:13:02<5:02:14,  3.05s/batch]Batch 14200/20010 Done, mean position loss: 22.755996358394622\n",
      "Training growing_up:  72%|█████▋  | 14321/20010 [11:13:12<4:38:20,  2.94s/batch]Batch 14100/20010 Done, mean position loss: 22.950898196697235\n",
      "Training growing_up:  71%|█████▋  | 14193/20010 [11:13:14<5:12:50,  3.23s/batch]Batch 14400/20010 Done, mean position loss: 22.088014760017394\n",
      "Training growing_up:  71%|█████▋  | 14187/20010 [11:13:14<4:51:17,  3.00s/batch]Batch 14100/20010 Done, mean position loss: 22.714279453754425\n",
      "Training growing_up:  71%|█████▋  | 14109/20010 [11:13:37<5:02:50,  3.08s/batch]Batch 14200/20010 Done, mean position loss: 22.543325209617613\n",
      "Training growing_up:  69%|█████▌  | 13856/20010 [11:13:54<4:48:58,  2.82s/batch]Batch 14100/20010 Done, mean position loss: 23.23418292760849\n",
      "Training growing_up:  72%|█████▋  | 14335/20010 [11:13:58<5:19:21,  3.38s/batch]Batch 14200/20010 Done, mean position loss: 23.029880032539367\n",
      "Training growing_up:  71%|█████▋  | 14258/20010 [11:14:09<4:59:52,  3.13s/batch]Batch 14300/20010 Done, mean position loss: 22.37670919418335\n",
      "Training growing_up:  70%|█████▋  | 14092/20010 [11:14:11<5:06:29,  3.11s/batch]Batch 14200/20010 Done, mean position loss: 22.99503807067871\n",
      "Training growing_up:  69%|█████▌  | 13892/20010 [11:14:24<4:34:27,  2.69s/batch]Batch 14200/20010 Done, mean position loss: 22.105539145469667\n",
      "Training growing_up:  72%|█████▊  | 14426/20010 [11:14:29<4:50:29,  3.12s/batch]Batch 14100/20010 Done, mean position loss: 23.244283480644228\n",
      "Training growing_up:  71%|█████▋  | 14126/20010 [11:14:31<5:02:52,  3.09s/batch]Batch 14300/20010 Done, mean position loss: 23.010276427268984\n",
      "Training growing_up:  71%|█████▋  | 14135/20010 [11:14:37<4:48:40,  2.95s/batch]Batch 14000/20010 Done, mean position loss: 22.958840789794923\n",
      "Training growing_up:  71%|█████▋  | 14199/20010 [11:14:39<4:38:27,  2.88s/batch]Batch 14100/20010 Done, mean position loss: 22.363821845054627\n",
      "Training growing_up:  70%|█████▋  | 14101/20010 [11:14:39<5:07:01,  3.12s/batch]Batch 14200/20010 Done, mean position loss: 22.596941416263583\n",
      "Training growing_up:  71%|█████▋  | 14305/20010 [11:14:42<4:27:36,  2.81s/batch]Batch 14100/20010 Done, mean position loss: 22.85029926776886\n",
      "Training growing_up:  70%|█████▋  | 14106/20010 [11:14:45<5:15:13,  3.20s/batch]Batch 14200/20010 Done, mean position loss: 22.793053238391877\n",
      "Training growing_up:  72%|█████▋  | 14350/20010 [11:14:48<5:08:05,  3.27s/batch]Batch 14100/20010 Done, mean position loss: 23.11405258178711\n",
      "Training growing_up:  71%|█████▋  | 14205/20010 [11:14:49<4:04:12,  2.52s/batch]Batch 13900/20010 Done, mean position loss: 22.57211898803711\n",
      "Training growing_up:  71%|█████▋  | 14227/20010 [11:14:55<4:45:32,  2.96s/batch]Batch 14300/20010 Done, mean position loss: 22.749330735206605\n",
      "Training growing_up:  71%|█████▋  | 14216/20010 [11:14:56<4:49:00,  2.99s/batch]Batch 14200/20010 Done, mean position loss: 22.17735582590103\n",
      "Training growing_up:  71%|█████▋  | 14108/20010 [11:15:09<5:07:11,  3.12s/batch]Batch 14200/20010 Done, mean position loss: 22.744025540351863\n",
      "Training growing_up:  71%|█████▋  | 14224/20010 [11:15:20<4:46:10,  2.97s/batch]Batch 14100/20010 Done, mean position loss: 23.88197544336319\n",
      "Training growing_up:  71%|█████▋  | 14282/20010 [11:15:25<5:16:17,  3.31s/batch]Batch 14100/20010 Done, mean position loss: 22.5440083694458\n",
      "Training growing_up:  71%|█████▋  | 14240/20010 [11:15:36<5:27:39,  3.41s/batch]Batch 14200/20010 Done, mean position loss: 22.785573804378508\n",
      "Training growing_up:  72%|█████▋  | 14333/20010 [11:15:42<4:59:35,  3.17s/batch]Batch 14200/20010 Done, mean position loss: 23.402940504550934\n",
      "Training growing_up:  71%|█████▋  | 14212/20010 [11:16:16<4:51:29,  3.02s/batch]Batch 13900/20010 Done, mean position loss: 22.039945907592774\n",
      "Training growing_up:  71%|█████▋  | 14179/20010 [11:16:22<4:43:27,  2.92s/batch]Batch 14300/20010 Done, mean position loss: 22.198088009357452\n",
      "Batch 14100/20010 Done, mean position loss: 22.802513430118562\n",
      "Training growing_up:  71%|█████▋  | 14246/20010 [11:16:52<4:43:44,  2.95s/batch]Batch 14000/20010 Done, mean position loss: 22.3090274977684\n",
      "Training growing_up:  71%|█████▋  | 14226/20010 [11:17:00<5:16:13,  3.28s/batch]Batch 14000/20010 Done, mean position loss: 23.56664213657379\n",
      "Training growing_up:  71%|█████▋  | 14191/20010 [11:17:22<4:40:53,  2.90s/batch]Batch 14400/20010 Done, mean position loss: 22.905249993801117\n",
      "Training growing_up:  71%|█████▋  | 14271/20010 [11:17:25<4:45:24,  2.98s/batch]Batch 14300/20010 Done, mean position loss: 22.34364416122437\n",
      "Training growing_up:  70%|█████▌  | 13987/20010 [11:17:27<4:59:40,  2.99s/batch]Batch 14300/20010 Done, mean position loss: 21.788112630844118\n",
      "Batch 14200/20010 Done, mean position loss: 22.3005256152153\n",
      "Training growing_up:  70%|█████▌  | 13994/20010 [11:17:46<4:53:32,  2.93s/batch]Batch 14100/20010 Done, mean position loss: 22.4740047454834\n",
      "Training growing_up:  70%|█████▌  | 13931/20010 [11:17:49<4:34:12,  2.71s/batch]Batch 14300/20010 Done, mean position loss: 23.457115318775177\n",
      "Training growing_up:  71%|█████▋  | 14165/20010 [11:17:54<4:29:19,  2.76s/batch]Batch 14200/20010 Done, mean position loss: 22.711198341846465\n",
      "Training growing_up:  72%|█████▊  | 14415/20010 [11:18:02<3:45:16,  2.42s/batch]Batch 14000/20010 Done, mean position loss: 22.984677486419677\n",
      "Training growing_up:  71%|█████▋  | 14153/20010 [11:18:03<4:24:59,  2.71s/batch]Batch 14000/20010 Done, mean position loss: 21.800996189117434\n",
      "Training growing_up:  72%|█████▋  | 14335/20010 [11:18:05<4:19:15,  2.74s/batch]Batch 14300/20010 Done, mean position loss: 22.602231714725495\n",
      "Training growing_up:  71%|█████▋  | 14262/20010 [11:18:10<3:54:30,  2.45s/batch]Batch 14500/20010 Done, mean position loss: 22.219326515197753\n",
      "Training growing_up:  72%|█████▋  | 14319/20010 [11:18:14<3:56:57,  2.50s/batch]Batch 14200/20010 Done, mean position loss: 22.999798521995544\n",
      "Training growing_up:  71%|█████▋  | 14265/20010 [11:18:18<4:02:20,  2.53s/batch]Batch 14200/20010 Done, mean position loss: 22.695260334014893\n",
      "Training growing_up:  72%|█████▋  | 14346/20010 [11:18:33<4:10:21,  2.65s/batch]Batch 14300/20010 Done, mean position loss: 22.39763595342636\n",
      "Training growing_up:  70%|█████▌  | 14018/20010 [11:18:45<4:23:13,  2.64s/batch]Batch 14200/20010 Done, mean position loss: 22.905559239387514\n",
      "Training growing_up:  71%|█████▋  | 14270/20010 [11:18:49<4:05:43,  2.57s/batch]Batch 14300/20010 Done, mean position loss: 22.72228740692139\n",
      "Training growing_up:  71%|█████▋  | 14191/20010 [11:18:53<4:13:45,  2.62s/batch]Batch 14400/20010 Done, mean position loss: 22.452977833747862\n",
      "Training growing_up:  72%|█████▋  | 14326/20010 [11:18:57<4:26:32,  2.81s/batch]Batch 14300/20010 Done, mean position loss: 22.98600692987442\n",
      "Training growing_up:  71%|█████▋  | 14307/20010 [11:19:12<4:12:56,  2.66s/batch]Batch 14300/20010 Done, mean position loss: 22.14412695169449\n",
      "Training growing_up:  71%|█████▋  | 14295/20010 [11:19:18<3:36:21,  2.27s/batch]Batch 14200/20010 Done, mean position loss: 23.79042592048645\n",
      "Training growing_up:  71%|█████▋  | 14201/20010 [11:19:18<3:55:56,  2.44s/batch]Batch 14400/20010 Done, mean position loss: 23.008932695388793\n",
      "Training growing_up:  70%|█████▋  | 14099/20010 [11:19:25<3:36:42,  2.20s/batch]Batch 14300/20010 Done, mean position loss: 22.902995839118958\n",
      "Training growing_up:  70%|█████▌  | 14034/20010 [11:19:28<4:49:51,  2.91s/batch]Batch 14200/20010 Done, mean position loss: 22.509173285961154\n",
      "Training growing_up:  71%|█████▋  | 14231/20010 [11:19:30<3:42:30,  2.31s/batch]Batch 14100/20010 Done, mean position loss: 23.209869706630705\n",
      "Training growing_up:  72%|█████▋  | 14350/20010 [11:19:32<3:31:27,  2.24s/batch]Batch 14300/20010 Done, mean position loss: 22.606110711097717\n",
      "Training growing_up:  72%|█████▋  | 14316/20010 [11:19:36<4:10:32,  2.64s/batch]Batch 14200/20010 Done, mean position loss: 22.836250612735746\n",
      "Training growing_up:  71%|█████▋  | 14252/20010 [11:19:36<4:13:47,  2.64s/batch]Batch 14000/20010 Done, mean position loss: 22.864541461467745\n",
      "Training growing_up:  72%|█████▊  | 14450/20010 [11:19:36<4:28:38,  2.90s/batch]Batch 14200/20010 Done, mean position loss: 22.869137337207796\n",
      "Training growing_up:  71%|█████▋  | 14289/20010 [11:19:39<3:58:57,  2.51s/batch]Batch 14300/20010 Done, mean position loss: 22.392380349636078\n",
      "Training growing_up:  70%|█████▌  | 14058/20010 [11:19:44<3:44:50,  2.27s/batch]Batch 14400/20010 Done, mean position loss: 22.37487326860428\n",
      "Training growing_up:  72%|█████▊  | 14455/20010 [11:19:50<4:20:10,  2.81s/batch]Batch 14300/20010 Done, mean position loss: 22.659291813373564\n",
      "Training growing_up:  72%|█████▋  | 14313/20010 [11:19:54<4:05:51,  2.59s/batch]Batch 14200/20010 Done, mean position loss: 23.66240262031555\n",
      "Training growing_up:  72%|█████▋  | 14316/20010 [11:20:13<4:38:42,  2.94s/batch]Batch 14300/20010 Done, mean position loss: 23.051539220809936\n",
      "Training growing_up:  70%|█████▋  | 14071/20010 [11:20:20<4:44:59,  2.88s/batch]Batch 14200/20010 Done, mean position loss: 22.54305746793747\n",
      "Training growing_up:  70%|█████▋  | 14073/20010 [11:20:27<4:56:48,  3.00s/batch]Batch 14300/20010 Done, mean position loss: 23.12649803161621\n",
      "Training growing_up:  71%|█████▋  | 14267/20010 [11:20:55<4:17:47,  2.69s/batch]Batch 14200/20010 Done, mean position loss: 22.993617973327638\n",
      "Training growing_up:  71%|█████▋  | 14239/20010 [11:21:03<4:46:00,  2.97s/batch]Batch 14400/20010 Done, mean position loss: 22.392568230628967\n",
      "Training growing_up:  72%|█████▋  | 14328/20010 [11:21:10<4:26:45,  2.82s/batch]Batch 14000/20010 Done, mean position loss: 21.96290053844452\n",
      "Training growing_up:  72%|█████▋  | 14359/20010 [11:21:30<5:00:53,  3.19s/batch]Batch 14100/20010 Done, mean position loss: 22.825220487117768\n",
      "Training growing_up:  71%|█████▋  | 14285/20010 [11:21:49<4:41:25,  2.95s/batch]Batch 14100/20010 Done, mean position loss: 23.636297087669373\n",
      "Training growing_up:  72%|█████▊  | 14500/20010 [11:22:00<4:38:24,  3.03s/batch]Batch 14400/20010 Done, mean position loss: 21.87871229171753\n",
      "Training growing_up:  72%|█████▊  | 14468/20010 [11:22:02<4:34:51,  2.98s/batch]Batch 14500/20010 Done, mean position loss: 22.897612628936766\n",
      "Training growing_up:  72%|█████▋  | 14340/20010 [11:22:04<4:45:15,  3.02s/batch]Batch 14300/20010 Done, mean position loss: 22.02197085618973\n",
      "Training growing_up:  70%|█████▋  | 14086/20010 [11:22:06<5:15:13,  3.19s/batch]Batch 14400/20010 Done, mean position loss: 22.21514662027359\n",
      "Training growing_up:  73%|█████▊  | 14595/20010 [11:22:29<4:16:39,  2.84s/batch]Batch 14200/20010 Done, mean position loss: 22.326336154937742\n",
      "Training growing_up:  72%|█████▋  | 14345/20010 [11:22:34<5:01:10,  3.19s/batch]Batch 14400/20010 Done, mean position loss: 23.21812418937683\n",
      "Training growing_up:  72%|█████▋  | 14373/20010 [11:22:37<4:27:09,  2.84s/batch]Batch 14300/20010 Done, mean position loss: 23.0689537191391\n",
      "Training growing_up:  73%|█████▊  | 14516/20010 [11:22:46<4:24:01,  2.88s/batch]Batch 14400/20010 Done, mean position loss: 22.588596885204318\n",
      "Training growing_up:  72%|█████▋  | 14360/20010 [11:22:48<5:07:40,  3.27s/batch]Batch 14600/20010 Done, mean position loss: 22.00547237634659\n",
      "Training growing_up:  72%|█████▊  | 14466/20010 [11:22:51<4:11:09,  2.72s/batch]Batch 14100/20010 Done, mean position loss: 21.621156401634217\n",
      "Training growing_up:  71%|█████▋  | 14296/20010 [11:22:52<4:55:47,  3.11s/batch]Batch 14100/20010 Done, mean position loss: 23.076981041431427\n",
      "Training growing_up:  71%|█████▋  | 14210/20010 [11:22:57<5:14:23,  3.25s/batch]Batch 14300/20010 Done, mean position loss: 22.951311376094818\n",
      "Training growing_up:  71%|█████▋  | 14305/20010 [11:23:07<4:03:56,  2.57s/batch]Batch 14300/20010 Done, mean position loss: 22.490067982673644\n",
      "Training growing_up:  72%|█████▊  | 14483/20010 [11:23:16<4:20:23,  2.83s/batch]Batch 14400/20010 Done, mean position loss: 22.376409180164337\n",
      "Training growing_up:  71%|█████▋  | 14181/20010 [11:23:31<4:42:57,  2.91s/batch]Batch 14400/20010 Done, mean position loss: 22.952730383872986\n",
      "Training growing_up:  71%|█████▋  | 14274/20010 [11:23:35<5:00:32,  3.14s/batch]Batch 14500/20010 Done, mean position loss: 22.13664061307907\n",
      "Training growing_up:  72%|█████▊  | 14389/20010 [11:23:37<4:49:55,  3.09s/batch]Batch 14300/20010 Done, mean position loss: 22.93777562379837\n",
      "Training growing_up:  72%|█████▊  | 14493/20010 [11:23:45<4:16:27,  2.79s/batch]Batch 14400/20010 Done, mean position loss: 23.395442008972168\n",
      "Training growing_up:  72%|█████▊  | 14488/20010 [11:23:56<4:43:12,  3.08s/batch]Batch 14400/20010 Done, mean position loss: 21.97414986848831\n",
      "Training growing_up:  72%|█████▊  | 14464/20010 [11:24:07<4:31:35,  2.94s/batch]Batch 14300/20010 Done, mean position loss: 23.170972669124602\n",
      "Training growing_up:  71%|█████▋  | 14286/20010 [11:24:09<4:41:57,  2.96s/batch]Batch 14500/20010 Done, mean position loss: 23.331282954216004\n",
      "Training growing_up:  71%|█████▋  | 14294/20010 [11:24:14<4:23:18,  2.76s/batch]Batch 14400/20010 Done, mean position loss: 22.499706411361693\n",
      "Training growing_up:  72%|█████▊  | 14434/20010 [11:24:21<4:39:40,  3.01s/batch]Batch 14400/20010 Done, mean position loss: 22.85396369934082\n",
      "Training growing_up:  72%|█████▋  | 14317/20010 [11:24:26<5:05:58,  3.22s/batch]Batch 14300/20010 Done, mean position loss: 22.43211387395859\n",
      "Training growing_up:  71%|█████▋  | 14241/20010 [11:24:31<5:03:23,  3.16s/batch]Batch 14400/20010 Done, mean position loss: 22.351095695495605\n",
      "Training growing_up:  71%|█████▋  | 14155/20010 [11:24:33<4:45:28,  2.93s/batch]Batch 14200/20010 Done, mean position loss: 22.890275797843934\n",
      "Training growing_up:  72%|█████▊  | 14500/20010 [11:24:34<4:37:20,  3.02s/batch]Batch 14300/20010 Done, mean position loss: 22.61508999109268\n",
      "Training growing_up:  72%|█████▊  | 14422/20010 [11:24:35<4:22:27,  2.82s/batch]Batch 14100/20010 Done, mean position loss: 22.496342945098874\n",
      "Training growing_up:  70%|█████▋  | 14102/20010 [11:24:37<4:42:55,  2.87s/batch]Batch 14500/20010 Done, mean position loss: 22.092998840808868\n",
      "Batch 14300/20010 Done, mean position loss: 22.73255595445633\n",
      "Training growing_up:  72%|█████▊  | 14505/20010 [11:24:48<4:14:09,  2.77s/batch]Batch 14400/20010 Done, mean position loss: 22.498099031448366\n",
      "Training growing_up:  73%|█████▊  | 14527/20010 [11:24:54<4:32:21,  2.98s/batch]Batch 14300/20010 Done, mean position loss: 23.507418208122253\n",
      "Training growing_up:  71%|█████▋  | 14283/20010 [11:24:59<4:55:28,  3.10s/batch]Batch 14400/20010 Done, mean position loss: 23.095274641513825\n",
      "Training growing_up:  72%|█████▊  | 14416/20010 [11:25:16<4:34:18,  2.94s/batch]Batch 14300/20010 Done, mean position loss: 22.557703816890715\n",
      "Training growing_up:  71%|█████▋  | 14150/20010 [11:25:20<4:42:54,  2.90s/batch]Batch 14400/20010 Done, mean position loss: 23.395122623443605\n",
      "Training growing_up:  72%|█████▊  | 14455/20010 [11:25:52<4:46:17,  3.09s/batch]Batch 14300/20010 Done, mean position loss: 22.986474604606627\n",
      "Training growing_up:  73%|█████▊  | 14538/20010 [11:25:58<4:18:23,  2.83s/batch]Batch 14500/20010 Done, mean position loss: 22.11904976129532\n",
      "Training growing_up:  72%|█████▊  | 14454/20010 [11:26:11<4:28:08,  2.90s/batch]Batch 14100/20010 Done, mean position loss: 22.114954938888552\n",
      "Training growing_up:  71%|█████▋  | 14279/20010 [11:26:34<4:58:09,  3.12s/batch]Batch 14200/20010 Done, mean position loss: 22.352869784832002\n",
      "Training growing_up:  72%|█████▊  | 14432/20010 [11:26:58<4:46:33,  3.08s/batch]Batch 14200/20010 Done, mean position loss: 23.606463367938996\n",
      "Training growing_up:  72%|█████▊  | 14453/20010 [11:27:02<4:36:42,  2.99s/batch]Batch 14500/20010 Done, mean position loss: 21.95701430559158\n",
      "Training growing_up:  71%|█████▋  | 14119/20010 [11:27:03<4:37:35,  2.83s/batch]Batch 14600/20010 Done, mean position loss: 22.89966603755951\n",
      "Training growing_up:  72%|█████▊  | 14461/20010 [11:27:04<4:56:34,  3.21s/batch]Batch 14400/20010 Done, mean position loss: 22.01302868127823\n",
      "Training growing_up:  73%|█████▊  | 14604/20010 [11:27:12<4:41:19,  3.12s/batch]Batch 14500/20010 Done, mean position loss: 22.1928950047493\n",
      "Training growing_up:  71%|█████▋  | 14217/20010 [11:27:30<5:18:04,  3.29s/batch]Batch 14400/20010 Done, mean position loss: 22.44890722513199\n",
      "Training growing_up:  72%|█████▊  | 14404/20010 [11:27:38<4:29:35,  2.89s/batch]Batch 14300/20010 Done, mean position loss: 21.994498114585877\n",
      "Training growing_up:  73%|█████▊  | 14536/20010 [11:27:39<4:12:02,  2.76s/batch]Batch 14500/20010 Done, mean position loss: 23.208131775856017\n",
      "Training growing_up:  72%|█████▊  | 14447/20010 [11:27:41<4:07:51,  2.67s/batch]Batch 14700/20010 Done, mean position loss: 22.081214923858646\n",
      "Training growing_up:  72%|█████▊  | 14448/20010 [11:27:44<4:11:48,  2.72s/batch]Batch 14500/20010 Done, mean position loss: 22.66307445526123\n",
      "Training growing_up:  71%|█████▋  | 14307/20010 [11:27:56<4:44:12,  2.99s/batch]Batch 14200/20010 Done, mean position loss: 23.117178423404695\n",
      "Training growing_up:  72%|█████▋  | 14377/20010 [11:27:57<4:36:06,  2.94s/batch]Batch 14200/20010 Done, mean position loss: 21.847515709400177\n",
      "Training growing_up:  71%|█████▋  | 14139/20010 [11:28:01<4:40:58,  2.87s/batch]Batch 14400/20010 Done, mean position loss: 22.84562061309815\n",
      "Training growing_up:  73%|█████▊  | 14570/20010 [11:28:07<4:16:26,  2.83s/batch]Batch 14400/20010 Done, mean position loss: 22.359059624671936\n",
      "Training growing_up:  72%|█████▋  | 14348/20010 [11:28:07<4:56:47,  3.15s/batch]Batch 14500/20010 Done, mean position loss: 22.31293613433838\n",
      "Training growing_up:  72%|█████▊  | 14474/20010 [11:28:32<4:34:52,  2.98s/batch]Batch 14500/20010 Done, mean position loss: 22.83967166900635\n",
      "Training growing_up:  72%|█████▊  | 14389/20010 [11:28:33<4:48:35,  3.08s/batch]Batch 14600/20010 Done, mean position loss: 22.19159951210022\n",
      "Training growing_up:  72%|█████▊  | 14479/20010 [11:28:48<4:58:59,  3.24s/batch]Batch 14400/20010 Done, mean position loss: 23.01390014886856\n",
      "Training growing_up:  71%|█████▋  | 14155/20010 [11:28:48<4:27:20,  2.74s/batch]Batch 14500/20010 Done, mean position loss: 23.228215579986575\n",
      "Training growing_up:  73%|█████▊  | 14589/20010 [11:29:04<4:11:53,  2.79s/batch]Batch 14600/20010 Done, mean position loss: 22.815153982639313\n",
      "Training growing_up:  72%|█████▊  | 14441/20010 [11:29:04<4:40:47,  3.03s/batch]Batch 14500/20010 Done, mean position loss: 22.163481743335726\n",
      "Training growing_up:  72%|█████▊  | 14394/20010 [11:29:09<5:07:14,  3.28s/batch]Batch 14400/20010 Done, mean position loss: 23.364995927810668\n",
      "Training growing_up:  73%|█████▊  | 14645/20010 [11:29:13<4:09:51,  2.79s/batch]Batch 14500/20010 Done, mean position loss: 22.60091634273529\n",
      "Training growing_up:  73%|█████▊  | 14543/20010 [11:29:23<4:27:13,  2.93s/batch]Batch 14500/20010 Done, mean position loss: 22.733274219036105\n",
      "Training growing_up:  73%|█████▊  | 14537/20010 [11:29:29<5:01:50,  3.31s/batch]Batch 14400/20010 Done, mean position loss: 22.187470870018004\n",
      "Training growing_up:  71%|█████▋  | 14300/20010 [11:29:31<5:14:58,  3.31s/batch]Batch 14500/20010 Done, mean position loss: 22.452651715278627\n",
      "Training growing_up:  74%|█████▉  | 14739/20010 [11:29:33<4:14:38,  2.90s/batch]Batch 14300/20010 Done, mean position loss: 22.733504951000214\n",
      "Training growing_up:  72%|█████▊  | 14506/20010 [11:29:37<4:14:50,  2.78s/batch]Batch 14400/20010 Done, mean position loss: 22.484932103157046\n",
      "Training growing_up:  72%|█████▊  | 14454/20010 [11:29:40<4:38:58,  3.01s/batch]Batch 14400/20010 Done, mean position loss: 22.952493026256562\n",
      "Training growing_up:  73%|█████▊  | 14541/20010 [11:29:41<4:47:39,  3.16s/batch]Batch 14600/20010 Done, mean position loss: 22.362980296611788\n",
      "Training growing_up:  71%|█████▋  | 14236/20010 [11:29:44<5:01:15,  3.13s/batch]Batch 14200/20010 Done, mean position loss: 22.62425498008728\n",
      "Training growing_up:  73%|█████▊  | 14545/20010 [11:29:54<4:05:01,  2.69s/batch]Batch 14500/20010 Done, mean position loss: 22.622958526611328\n",
      "Training growing_up:  72%|█████▊  | 14384/20010 [11:29:56<3:59:12,  2.55s/batch]Batch 14400/20010 Done, mean position loss: 23.74846942901611\n",
      "Training growing_up:  73%|█████▊  | 14584/20010 [11:30:00<4:09:46,  2.76s/batch]Batch 14500/20010 Done, mean position loss: 22.99651378154755\n",
      "Training growing_up:  73%|█████▊  | 14553/20010 [11:30:16<4:08:06,  2.73s/batch]Batch 14400/20010 Done, mean position loss: 22.495510423183443\n",
      "Training growing_up:  72%|█████▊  | 14426/20010 [11:30:26<4:23:42,  2.83s/batch]Batch 14500/20010 Done, mean position loss: 23.25989271879196\n",
      "Training growing_up:  73%|█████▊  | 14541/20010 [11:30:49<4:26:29,  2.92s/batch]Batch 14400/20010 Done, mean position loss: 23.096171665191648\n",
      "Training growing_up:  73%|█████▊  | 14565/20010 [11:30:52<4:33:21,  3.01s/batch]Batch 14600/20010 Done, mean position loss: 21.91556885719299\n",
      "Training growing_up:  73%|█████▊  | 14643/20010 [11:31:13<4:26:42,  2.98s/batch]Batch 14200/20010 Done, mean position loss: 21.78945763349533\n",
      "Training growing_up:  72%|█████▊  | 14439/20010 [11:31:51<4:36:36,  2.98s/batch]Batch 14300/20010 Done, mean position loss: 22.441827142238616\n",
      "Training growing_up:  73%|█████▊  | 14586/20010 [11:31:56<4:12:11,  2.79s/batch]Batch 14500/20010 Done, mean position loss: 22.145712299346926\n",
      "Training growing_up:  71%|█████▋  | 14297/20010 [11:31:57<4:20:20,  2.73s/batch]Batch 14700/20010 Done, mean position loss: 22.78786014318466\n",
      "Training growing_up:  73%|█████▊  | 14550/20010 [11:32:03<4:34:19,  3.01s/batch]Batch 14600/20010 Done, mean position loss: 21.929293985366822\n",
      "Training growing_up:  73%|█████▊  | 14557/20010 [11:32:08<4:00:21,  2.64s/batch]Batch 14300/20010 Done, mean position loss: 23.35450792312622\n",
      "Training growing_up:  74%|█████▉  | 14710/20010 [11:32:20<3:29:11,  2.37s/batch]Batch 14600/20010 Done, mean position loss: 22.055229308605192\n",
      "Training growing_up:  73%|█████▊  | 14554/20010 [11:32:29<3:53:39,  2.57s/batch]Batch 14500/20010 Done, mean position loss: 22.44446195602417\n",
      "Training growing_up:  71%|█████▋  | 14293/20010 [11:32:34<4:27:29,  2.81s/batch]Batch 14400/20010 Done, mean position loss: 22.041752638816835\n",
      "Training growing_up:  73%|█████▊  | 14555/20010 [11:32:37<4:29:36,  2.97s/batch]Batch 14600/20010 Done, mean position loss: 22.35780174255371\n",
      "Training growing_up:  72%|█████▋  | 14317/20010 [11:32:37<4:34:09,  2.89s/batch]Batch 14600/20010 Done, mean position loss: 23.15138293981552\n",
      "Training growing_up:  73%|█████▊  | 14602/20010 [11:32:39<3:50:45,  2.56s/batch]Batch 14800/20010 Done, mean position loss: 21.955434837341308\n",
      "Training growing_up:  72%|█████▋  | 14368/20010 [11:32:55<4:30:12,  2.87s/batch]Batch 14500/20010 Done, mean position loss: 22.966155269145965\n",
      "Batch 14300/20010 Done, mean position loss: 23.096371133327484\n",
      "Training growing_up:  72%|█████▊  | 14468/20010 [11:32:56<3:48:03,  2.47s/batch]Batch 14300/20010 Done, mean position loss: 21.69799474477768\n",
      "Training growing_up:  72%|█████▊  | 14446/20010 [11:32:57<4:19:12,  2.80s/batch]Batch 14600/20010 Done, mean position loss: 22.303404376506805\n",
      "Training growing_up:  73%|█████▉  | 14696/20010 [11:33:06<4:00:34,  2.72s/batch]Batch 14500/20010 Done, mean position loss: 22.767748479843142\n",
      "Training growing_up:  73%|█████▊  | 14593/20010 [11:33:15<4:05:12,  2.72s/batch]Batch 14600/20010 Done, mean position loss: 22.75622565984726\n",
      "Training growing_up:  73%|█████▊  | 14520/20010 [11:33:18<3:58:09,  2.60s/batch]Batch 14700/20010 Done, mean position loss: 22.29461785554886\n",
      "Training growing_up:  73%|█████▊  | 14588/20010 [11:33:37<4:18:41,  2.86s/batch]Batch 14600/20010 Done, mean position loss: 22.931144571304323\n",
      "Training growing_up:  73%|█████▊  | 14540/20010 [11:33:42<4:29:13,  2.95s/batch]Batch 14500/20010 Done, mean position loss: 22.787823362350466\n",
      "Training growing_up:  73%|█████▊  | 14519/20010 [11:33:53<3:52:16,  2.54s/batch]Batch 14700/20010 Done, mean position loss: 23.21968522310257\n",
      "Training growing_up:  73%|█████▊  | 14630/20010 [11:33:53<3:48:17,  2.55s/batch]Batch 14600/20010 Done, mean position loss: 22.1142427945137\n",
      "Training growing_up:  73%|█████▊  | 14626/20010 [11:33:59<3:35:12,  2.40s/batch]Batch 14500/20010 Done, mean position loss: 23.29040596246719\n",
      "Training growing_up:  72%|█████▊  | 14502/20010 [11:34:02<3:48:53,  2.49s/batch]Batch 14600/20010 Done, mean position loss: 22.67820690870285\n",
      "Training growing_up:  74%|█████▉  | 14752/20010 [11:34:11<3:38:38,  2.49s/batch]Batch 14600/20010 Done, mean position loss: 22.697086625099182\n",
      "Training growing_up:  71%|█████▋  | 14295/20010 [11:34:17<4:23:00,  2.76s/batch]Batch 14500/20010 Done, mean position loss: 22.249396419525148\n",
      "Batch 14600/20010 Done, mean position loss: 22.40186060190201\n",
      "Training growing_up:  73%|█████▊  | 14618/20010 [11:34:20<3:48:33,  2.54s/batch]Batch 14700/20010 Done, mean position loss: 22.277456188201903\n",
      "Training growing_up:  73%|█████▊  | 14634/20010 [11:34:21<3:40:15,  2.46s/batch]Batch 14500/20010 Done, mean position loss: 22.830218760967256\n",
      "Training growing_up:  74%|█████▉  | 14757/20010 [11:34:23<3:42:31,  2.54s/batch]Batch 14400/20010 Done, mean position loss: 22.765149631500243\n",
      "Training growing_up:  73%|█████▊  | 14646/20010 [11:34:27<3:54:58,  2.63s/batch]Batch 14500/20010 Done, mean position loss: 22.782658774852752\n",
      "Training growing_up:  71%|█████▋  | 14300/20010 [11:34:29<3:53:00,  2.45s/batch]Batch 14600/20010 Done, mean position loss: 22.80497358083725\n",
      "Training growing_up:  73%|█████▊  | 14513/20010 [11:34:30<4:02:10,  2.64s/batch]Batch 14500/20010 Done, mean position loss: 23.505010204315184\n",
      "Training growing_up:  73%|█████▊  | 14588/20010 [11:34:32<4:14:50,  2.82s/batch]Batch 14300/20010 Done, mean position loss: 22.44927519321442\n",
      "Training growing_up:  73%|█████▊  | 14619/20010 [11:34:41<3:52:14,  2.58s/batch]Batch 14600/20010 Done, mean position loss: 22.994575319290163\n",
      "Training growing_up:  73%|█████▊  | 14669/20010 [11:34:59<3:56:16,  2.65s/batch]Batch 14500/20010 Done, mean position loss: 22.509371795654296\n",
      "Training growing_up:  74%|█████▉  | 14729/20010 [11:35:07<3:19:40,  2.27s/batch]Batch 14600/20010 Done, mean position loss: 23.585131511688232\n",
      "Training growing_up:  73%|█████▊  | 14607/20010 [11:35:22<4:05:27,  2.73s/batch]Batch 14500/20010 Done, mean position loss: 23.308713586330413\n",
      "Training growing_up:  72%|█████▊  | 14425/20010 [11:35:24<3:44:03,  2.41s/batch]Batch 14700/20010 Done, mean position loss: 22.04020318031311\n",
      "Training growing_up:  73%|█████▊  | 14531/20010 [11:35:47<4:32:26,  2.98s/batch]Batch 14300/20010 Done, mean position loss: 21.68682175397873\n",
      "Training growing_up:  74%|█████▉  | 14755/20010 [11:36:20<3:59:13,  2.73s/batch]Batch 14800/20010 Done, mean position loss: 22.527631752490997\n",
      "Training growing_up:  74%|█████▉  | 14773/20010 [11:36:28<4:06:46,  2.83s/batch]Batch 14700/20010 Done, mean position loss: 21.82244045972824\n",
      "Training growing_up:  74%|█████▉  | 14806/20010 [11:36:33<3:42:15,  2.56s/batch]Batch 14600/20010 Done, mean position loss: 22.110775692462923\n",
      "Batch 14400/20010 Done, mean position loss: 22.6998841047287\n",
      "Training growing_up:  73%|█████▉  | 14704/20010 [11:36:35<3:51:13,  2.61s/batch]Batch 14400/20010 Done, mean position loss: 23.18076151371002\n",
      "Training growing_up:  73%|█████▊  | 14592/20010 [11:36:57<4:09:10,  2.76s/batch]Batch 14700/20010 Done, mean position loss: 22.056443021297454\n",
      "Training growing_up:  72%|█████▊  | 14498/20010 [11:37:02<4:17:34,  2.80s/batch]Batch 14600/20010 Done, mean position loss: 22.533233432769777\n",
      "Training growing_up:  73%|█████▊  | 14536/20010 [11:37:06<4:30:18,  2.96s/batch]Batch 14700/20010 Done, mean position loss: 22.72956637620926\n",
      "Training growing_up:  74%|█████▉  | 14738/20010 [11:37:09<3:45:51,  2.57s/batch]Batch 14700/20010 Done, mean position loss: 23.165209715366366\n",
      "Training growing_up:  73%|█████▊  | 14661/20010 [11:37:09<4:02:57,  2.73s/batch]Batch 14900/20010 Done, mean position loss: 21.89117022037506\n",
      "Training growing_up:  73%|█████▊  | 14615/20010 [11:37:11<4:09:11,  2.77s/batch]Batch 14500/20010 Done, mean position loss: 21.788890647888184\n",
      "Training growing_up:  73%|█████▊  | 14552/20010 [11:37:23<4:09:39,  2.74s/batch]Batch 14600/20010 Done, mean position loss: 22.938101427555083\n",
      "Training growing_up:  73%|█████▊  | 14569/20010 [11:37:28<4:27:55,  2.95s/batch]Batch 14400/20010 Done, mean position loss: 21.551831102371217\n",
      "Training growing_up:  73%|█████▊  | 14693/20010 [11:37:29<3:54:48,  2.65s/batch]Batch 14700/20010 Done, mean position loss: 22.305490918159485\n",
      "Training growing_up:  73%|█████▊  | 14558/20010 [11:37:40<4:22:32,  2.89s/batch]Batch 14400/20010 Done, mean position loss: 22.770056092739107\n",
      "Training growing_up:  73%|█████▊  | 14680/20010 [11:37:48<4:21:10,  2.94s/batch]Batch 14800/20010 Done, mean position loss: 22.166380643844604\n",
      "Batch 14600/20010 Done, mean position loss: 22.4075176858902\n",
      "Training growing_up:  74%|█████▉  | 14803/20010 [11:37:53<3:52:55,  2.68s/batch]Batch 14700/20010 Done, mean position loss: 22.873297300338745\n",
      "Training growing_up:  73%|█████▊  | 14677/20010 [11:38:11<4:20:58,  2.94s/batch]Batch 14700/20010 Done, mean position loss: 22.82192351818085\n",
      "Training growing_up:  73%|█████▊  | 14685/20010 [11:38:21<4:33:43,  3.08s/batch]Batch 14600/20010 Done, mean position loss: 23.319167737960814\n",
      "Training growing_up:  73%|█████▊  | 14592/20010 [11:38:33<4:11:12,  2.78s/batch]Batch 14800/20010 Done, mean position loss: 22.954604058265687\n",
      "Training growing_up:  74%|█████▉  | 14801/20010 [11:38:34<4:27:39,  3.08s/batch]Batch 14700/20010 Done, mean position loss: 22.10021171092987\n",
      "Training growing_up:  73%|█████▊  | 14646/20010 [11:38:41<4:24:12,  2.96s/batch]Batch 14600/20010 Done, mean position loss: 23.616334161758424\n",
      "Training growing_up:  74%|█████▉  | 14726/20010 [11:38:43<4:51:42,  3.31s/batch]Batch 14700/20010 Done, mean position loss: 22.699190526008607\n",
      "Training growing_up:  74%|█████▉  | 14773/20010 [11:38:48<3:49:05,  2.62s/batch]Batch 14700/20010 Done, mean position loss: 22.682321639060973\n",
      "Training growing_up:  73%|█████▊  | 14592/20010 [11:38:49<4:28:05,  2.97s/batch]Batch 14800/20010 Done, mean position loss: 22.1951482796669\n",
      "Training growing_up:  73%|█████▊  | 14595/20010 [11:38:58<4:35:57,  3.06s/batch]Batch 14600/20010 Done, mean position loss: 22.13834807634354\n",
      "Training growing_up:  72%|█████▋  | 14370/20010 [11:39:05<4:33:39,  2.91s/batch]Batch 14700/20010 Done, mean position loss: 22.759591097831727\n",
      "Batch 14500/20010 Done, mean position loss: 22.609897284507753\n",
      "Training growing_up:  73%|█████▊  | 14685/20010 [11:39:08<4:41:57,  3.18s/batch]Batch 14600/20010 Done, mean position loss: 22.99428602218628\n",
      "Training growing_up:  72%|█████▊  | 14453/20010 [11:39:08<4:55:22,  3.19s/batch]Batch 14600/20010 Done, mean position loss: 22.735237698554993\n",
      "Training growing_up:  74%|█████▉  | 14742/20010 [11:39:08<4:16:19,  2.92s/batch]Batch 14700/20010 Done, mean position loss: 22.591604430675506\n",
      "Training growing_up:  74%|█████▉  | 14745/20010 [11:39:17<4:19:53,  2.96s/batch]Batch 14600/20010 Done, mean position loss: 23.616057965755463\n",
      "Training growing_up:  74%|█████▉  | 14713/20010 [11:39:22<4:13:37,  2.87s/batch]Batch 14700/20010 Done, mean position loss: 22.96714203119278\n",
      "Training growing_up:  74%|█████▉  | 14834/20010 [11:39:24<4:09:47,  2.90s/batch]Batch 14400/20010 Done, mean position loss: 22.775925529003146\n",
      "Training growing_up:  73%|█████▊  | 14630/20010 [11:39:46<4:01:24,  2.69s/batch]Batch 14600/20010 Done, mean position loss: 22.641356775760652\n",
      "Training growing_up:  73%|█████▊  | 14620/20010 [11:39:53<4:45:26,  3.18s/batch]Batch 14700/20010 Done, mean position loss: 23.624626610279087\n",
      "Training growing_up:  73%|█████▊  | 14651/20010 [11:40:12<4:31:15,  3.04s/batch]Batch 14800/20010 Done, mean position loss: 21.853293647766115\n",
      "Training growing_up:  74%|█████▉  | 14853/20010 [11:40:20<4:58:28,  3.47s/batch]Batch 14600/20010 Done, mean position loss: 22.925040678977965\n",
      "Training growing_up:  73%|█████▊  | 14572/20010 [11:40:37<4:12:24,  2.79s/batch]Batch 14400/20010 Done, mean position loss: 21.78740742921829\n",
      "Training growing_up:  73%|█████▉  | 14697/20010 [11:41:14<4:00:32,  2.72s/batch]Batch 14900/20010 Done, mean position loss: 22.957900166511536\n",
      "Training growing_up:  74%|█████▉  | 14757/20010 [11:41:21<5:02:55,  3.46s/batch]Batch 14800/20010 Done, mean position loss: 21.944114022254944\n",
      "Training growing_up:  73%|█████▊  | 14658/20010 [11:41:29<4:10:04,  2.80s/batch]Batch 14700/20010 Done, mean position loss: 22.41394107341766\n",
      "Training growing_up:  73%|█████▊  | 14626/20010 [11:41:32<4:11:09,  2.80s/batch]Batch 14500/20010 Done, mean position loss: 22.20258245944977\n",
      "Training growing_up:  72%|█████▊  | 14444/20010 [11:41:35<4:43:11,  3.05s/batch]Batch 14500/20010 Done, mean position loss: 23.056607282161714\n",
      "Training growing_up:  73%|█████▊  | 14685/20010 [11:41:54<4:36:47,  3.12s/batch]Batch 14800/20010 Done, mean position loss: 22.12014743328095\n",
      "Training growing_up:  72%|█████▊  | 14485/20010 [11:42:00<5:32:12,  3.61s/batch]Batch 14700/20010 Done, mean position loss: 22.63742222070694\n",
      "Training growing_up:  73%|█████▊  | 14559/20010 [11:42:03<5:18:46,  3.51s/batch]Batch 14800/20010 Done, mean position loss: 23.087374355792996\n",
      "Training growing_up:  74%|█████▉  | 14745/20010 [11:42:04<4:02:52,  2.77s/batch]Batch 14600/20010 Done, mean position loss: 22.198477747440336\n",
      "Training growing_up:  74%|█████▉  | 14874/20010 [11:42:04<4:22:38,  3.07s/batch]Batch 15000/20010 Done, mean position loss: 22.00988377571106\n",
      "Training growing_up:  74%|█████▉  | 14769/20010 [11:42:09<4:38:39,  3.19s/batch]Batch 14800/20010 Done, mean position loss: 22.551766717433928\n",
      "Training growing_up:  73%|█████▊  | 14648/20010 [11:42:17<4:47:43,  3.22s/batch]Batch 14700/20010 Done, mean position loss: 23.148523666858672\n",
      "Training growing_up:  73%|█████▊  | 14671/20010 [11:42:27<4:44:25,  3.20s/batch]Batch 14500/20010 Done, mean position loss: 21.42349390745163\n",
      "Training growing_up:  74%|█████▉  | 14776/20010 [11:42:35<4:41:51,  3.23s/batch]Batch 14800/20010 Done, mean position loss: 22.14682431459427\n",
      "Training growing_up:  74%|█████▉  | 14711/20010 [11:42:44<4:13:57,  2.88s/batch]Batch 14900/20010 Done, mean position loss: 22.288200087547303\n",
      "Training growing_up:  75%|██████  | 15015/20010 [11:42:45<3:46:14,  2.72s/batch]Batch 14700/20010 Done, mean position loss: 22.472691004276278\n",
      "Training growing_up:  73%|█████▊  | 14615/20010 [11:42:47<4:38:18,  3.10s/batch]Batch 14500/20010 Done, mean position loss: 22.854576964378357\n",
      "Training growing_up:  74%|█████▉  | 14834/20010 [11:42:58<4:00:12,  2.78s/batch]Batch 14800/20010 Done, mean position loss: 22.73930233001709\n",
      "Training growing_up:  74%|█████▉  | 14767/20010 [11:43:13<4:31:13,  3.10s/batch]Batch 14800/20010 Done, mean position loss: 22.81841309785843\n",
      "Training growing_up:  74%|█████▉  | 14792/20010 [11:43:26<4:17:13,  2.96s/batch]Batch 14900/20010 Done, mean position loss: 23.005813164711\n",
      "Training growing_up:  74%|█████▉  | 14740/20010 [11:43:29<4:08:15,  2.83s/batch]Batch 14700/20010 Done, mean position loss: 22.79330817461014\n",
      "Training growing_up:  74%|█████▉  | 14867/20010 [11:43:35<4:50:22,  3.39s/batch]Batch 14800/20010 Done, mean position loss: 22.21433225393295\n",
      "Training growing_up:  73%|█████▊  | 14632/20010 [11:43:39<4:39:49,  3.12s/batch]Batch 14700/20010 Done, mean position loss: 23.457922542095183\n",
      "Training growing_up:  74%|█████▉  | 14720/20010 [11:43:44<5:14:59,  3.57s/batch]Batch 14800/20010 Done, mean position loss: 22.439434151649472\n",
      "Training growing_up:  74%|█████▉  | 14788/20010 [11:43:46<4:30:19,  3.11s/batch]Batch 14900/20010 Done, mean position loss: 22.1591077208519\n",
      "Training growing_up:  74%|█████▉  | 14740/20010 [11:43:56<4:08:10,  2.83s/batch]Batch 14800/20010 Done, mean position loss: 22.708800361156463\n",
      "Training growing_up:  73%|█████▊  | 14599/20010 [11:44:03<4:27:54,  2.97s/batch]Batch 14700/20010 Done, mean position loss: 22.251121759414673\n",
      "Training growing_up:  72%|█████▊  | 14494/20010 [11:44:07<4:48:40,  3.14s/batch]Batch 14800/20010 Done, mean position loss: 22.667031116485596\n",
      "Training growing_up:  73%|█████▊  | 14684/20010 [11:44:10<4:48:02,  3.24s/batch]Batch 14600/20010 Done, mean position loss: 22.822859160900116\n",
      "Training growing_up:  75%|█████▉  | 14959/20010 [11:44:11<3:59:53,  2.85s/batch]Batch 14700/20010 Done, mean position loss: 22.764298207759857\n",
      "Training growing_up:  73%|█████▊  | 14535/20010 [11:44:10<4:49:54,  3.18s/batch]Batch 14800/20010 Done, mean position loss: 22.505515971183776\n",
      "Training growing_up:  74%|█████▉  | 14787/20010 [11:44:11<4:26:51,  3.07s/batch]Batch 14700/20010 Done, mean position loss: 22.885214629173277\n",
      "Training growing_up:  73%|█████▊  | 14553/20010 [11:44:16<4:51:32,  3.21s/batch]Batch 14700/20010 Done, mean position loss: 23.572703864574436\n",
      "Training growing_up:  72%|█████▊  | 14499/20010 [11:44:24<5:16:43,  3.45s/batch]Batch 14800/20010 Done, mean position loss: 22.95634062290192\n",
      "Training growing_up:  74%|█████▉  | 14818/20010 [11:44:30<4:47:59,  3.33s/batch]Batch 14500/20010 Done, mean position loss: 22.759770460128784\n",
      "Training growing_up:  74%|█████▉  | 14860/20010 [11:44:53<4:09:15,  2.90s/batch]Batch 14800/20010 Done, mean position loss: 23.289672985076905\n",
      "Training growing_up:  74%|█████▉  | 14895/20010 [11:45:00<4:24:59,  3.11s/batch]Batch 14700/20010 Done, mean position loss: 22.38891846418381\n",
      "Training growing_up:  74%|█████▉  | 14720/20010 [11:45:17<4:31:10,  3.08s/batch]Batch 14900/20010 Done, mean position loss: 21.7723481965065\n",
      "Training growing_up:  74%|█████▉  | 14826/20010 [11:45:19<3:58:48,  2.76s/batch]Batch 14700/20010 Done, mean position loss: 23.047019047737123\n",
      "Training growing_up:  74%|█████▉  | 14887/20010 [11:45:41<4:13:15,  2.97s/batch]Batch 14500/20010 Done, mean position loss: 22.320659291744235\n",
      "Training growing_up:  74%|█████▉  | 14783/20010 [11:46:21<4:02:36,  2.78s/batch]Batch 15000/20010 Done, mean position loss: 22.639668438434597\n",
      "Training growing_up:  74%|█████▉  | 14749/20010 [11:46:22<3:56:30,  2.70s/batch]Batch 14900/20010 Done, mean position loss: 22.04216530799866\n",
      "Training growing_up:  74%|█████▉  | 14731/20010 [11:46:32<4:39:07,  3.17s/batch]Batch 14800/20010 Done, mean position loss: 22.454415059089662\n",
      "Training growing_up:  75%|█████▉  | 14928/20010 [11:46:41<4:38:02,  3.28s/batch]Batch 14600/20010 Done, mean position loss: 23.276299300193784\n",
      "Training growing_up:  75%|█████▉  | 14963/20010 [11:46:51<4:31:09,  3.22s/batch]Batch 14600/20010 Done, mean position loss: 22.272538909912107\n",
      "Training growing_up:  74%|█████▉  | 14793/20010 [11:46:53<4:28:56,  3.09s/batch]Batch 14900/20010 Done, mean position loss: 21.95939881324768\n",
      "Training growing_up:  74%|█████▉  | 14879/20010 [11:47:02<4:00:30,  2.81s/batch]Batch 14800/20010 Done, mean position loss: 22.566808245182038\n",
      "Training growing_up:  74%|█████▉  | 14755/20010 [11:47:06<4:39:46,  3.19s/batch]Batch 15100/20010 Done, mean position loss: 21.908575146198274\n",
      "Batch 14900/20010 Done, mean position loss: 23.126317465305327\n",
      "Training growing_up:  74%|█████▉  | 14803/20010 [11:47:07<3:41:14,  2.55s/batch]Batch 14900/20010 Done, mean position loss: 22.671098954677582\n",
      "Training growing_up:  74%|█████▉  | 14799/20010 [11:47:12<4:25:22,  3.06s/batch]Batch 14700/20010 Done, mean position loss: 21.882966699600217\n",
      "Training growing_up:  74%|█████▉  | 14872/20010 [11:47:18<4:23:08,  3.07s/batch]Batch 14800/20010 Done, mean position loss: 22.95434923648834\n",
      "Training growing_up:  74%|█████▉  | 14782/20010 [11:47:32<4:49:54,  3.33s/batch]Batch 14600/20010 Done, mean position loss: 21.432196829319\n",
      "Training growing_up:  73%|█████▊  | 14617/20010 [11:47:42<4:11:54,  2.80s/batch]Batch 15000/20010 Done, mean position loss: 22.14568593978882\n",
      "Training growing_up:  74%|█████▉  | 14712/20010 [11:47:44<4:11:37,  2.85s/batch]Batch 14900/20010 Done, mean position loss: 22.30905785083771\n",
      "Training growing_up:  74%|█████▉  | 14818/20010 [11:47:52<4:19:57,  3.00s/batch]Batch 14800/20010 Done, mean position loss: 22.43154975414276\n",
      "Training growing_up:  73%|█████▊  | 14622/20010 [11:47:56<4:14:04,  2.83s/batch]Batch 14600/20010 Done, mean position loss: 22.69787873506546\n",
      "Training growing_up:  74%|█████▉  | 14831/20010 [11:47:57<4:24:40,  3.07s/batch]Batch 14900/20010 Done, mean position loss: 22.871100354194642\n",
      "Training growing_up:  74%|█████▉  | 14758/20010 [11:48:09<5:11:32,  3.56s/batch]Batch 14900/20010 Done, mean position loss: 22.512007863521575\n",
      "Training growing_up:  73%|█████▊  | 14608/20010 [11:48:17<4:35:11,  3.06s/batch]Batch 15000/20010 Done, mean position loss: 22.80228751182556\n",
      "Training growing_up:  74%|█████▉  | 14728/20010 [11:48:29<3:45:08,  2.56s/batch]Batch 14800/20010 Done, mean position loss: 22.9351012635231\n",
      "Training growing_up:  74%|█████▉  | 14895/20010 [11:48:35<4:08:32,  2.92s/batch]Batch 14900/20010 Done, mean position loss: 22.093622324466704\n",
      "Training growing_up:  74%|█████▉  | 14828/20010 [11:48:39<3:51:05,  2.68s/batch]Batch 14800/20010 Done, mean position loss: 23.358636465072635\n",
      "Training growing_up:  73%|█████▊  | 14561/20010 [11:48:41<4:35:39,  3.04s/batch]Batch 14900/20010 Done, mean position loss: 22.236600058078764\n",
      "Training growing_up:  75%|█████▉  | 14934/20010 [11:48:42<3:43:22,  2.64s/batch]Batch 15000/20010 Done, mean position loss: 22.225850095748903\n",
      "Training growing_up:  74%|█████▉  | 14905/20010 [11:48:52<4:11:45,  2.96s/batch]Batch 14800/20010 Done, mean position loss: 22.193682186603546\n",
      "Training growing_up:  74%|█████▉  | 14801/20010 [11:48:53<4:32:43,  3.14s/batch]Batch 14900/20010 Done, mean position loss: 22.655028281211855\n",
      "Training growing_up:  75%|██████  | 15016/20010 [11:48:57<3:29:27,  2.52s/batch]Batch 14900/20010 Done, mean position loss: 22.699104981422426\n",
      "Training growing_up:  75%|█████▉  | 14918/20010 [11:48:58<3:55:34,  2.78s/batch]Batch 14900/20010 Done, mean position loss: 22.298475213050843\n",
      "Training growing_up:  74%|█████▉  | 14887/20010 [11:49:03<4:10:14,  2.93s/batch]Batch 14800/20010 Done, mean position loss: 22.67239317893982\n",
      "Training growing_up:  75%|██████  | 15034/20010 [11:49:10<3:28:25,  2.51s/batch]Batch 14800/20010 Done, mean position loss: 22.93607806444168\n",
      "Training growing_up:  74%|█████▉  | 14907/20010 [11:49:10<3:58:01,  2.80s/batch]Batch 14700/20010 Done, mean position loss: 22.787377972602844\n",
      "Training growing_up:  73%|█████▊  | 14652/20010 [11:49:17<3:47:42,  2.55s/batch]Batch 14800/20010 Done, mean position loss: 23.518584764003755\n",
      "Training growing_up:  73%|█████▊  | 14630/20010 [11:49:18<3:36:21,  2.41s/batch]Batch 14900/20010 Done, mean position loss: 22.732518396377564\n",
      "Training growing_up:  74%|█████▉  | 14811/20010 [11:49:29<3:54:33,  2.71s/batch]Batch 14600/20010 Done, mean position loss: 22.461984388828277\n",
      "Training growing_up:  74%|█████▉  | 14798/20010 [11:49:42<4:08:28,  2.86s/batch]Batch 14900/20010 Done, mean position loss: 23.354747576713564\n",
      "Training growing_up:  75%|█████▉  | 14930/20010 [11:49:50<3:39:22,  2.59s/batch]Batch 14800/20010 Done, mean position loss: 22.615129868984223\n",
      "Training growing_up:  73%|█████▊  | 14592/20010 [11:50:03<3:40:33,  2.44s/batch]Batch 14800/20010 Done, mean position loss: 22.92438269615173\n",
      "Training growing_up:  74%|█████▉  | 14861/20010 [11:50:07<4:03:47,  2.84s/batch]Batch 15000/20010 Done, mean position loss: 22.121013195514678\n",
      "Training growing_up:  75%|██████  | 15092/20010 [11:50:28<3:35:01,  2.62s/batch]Batch 14600/20010 Done, mean position loss: 22.00755733728409\n",
      "Training growing_up:  75%|█████▉  | 14947/20010 [11:50:54<4:11:30,  2.98s/batch]Batch 15100/20010 Done, mean position loss: 22.618866977691653\n",
      "Training growing_up:  74%|█████▉  | 14866/20010 [11:50:57<4:32:59,  3.18s/batch]Batch 15000/20010 Done, mean position loss: 21.84627918958664\n",
      "Training growing_up:  75%|█████▉  | 14950/20010 [11:51:07<4:00:11,  2.85s/batch]Batch 14900/20010 Done, mean position loss: 21.914544751644137\n",
      "Training growing_up:  74%|█████▉  | 14897/20010 [11:51:29<4:08:39,  2.92s/batch]Batch 14700/20010 Done, mean position loss: 23.297907032966613\n",
      "Training growing_up:  75%|█████▉  | 14951/20010 [11:51:31<4:04:40,  2.90s/batch]Batch 15000/20010 Done, mean position loss: 21.871244771480562\n",
      "Training growing_up:  73%|█████▊  | 14678/20010 [11:51:37<4:47:28,  3.23s/batch]Batch 14700/20010 Done, mean position loss: 22.311152906417846\n",
      "Training growing_up:  75%|██████  | 15076/20010 [11:51:42<4:19:41,  3.16s/batch]Batch 15200/20010 Done, mean position loss: 21.92467401266098\n",
      "Training growing_up:  75%|█████▉  | 14983/20010 [11:51:43<4:28:15,  3.20s/batch]Batch 14900/20010 Done, mean position loss: 22.70513178110123\n",
      "Training growing_up:  74%|█████▉  | 14860/20010 [11:51:44<4:30:50,  3.16s/batch]Batch 15000/20010 Done, mean position loss: 22.614348273277283\n",
      "Training growing_up:  75%|█████▉  | 14985/20010 [11:51:49<4:27:29,  3.19s/batch]Batch 15000/20010 Done, mean position loss: 23.029286336898803\n",
      "Training growing_up:  74%|█████▉  | 14840/20010 [11:51:57<4:12:04,  2.93s/batch]Batch 14800/20010 Done, mean position loss: 21.70350593805313\n",
      "Training growing_up:  74%|█████▉  | 14845/20010 [11:52:09<3:39:15,  2.55s/batch]Batch 14900/20010 Done, mean position loss: 23.29324742555618\n",
      "Training growing_up:  74%|█████▉  | 14863/20010 [11:52:15<3:49:35,  2.68s/batch]Batch 14700/20010 Done, mean position loss: 21.78185565948486\n",
      "Training growing_up:  74%|█████▉  | 14877/20010 [11:52:20<3:54:56,  2.75s/batch]Batch 15100/20010 Done, mean position loss: 22.033351209163666\n",
      "Training growing_up:  74%|█████▉  | 14883/20010 [11:52:38<4:07:24,  2.90s/batch]Batch 15000/20010 Done, mean position loss: 22.248634152412414\n",
      "Training growing_up:  74%|█████▉  | 14884/20010 [11:52:40<4:05:03,  2.87s/batch]Batch 14900/20010 Done, mean position loss: 22.43798788547516\n",
      "Training growing_up:  75%|█████▉  | 14984/20010 [11:52:42<3:43:59,  2.67s/batch]Batch 14700/20010 Done, mean position loss: 22.50286269426346\n",
      "Training growing_up:  74%|█████▉  | 14876/20010 [11:52:43<4:03:35,  2.85s/batch]Batch 15000/20010 Done, mean position loss: 22.95357318401337\n",
      "Training growing_up:  75%|██████  | 15028/20010 [11:52:47<3:57:03,  2.85s/batch]Batch 15100/20010 Done, mean position loss: 23.01978896141052\n",
      "Training growing_up:  74%|█████▉  | 14883/20010 [11:52:50<3:58:11,  2.79s/batch]Batch 15000/20010 Done, mean position loss: 22.990937576293945\n",
      "Training growing_up:  75%|█████▉  | 14997/20010 [11:53:07<4:01:02,  2.89s/batch]Batch 14900/20010 Done, mean position loss: 22.886234605312346\n",
      "Training growing_up:  75%|██████  | 15015/20010 [11:53:19<4:06:59,  2.97s/batch]Batch 15000/20010 Done, mean position loss: 22.263814809322355\n",
      "Training growing_up:  75%|█████▉  | 15002/20010 [11:53:22<4:00:49,  2.89s/batch]Batch 14900/20010 Done, mean position loss: 23.40240274190903\n",
      "Training growing_up:  75%|██████  | 15052/20010 [11:53:24<4:01:06,  2.92s/batch]Batch 15100/20010 Done, mean position loss: 22.224512705802915\n",
      "Training growing_up:  75%|██████  | 15014/20010 [11:53:28<4:29:12,  3.23s/batch]Batch 15000/20010 Done, mean position loss: 22.531617603302003\n",
      "Training growing_up:  74%|█████▉  | 14831/20010 [11:53:29<4:39:08,  3.23s/batch]Batch 14900/20010 Done, mean position loss: 22.15475753068924\n",
      "Training growing_up:  74%|█████▉  | 14832/20010 [11:53:33<4:58:11,  3.46s/batch]Batch 15000/20010 Done, mean position loss: 22.91334525346756\n",
      "Training growing_up:  75%|██████  | 15018/20010 [11:53:33<4:22:54,  3.16s/batch]Batch 15000/20010 Done, mean position loss: 22.577620282173157\n",
      "Training growing_up:  75%|█████▉  | 14997/20010 [11:53:43<3:55:47,  2.82s/batch]Batch 14900/20010 Done, mean position loss: 22.872075345516205\n",
      "Training growing_up:  75%|█████▉  | 14955/20010 [11:53:46<3:51:25,  2.75s/batch]Batch 15000/20010 Done, mean position loss: 22.334036922454835\n",
      "Training growing_up:  75%|██████  | 15008/20010 [11:53:54<4:24:24,  3.17s/batch]Batch 14900/20010 Done, mean position loss: 22.658391726016998\n",
      "Training growing_up:  76%|██████  | 15124/20010 [11:53:55<3:52:10,  2.85s/batch]Batch 15000/20010 Done, mean position loss: 22.891449313163758\n",
      "Training growing_up:  74%|█████▉  | 14844/20010 [11:54:08<4:23:29,  3.06s/batch]Batch 14800/20010 Done, mean position loss: 22.92487032413483\n",
      "Training growing_up:  75%|█████▉  | 14950/20010 [11:54:07<4:23:23,  3.12s/batch]Batch 14900/20010 Done, mean position loss: 23.72163008451462\n",
      "Training growing_up:  75%|██████  | 15015/20010 [11:54:14<3:59:57,  2.88s/batch]Batch 14700/20010 Done, mean position loss: 22.486488809585573\n",
      "Training growing_up:  74%|█████▉  | 14897/20010 [11:54:31<4:31:11,  3.18s/batch]Batch 15000/20010 Done, mean position loss: 23.333996391296388\n",
      "Training growing_up:  75%|█████▉  | 14928/20010 [11:54:43<4:06:12,  2.91s/batch]Batch 14900/20010 Done, mean position loss: 22.49001345157623\n",
      "Training growing_up:  75%|█████▉  | 14919/20010 [11:54:45<3:52:52,  2.74s/batch]Batch 15100/20010 Done, mean position loss: 21.922198865413666\n",
      "Training growing_up:  75%|█████▉  | 14957/20010 [11:54:54<4:08:54,  2.96s/batch]Batch 14900/20010 Done, mean position loss: 22.870231387615203\n",
      "Training growing_up:  75%|██████  | 15078/20010 [11:55:39<4:46:24,  3.48s/batch]Batch 14700/20010 Done, mean position loss: 21.878883976936343\n",
      "Training growing_up:  75%|█████▉  | 14954/20010 [11:55:43<3:55:14,  2.79s/batch]Batch 15200/20010 Done, mean position loss: 22.327127528190612\n",
      "Training growing_up:  75%|██████  | 15062/20010 [11:55:49<4:11:16,  3.05s/batch]Batch 15100/20010 Done, mean position loss: 21.823934473991393\n",
      "Training growing_up:  75%|██████  | 15071/20010 [11:56:06<3:55:31,  2.86s/batch]Batch 15000/20010 Done, mean position loss: 22.33810670852661\n",
      "Training growing_up:  75%|██████  | 15058/20010 [11:56:24<4:13:51,  3.08s/batch]Batch 15100/20010 Done, mean position loss: 21.903496446609495\n",
      "Training growing_up:  74%|█████▉  | 14775/20010 [11:56:25<4:46:19,  3.28s/batch]Batch 14800/20010 Done, mean position loss: 23.170961055755615\n",
      "Training growing_up:  75%|█████▉  | 14953/20010 [11:56:30<4:05:43,  2.92s/batch]Batch 15100/20010 Done, mean position loss: 22.33043239593506\n",
      "Training growing_up:  74%|█████▉  | 14891/20010 [11:56:32<4:28:11,  3.14s/batch]Batch 15300/20010 Done, mean position loss: 21.740421311855314\n",
      "Training growing_up:  75%|█████▉  | 14964/20010 [11:56:38<4:25:24,  3.16s/batch]Batch 14800/20010 Done, mean position loss: 22.32085431098938\n",
      "Training growing_up:  74%|█████▉  | 14780/20010 [11:56:40<4:28:07,  3.08s/batch]Batch 15000/20010 Done, mean position loss: 22.720917749404908\n",
      "Training growing_up:  75%|██████  | 15016/20010 [11:56:51<3:54:20,  2.82s/batch]Batch 15100/20010 Done, mean position loss: 22.92642512321472\n",
      "Training growing_up:  75%|██████  | 15008/20010 [11:57:01<4:11:15,  3.01s/batch]Batch 14900/20010 Done, mean position loss: 21.781798870563506\n",
      "Training growing_up:  75%|█████▉  | 14944/20010 [11:57:06<4:41:34,  3.33s/batch]Batch 15000/20010 Done, mean position loss: 23.013280198574066\n",
      "Training growing_up:  77%|██████  | 15315/20010 [11:57:12<3:46:33,  2.90s/batch]Batch 15200/20010 Done, mean position loss: 21.9466933965683\n",
      "Training growing_up:  75%|██████  | 15076/20010 [11:57:20<4:18:37,  3.14s/batch]Batch 14800/20010 Done, mean position loss: 21.704597067832943\n",
      "Training growing_up:  75%|█████▉  | 14976/20010 [11:57:39<3:54:32,  2.80s/batch]Batch 15100/20010 Done, mean position loss: 22.625823385715485\n",
      "Training growing_up:  75%|██████  | 15022/20010 [11:57:41<3:40:50,  2.66s/batch]Batch 15100/20010 Done, mean position loss: 22.119790999889375\n",
      "Training growing_up:  76%|██████  | 15126/20010 [11:57:43<3:46:57,  2.79s/batch]Batch 15000/20010 Done, mean position loss: 22.666397593021394\n",
      "Training growing_up:  75%|█████▉  | 14915/20010 [11:57:44<4:12:38,  2.98s/batch]Batch 15200/20010 Done, mean position loss: 23.02606369495392\n",
      "Training growing_up:  76%|██████  | 15119/20010 [11:57:46<3:57:52,  2.92s/batch]Batch 14800/20010 Done, mean position loss: 22.783999962806703\n",
      "Training growing_up:  76%|██████  | 15129/20010 [11:57:52<3:54:13,  2.88s/batch]Batch 15100/20010 Done, mean position loss: 22.904386882781985\n",
      "Training growing_up:  75%|██████  | 15094/20010 [11:58:13<3:57:41,  2.90s/batch]Batch 15000/20010 Done, mean position loss: 22.611603236198427\n",
      "Training growing_up:  76%|██████  | 15212/20010 [11:58:20<4:15:07,  3.19s/batch]Batch 15100/20010 Done, mean position loss: 22.431019937992097\n",
      "Training growing_up:  76%|██████  | 15224/20010 [11:58:24<4:21:29,  3.28s/batch]Batch 15200/20010 Done, mean position loss: 22.182862629890444\n",
      "Training growing_up:  74%|█████▉  | 14887/20010 [11:58:30<4:21:45,  3.07s/batch]Batch 15100/20010 Done, mean position loss: 22.499494290351866\n",
      "Training growing_up:  75%|█████▉  | 14973/20010 [11:58:33<3:57:21,  2.83s/batch]Batch 15000/20010 Done, mean position loss: 23.252921805381774\n",
      "Training growing_up:  76%|██████  | 15176/20010 [11:58:35<3:44:30,  2.79s/batch]Batch 15100/20010 Done, mean position loss: 22.47564039707184\n",
      "Training growing_up:  75%|██████  | 15097/20010 [11:58:36<4:02:32,  2.96s/batch]Batch 15100/20010 Done, mean position loss: 22.63551470279694\n",
      "Training growing_up:  75%|██████  | 15102/20010 [11:58:38<3:54:44,  2.87s/batch]Batch 15000/20010 Done, mean position loss: 22.377815201282502\n",
      "Training growing_up:  76%|██████  | 15145/20010 [11:58:45<3:57:18,  2.93s/batch]Batch 15000/20010 Done, mean position loss: 22.907663357257842\n",
      "Training growing_up:  74%|█████▉  | 14847/20010 [11:58:48<4:21:19,  3.04s/batch]Batch 15100/20010 Done, mean position loss: 22.210741369724275\n",
      "Training growing_up:  76%|██████  | 15161/20010 [11:58:54<4:04:19,  3.02s/batch]Batch 15100/20010 Done, mean position loss: 22.871955194473266\n",
      "Training growing_up:  75%|█████▉  | 14937/20010 [11:58:56<4:43:39,  3.35s/batch]Batch 15000/20010 Done, mean position loss: 22.54256940364838\n",
      "Training growing_up:  74%|█████▉  | 14836/20010 [11:59:09<4:26:25,  3.09s/batch]Batch 15000/20010 Done, mean position loss: 23.509618003368377\n",
      "Training growing_up:  74%|█████▉  | 14797/20010 [11:59:14<4:19:20,  2.98s/batch]Batch 14900/20010 Done, mean position loss: 22.42167998790741\n",
      "Training growing_up:  76%|██████  | 15119/20010 [11:59:27<4:36:19,  3.39s/batch]Batch 14800/20010 Done, mean position loss: 22.6291868853569\n",
      "Training growing_up:  74%|█████▉  | 14858/20010 [11:59:38<5:00:32,  3.50s/batch]Batch 15100/20010 Done, mean position loss: 23.434161677360535\n",
      "Training growing_up:  75%|█████▉  | 14953/20010 [11:59:45<4:31:49,  3.23s/batch]Batch 15200/20010 Done, mean position loss: 21.759580771923066\n",
      "Training growing_up:  76%|██████  | 15124/20010 [11:59:46<4:10:46,  3.08s/batch]Batch 15000/20010 Done, mean position loss: 22.412010357379913\n",
      "Training growing_up:  75%|█████▉  | 14957/20010 [11:59:59<5:02:06,  3.59s/batch]Batch 15000/20010 Done, mean position loss: 22.901136116981505\n",
      "Training growing_up:  75%|██████  | 15062/20010 [12:00:47<4:00:35,  2.92s/batch]Batch 15300/20010 Done, mean position loss: 22.68800570011139\n",
      "Training growing_up:  76%|██████  | 15159/20010 [12:00:49<3:57:57,  2.94s/batch]Batch 14800/20010 Done, mean position loss: 21.94038026571274\n",
      "Training growing_up:  75%|█████▉  | 14934/20010 [12:00:58<4:37:15,  3.28s/batch]Batch 15200/20010 Done, mean position loss: 21.664320480823516\n",
      "Training growing_up:  76%|██████  | 15146/20010 [12:01:09<4:08:01,  3.06s/batch]Batch 15100/20010 Done, mean position loss: 22.184633691310882\n",
      "Training growing_up:  77%|██████  | 15317/20010 [12:01:35<4:01:17,  3.08s/batch]Batch 15200/20010 Done, mean position loss: 22.41726287841797\n",
      "Training growing_up:  75%|██████  | 15039/20010 [12:01:38<4:08:49,  3.00s/batch]Batch 15200/20010 Done, mean position loss: 22.029977235794068\n",
      "Batch 14900/20010 Done, mean position loss: 23.23324177980423\n",
      "Batch 15400/20010 Done, mean position loss: 21.74111817598343\n",
      "Training growing_up:  74%|█████▉  | 14906/20010 [12:01:51<3:49:25,  2.70s/batch]Batch 15100/20010 Done, mean position loss: 22.589241149425504\n",
      "Training growing_up:  75%|██████  | 15064/20010 [12:01:52<3:44:32,  2.72s/batch]Batch 14900/20010 Done, mean position loss: 22.070046794414523\n",
      "Training growing_up:  76%|██████  | 15184/20010 [12:02:00<3:57:28,  2.95s/batch]Batch 15200/20010 Done, mean position loss: 23.089765107631685\n",
      "Training growing_up:  76%|██████  | 15224/20010 [12:02:07<4:14:43,  3.19s/batch]Batch 15100/20010 Done, mean position loss: 23.431060669422152\n",
      "Training growing_up:  75%|██████  | 15107/20010 [12:02:08<4:14:00,  3.11s/batch]Batch 15300/20010 Done, mean position loss: 22.09786688327789\n",
      "Training growing_up:  75%|██████  | 15046/20010 [12:02:16<4:16:15,  3.10s/batch]Batch 15000/20010 Done, mean position loss: 21.903776659965516\n",
      "Training growing_up:  75%|██████  | 15084/20010 [12:02:30<4:08:09,  3.02s/batch]Batch 14900/20010 Done, mean position loss: 21.41919838666916\n",
      "Training growing_up:  76%|██████  | 15119/20010 [12:02:44<4:08:55,  3.05s/batch]Batch 15100/20010 Done, mean position loss: 22.542765684127808\n",
      "Training growing_up:  76%|██████  | 15120/20010 [12:02:48<4:22:55,  3.23s/batch]Batch 15200/20010 Done, mean position loss: 22.19724123954773\n",
      "Training growing_up:  76%|██████  | 15192/20010 [12:02:52<3:59:40,  2.98s/batch]Batch 15200/20010 Done, mean position loss: 22.682013180255886\n",
      "Training growing_up:  76%|██████  | 15180/20010 [12:02:53<4:07:45,  3.08s/batch]Batch 15300/20010 Done, mean position loss: 22.869929575920104\n",
      "Training growing_up:  74%|█████▉  | 14900/20010 [12:02:59<4:26:36,  3.13s/batch]Batch 15200/20010 Done, mean position loss: 22.684826905727384\n",
      "Training growing_up:  76%|██████  | 15202/20010 [12:03:02<4:09:26,  3.11s/batch]Batch 14900/20010 Done, mean position loss: 22.70257379770279\n",
      "Training growing_up:  76%|██████  | 15130/20010 [12:03:20<4:12:33,  3.11s/batch]Batch 15200/20010 Done, mean position loss: 22.079388608932497\n",
      "Training growing_up:  75%|█████▉  | 14908/20010 [12:03:23<4:04:05,  2.87s/batch]Batch 15300/20010 Done, mean position loss: 21.975401093959807\n",
      "Training growing_up:  74%|█████▉  | 14879/20010 [12:03:24<4:01:24,  2.82s/batch]Batch 15100/20010 Done, mean position loss: 22.748340814113618\n",
      "Training growing_up:  75%|██████  | 15074/20010 [12:03:35<3:35:34,  2.62s/batch]Batch 15200/20010 Done, mean position loss: 22.746686017513277\n",
      "Training growing_up:  77%|██████▏ | 15357/20010 [12:03:38<3:43:40,  2.88s/batch]Batch 15100/20010 Done, mean position loss: 22.736294915676115\n",
      "Training growing_up:  76%|██████  | 15244/20010 [12:03:39<3:35:25,  2.71s/batch]Batch 15100/20010 Done, mean position loss: 22.09884683609009\n",
      "Training growing_up:  76%|██████  | 15182/20010 [12:03:41<3:42:16,  2.76s/batch]Batch 15200/20010 Done, mean position loss: 22.574921712875366\n",
      "Training growing_up:  76%|██████  | 15201/20010 [12:03:41<3:42:29,  2.78s/batch]Batch 15100/20010 Done, mean position loss: 23.22901011705399\n",
      "Training growing_up:  76%|██████  | 15151/20010 [12:03:43<3:50:07,  2.84s/batch]Batch 15200/20010 Done, mean position loss: 22.581225872039795\n",
      "Training growing_up:  76%|██████  | 15207/20010 [12:03:51<3:43:37,  2.79s/batch]Batch 15200/20010 Done, mean position loss: 22.734139614105224\n",
      "Training growing_up:  76%|██████  | 15142/20010 [12:03:54<3:25:34,  2.53s/batch]Batch 15200/20010 Done, mean position loss: 22.297317211627963\n",
      "Training growing_up:  77%|██████▏ | 15340/20010 [12:03:59<3:25:59,  2.65s/batch]Batch 15100/20010 Done, mean position loss: 22.814838342666626\n",
      "Training growing_up:  76%|██████  | 15212/20010 [12:04:10<3:51:06,  2.89s/batch]Batch 15100/20010 Done, mean position loss: 23.413560025691986\n",
      "Training growing_up:  77%|██████▏ | 15371/20010 [12:04:16<2:59:45,  2.32s/batch]Batch 15000/20010 Done, mean position loss: 22.451961979866027\n",
      "Training growing_up:  76%|██████  | 15198/20010 [12:04:23<3:44:28,  2.80s/batch]Batch 14900/20010 Done, mean position loss: 22.611079964637756\n",
      "Training growing_up:  75%|█████▉  | 14933/20010 [12:04:31<3:19:38,  2.36s/batch]Batch 15200/20010 Done, mean position loss: 23.07326174020767\n",
      "Training growing_up:  76%|██████  | 15215/20010 [12:04:32<4:06:10,  3.08s/batch]Batch 15300/20010 Done, mean position loss: 21.833876185417175\n",
      "Training growing_up:  76%|██████  | 15304/20010 [12:04:40<3:46:54,  2.89s/batch]Batch 15100/20010 Done, mean position loss: 22.717684192657472\n",
      "Training growing_up:  76%|██████  | 15234/20010 [12:04:49<3:55:08,  2.95s/batch]Batch 15100/20010 Done, mean position loss: 22.780090665817262\n",
      "Training growing_up:  76%|██████  | 15225/20010 [12:05:40<3:28:40,  2.62s/batch]Batch 15400/20010 Done, mean position loss: 22.732288513183594\n",
      "Training growing_up:  76%|██████  | 15193/20010 [12:05:41<3:52:04,  2.89s/batch]Batch 14900/20010 Done, mean position loss: 21.846278626918792\n",
      "Training growing_up:  76%|██████  | 15196/20010 [12:05:48<3:22:34,  2.52s/batch]Batch 15300/20010 Done, mean position loss: 21.966735792160033\n",
      "Training growing_up:  75%|█████▉  | 14966/20010 [12:06:02<3:32:40,  2.53s/batch]Batch 15200/20010 Done, mean position loss: 22.130667505264285\n",
      "Training growing_up:  76%|██████  | 15147/20010 [12:06:19<3:43:42,  2.76s/batch]Batch 15300/20010 Done, mean position loss: 22.55564864873886\n",
      "Training growing_up:  76%|██████  | 15190/20010 [12:06:22<3:32:05,  2.64s/batch]Batch 15500/20010 Done, mean position loss: 21.627893564701083\n",
      "Training growing_up:  76%|██████  | 15260/20010 [12:06:26<4:10:22,  3.16s/batch]Batch 15000/20010 Done, mean position loss: 23.397090561389923\n",
      "Training growing_up:  77%|██████  | 15316/20010 [12:06:30<4:23:58,  3.37s/batch]Batch 15300/20010 Done, mean position loss: 21.945963995456694\n",
      "Training growing_up:  76%|██████  | 15304/20010 [12:06:39<4:03:37,  3.11s/batch]Batch 15200/20010 Done, mean position loss: 22.32585217475891\n",
      "Training growing_up:  75%|██████  | 15008/20010 [12:06:48<4:01:48,  2.90s/batch]Batch 15300/20010 Done, mean position loss: 22.944484012126924\n",
      "Training growing_up:  76%|██████  | 15217/20010 [12:06:48<4:13:56,  3.18s/batch]Batch 15000/20010 Done, mean position loss: 22.197882282733918\n",
      "Training growing_up:  76%|██████  | 15163/20010 [12:06:56<4:22:35,  3.25s/batch]Batch 15200/20010 Done, mean position loss: 23.04676957845688\n",
      "Training growing_up:  76%|██████  | 15148/20010 [12:06:59<4:13:01,  3.12s/batch]Batch 15400/20010 Done, mean position loss: 22.20305964946747\n",
      "Training growing_up:  75%|█████▉  | 14998/20010 [12:07:10<4:12:38,  3.02s/batch]Batch 15100/20010 Done, mean position loss: 22.034256010055543\n",
      "Training growing_up:  75%|█████▉  | 14933/20010 [12:07:19<4:47:07,  3.39s/batch]Batch 15000/20010 Done, mean position loss: 21.36684876203537\n",
      "Training growing_up:  76%|██████  | 15276/20010 [12:07:24<3:49:04,  2.90s/batch]Batch 15200/20010 Done, mean position loss: 22.41541121006012\n",
      "Training growing_up:  75%|█████▉  | 14995/20010 [12:07:34<4:19:11,  3.10s/batch]Batch 15300/20010 Done, mean position loss: 22.345528721809387\n",
      "Training growing_up:  76%|██████  | 15302/20010 [12:07:36<3:35:40,  2.75s/batch]Batch 15300/20010 Done, mean position loss: 22.766301312446593\n",
      "Training growing_up:  77%|██████▏ | 15329/20010 [12:07:41<3:42:18,  2.85s/batch]Batch 15400/20010 Done, mean position loss: 22.903153665065766\n",
      "Training growing_up:  75%|█████▉  | 14940/20010 [12:07:42<4:43:10,  3.35s/batch]Batch 15300/20010 Done, mean position loss: 22.71416325330734\n",
      "Training growing_up:  76%|██████  | 15220/20010 [12:07:52<4:09:19,  3.12s/batch]Batch 15000/20010 Done, mean position loss: 22.516146559715274\n",
      "Training growing_up:  75%|██████  | 15034/20010 [12:08:03<4:00:57,  2.91s/batch]Batch 15200/20010 Done, mean position loss: 23.002862310409547\n",
      "Training growing_up:  76%|██████  | 15170/20010 [12:08:04<4:00:23,  2.98s/batch]Batch 15300/20010 Done, mean position loss: 21.955560848712924\n",
      "Training growing_up:  76%|██████  | 15196/20010 [12:08:09<3:34:29,  2.67s/batch]Batch 15400/20010 Done, mean position loss: 22.207212474346164\n",
      "Training growing_up:  75%|██████  | 15087/20010 [12:08:21<3:58:08,  2.90s/batch]Batch 15300/20010 Done, mean position loss: 22.289445519447327\n",
      "Training growing_up:  75%|██████  | 15032/20010 [12:08:22<4:06:16,  2.97s/batch]Batch 15200/20010 Done, mean position loss: 22.77378862857819\n",
      "Training growing_up:  76%|██████  | 15237/20010 [12:08:23<3:55:39,  2.96s/batch]Batch 15200/20010 Done, mean position loss: 22.249439864158628\n",
      "Training growing_up:  77%|██████▏ | 15382/20010 [12:08:27<3:46:51,  2.94s/batch]Batch 15200/20010 Done, mean position loss: 23.455413019657136\n",
      "Training growing_up:  76%|██████  | 15252/20010 [12:08:30<3:37:25,  2.74s/batch]Batch 15300/20010 Done, mean position loss: 22.679153542518616\n",
      "Training growing_up:  76%|██████  | 15299/20010 [12:08:37<3:29:53,  2.67s/batch]Batch 15300/20010 Done, mean position loss: 22.307035048007965\n",
      "Training growing_up:  76%|██████  | 15254/20010 [12:08:36<3:58:15,  3.01s/batch]Batch 15300/20010 Done, mean position loss: 22.86578979253769\n",
      "Training growing_up:  76%|██████  | 15183/20010 [12:08:43<3:56:03,  2.93s/batch]Batch 15300/20010 Done, mean position loss: 22.311379477977752\n",
      "Training growing_up:  75%|██████  | 15033/20010 [12:08:52<4:00:30,  2.90s/batch]Batch 15200/20010 Done, mean position loss: 22.44643295764923\n",
      "Training growing_up:  75%|██████  | 15054/20010 [12:09:01<3:48:13,  2.76s/batch]Batch 15100/20010 Done, mean position loss: 22.6715302658081\n",
      "Training growing_up:  75%|██████  | 15102/20010 [12:09:03<3:39:30,  2.68s/batch]Batch 15200/20010 Done, mean position loss: 23.580987820625307\n",
      "Training growing_up:  76%|██████  | 15145/20010 [12:09:21<4:30:56,  3.34s/batch]Batch 15400/20010 Done, mean position loss: 22.23128259897232\n",
      "Training growing_up:  77%|██████▏ | 15435/20010 [12:09:22<3:31:44,  2.78s/batch]Batch 15000/20010 Done, mean position loss: 22.61158720731735\n",
      "Training growing_up:  76%|██████  | 15211/20010 [12:09:22<3:48:48,  2.86s/batch]Batch 15300/20010 Done, mean position loss: 23.16225203514099\n",
      "Training growing_up:  77%|██████▏ | 15327/20010 [12:09:34<3:44:15,  2.87s/batch]Batch 15200/20010 Done, mean position loss: 22.756229259967803\n",
      "Training growing_up:  77%|██████  | 15308/20010 [12:09:42<3:37:23,  2.77s/batch]Batch 15200/20010 Done, mean position loss: 22.650941584110264\n",
      "Training growing_up:  75%|██████  | 15053/20010 [12:10:26<4:03:45,  2.95s/batch]Batch 15500/20010 Done, mean position loss: 22.66368351697922\n",
      "Training growing_up:  76%|██████  | 15248/20010 [12:10:41<3:53:16,  2.94s/batch]Batch 15000/20010 Done, mean position loss: 21.712325549125673\n",
      "Training growing_up:  76%|██████  | 15224/20010 [12:10:50<3:55:13,  2.95s/batch]Batch 15400/20010 Done, mean position loss: 21.631610176563264\n",
      "Training growing_up:  76%|██████  | 15270/20010 [12:10:56<3:58:46,  3.02s/batch]Batch 15300/20010 Done, mean position loss: 21.90976089000702\n",
      "Training growing_up:  77%|██████▏ | 15377/20010 [12:11:14<4:02:39,  3.14s/batch]Batch 15600/20010 Done, mean position loss: 21.738522622585297\n",
      "Training growing_up:  76%|██████  | 15289/20010 [12:11:15<3:54:40,  2.98s/batch]Batch 15400/20010 Done, mean position loss: 22.738923904895785\n",
      "Training growing_up:  77%|██████▏ | 15358/20010 [12:11:21<3:51:39,  2.99s/batch]Batch 15100/20010 Done, mean position loss: 23.067642722129825\n",
      "Training growing_up:  75%|██████  | 15076/20010 [12:11:34<3:47:27,  2.77s/batch]Batch 15300/20010 Done, mean position loss: 22.610535664558412\n",
      "Training growing_up:  76%|██████  | 15266/20010 [12:11:37<3:45:53,  2.86s/batch]Batch 15400/20010 Done, mean position loss: 22.283438811302183\n",
      "Training growing_up:  76%|██████  | 15155/20010 [12:11:48<4:35:48,  3.41s/batch]Batch 15400/20010 Done, mean position loss: 23.37671141386032\n",
      "Training growing_up:  77%|██████▏ | 15476/20010 [12:11:48<3:28:26,  2.76s/batch]Batch 15100/20010 Done, mean position loss: 22.426141679286957\n",
      "Training growing_up:  78%|██████▏ | 15530/20010 [12:11:53<3:57:06,  3.18s/batch]Batch 15300/20010 Done, mean position loss: 22.874680280685425\n",
      "Training growing_up:  75%|██████  | 15050/20010 [12:11:54<4:49:30,  3.50s/batch]Batch 15500/20010 Done, mean position loss: 22.08012152194977\n",
      "Training growing_up:  76%|██████  | 15109/20010 [12:12:13<4:09:23,  3.05s/batch]Batch 15200/20010 Done, mean position loss: 21.79144991874695\n",
      "Training growing_up:  78%|██████▏ | 15537/20010 [12:12:15<4:02:29,  3.25s/batch]Batch 15100/20010 Done, mean position loss: 21.707693433761598\n",
      "Training growing_up:  78%|██████▏ | 15624/20010 [12:12:26<3:51:36,  3.17s/batch]Batch 15400/20010 Done, mean position loss: 22.814747710227966\n",
      "Training growing_up:  78%|██████▏ | 15512/20010 [12:12:28<3:53:48,  3.12s/batch]Batch 15400/20010 Done, mean position loss: 22.172802143096924\n",
      "Training growing_up:  76%|██████  | 15273/20010 [12:12:36<3:29:20,  2.65s/batch]Batch 15300/20010 Done, mean position loss: 22.417599375247956\n",
      "Training growing_up:  78%|██████▏ | 15515/20010 [12:12:36<3:38:09,  2.91s/batch]Batch 15400/20010 Done, mean position loss: 22.85978854894638\n",
      "Training growing_up:  77%|██████▏ | 15384/20010 [12:12:37<4:01:46,  3.14s/batch]Batch 15500/20010 Done, mean position loss: 22.784766678810122\n",
      "Training growing_up:  76%|██████  | 15263/20010 [12:12:51<3:56:34,  2.99s/batch]Batch 15100/20010 Done, mean position loss: 22.496593437194825\n",
      "Training growing_up:  78%|██████▏ | 15551/20010 [12:12:58<3:31:40,  2.85s/batch]Batch 15400/20010 Done, mean position loss: 22.03095053434372\n",
      "Training growing_up:  77%|██████▏ | 15332/20010 [12:13:04<3:45:26,  2.89s/batch]Batch 15500/20010 Done, mean position loss: 22.11798409938812\n",
      "Training growing_up:  77%|██████▏ | 15404/20010 [12:13:06<3:26:31,  2.69s/batch]Batch 15300/20010 Done, mean position loss: 22.736569645404813\n",
      "Training growing_up:  77%|██████  | 15315/20010 [12:13:17<3:57:33,  3.04s/batch]Batch 15300/20010 Done, mean position loss: 22.22222324371338\n",
      "Training growing_up:  78%|██████▏ | 15561/20010 [12:13:28<3:26:25,  2.78s/batch]Batch 15400/20010 Done, mean position loss: 22.344000494480134\n",
      "Training growing_up:  75%|██████  | 15054/20010 [12:13:28<4:18:07,  3.13s/batch]Batch 15300/20010 Done, mean position loss: 22.828690650463106\n",
      "Training growing_up:  77%|██████▏ | 15382/20010 [12:13:28<4:02:16,  3.14s/batch]Batch 15300/20010 Done, mean position loss: 23.28172881126404\n",
      "Training growing_up:  76%|██████  | 15142/20010 [12:13:30<4:26:28,  3.28s/batch]Batch 15400/20010 Done, mean position loss: 22.89631217241287\n",
      "Training growing_up:  77%|██████▏ | 15421/20010 [12:13:39<3:57:44,  3.11s/batch]Batch 15400/20010 Done, mean position loss: 22.272082245349885\n",
      "Training growing_up:  77%|██████▏ | 15344/20010 [12:13:40<3:47:26,  2.92s/batch]Batch 15400/20010 Done, mean position loss: 22.380154461860656\n",
      "Training growing_up:  77%|██████  | 15308/20010 [12:13:49<3:54:40,  2.99s/batch]Batch 15400/20010 Done, mean position loss: 23.027008368968964\n",
      "Training growing_up:  78%|██████▏ | 15529/20010 [12:14:00<3:17:03,  2.64s/batch]Batch 15300/20010 Done, mean position loss: 22.44625054359436\n",
      "Training growing_up:  77%|██████▏ | 15352/20010 [12:14:05<4:24:41,  3.41s/batch]Batch 15300/20010 Done, mean position loss: 23.51958617925644\n",
      "Training growing_up:  77%|██████  | 15320/20010 [12:14:06<4:05:15,  3.14s/batch]Batch 15200/20010 Done, mean position loss: 22.47360059738159\n",
      "Training growing_up:  76%|██████  | 15306/20010 [12:14:20<3:50:36,  2.94s/batch]Batch 15500/20010 Done, mean position loss: 21.820948317050934\n",
      "Training growing_up:  77%|██████▏ | 15463/20010 [12:14:28<4:08:47,  3.28s/batch]Batch 15400/20010 Done, mean position loss: 23.263108308315278\n",
      "Training growing_up:  76%|██████  | 15162/20010 [12:14:34<4:37:21,  3.43s/batch]Batch 15100/20010 Done, mean position loss: 22.65924502134323\n",
      "Training growing_up:  76%|██████  | 15298/20010 [12:14:39<4:01:28,  3.07s/batch]Batch 15300/20010 Done, mean position loss: 22.35561507463455\n",
      "Training growing_up:  77%|██████▏ | 15480/20010 [12:14:48<3:27:02,  2.74s/batch]Batch 15300/20010 Done, mean position loss: 22.75913244485855\n",
      "Training growing_up:  77%|██████▏ | 15391/20010 [12:15:37<3:57:04,  3.08s/batch]Batch 15600/20010 Done, mean position loss: 22.466504747867585\n",
      "Training growing_up:  78%|██████▏ | 15557/20010 [12:15:52<3:32:42,  2.87s/batch]Batch 15500/20010 Done, mean position loss: 22.00376811981201\n",
      "Training growing_up:  77%|██████▏ | 15492/20010 [12:15:53<3:52:23,  3.09s/batch]Batch 15100/20010 Done, mean position loss: 21.51781482934952\n",
      "Training growing_up:  77%|██████▏ | 15506/20010 [12:16:07<3:44:20,  2.99s/batch]Batch 15400/20010 Done, mean position loss: 22.055460667610166\n",
      "Training growing_up:  77%|██████▏ | 15346/20010 [12:16:16<3:51:24,  2.98s/batch]Batch 15700/20010 Done, mean position loss: 21.83319161653519\n",
      "Training growing_up:  77%|██████▏ | 15481/20010 [12:16:21<3:31:30,  2.80s/batch]Batch 15500/20010 Done, mean position loss: 22.995632162094118\n",
      "Training growing_up:  77%|██████▏ | 15350/20010 [12:16:28<3:48:06,  2.94s/batch]Batch 15200/20010 Done, mean position loss: 22.936265571117403\n",
      "Training growing_up:  77%|██████▏ | 15338/20010 [12:16:33<3:48:34,  2.94s/batch]Batch 15400/20010 Done, mean position loss: 22.396198534965514\n",
      "Training growing_up:  77%|██████▏ | 15396/20010 [12:16:45<4:00:16,  3.12s/batch]Batch 15500/20010 Done, mean position loss: 23.083490726947783\n",
      "Training growing_up:  76%|██████  | 15144/20010 [12:16:49<4:30:36,  3.34s/batch]Batch 15500/20010 Done, mean position loss: 21.789361238479614\n",
      "Training growing_up:  78%|██████▏ | 15625/20010 [12:16:52<3:49:39,  3.14s/batch]Batch 15200/20010 Done, mean position loss: 22.21895363330841\n",
      "Training growing_up:  78%|██████▏ | 15554/20010 [12:16:56<3:41:32,  2.98s/batch]Batch 15600/20010 Done, mean position loss: 21.916460008621215\n",
      "Training growing_up:  77%|██████▏ | 15387/20010 [12:17:00<4:00:56,  3.13s/batch]Batch 15400/20010 Done, mean position loss: 23.060927391052246\n",
      "Training growing_up:  76%|██████  | 15210/20010 [12:17:20<4:25:49,  3.32s/batch]Batch 15300/20010 Done, mean position loss: 21.79959742307663\n",
      "Training growing_up:  78%|██████▎ | 15634/20010 [12:17:21<4:02:33,  3.33s/batch]Batch 15500/20010 Done, mean position loss: 23.182843143939973\n",
      "Training growing_up:  76%|██████  | 15191/20010 [12:17:29<4:03:32,  3.03s/batch]Batch 15200/20010 Done, mean position loss: 21.82724557876587\n",
      "Training growing_up:  76%|██████  | 15267/20010 [12:17:32<4:19:20,  3.28s/batch]Batch 15500/20010 Done, mean position loss: 22.12535987138748\n",
      "Training growing_up:  77%|██████▏ | 15371/20010 [12:17:33<3:46:51,  2.93s/batch]Batch 15600/20010 Done, mean position loss: 22.90546503305435\n",
      "Training growing_up:  77%|██████▏ | 15385/20010 [12:17:40<3:56:11,  3.06s/batch]Batch 15500/20010 Done, mean position loss: 22.688910503387454\n",
      "Training growing_up:  77%|██████▏ | 15466/20010 [12:17:44<3:38:37,  2.89s/batch]Batch 15400/20010 Done, mean position loss: 22.2563622713089\n",
      "Training growing_up:  77%|██████▏ | 15499/20010 [12:18:00<3:46:54,  3.02s/batch]Batch 15200/20010 Done, mean position loss: 22.87143614768982\n",
      "Training growing_up:  76%|██████  | 15203/20010 [12:18:06<3:58:47,  2.98s/batch]Batch 15500/20010 Done, mean position loss: 22.098562211990355\n",
      "Training growing_up:  77%|██████▏ | 15396/20010 [12:18:07<3:46:43,  2.95s/batch]Batch 15600/20010 Done, mean position loss: 22.23019335985184\n",
      "Training growing_up:  78%|██████▏ | 15514/20010 [12:18:20<3:42:18,  2.97s/batch]Batch 15400/20010 Done, mean position loss: 22.662491908073427\n",
      "Training growing_up:  78%|██████▏ | 15515/20010 [12:18:23<3:51:41,  3.09s/batch]Batch 15400/20010 Done, mean position loss: 22.09447775363922\n",
      "Training growing_up:  77%|██████▏ | 15492/20010 [12:18:27<3:39:28,  2.91s/batch]Batch 15400/20010 Done, mean position loss: 22.898094930648803\n",
      "Training growing_up:  78%|██████▏ | 15585/20010 [12:18:28<3:39:25,  2.98s/batch]Batch 15500/20010 Done, mean position loss: 22.14300557374954\n",
      "Training growing_up:  77%|██████▏ | 15439/20010 [12:18:29<4:05:19,  3.22s/batch]Batch 15400/20010 Done, mean position loss: 22.812198140621188\n",
      "Training growing_up:  77%|██████▏ | 15419/20010 [12:18:38<3:41:34,  2.90s/batch]Batch 15500/20010 Done, mean position loss: 22.586556036472324\n",
      "Training growing_up:  76%|██████  | 15180/20010 [12:18:40<3:59:00,  2.97s/batch]Batch 15500/20010 Done, mean position loss: 22.64083018541336\n",
      "Training growing_up:  77%|██████▏ | 15502/20010 [12:18:41<3:56:01,  3.14s/batch]Batch 15500/20010 Done, mean position loss: 22.19895118713379\n",
      "Training growing_up:  78%|██████▏ | 15553/20010 [12:18:53<3:12:42,  2.59s/batch]Batch 15500/20010 Done, mean position loss: 22.840720043182372\n",
      "Training growing_up:  78%|██████▏ | 15621/20010 [12:19:06<3:29:59,  2.87s/batch]Batch 15400/20010 Done, mean position loss: 22.712641451358795\n",
      "Training growing_up:  78%|██████▏ | 15518/20010 [12:19:17<3:46:45,  3.03s/batch]Batch 15600/20010 Done, mean position loss: 21.84442564725876\n",
      "Training growing_up:  78%|██████▏ | 15514/20010 [12:19:17<3:33:48,  2.85s/batch]Batch 15400/20010 Done, mean position loss: 23.558940052986145\n",
      "Training growing_up:  77%|██████▏ | 15342/20010 [12:19:20<3:43:31,  2.87s/batch]Batch 15300/20010 Done, mean position loss: 22.53738740682602\n",
      "Training growing_up:  78%|██████▏ | 15576/20010 [12:19:29<3:48:39,  3.09s/batch]Batch 15500/20010 Done, mean position loss: 22.952638187408446\n",
      "Training growing_up:  78%|██████▎ | 15682/20010 [12:19:40<3:18:07,  2.75s/batch]Batch 15200/20010 Done, mean position loss: 22.63656546354294\n",
      "Training growing_up:  76%|██████  | 15246/20010 [12:19:40<3:24:30,  2.58s/batch]Batch 15400/20010 Done, mean position loss: 22.372384927272797\n",
      "Training growing_up:  77%|██████▏ | 15430/20010 [12:19:54<4:18:49,  3.39s/batch]Batch 15400/20010 Done, mean position loss: 22.871178860664365\n",
      "Training growing_up:  78%|██████▏ | 15590/20010 [12:20:38<2:55:58,  2.39s/batch]Batch 15700/20010 Done, mean position loss: 22.559755637645722\n",
      "Training growing_up:  77%|██████▏ | 15447/20010 [12:20:43<3:10:20,  2.50s/batch]Batch 15600/20010 Done, mean position loss: 21.514722375869752\n",
      "Training growing_up:  77%|██████▏ | 15480/20010 [12:20:53<3:14:07,  2.57s/batch]Batch 15200/20010 Done, mean position loss: 21.70965917110443\n",
      "Training growing_up:  77%|██████▏ | 15429/20010 [12:21:01<3:29:45,  2.75s/batch]Batch 15500/20010 Done, mean position loss: 22.139591064453125\n",
      "Training growing_up:  78%|██████▏ | 15588/20010 [12:21:07<3:26:08,  2.80s/batch]Batch 15600/20010 Done, mean position loss: 22.61947543144226\n",
      "Training growing_up:  78%|██████▏ | 15552/20010 [12:21:09<3:32:33,  2.86s/batch]Batch 15800/20010 Done, mean position loss: 21.935660407543182\n",
      "Training growing_up:  79%|██████▎ | 15719/20010 [12:21:27<3:11:17,  2.67s/batch]Batch 15300/20010 Done, mean position loss: 23.20075632810593\n",
      "Training growing_up:  77%|██████▏ | 15447/20010 [12:21:28<3:15:38,  2.57s/batch]Batch 15500/20010 Done, mean position loss: 22.52209887266159\n",
      "Training growing_up:  78%|██████▎ | 15649/20010 [12:21:31<3:27:11,  2.85s/batch]Batch 15600/20010 Done, mean position loss: 23.028582127094268\n",
      "Training growing_up:  76%|██████  | 15307/20010 [12:21:45<3:45:52,  2.88s/batch]Batch 15600/20010 Done, mean position loss: 22.096157205104827\n",
      "Training growing_up:  78%|██████▏ | 15571/20010 [12:21:49<3:51:24,  3.13s/batch]Batch 15700/20010 Done, mean position loss: 22.031060013771057\n",
      "Training growing_up:  78%|██████▎ | 15679/20010 [12:21:50<3:35:41,  2.99s/batch]Batch 15300/20010 Done, mean position loss: 22.211048855781556\n",
      "Training growing_up:  78%|██████▎ | 15703/20010 [12:21:55<3:22:52,  2.83s/batch]Batch 15500/20010 Done, mean position loss: 23.31134270429611\n",
      "Training growing_up:  78%|██████▏ | 15577/20010 [12:22:07<3:39:07,  2.97s/batch]Batch 15600/20010 Done, mean position loss: 22.801971666812896\n",
      "Training growing_up:  78%|██████▏ | 15516/20010 [12:22:12<3:28:35,  2.79s/batch]Batch 15400/20010 Done, mean position loss: 21.81929579973221\n",
      "Training growing_up:  77%|██████▏ | 15482/20010 [12:22:17<3:35:41,  2.86s/batch]Batch 15700/20010 Done, mean position loss: 22.87631843328476\n",
      "Training growing_up:  78%|██████▏ | 15510/20010 [12:22:21<3:35:09,  2.87s/batch]Batch 15300/20010 Done, mean position loss: 21.556920318603517\n",
      "Training growing_up:  77%|██████▏ | 15458/20010 [12:22:24<3:37:52,  2.87s/batch]Batch 15600/20010 Done, mean position loss: 22.05852872133255\n",
      "Training growing_up:  77%|██████▏ | 15321/20010 [12:22:26<4:06:11,  3.15s/batch]Batch 15600/20010 Done, mean position loss: 22.52746819496155\n",
      "Training growing_up:  78%|██████▏ | 15584/20010 [12:22:39<3:39:23,  2.97s/batch]Batch 15500/20010 Done, mean position loss: 22.089477987289428\n",
      "Training growing_up:  77%|██████▏ | 15496/20010 [12:22:53<3:23:51,  2.71s/batch]Batch 15700/20010 Done, mean position loss: 22.122819805145262\n",
      "Training growing_up:  78%|██████▏ | 15631/20010 [12:22:59<3:33:32,  2.93s/batch]Batch 15300/20010 Done, mean position loss: 22.676993260383604\n",
      "Training growing_up:  77%|██████▏ | 15500/20010 [12:23:04<3:26:10,  2.74s/batch]Batch 15600/20010 Done, mean position loss: 21.75800808429718\n",
      "Training growing_up:  76%|██████  | 15273/20010 [12:23:07<3:56:21,  2.99s/batch]Batch 15500/20010 Done, mean position loss: 22.168065586090087\n",
      "Training growing_up:  77%|██████▏ | 15380/20010 [12:23:11<3:47:37,  2.95s/batch]Batch 15500/20010 Done, mean position loss: 22.67196253538132\n",
      "Training growing_up:  76%|██████  | 15276/20010 [12:23:17<4:02:13,  3.07s/batch]Batch 15500/20010 Done, mean position loss: 22.686152000427246\n",
      "Training growing_up:  78%|██████▏ | 15626/20010 [12:23:19<3:29:10,  2.86s/batch]Batch 15500/20010 Done, mean position loss: 22.96646957397461\n",
      "Training growing_up:  77%|██████▏ | 15502/20010 [12:23:20<3:29:37,  2.79s/batch]Batch 15600/20010 Done, mean position loss: 22.22152053356171\n",
      "Training growing_up:  77%|██████▏ | 15491/20010 [12:23:25<3:36:29,  2.87s/batch]Batch 15600/20010 Done, mean position loss: 22.215069696903228\n",
      "Training growing_up:  79%|██████▎ | 15763/20010 [12:23:32<3:18:49,  2.81s/batch]Batch 15600/20010 Done, mean position loss: 22.568252313137055\n",
      "Training growing_up:  78%|██████▏ | 15554/20010 [12:23:36<3:51:41,  3.12s/batch]Batch 15600/20010 Done, mean position loss: 22.073628084659575\n",
      "Training growing_up:  78%|██████▏ | 15609/20010 [12:23:41<3:23:25,  2.77s/batch]Batch 15600/20010 Done, mean position loss: 22.616119487285616\n",
      "Training growing_up:  78%|██████▏ | 15514/20010 [12:23:54<3:30:21,  2.81s/batch]Batch 15500/20010 Done, mean position loss: 22.55769189119339\n",
      "Training growing_up:  78%|██████▏ | 15542/20010 [12:23:55<3:48:09,  3.06s/batch]Batch 15700/20010 Done, mean position loss: 21.8927156829834\n",
      "Training growing_up:  78%|██████▏ | 15556/20010 [12:24:09<3:54:13,  3.16s/batch]Batch 15500/20010 Done, mean position loss: 23.59966052532196\n",
      "Training growing_up:  78%|██████▎ | 15674/20010 [12:24:13<3:20:18,  2.77s/batch]Batch 15600/20010 Done, mean position loss: 23.211025731563566\n",
      "Training growing_up:  78%|██████▏ | 15533/20010 [12:24:13<3:57:35,  3.18s/batch]Batch 15400/20010 Done, mean position loss: 22.78791535615921\n",
      "Training growing_up:  79%|██████▎ | 15713/20010 [12:24:30<3:29:39,  2.93s/batch]Batch 15300/20010 Done, mean position loss: 22.61591393232346\n",
      "Training growing_up:  77%|██████▏ | 15365/20010 [12:24:36<3:55:02,  3.04s/batch]Batch 15500/20010 Done, mean position loss: 22.331381177902223\n",
      "Training growing_up:  78%|██████▏ | 15569/20010 [12:24:48<3:08:19,  2.54s/batch]Batch 15500/20010 Done, mean position loss: 22.562500112056732\n",
      "Training growing_up:  78%|██████▎ | 15634/20010 [12:25:20<3:34:25,  2.94s/batch]Batch 15800/20010 Done, mean position loss: 22.502031779289247\n",
      "Training growing_up:  78%|██████▎ | 15684/20010 [12:25:33<3:38:49,  3.03s/batch]Batch 15700/20010 Done, mean position loss: 21.71820605993271\n",
      "Training growing_up:  78%|██████▏ | 15564/20010 [12:25:47<3:39:27,  2.96s/batch]Batch 15300/20010 Done, mean position loss: 21.96122022151947\n",
      "Training growing_up:  78%|██████▎ | 15646/20010 [12:25:54<3:29:42,  2.88s/batch]Batch 15600/20010 Done, mean position loss: 22.044949712753294\n",
      "Training growing_up:  78%|██████▏ | 15555/20010 [12:25:58<3:50:56,  3.11s/batch]Batch 15700/20010 Done, mean position loss: 22.420999536514284\n",
      "Training growing_up:  78%|██████▎ | 15649/20010 [12:26:04<3:45:34,  3.10s/batch]Batch 15900/20010 Done, mean position loss: 22.040021522045137\n",
      "Training growing_up:  78%|██████▎ | 15681/20010 [12:26:22<3:55:18,  3.26s/batch]Batch 15400/20010 Done, mean position loss: 23.13657722711563\n",
      "Training growing_up:  78%|██████▎ | 15688/20010 [12:26:24<4:04:39,  3.40s/batch]Batch 15700/20010 Done, mean position loss: 23.26355424880981\n",
      "Training growing_up:  79%|██████▎ | 15788/20010 [12:26:26<3:26:04,  2.93s/batch]Batch 15600/20010 Done, mean position loss: 22.70749542236328\n",
      "Training growing_up:  77%|██████▏ | 15408/20010 [12:26:46<4:14:26,  3.32s/batch]Batch 15800/20010 Done, mean position loss: 22.256965603828434\n",
      "Training growing_up:  78%|██████▏ | 15539/20010 [12:26:46<4:07:35,  3.32s/batch]Batch 15700/20010 Done, mean position loss: 21.881124904155733\n",
      "Training growing_up:  79%|██████▎ | 15796/20010 [12:26:53<3:39:34,  3.13s/batch]Batch 15400/20010 Done, mean position loss: 22.372383828163144\n",
      "Training growing_up:  79%|██████▎ | 15834/20010 [12:26:59<3:19:23,  2.86s/batch]Batch 15600/20010 Done, mean position loss: 23.14238193511963\n",
      "Training growing_up:  78%|██████▏ | 15549/20010 [12:27:03<3:29:34,  2.82s/batch]Batch 15700/20010 Done, mean position loss: 22.59600732088089\n",
      "Training growing_up:  79%|██████▎ | 15732/20010 [12:27:07<3:49:05,  3.21s/batch]Batch 15800/20010 Done, mean position loss: 22.82845211982727\n",
      "Training growing_up:  78%|██████▎ | 15677/20010 [12:27:16<3:53:21,  3.23s/batch]Batch 15500/20010 Done, mean position loss: 21.557807931900022\n",
      "Training growing_up:  78%|██████▎ | 15673/20010 [12:27:20<3:47:53,  3.15s/batch]Batch 15700/20010 Done, mean position loss: 22.72555290222168\n",
      "Training growing_up:  77%|██████▏ | 15419/20010 [12:27:23<3:53:23,  3.05s/batch]Batch 15400/20010 Done, mean position loss: 21.832049713134765\n",
      "Training growing_up:  78%|██████▎ | 15678/20010 [12:27:27<3:45:13,  3.12s/batch]Batch 15700/20010 Done, mean position loss: 22.025532050132753\n",
      "Training growing_up:  77%|██████▏ | 15392/20010 [12:27:41<3:42:58,  2.90s/batch]Batch 15600/20010 Done, mean position loss: 22.279426939487458\n",
      "Training growing_up:  78%|██████▏ | 15561/20010 [12:27:54<3:42:05,  3.00s/batch]Batch 15800/20010 Done, mean position loss: 22.276882524490357\n",
      "Training growing_up:  79%|██████▎ | 15856/20010 [12:28:04<3:23:24,  2.94s/batch]Batch 15700/20010 Done, mean position loss: 21.854765007495878\n",
      "Training growing_up:  78%|██████▎ | 15694/20010 [12:28:07<3:43:33,  3.11s/batch]Batch 15600/20010 Done, mean position loss: 22.890941700935365\n",
      "Training growing_up:  78%|██████▏ | 15599/20010 [12:28:09<3:53:37,  3.18s/batch]Batch 15400/20010 Done, mean position loss: 22.509508428573607\n",
      "Training growing_up:  79%|██████▎ | 15716/20010 [12:28:13<3:34:23,  3.00s/batch]Batch 15600/20010 Done, mean position loss: 22.153607997894287\n",
      "Training growing_up:  79%|██████▎ | 15830/20010 [12:28:14<3:36:45,  3.11s/batch]Batch 15600/20010 Done, mean position loss: 22.77405551671982\n",
      "Training growing_up:  79%|██████▎ | 15789/20010 [12:28:19<3:07:21,  2.66s/batch]Batch 15700/20010 Done, mean position loss: 22.105335643291475\n",
      "Training growing_up:  78%|██████▎ | 15684/20010 [12:28:22<3:27:29,  2.88s/batch]Batch 15700/20010 Done, mean position loss: 22.20312286615372\n",
      "Training growing_up:  78%|██████▏ | 15604/20010 [12:28:23<3:39:30,  2.99s/batch]Batch 15600/20010 Done, mean position loss: 22.942316336631777\n",
      "Training growing_up:  79%|██████▎ | 15709/20010 [12:28:28<3:45:46,  3.15s/batch]Batch 15700/20010 Done, mean position loss: 22.67321870803833\n",
      "Training growing_up:  77%|██████▏ | 15432/20010 [12:28:34<4:05:59,  3.22s/batch]Batch 15700/20010 Done, mean position loss: 22.214889321327206\n",
      "Training growing_up:  78%|██████▏ | 15622/20010 [12:28:44<3:35:12,  2.94s/batch]Batch 15700/20010 Done, mean position loss: 22.97831080198288\n",
      "Training growing_up:  77%|██████▏ | 15439/20010 [12:28:54<3:34:45,  2.82s/batch]Batch 15800/20010 Done, mean position loss: 21.63123535871506\n",
      "Training growing_up:  79%|██████▎ | 15769/20010 [12:28:58<3:17:08,  2.79s/batch]Batch 15600/20010 Done, mean position loss: 22.383369145393374\n",
      "Training growing_up:  78%|██████▏ | 15586/20010 [12:29:07<3:52:21,  3.15s/batch]Batch 15600/20010 Done, mean position loss: 23.466637210845946\n",
      "Training growing_up:  78%|██████▏ | 15594/20010 [12:29:14<3:39:32,  2.98s/batch]Batch 15700/20010 Done, mean position loss: 23.20356427192688\n",
      "Training growing_up:  79%|██████▎ | 15739/20010 [12:29:21<4:12:24,  3.55s/batch]Batch 15500/20010 Done, mean position loss: 22.43761895895004\n",
      "Training growing_up:  79%|██████▎ | 15744/20010 [12:29:35<3:18:30,  2.79s/batch]Batch 15600/20010 Done, mean position loss: 22.40005303621292\n",
      "Training growing_up:  79%|██████▎ | 15732/20010 [12:29:37<3:13:19,  2.71s/batch]Batch 15400/20010 Done, mean position loss: 22.54235584497452\n",
      "Training growing_up:  77%|██████▏ | 15405/20010 [12:29:50<4:03:13,  3.17s/batch]Batch 15600/20010 Done, mean position loss: 22.484528942108156\n",
      "Training growing_up:  79%|██████▎ | 15732/20010 [12:30:15<3:23:59,  2.86s/batch]Batch 15900/20010 Done, mean position loss: 22.401685800552368\n",
      "Training growing_up:  79%|██████▎ | 15870/20010 [12:30:34<3:20:09,  2.90s/batch]Batch 15800/20010 Done, mean position loss: 21.556159946918488\n",
      "Training growing_up:  79%|██████▎ | 15734/20010 [12:30:51<3:40:43,  3.10s/batch]Batch 15400/20010 Done, mean position loss: 21.943916656970977\n",
      "Training growing_up:  78%|██████▎ | 15666/20010 [12:30:53<3:31:00,  2.91s/batch]Batch 15700/20010 Done, mean position loss: 21.855238783359525\n",
      "Training growing_up:  79%|██████▎ | 15749/20010 [12:30:54<3:44:34,  3.16s/batch]Batch 15800/20010 Done, mean position loss: 22.255037813186647\n",
      "Training growing_up:  79%|██████▎ | 15752/20010 [12:31:04<3:58:30,  3.36s/batch]Batch 16000/20010 Done, mean position loss: 21.702440400123596\n",
      "Training growing_up:  78%|██████▎ | 15650/20010 [12:31:22<3:22:38,  2.79s/batch]Batch 15500/20010 Done, mean position loss: 23.16151486158371\n",
      "Training growing_up:  79%|██████▎ | 15782/20010 [12:31:26<3:21:54,  2.87s/batch]Batch 15800/20010 Done, mean position loss: 23.051574807167054\n",
      "Training growing_up:  79%|██████▎ | 15762/20010 [12:31:33<3:20:47,  2.84s/batch]Batch 15700/20010 Done, mean position loss: 22.610251162052155\n",
      "Training growing_up:  78%|██████▎ | 15643/20010 [12:31:41<3:46:50,  3.12s/batch]Batch 15800/20010 Done, mean position loss: 21.64789959192276\n",
      "Training growing_up:  77%|██████▏ | 15496/20010 [12:31:45<3:34:23,  2.85s/batch]Batch 15900/20010 Done, mean position loss: 22.38139045238495\n",
      "Training growing_up:  78%|██████▎ | 15663/20010 [12:32:00<3:23:09,  2.80s/batch]Batch 15500/20010 Done, mean position loss: 22.059346206188202\n",
      "Training growing_up:  80%|██████▍ | 16023/20010 [12:32:05<3:05:50,  2.80s/batch]Batch 15800/20010 Done, mean position loss: 22.570777368545528\n",
      "Training growing_up:  79%|██████▎ | 15887/20010 [12:32:06<3:14:55,  2.84s/batch]Batch 15700/20010 Done, mean position loss: 22.922700376510623\n",
      "Training growing_up:  79%|██████▎ | 15760/20010 [12:32:07<3:03:03,  2.58s/batch]Batch 15900/20010 Done, mean position loss: 22.9984415102005\n",
      "Training growing_up:  77%|██████▏ | 15481/20010 [12:32:17<3:47:25,  3.01s/batch]Batch 15600/20010 Done, mean position loss: 21.823907837867736\n",
      "Training growing_up:  79%|██████▎ | 15870/20010 [12:32:18<3:24:44,  2.97s/batch]Batch 15800/20010 Done, mean position loss: 22.58217482805252\n",
      "Training growing_up:  78%|██████▎ | 15665/20010 [12:32:21<3:51:59,  3.20s/batch]Batch 15800/20010 Done, mean position loss: 22.217177917957308\n",
      "Training growing_up:  79%|██████▎ | 15733/20010 [12:32:26<3:14:15,  2.73s/batch]Batch 15500/20010 Done, mean position loss: 21.73268676042557\n",
      "Training growing_up:  78%|██████▏ | 15607/20010 [12:32:34<3:36:07,  2.95s/batch]Batch 15700/20010 Done, mean position loss: 22.296266615390778\n",
      "Training growing_up:  78%|██████▏ | 15530/20010 [12:32:48<3:38:59,  2.93s/batch]Batch 15900/20010 Done, mean position loss: 22.10529680252075\n",
      "Training growing_up:  78%|██████▎ | 15698/20010 [12:33:00<3:21:51,  2.81s/batch]Batch 15700/20010 Done, mean position loss: 22.930762128829954\n",
      "Training growing_up:  80%|██████▍ | 16041/20010 [12:33:00<3:20:37,  3.03s/batch]Batch 15800/20010 Done, mean position loss: 21.98365833044052\n",
      "Training growing_up:  77%|██████▏ | 15469/20010 [12:33:11<3:40:28,  2.91s/batch]Batch 15700/20010 Done, mean position loss: 22.557155604362485\n",
      "Training growing_up:  78%|██████▎ | 15703/20010 [12:33:15<3:14:27,  2.71s/batch]Batch 15500/20010 Done, mean position loss: 22.588263864517213\n",
      "Training growing_up:  79%|██████▎ | 15785/20010 [12:33:18<3:11:53,  2.72s/batch]Batch 15800/20010 Done, mean position loss: 22.3135373878479\n",
      "Training growing_up:  78%|██████▎ | 15700/20010 [12:33:19<3:14:52,  2.71s/batch]Batch 15700/20010 Done, mean position loss: 22.25820477962494\n",
      "Training growing_up:  79%|██████▎ | 15738/20010 [12:33:20<3:47:21,  3.19s/batch]Batch 15800/20010 Done, mean position loss: 22.202396721839904\n",
      "Training growing_up:  78%|██████▎ | 15673/20010 [12:33:21<3:12:08,  2.66s/batch]Batch 15700/20010 Done, mean position loss: 23.176870465278625\n",
      "Training growing_up:  80%|██████▍ | 15967/20010 [12:33:30<3:22:19,  3.00s/batch]Batch 15800/20010 Done, mean position loss: 22.443866181373597\n",
      "Training growing_up:  79%|██████▎ | 15732/20010 [12:33:37<3:07:00,  2.62s/batch]Batch 15800/20010 Done, mean position loss: 23.147545764446257\n",
      "Training growing_up:  79%|██████▎ | 15801/20010 [12:33:37<3:22:19,  2.88s/batch]Batch 15800/20010 Done, mean position loss: 22.287421531677246\n",
      "Training growing_up:  79%|██████▎ | 15747/20010 [12:33:47<3:22:44,  2.85s/batch]Batch 15900/20010 Done, mean position loss: 21.78299114465714\n",
      "Training growing_up:  79%|██████▎ | 15832/20010 [12:33:50<2:58:51,  2.57s/batch]Batch 15700/20010 Done, mean position loss: 22.57143005132675\n",
      "Training growing_up:  79%|██████▎ | 15823/20010 [12:34:05<3:08:12,  2.70s/batch]Batch 15800/20010 Done, mean position loss: 23.052119410037996\n",
      "Training growing_up:  80%|██████▎ | 15909/20010 [12:34:09<3:11:30,  2.80s/batch]Batch 15700/20010 Done, mean position loss: 23.5171693277359\n",
      "Training growing_up:  80%|██████▍ | 15986/20010 [12:34:23<2:52:32,  2.57s/batch]Batch 15600/20010 Done, mean position loss: 22.505973360538484\n",
      "Training growing_up:  79%|██████▎ | 15750/20010 [12:34:28<3:25:36,  2.90s/batch]Batch 15700/20010 Done, mean position loss: 22.437174661159517\n",
      "Training growing_up:  79%|██████▎ | 15888/20010 [12:34:42<2:41:56,  2.36s/batch]Batch 15700/20010 Done, mean position loss: 22.66207040309906\n",
      "Training growing_up:  79%|██████▎ | 15852/20010 [12:34:45<3:03:09,  2.64s/batch]Batch 15500/20010 Done, mean position loss: 22.330592644214633\n",
      "Training growing_up:  79%|██████▎ | 15832/20010 [12:35:02<3:17:19,  2.83s/batch]Batch 16000/20010 Done, mean position loss: 22.64901405334473\n",
      "Training growing_up:  80%|██████▍ | 15970/20010 [12:35:22<3:12:11,  2.85s/batch]Batch 15900/20010 Done, mean position loss: 21.96673443078995\n",
      "Training growing_up:  78%|██████▏ | 15588/20010 [12:35:34<3:04:30,  2.50s/batch]Batch 15800/20010 Done, mean position loss: 21.81384225845337\n",
      "Training growing_up:  80%|██████▍ | 15984/20010 [12:35:40<2:42:08,  2.42s/batch]Batch 16100/20010 Done, mean position loss: 21.75537586450577\n",
      "Training growing_up:  79%|██████▎ | 15755/20010 [12:35:48<2:45:50,  2.34s/batch]Batch 15900/20010 Done, mean position loss: 22.576790652275083\n",
      "Training growing_up:  78%|██████▎ | 15633/20010 [12:35:51<3:12:42,  2.64s/batch]Batch 15500/20010 Done, mean position loss: 21.898995010852815\n",
      "Training growing_up:  79%|██████▎ | 15881/20010 [12:36:06<3:27:58,  3.02s/batch]Batch 15900/20010 Done, mean position loss: 22.91271795988083\n",
      "Training growing_up:  80%|██████▍ | 16025/20010 [12:36:07<2:42:08,  2.44s/batch]Batch 15600/20010 Done, mean position loss: 22.996875660419462\n",
      "Training growing_up:  78%|██████▏ | 15602/20010 [12:36:10<3:03:15,  2.49s/batch]Batch 15800/20010 Done, mean position loss: 22.556288626194\n",
      "Training growing_up:  80%|██████▎ | 15915/20010 [12:36:27<3:38:17,  3.20s/batch]Batch 16000/20010 Done, mean position loss: 22.19964196920395\n",
      "Training growing_up:  79%|██████▎ | 15822/20010 [12:36:31<3:56:35,  3.39s/batch]Batch 15900/20010 Done, mean position loss: 21.834614942073824\n",
      "Training growing_up:  79%|██████▎ | 15868/20010 [12:36:43<3:24:47,  2.97s/batch]Batch 15600/20010 Done, mean position loss: 22.129318058490753\n",
      "Training growing_up:  79%|██████▎ | 15751/20010 [12:36:47<3:23:26,  2.87s/batch]Batch 16000/20010 Done, mean position loss: 22.783457062244416\n",
      "Training growing_up:  78%|██████▎ | 15698/20010 [12:36:51<3:24:29,  2.85s/batch]Batch 15800/20010 Done, mean position loss: 22.717028517723083\n",
      "Training growing_up:  80%|██████▎ | 15909/20010 [12:36:54<3:18:10,  2.90s/batch]Batch 15900/20010 Done, mean position loss: 22.672947230339048\n",
      "Training growing_up:  79%|██████▎ | 15887/20010 [12:36:58<2:48:40,  2.45s/batch]Batch 15900/20010 Done, mean position loss: 22.512677323818206\n",
      "Training growing_up:  79%|██████▎ | 15755/20010 [12:36:59<3:30:57,  2.97s/batch]Batch 15700/20010 Done, mean position loss: 21.717057952880857\n",
      "Training growing_up:  78%|██████▏ | 15580/20010 [12:37:02<3:28:43,  2.83s/batch]Batch 15900/20010 Done, mean position loss: 22.12178678512573\n",
      "Training growing_up:  79%|██████▎ | 15772/20010 [12:37:11<3:26:14,  2.92s/batch]Batch 15600/20010 Done, mean position loss: 22.00237745523453\n",
      "Training growing_up:  79%|██████▎ | 15754/20010 [12:37:12<3:21:14,  2.84s/batch]Batch 15800/20010 Done, mean position loss: 22.44207677602768\n",
      "Training growing_up:  79%|██████▎ | 15793/20010 [12:37:33<3:06:01,  2.65s/batch]Batch 16000/20010 Done, mean position loss: 21.95973475933075\n",
      "Training growing_up:  79%|██████▎ | 15887/20010 [12:37:36<3:05:45,  2.70s/batch]Batch 15900/20010 Done, mean position loss: 21.874358658790587\n",
      "Training growing_up:  78%|██████▏ | 15612/20010 [12:37:43<3:23:04,  2.77s/batch]Batch 15800/20010 Done, mean position loss: 22.61968042612076\n",
      "Training growing_up:  80%|██████▎ | 15943/20010 [12:37:44<3:16:11,  2.89s/batch]Batch 15800/20010 Done, mean position loss: 22.58135046482086\n",
      "Training growing_up:  80%|██████▎ | 15921/20010 [12:37:55<2:48:58,  2.48s/batch]Batch 15900/20010 Done, mean position loss: 22.065698142051694\n",
      "Training growing_up:  79%|██████▎ | 15887/20010 [12:37:55<2:59:14,  2.61s/batch]Batch 15800/20010 Done, mean position loss: 22.047158713340757\n",
      "Training growing_up:  79%|██████▎ | 15802/20010 [12:37:58<2:56:39,  2.52s/batch]Batch 15800/20010 Done, mean position loss: 23.216558279991148\n",
      "Training growing_up:  79%|██████▎ | 15801/20010 [12:37:58<3:30:59,  3.01s/batch]Batch 15900/20010 Done, mean position loss: 22.35041516304016\n",
      "Training growing_up:  78%|██████▏ | 15620/20010 [12:38:05<3:18:14,  2.71s/batch]Batch 15600/20010 Done, mean position loss: 22.577393217086794\n",
      "Training growing_up:  79%|██████▎ | 15841/20010 [12:38:08<3:46:01,  3.25s/batch]Batch 15900/20010 Done, mean position loss: 22.68238445520401\n",
      "Training growing_up:  78%|██████▏ | 15631/20010 [12:38:10<3:27:44,  2.85s/batch]Batch 15900/20010 Done, mean position loss: 22.01753318786621\n",
      "Training growing_up:  79%|██████▎ | 15895/20010 [12:38:18<3:10:16,  2.77s/batch]Batch 15900/20010 Done, mean position loss: 22.555375425815583\n",
      "Training growing_up:  78%|██████▎ | 15634/20010 [12:38:19<3:40:15,  3.02s/batch]Batch 16000/20010 Done, mean position loss: 21.573062269687654\n",
      "Training growing_up:  79%|██████▎ | 15784/20010 [12:38:35<3:02:47,  2.60s/batch]Batch 15800/20010 Done, mean position loss: 22.29156317472458\n",
      "Training growing_up:  80%|██████▎ | 15923/20010 [12:38:36<3:10:42,  2.80s/batch]Batch 15900/20010 Done, mean position loss: 23.000911452770232\n",
      "Training growing_up:  80%|██████▎ | 15913/20010 [12:38:51<3:06:58,  2.74s/batch]Batch 15800/20010 Done, mean position loss: 23.79070244550705\n",
      "Training growing_up:  80%|██████▍ | 15964/20010 [12:39:05<3:08:15,  2.79s/batch]Batch 15700/20010 Done, mean position loss: 22.509747207164764\n",
      "Training growing_up:  79%|██████▎ | 15828/20010 [12:39:12<3:17:07,  2.83s/batch]Batch 15800/20010 Done, mean position loss: 22.697388288974764\n",
      "Training growing_up:  80%|██████▍ | 15955/20010 [12:39:27<3:19:56,  2.96s/batch]Batch 15800/20010 Done, mean position loss: 22.561252119541166\n",
      "Training growing_up:  80%|██████▍ | 16097/20010 [12:39:30<3:18:56,  3.05s/batch]Batch 15600/20010 Done, mean position loss: 22.316652982234956\n",
      "Training growing_up:  79%|██████▎ | 15756/20010 [12:39:42<3:19:04,  2.81s/batch]Batch 16100/20010 Done, mean position loss: 22.509853537082673\n",
      "Training growing_up:  80%|██████▍ | 16078/20010 [12:40:05<3:10:31,  2.91s/batch]Batch 16000/20010 Done, mean position loss: 21.895728769302366\n",
      "Training growing_up:  78%|██████▎ | 15665/20010 [12:40:17<3:35:55,  2.98s/batch]Batch 16200/20010 Done, mean position loss: 21.85935093164444\n",
      "Training growing_up:  80%|██████▎ | 15945/20010 [12:40:18<3:17:05,  2.91s/batch]Batch 15900/20010 Done, mean position loss: 21.925559058189393\n",
      "Training growing_up:  78%|██████▏ | 15621/20010 [12:40:30<3:25:36,  2.81s/batch]Batch 16000/20010 Done, mean position loss: 22.44435542821884\n",
      "Training growing_up:  80%|██████▎ | 15911/20010 [12:40:45<3:00:45,  2.65s/batch]Batch 15600/20010 Done, mean position loss: 21.823344140052797\n",
      "Training growing_up:  80%|██████▍ | 15956/20010 [12:40:53<3:18:33,  2.94s/batch]Batch 16000/20010 Done, mean position loss: 22.902962827682494\n",
      "Training growing_up:  79%|██████▎ | 15783/20010 [12:41:00<3:19:31,  2.83s/batch]Batch 15700/20010 Done, mean position loss: 22.884315292835236\n",
      "Training growing_up:  79%|██████▎ | 15866/20010 [12:41:03<3:03:27,  2.66s/batch]Batch 15900/20010 Done, mean position loss: 22.473082511425016\n",
      "Training growing_up:  80%|██████▍ | 15963/20010 [12:41:15<3:21:53,  2.99s/batch]Batch 16100/20010 Done, mean position loss: 21.91755550861359\n",
      "Training growing_up:  79%|██████▎ | 15875/20010 [12:41:23<3:33:56,  3.10s/batch]Batch 16000/20010 Done, mean position loss: 21.830223076343536\n",
      "Training growing_up:  79%|██████▎ | 15845/20010 [12:41:36<3:05:22,  2.67s/batch]Batch 16000/20010 Done, mean position loss: 22.800969681739808\n",
      "Training growing_up:  79%|██████▎ | 15880/20010 [12:41:40<3:45:33,  3.28s/batch]Batch 16100/20010 Done, mean position loss: 22.878669917583466\n",
      "Training growing_up:  80%|██████▍ | 15997/20010 [12:41:41<3:23:09,  3.04s/batch]Batch 15900/20010 Done, mean position loss: 22.89528562784195\n",
      "Training growing_up:  80%|██████▍ | 15998/20010 [12:41:43<3:08:28,  2.82s/batch]Batch 16000/20010 Done, mean position loss: 22.61270524978638\n",
      "Training growing_up:  80%|██████▎ | 15915/20010 [12:41:45<3:13:54,  2.84s/batch]Batch 15700/20010 Done, mean position loss: 21.665362000465393\n",
      "Training growing_up:  79%|██████▎ | 15899/20010 [12:41:53<3:17:36,  2.88s/batch]Batch 16000/20010 Done, mean position loss: 22.175942184925077\n",
      "Training growing_up:  79%|██████▎ | 15906/20010 [12:41:56<3:22:06,  2.95s/batch]Batch 15800/20010 Done, mean position loss: 21.769920532703402\n",
      "Training growing_up:  79%|██████▎ | 15887/20010 [12:42:01<4:16:09,  3.73s/batch]Batch 15900/20010 Done, mean position loss: 22.103470551967618\n",
      "Training growing_up:  80%|██████▍ | 16005/20010 [12:42:07<3:30:02,  3.15s/batch]Batch 15700/20010 Done, mean position loss: 21.686572632789613\n",
      "Training growing_up:  80%|██████▎ | 15909/20010 [12:42:27<3:43:48,  3.27s/batch]Batch 16100/20010 Done, mean position loss: 22.067327971458433\n",
      "Training growing_up:  79%|██████▎ | 15894/20010 [12:42:33<3:13:04,  2.81s/batch]Batch 16000/20010 Done, mean position loss: 21.905570673942563\n",
      "Training growing_up:  80%|██████▎ | 15914/20010 [12:42:43<3:38:11,  3.20s/batch]Batch 15900/20010 Done, mean position loss: 22.797404124736786\n",
      "Training growing_up:  80%|██████▍ | 16006/20010 [12:42:47<3:09:35,  2.84s/batch]Batch 15900/20010 Done, mean position loss: 22.527106921672818\n",
      "Training growing_up:  79%|██████▎ | 15886/20010 [12:42:50<3:21:25,  2.93s/batch]Batch 16000/20010 Done, mean position loss: 22.158158807754518\n",
      "Training growing_up:  80%|██████▍ | 15998/20010 [12:42:54<3:16:24,  2.94s/batch]Batch 16000/20010 Done, mean position loss: 22.371356778144836\n",
      "Training growing_up:  80%|██████▍ | 16025/20010 [12:42:55<3:17:58,  2.98s/batch]Batch 15900/20010 Done, mean position loss: 23.06659092903137\n",
      "Training growing_up:  80%|██████▍ | 16003/20010 [12:42:55<2:57:03,  2.65s/batch]Batch 15900/20010 Done, mean position loss: 21.97066190481186\n",
      "Training growing_up:  81%|██████▍ | 16113/20010 [12:43:02<3:00:58,  2.79s/batch]Batch 16000/20010 Done, mean position loss: 22.0226326918602\n",
      "Training growing_up:  80%|██████▍ | 16000/20010 [12:43:08<3:39:40,  3.29s/batch]Batch 15700/20010 Done, mean position loss: 22.75748812675476\n",
      "Training growing_up:  80%|██████▍ | 16005/20010 [12:43:11<2:43:08,  2.44s/batch]Batch 16000/20010 Done, mean position loss: 22.39254302978516\n",
      "Training growing_up:  80%|██████▎ | 15910/20010 [12:43:13<3:16:55,  2.88s/batch]Batch 16000/20010 Done, mean position loss: 22.733039362430574\n",
      "Training growing_up:  81%|██████▍ | 16141/20010 [12:43:18<3:07:49,  2.91s/batch]Batch 16100/20010 Done, mean position loss: 21.737524814605713\n",
      "Training growing_up:  80%|██████▍ | 16012/20010 [12:43:33<3:22:54,  3.05s/batch]Batch 16000/20010 Done, mean position loss: 23.085638432502748\n",
      "Training growing_up:  80%|██████▍ | 16038/20010 [12:43:33<3:05:58,  2.81s/batch]Batch 15900/20010 Done, mean position loss: 22.474620871543884\n",
      "Training growing_up:  80%|██████▍ | 16051/20010 [12:43:51<3:22:11,  3.06s/batch]Batch 15900/20010 Done, mean position loss: 23.359101724624633\n",
      "Training growing_up:  81%|██████▍ | 16191/20010 [12:44:07<3:13:14,  3.04s/batch]Batch 15800/20010 Done, mean position loss: 22.586436028480527\n",
      "Training growing_up:  80%|██████▍ | 15964/20010 [12:44:13<3:36:57,  3.22s/batch]Batch 15900/20010 Done, mean position loss: 22.390807423591614\n",
      "Training growing_up:  79%|██████▎ | 15770/20010 [12:44:31<3:21:37,  2.85s/batch]Batch 15900/20010 Done, mean position loss: 22.689387989044192\n",
      "Training growing_up:  80%|██████▍ | 15951/20010 [12:44:37<3:22:57,  3.00s/batch]Batch 16200/20010 Done, mean position loss: 22.575848777294162\n",
      "Training growing_up:  80%|██████▍ | 16055/20010 [12:44:38<3:34:48,  3.26s/batch]Batch 15700/20010 Done, mean position loss: 22.373778669834135\n",
      "Training growing_up:  80%|██████▍ | 16044/20010 [12:45:08<3:17:45,  2.99s/batch]Batch 16100/20010 Done, mean position loss: 21.741887493133547\n",
      "Training growing_up:  80%|██████▍ | 16041/20010 [12:45:15<3:17:01,  2.98s/batch]Batch 16300/20010 Done, mean position loss: 21.589981820583343\n",
      "Training growing_up:  80%|██████▍ | 15947/20010 [12:45:16<3:25:56,  3.04s/batch]Batch 16000/20010 Done, mean position loss: 22.107392058372497\n",
      "Training growing_up:  80%|██████▍ | 16076/20010 [12:45:41<3:25:00,  3.13s/batch]Batch 16100/20010 Done, mean position loss: 22.289549083709716\n",
      "Training growing_up:  80%|██████▎ | 15943/20010 [12:45:52<3:23:12,  3.00s/batch]Batch 15700/20010 Done, mean position loss: 21.793434963226318\n",
      "Training growing_up:  78%|██████▎ | 15701/20010 [12:45:53<3:45:43,  3.14s/batch]Batch 16100/20010 Done, mean position loss: 22.797270193099976\n",
      "Training growing_up:  81%|██████▍ | 16173/20010 [12:46:02<3:00:42,  2.83s/batch]Batch 15800/20010 Done, mean position loss: 23.059666533470153\n",
      "Training growing_up:  80%|██████▍ | 15967/20010 [12:46:03<3:20:16,  2.97s/batch]Batch 16000/20010 Done, mean position loss: 22.318594636917112\n",
      "Training growing_up:  81%|██████▍ | 16109/20010 [12:46:14<3:03:44,  2.83s/batch]Batch 16200/20010 Done, mean position loss: 21.84314965009689\n",
      "Training growing_up:  81%|██████▍ | 16162/20010 [12:46:19<3:14:55,  3.04s/batch]Batch 16100/20010 Done, mean position loss: 21.992077763080598\n",
      "Training growing_up:  80%|██████▍ | 15946/20010 [12:46:30<3:36:15,  3.19s/batch]Batch 16100/20010 Done, mean position loss: 22.584999637603758\n",
      "Training growing_up:  80%|██████▍ | 16067/20010 [12:46:35<3:38:05,  3.32s/batch]Batch 16000/20010 Done, mean position loss: 22.904657092094418\n",
      "Training growing_up:  81%|██████▍ | 16211/20010 [12:46:42<2:43:31,  2.58s/batch]Batch 16100/20010 Done, mean position loss: 22.521085398197172\n",
      "Training growing_up:  80%|██████▍ | 15946/20010 [12:46:46<3:46:38,  3.35s/batch]Batch 16200/20010 Done, mean position loss: 23.040073583126066\n",
      "Training growing_up:  81%|██████▍ | 16125/20010 [12:46:49<3:16:49,  3.04s/batch]Batch 15800/20010 Done, mean position loss: 22.020774924755095\n",
      "Training growing_up:  80%|██████▍ | 16090/20010 [12:46:55<2:54:00,  2.66s/batch]Batch 16100/20010 Done, mean position loss: 22.133776323795317\n",
      "Training growing_up:  81%|██████▍ | 16206/20010 [12:47:01<3:02:59,  2.89s/batch]Batch 15900/20010 Done, mean position loss: 21.945182807445526\n",
      "Training growing_up:  80%|██████▍ | 15988/20010 [12:47:07<3:18:18,  2.96s/batch]Batch 16000/20010 Done, mean position loss: 21.795368382930757\n",
      "Training growing_up:  80%|██████▍ | 16073/20010 [12:47:09<3:21:47,  3.08s/batch]Batch 15800/20010 Done, mean position loss: 21.70181679725647\n",
      "Training growing_up:  81%|██████▍ | 16115/20010 [12:47:23<3:17:23,  3.04s/batch]Batch 16200/20010 Done, mean position loss: 22.122361330986024\n",
      "Training growing_up:  79%|██████▎ | 15733/20010 [12:47:29<3:41:08,  3.10s/batch]Batch 16100/20010 Done, mean position loss: 21.790584037303926\n",
      "Training growing_up:  80%|██████▍ | 15992/20010 [12:47:34<3:13:12,  2.89s/batch]Batch 16000/20010 Done, mean position loss: 22.743438258171082\n",
      "Training growing_up:  80%|██████▍ | 16094/20010 [12:47:43<3:20:08,  3.07s/batch]Batch 16100/20010 Done, mean position loss: 22.545627558231352\n",
      "Training growing_up:  80%|██████▍ | 16090/20010 [12:47:45<3:04:42,  2.83s/batch]Batch 16000/20010 Done, mean position loss: 22.44713630914688\n",
      "Training growing_up:  81%|██████▍ | 16147/20010 [12:47:52<3:04:33,  2.87s/batch]Batch 16000/20010 Done, mean position loss: 22.100510773658755\n",
      "Batch 16100/20010 Done, mean position loss: 22.188327531814576\n",
      "Training growing_up:  81%|██████▍ | 16128/20010 [12:48:00<2:52:36,  2.67s/batch]Batch 16000/20010 Done, mean position loss: 23.009616966247556\n",
      "Training growing_up:  81%|██████▍ | 16160/20010 [12:48:04<3:53:27,  3.64s/batch]Batch 16100/20010 Done, mean position loss: 22.125657906532286\n",
      "Training growing_up:  80%|██████▍ | 16034/20010 [12:48:10<3:31:04,  3.19s/batch]Batch 16100/20010 Done, mean position loss: 22.514136283397672\n",
      "Training growing_up:  81%|██████▍ | 16111/20010 [12:48:13<3:23:33,  3.13s/batch]Batch 16200/20010 Done, mean position loss: 21.705518922805787\n",
      "Training growing_up:  80%|██████▍ | 15993/20010 [12:48:19<3:10:29,  2.85s/batch]Batch 16100/20010 Done, mean position loss: 22.824893157482148\n",
      "Training growing_up:  80%|██████▍ | 15985/20010 [12:48:24<3:22:36,  3.02s/batch]Batch 15800/20010 Done, mean position loss: 22.603782370090485\n",
      "Training growing_up:  80%|██████▍ | 16017/20010 [12:48:32<2:58:58,  2.69s/batch]Batch 16100/20010 Done, mean position loss: 22.94294598579407\n",
      "Training growing_up:  81%|██████▍ | 16124/20010 [12:48:38<3:03:19,  2.83s/batch]Batch 16000/20010 Done, mean position loss: 22.314381346702575\n",
      "Training growing_up:  80%|██████▍ | 16017/20010 [12:48:45<2:55:47,  2.64s/batch]Batch 16000/20010 Done, mean position loss: 23.429998416900638\n",
      "Training growing_up:  80%|██████▍ | 16028/20010 [12:49:05<3:06:45,  2.81s/batch]Batch 15900/20010 Done, mean position loss: 22.570855672359468\n",
      "Training growing_up:  82%|██████▌ | 16379/20010 [12:49:08<2:56:07,  2.91s/batch]Batch 16000/20010 Done, mean position loss: 22.1583198595047\n",
      "Training growing_up:  79%|██████▎ | 15848/20010 [12:49:27<2:56:30,  2.54s/batch]Batch 16000/20010 Done, mean position loss: 22.689150168895722\n",
      "Training growing_up:  80%|██████▍ | 16043/20010 [12:49:34<2:59:32,  2.72s/batch]Batch 16300/20010 Done, mean position loss: 22.487792220115665\n",
      "Training growing_up:  80%|██████▍ | 15956/20010 [12:49:43<3:09:13,  2.80s/batch]Batch 15800/20010 Done, mean position loss: 22.442664494514467\n",
      "Training growing_up:  79%|██████▎ | 15859/20010 [12:49:58<3:33:51,  3.09s/batch]Batch 16200/20010 Done, mean position loss: 21.67778916120529\n",
      "Training growing_up:  79%|██████▎ | 15837/20010 [12:50:06<3:14:56,  2.80s/batch]Batch 16400/20010 Done, mean position loss: 21.7199254155159\n",
      "Training growing_up:  79%|██████▎ | 15811/20010 [12:50:10<2:57:54,  2.54s/batch]Batch 16100/20010 Done, mean position loss: 21.877813034057617\n",
      "Training growing_up:  81%|██████▌ | 16277/20010 [12:50:25<2:40:08,  2.57s/batch]Batch 16200/20010 Done, mean position loss: 22.409494023323056\n",
      "Training growing_up:  80%|██████▍ | 16061/20010 [12:50:44<3:04:43,  2.81s/batch]Batch 16200/20010 Done, mean position loss: 22.996786236763\n",
      "Training growing_up:  81%|██████▍ | 16255/20010 [12:50:48<2:42:08,  2.59s/batch]Batch 15800/20010 Done, mean position loss: 21.79764687538147\n",
      "Training growing_up:  81%|██████▍ | 16256/20010 [12:50:51<2:50:22,  2.72s/batch]Batch 16300/20010 Done, mean position loss: 22.032635574340823\n",
      "Training growing_up:  81%|██████▍ | 16193/20010 [12:50:54<2:36:07,  2.45s/batch]Batch 16100/20010 Done, mean position loss: 22.439106979370116\n",
      "Training growing_up:  81%|██████▍ | 16154/20010 [12:50:55<2:40:17,  2.49s/batch]Batch 15900/20010 Done, mean position loss: 23.05126237630844\n",
      "Training growing_up:  80%|██████▍ | 15946/20010 [12:51:07<3:01:05,  2.67s/batch]Batch 16200/20010 Done, mean position loss: 21.809675946235657\n",
      "Training growing_up:  81%|██████▍ | 16175/20010 [12:51:13<3:12:00,  3.00s/batch]Batch 16200/20010 Done, mean position loss: 22.75035602092743\n",
      "Training growing_up:  81%|██████▍ | 16126/20010 [12:51:18<2:57:38,  2.74s/batch]Batch 16100/20010 Done, mean position loss: 22.787821421623228\n",
      "Training growing_up:  81%|██████▍ | 16165/20010 [12:51:25<2:50:53,  2.67s/batch]Batch 16200/20010 Done, mean position loss: 22.638330640792848\n",
      "Training growing_up:  80%|██████▍ | 16045/20010 [12:51:26<2:49:17,  2.56s/batch]Batch 16300/20010 Done, mean position loss: 22.87509200811386\n",
      "Training growing_up:  80%|██████▍ | 16078/20010 [12:51:34<3:32:00,  3.24s/batch]Batch 15900/20010 Done, mean position loss: 21.78136921405792\n",
      "Training growing_up:  80%|██████▍ | 16064/20010 [12:51:38<3:16:46,  2.99s/batch]Batch 16200/20010 Done, mean position loss: 22.28406498670578\n",
      "Training growing_up:  81%|██████▍ | 16136/20010 [12:51:46<2:59:26,  2.78s/batch]Batch 15900/20010 Done, mean position loss: 21.638971359729766\n",
      "Training growing_up:  79%|██████▎ | 15902/20010 [12:51:49<3:09:06,  2.76s/batch]Batch 16000/20010 Done, mean position loss: 21.765886538028717\n",
      "Batch 16100/20010 Done, mean position loss: 22.20680921316147\n",
      "Training growing_up:  82%|██████▌ | 16444/20010 [12:52:04<2:54:38,  2.94s/batch]Batch 16300/20010 Done, mean position loss: 22.122390632629394\n",
      "Training growing_up:  80%|██████▍ | 16009/20010 [12:52:12<3:19:36,  2.99s/batch]Batch 16200/20010 Done, mean position loss: 22.135036895275114\n",
      "Training growing_up:  81%|██████▍ | 16110/20010 [12:52:14<3:20:44,  3.09s/batch]Batch 16100/20010 Done, mean position loss: 22.787981569767\n",
      "Training growing_up:  81%|██████▍ | 16218/20010 [12:52:28<2:48:24,  2.66s/batch]Batch 16200/20010 Done, mean position loss: 21.9986806678772\n",
      "Training growing_up:  82%|██████▌ | 16367/20010 [12:52:30<2:41:13,  2.66s/batch]Batch 16100/20010 Done, mean position loss: 22.515692718029023\n",
      "Training growing_up:  79%|██████▎ | 15890/20010 [12:52:39<3:16:08,  2.86s/batch]Batch 16200/20010 Done, mean position loss: 22.16299720525742\n",
      "Training growing_up:  80%|██████▎ | 15919/20010 [12:52:39<3:17:37,  2.90s/batch]Batch 16100/20010 Done, mean position loss: 22.083220767974854\n",
      "Training growing_up:  80%|██████▍ | 16085/20010 [12:52:42<3:16:01,  3.00s/batch]Batch 16100/20010 Done, mean position loss: 23.025410182476044\n",
      "Training growing_up:  81%|██████▍ | 16194/20010 [12:52:49<3:03:37,  2.89s/batch]Batch 16200/20010 Done, mean position loss: 22.074333486557006\n",
      "Training growing_up:  82%|██████▌ | 16329/20010 [12:52:50<2:58:39,  2.91s/batch]Batch 16200/20010 Done, mean position loss: 22.811142778396608\n",
      "Training growing_up:  79%|██████▎ | 15848/20010 [12:53:00<3:08:03,  2.71s/batch]Batch 16300/20010 Done, mean position loss: 21.554112696647643\n",
      "Training growing_up:  81%|██████▍ | 16257/20010 [12:53:02<2:56:24,  2.82s/batch]Batch 16200/20010 Done, mean position loss: 22.804850463867187\n",
      "Training growing_up:  81%|██████▌ | 16271/20010 [12:53:12<3:13:26,  3.10s/batch]Batch 15900/20010 Done, mean position loss: 22.580465018749237\n",
      "Training growing_up:  81%|██████▍ | 16130/20010 [12:53:13<3:30:08,  3.25s/batch]Batch 16200/20010 Done, mean position loss: 23.184549412727357\n",
      "Training growing_up:  81%|██████▍ | 16121/20010 [12:53:30<2:56:30,  2.72s/batch]Batch 16100/20010 Done, mean position loss: 22.680676052570345\n",
      "Training growing_up:  81%|██████▍ | 16249/20010 [12:53:32<2:47:14,  2.67s/batch]Batch 16100/20010 Done, mean position loss: 23.149870140552522\n",
      "Training growing_up:  80%|██████▎ | 15912/20010 [12:53:48<3:33:25,  3.12s/batch]Batch 16100/20010 Done, mean position loss: 22.183934674263\n",
      "Training growing_up:  81%|██████▌ | 16267/20010 [12:53:50<3:11:04,  3.06s/batch]Batch 16000/20010 Done, mean position loss: 22.331025202274326\n",
      "Training growing_up:  81%|██████▍ | 16112/20010 [12:54:05<2:57:52,  2.74s/batch]Batch 16400/20010 Done, mean position loss: 22.76633683681488\n",
      "Training growing_up:  82%|██████▌ | 16406/20010 [12:54:19<2:49:34,  2.82s/batch]Batch 16100/20010 Done, mean position loss: 22.60895290851593\n",
      "Training growing_up:  81%|██████▍ | 16243/20010 [12:54:30<3:09:16,  3.01s/batch]Batch 15900/20010 Done, mean position loss: 22.329255340099337\n",
      "Training growing_up:  81%|██████▍ | 16243/20010 [12:54:41<2:58:58,  2.85s/batch]Batch 16300/20010 Done, mean position loss: 21.738926620483397\n",
      "Training growing_up:  80%|██████▍ | 16062/20010 [12:54:49<2:58:34,  2.71s/batch]Batch 16500/20010 Done, mean position loss: 21.562794899940492\n",
      "Training growing_up:  81%|██████▍ | 16245/20010 [12:54:56<2:51:24,  2.73s/batch]Batch 16200/20010 Done, mean position loss: 21.817969863414763\n",
      "Training growing_up:  81%|██████▍ | 16245/20010 [12:55:10<2:50:52,  2.72s/batch]Batch 16300/20010 Done, mean position loss: 22.36223989725113\n",
      "Training growing_up:  81%|██████▍ | 16245/20010 [12:55:22<2:49:30,  2.70s/batch]Batch 16300/20010 Done, mean position loss: 22.792279629707334\n",
      "Training growing_up:  81%|██████▍ | 16127/20010 [12:55:36<3:11:33,  2.96s/batch]Batch 16400/20010 Done, mean position loss: 21.947143127918245\n",
      "Training growing_up:  81%|██████▌ | 16273/20010 [12:55:39<3:18:27,  3.19s/batch]Batch 15900/20010 Done, mean position loss: 21.4721875\n",
      "Training growing_up:  82%|██████▌ | 16356/20010 [12:55:43<3:16:39,  3.23s/batch]Batch 16200/20010 Done, mean position loss: 22.545679285526276\n",
      "Training growing_up:  81%|██████▍ | 16163/20010 [12:55:48<3:14:47,  3.04s/batch]Batch 16000/20010 Done, mean position loss: 22.884987795352934\n",
      "Training growing_up:  80%|██████▍ | 16084/20010 [12:55:55<3:18:32,  3.03s/batch]Batch 16300/20010 Done, mean position loss: 21.886775889396667\n",
      "Training growing_up:  80%|██████▍ | 15958/20010 [12:56:03<3:19:06,  2.95s/batch]Batch 16300/20010 Done, mean position loss: 22.571067507266996\n",
      "Training growing_up:  80%|██████▍ | 16010/20010 [12:56:13<2:57:36,  2.66s/batch]Batch 16300/20010 Done, mean position loss: 22.6710555434227\n",
      "Training growing_up:  81%|██████▌ | 16285/20010 [12:56:15<3:07:09,  3.01s/batch]Batch 16200/20010 Done, mean position loss: 22.8397958111763\n",
      "Training growing_up:  82%|██████▌ | 16324/20010 [12:56:15<2:52:05,  2.80s/batch]Batch 16400/20010 Done, mean position loss: 23.01260773897171\n",
      "Training growing_up:  80%|██████▍ | 15997/20010 [12:56:34<3:43:54,  3.35s/batch]Batch 16000/20010 Done, mean position loss: 21.881368901729584\n",
      "Training growing_up:  81%|██████▍ | 16157/20010 [12:56:36<3:28:18,  3.24s/batch]Batch 16300/20010 Done, mean position loss: 22.250586104393008\n",
      "Training growing_up:  81%|██████▍ | 16208/20010 [12:56:37<3:29:05,  3.30s/batch]Batch 16200/20010 Done, mean position loss: 21.922125079631805\n",
      "Training growing_up:  82%|██████▌ | 16424/20010 [12:56:47<3:10:18,  3.18s/batch]Batch 16100/20010 Done, mean position loss: 21.602502663135528\n",
      "Training growing_up:  81%|██████▍ | 16187/20010 [12:56:48<3:02:31,  2.86s/batch]Batch 16000/20010 Done, mean position loss: 21.576979739665987\n",
      "Training growing_up:  81%|██████▍ | 16195/20010 [12:56:59<4:03:42,  3.83s/batch]Batch 16400/20010 Done, mean position loss: 22.07162901163101\n",
      "Training growing_up:  81%|██████▌ | 16286/20010 [12:57:04<3:12:56,  3.11s/batch]Batch 16300/20010 Done, mean position loss: 22.050607662200928\n",
      "Training growing_up:  82%|██████▌ | 16327/20010 [12:57:19<3:17:37,  3.22s/batch]Batch 16200/20010 Done, mean position loss: 22.6600869512558\n",
      "Training growing_up:  81%|██████▍ | 16173/20010 [12:57:26<3:14:55,  3.05s/batch]Batch 16300/20010 Done, mean position loss: 22.082326374053956\n",
      "Training growing_up:  81%|██████▌ | 16286/20010 [12:57:29<3:32:48,  3.43s/batch]Batch 16200/20010 Done, mean position loss: 22.76649373292923\n",
      "Training growing_up:  82%|██████▌ | 16414/20010 [12:57:38<3:14:47,  3.25s/batch]Batch 16300/20010 Done, mean position loss: 22.4330739402771\n",
      "Training growing_up:  81%|██████▍ | 16254/20010 [12:57:39<3:08:16,  3.01s/batch]Batch 16200/20010 Done, mean position loss: 22.261270411014557\n",
      "Training growing_up:  81%|██████▌ | 16306/20010 [12:57:40<2:49:32,  2.75s/batch]Batch 16200/20010 Done, mean position loss: 22.88097809553146\n",
      "Training growing_up:  80%|██████▎ | 15943/20010 [12:57:51<3:46:41,  3.34s/batch]Batch 16300/20010 Done, mean position loss: 22.38220360279083\n",
      "Training growing_up:  82%|██████▌ | 16332/20010 [12:57:51<3:21:56,  3.29s/batch]Batch 16300/20010 Done, mean position loss: 22.090027606487276\n",
      "Training growing_up:  82%|██████▌ | 16337/20010 [12:57:56<2:55:41,  2.87s/batch]Batch 16400/20010 Done, mean position loss: 21.811137824058534\n",
      "Training growing_up:  82%|██████▌ | 16422/20010 [12:58:01<3:01:54,  3.04s/batch]Batch 16300/20010 Done, mean position loss: 22.47945138692856\n",
      "Training growing_up:  81%|██████▍ | 16232/20010 [12:58:15<3:17:11,  3.13s/batch]Batch 16300/20010 Done, mean position loss: 22.967667891979218\n",
      "Training growing_up:  82%|██████▌ | 16455/20010 [12:58:17<2:52:49,  2.92s/batch]Batch 16000/20010 Done, mean position loss: 22.661202352046967\n",
      "Training growing_up:  81%|██████▍ | 16194/20010 [12:58:29<2:56:08,  2.77s/batch]Batch 16200/20010 Done, mean position loss: 23.35059048175812\n",
      "Training growing_up:  82%|██████▌ | 16434/20010 [12:58:34<2:33:36,  2.58s/batch]Batch 16200/20010 Done, mean position loss: 22.51319897890091\n",
      "Training growing_up:  82%|██████▌ | 16382/20010 [12:58:49<3:01:57,  3.01s/batch]Batch 16200/20010 Done, mean position loss: 22.19770509004593\n",
      "Training growing_up:  82%|██████▌ | 16326/20010 [12:58:51<2:48:50,  2.75s/batch]Batch 16100/20010 Done, mean position loss: 22.24747408390045\n",
      "Training growing_up:  82%|██████▌ | 16346/20010 [12:58:57<3:00:52,  2.96s/batch]Batch 16500/20010 Done, mean position loss: 22.584886443614963\n",
      "Training growing_up:  82%|██████▌ | 16329/20010 [12:59:18<3:23:40,  3.32s/batch]Batch 16200/20010 Done, mean position loss: 22.94682992696762\n",
      "Training growing_up:  82%|██████▌ | 16352/20010 [12:59:36<2:46:33,  2.73s/batch]Batch 16000/20010 Done, mean position loss: 22.42097414493561\n",
      "Training growing_up:  82%|██████▌ | 16363/20010 [12:59:44<3:06:09,  3.06s/batch]Batch 16600/20010 Done, mean position loss: 21.51893277645111\n",
      "Training growing_up:  82%|██████▌ | 16393/20010 [12:59:45<3:10:12,  3.16s/batch]Batch 16400/20010 Done, mean position loss: 21.93132844209671\n",
      "Training growing_up:  82%|██████▌ | 16377/20010 [12:59:55<2:59:26,  2.96s/batch]Batch 16300/20010 Done, mean position loss: 22.109425134658814\n",
      "Training growing_up:  82%|██████▌ | 16353/20010 [13:00:10<2:58:09,  2.92s/batch]Batch 16400/20010 Done, mean position loss: 22.407042429447174\n",
      "Training growing_up:  82%|██████▌ | 16359/20010 [13:00:15<2:58:30,  2.93s/batch]Batch 16400/20010 Done, mean position loss: 22.881873230934143\n",
      "Training growing_up:  81%|██████▍ | 16241/20010 [13:00:30<3:17:32,  3.14s/batch]Batch 16500/20010 Done, mean position loss: 22.016034581661224\n",
      "Training growing_up:  81%|██████▍ | 16140/20010 [13:00:44<3:11:31,  2.97s/batch]Batch 16300/20010 Done, mean position loss: 22.506371681690215\n",
      "Training growing_up:  82%|██████▌ | 16411/20010 [13:00:46<2:57:34,  2.96s/batch]Batch 16000/20010 Done, mean position loss: 21.722710297107696\n",
      "Training growing_up:  80%|██████▍ | 16052/20010 [13:00:52<3:18:06,  3.00s/batch]Batch 16100/20010 Done, mean position loss: 22.949579260349275\n",
      "Training growing_up:  82%|██████▌ | 16426/20010 [13:01:00<2:42:43,  2.72s/batch]Batch 16400/20010 Done, mean position loss: 22.75350151062012\n",
      "Training growing_up:  80%|██████▍ | 16028/20010 [13:01:01<3:42:19,  3.35s/batch]Batch 16400/20010 Done, mean position loss: 21.9084588599205\n",
      "Training growing_up:  83%|██████▌ | 16514/20010 [13:01:08<2:56:02,  3.02s/batch]Batch 16500/20010 Done, mean position loss: 22.817306396961214\n",
      "Training growing_up:  81%|██████▌ | 16273/20010 [13:01:14<2:53:22,  2.78s/batch]Batch 16400/20010 Done, mean position loss: 22.827184872627257\n",
      "Training growing_up:  82%|██████▌ | 16490/20010 [13:01:20<2:46:58,  2.85s/batch]Batch 16300/20010 Done, mean position loss: 22.888084094524384\n",
      "Training growing_up:  80%|██████▍ | 16095/20010 [13:01:29<3:18:11,  3.04s/batch]Batch 16400/20010 Done, mean position loss: 22.10859736919403\n",
      "Training growing_up:  82%|██████▌ | 16430/20010 [13:01:41<3:08:18,  3.16s/batch]Batch 16100/20010 Done, mean position loss: 22.015821573734286\n",
      "Training growing_up:  82%|██████▌ | 16377/20010 [13:01:43<2:58:44,  2.95s/batch]Batch 16200/20010 Done, mean position loss: 21.91969409942627\n",
      "Training growing_up:  81%|██████▍ | 16201/20010 [13:01:44<2:48:33,  2.66s/batch]Batch 16300/20010 Done, mean position loss: 22.135267384052277\n",
      "Training growing_up:  82%|██████▌ | 16407/20010 [13:01:47<2:57:01,  2.95s/batch]Batch 16100/20010 Done, mean position loss: 21.694244339466096\n",
      "Training growing_up:  82%|██████▌ | 16388/20010 [13:01:51<2:46:15,  2.75s/batch]Batch 16500/20010 Done, mean position loss: 22.06716493368149\n",
      "Training growing_up:  82%|██████▌ | 16384/20010 [13:02:01<2:58:48,  2.96s/batch]Batch 16400/20010 Done, mean position loss: 21.993922026157378\n",
      "Training growing_up:  81%|██████▌ | 16295/20010 [13:02:20<2:56:26,  2.85s/batch]Batch 16400/20010 Done, mean position loss: 22.12618781089783\n",
      "Training growing_up:  81%|██████▍ | 16112/20010 [13:02:21<3:18:06,  3.05s/batch]Batch 16300/20010 Done, mean position loss: 22.73355787754059\n",
      "Training growing_up:  82%|██████▌ | 16334/20010 [13:02:25<3:18:19,  3.24s/batch]Batch 16300/20010 Done, mean position loss: 22.832230930328368\n",
      "Training growing_up:  81%|██████▌ | 16275/20010 [13:02:32<3:08:04,  3.02s/batch]Batch 16400/20010 Done, mean position loss: 22.3941814994812\n",
      "Training growing_up:  82%|██████▌ | 16497/20010 [13:02:38<3:03:50,  3.14s/batch]Batch 16300/20010 Done, mean position loss: 22.00424969434738\n",
      "Training growing_up:  82%|██████▌ | 16392/20010 [13:02:42<2:52:33,  2.86s/batch]Batch 16300/20010 Done, mean position loss: 22.92844656229019\n",
      "Training growing_up:  81%|██████▌ | 16286/20010 [13:02:48<3:13:43,  3.12s/batch]Batch 16400/20010 Done, mean position loss: 22.620583064556122\n",
      "Training growing_up:  82%|██████▌ | 16439/20010 [13:02:51<3:00:38,  3.04s/batch]Batch 16500/20010 Done, mean position loss: 21.55494417667389\n",
      "Training growing_up:  81%|██████▌ | 16281/20010 [13:02:51<3:05:43,  2.99s/batch]Batch 16400/20010 Done, mean position loss: 22.52606618642807\n",
      "Training growing_up:  83%|██████▋ | 16580/20010 [13:02:55<2:56:07,  3.08s/batch]Batch 16400/20010 Done, mean position loss: 22.10858738660812\n",
      "Training growing_up:  82%|██████▌ | 16338/20010 [13:03:09<2:55:27,  2.87s/batch]Batch 16400/20010 Done, mean position loss: 22.782470960617065\n",
      "Training growing_up:  82%|██████▌ | 16465/20010 [13:03:22<2:57:06,  3.00s/batch]Batch 16100/20010 Done, mean position loss: 22.50551263093948\n",
      "Training growing_up:  81%|██████▍ | 16237/20010 [13:03:31<3:11:05,  3.04s/batch]Batch 16300/20010 Done, mean position loss: 23.25265049695969\n",
      "Training growing_up:  82%|██████▌ | 16416/20010 [13:03:38<3:00:50,  3.02s/batch]Batch 16300/20010 Done, mean position loss: 22.526550774574282\n",
      "Training growing_up:  82%|██████▌ | 16455/20010 [13:03:47<3:17:22,  3.33s/batch]Batch 16200/20010 Done, mean position loss: 22.37024390459061\n",
      "Training growing_up:  81%|██████▍ | 16243/20010 [13:03:48<3:02:33,  2.91s/batch]Batch 16300/20010 Done, mean position loss: 22.29134351491928\n",
      "Training growing_up:  80%|██████▍ | 16088/20010 [13:03:59<3:11:39,  2.93s/batch]Batch 16600/20010 Done, mean position loss: 22.361983160972596\n",
      "Training growing_up:  82%|██████▌ | 16482/20010 [13:04:11<3:13:10,  3.29s/batch]Batch 16300/20010 Done, mean position loss: 22.718593876361847\n",
      "Training growing_up:  83%|██████▋ | 16573/20010 [13:04:38<2:31:27,  2.64s/batch]Batch 16100/20010 Done, mean position loss: 22.47449185371399\n",
      "Training growing_up:  81%|██████▍ | 16157/20010 [13:04:44<3:25:55,  3.21s/batch]Batch 16500/20010 Done, mean position loss: 21.6778128695488\n",
      "Training growing_up:  82%|██████▌ | 16438/20010 [13:04:45<2:59:44,  3.02s/batch]Batch 16400/20010 Done, mean position loss: 22.187348575592043\n",
      "Training growing_up:  81%|██████▌ | 16264/20010 [13:04:49<3:20:47,  3.22s/batch]Batch 16700/20010 Done, mean position loss: 21.31456080198288\n",
      "Training growing_up:  83%|██████▋ | 16593/20010 [13:05:04<2:49:46,  2.98s/batch]Batch 16500/20010 Done, mean position loss: 22.57961984872818\n",
      "Training growing_up:  82%|██████▌ | 16502/20010 [13:05:06<2:31:47,  2.60s/batch]Batch 16500/20010 Done, mean position loss: 22.922401132583616\n",
      "Training growing_up:  83%|██████▌ | 16555/20010 [13:05:26<2:34:58,  2.69s/batch]Batch 16600/20010 Done, mean position loss: 21.869325225353244\n",
      "Training growing_up:  82%|██████▌ | 16366/20010 [13:05:50<3:16:48,  3.24s/batch]Batch 16400/20010 Done, mean position loss: 22.488071126937868\n",
      "Batch 16200/20010 Done, mean position loss: 22.587955298423765\n",
      "Training growing_up:  81%|██████▍ | 16126/20010 [13:05:51<3:33:25,  3.30s/batch]Batch 16500/20010 Done, mean position loss: 22.67509203910828\n",
      "Training growing_up:  82%|██████▌ | 16402/20010 [13:05:51<3:06:43,  3.11s/batch]Batch 16100/20010 Done, mean position loss: 21.97966295480728\n",
      "Training growing_up:  83%|██████▌ | 16518/20010 [13:05:58<2:44:40,  2.83s/batch]Batch 16600/20010 Done, mean position loss: 22.80422909736633\n",
      "Training growing_up:  82%|██████▌ | 16349/20010 [13:06:02<2:56:22,  2.89s/batch]Batch 16500/20010 Done, mean position loss: 21.73499783039093\n",
      "Training growing_up:  81%|██████▍ | 16208/20010 [13:06:11<3:21:55,  3.19s/batch]Batch 16500/20010 Done, mean position loss: 22.325406739711763\n",
      "Training growing_up:  81%|██████▍ | 16133/20010 [13:06:13<3:24:31,  3.17s/batch]Batch 16400/20010 Done, mean position loss: 23.05857427120209\n",
      "Training growing_up:  82%|██████▌ | 16379/20010 [13:06:28<2:43:13,  2.70s/batch]Batch 16500/20010 Done, mean position loss: 22.120994188785552\n",
      "Training growing_up:  83%|██████▋ | 16655/20010 [13:06:35<2:31:52,  2.72s/batch]Batch 16300/20010 Done, mean position loss: 21.794786829948425\n",
      "Training growing_up:  82%|██████▌ | 16382/20010 [13:06:36<2:51:34,  2.84s/batch]Batch 16600/20010 Done, mean position loss: 22.18890575885773\n",
      "Training growing_up:  84%|██████▋ | 16741/20010 [13:06:39<2:25:20,  2.67s/batch]Batch 16200/20010 Done, mean position loss: 22.016495673656465\n",
      "Training growing_up:  83%|██████▌ | 16519/20010 [13:06:43<2:37:55,  2.71s/batch]Batch 16400/20010 Done, mean position loss: 22.167056145668028\n",
      "Training growing_up:  83%|██████▌ | 16509/20010 [13:06:50<2:35:01,  2.66s/batch]Batch 16200/20010 Done, mean position loss: 21.94449387550354\n",
      "Training growing_up:  82%|██████▌ | 16415/20010 [13:06:50<2:32:54,  2.55s/batch]Batch 16500/20010 Done, mean position loss: 21.861363878250124\n",
      "Training growing_up:  82%|██████▌ | 16507/20010 [13:07:06<2:36:16,  2.68s/batch]Batch 16400/20010 Done, mean position loss: 22.77932163476944\n",
      "Training growing_up:  82%|██████▌ | 16403/20010 [13:07:10<2:39:13,  2.65s/batch]Batch 16500/20010 Done, mean position loss: 21.990553376674654\n",
      "Training growing_up:  82%|██████▌ | 16374/20010 [13:07:19<2:34:34,  2.55s/batch]Batch 16400/20010 Done, mean position loss: 22.576783578395844\n",
      "Training growing_up:  82%|██████▌ | 16377/20010 [13:07:23<2:45:55,  2.74s/batch]Batch 16500/20010 Done, mean position loss: 22.11976000547409\n",
      "Training growing_up:  82%|██████▌ | 16459/20010 [13:07:31<2:36:00,  2.64s/batch]Batch 16400/20010 Done, mean position loss: 22.22937216043472\n",
      "Training growing_up:  82%|██████▌ | 16321/20010 [13:07:32<3:03:10,  2.98s/batch]Batch 16400/20010 Done, mean position loss: 22.962987105846402\n",
      "Training growing_up:  84%|██████▋ | 16761/20010 [13:07:33<2:22:01,  2.62s/batch]Batch 16600/20010 Done, mean position loss: 21.7727897977829\n",
      "Training growing_up:  83%|██████▋ | 16635/20010 [13:07:36<2:48:19,  2.99s/batch]Batch 16500/20010 Done, mean position loss: 22.487533345222474\n",
      "Training growing_up:  83%|██████▌ | 16534/20010 [13:07:43<2:39:11,  2.75s/batch]Batch 16500/20010 Done, mean position loss: 22.658127405643462\n",
      "Training growing_up:  82%|██████▌ | 16443/20010 [13:07:47<2:41:18,  2.71s/batch]Batch 16500/20010 Done, mean position loss: 22.247197659015654\n",
      "Training growing_up:  83%|██████▌ | 16514/20010 [13:08:00<2:37:50,  2.71s/batch]Batch 16500/20010 Done, mean position loss: 22.564293203353884\n",
      "Training growing_up:  82%|██████▌ | 16396/20010 [13:08:23<2:44:11,  2.73s/batch]Batch 16200/20010 Done, mean position loss: 22.33684303998947\n",
      "Training growing_up:  83%|██████▋ | 16651/20010 [13:08:23<2:53:41,  3.10s/batch]Batch 16400/20010 Done, mean position loss: 23.43039202213287\n",
      "Training growing_up:  82%|██████▌ | 16342/20010 [13:08:35<3:04:17,  3.01s/batch]Batch 16400/20010 Done, mean position loss: 22.4340669131279\n",
      "Training growing_up:  83%|██████▌ | 16513/20010 [13:08:35<2:58:17,  3.06s/batch]Batch 16300/20010 Done, mean position loss: 22.37729906797409\n",
      "Training growing_up:  83%|██████▌ | 16531/20010 [13:08:37<3:01:35,  3.13s/batch]Batch 16400/20010 Done, mean position loss: 22.33228441953659\n",
      "Training growing_up:  83%|██████▋ | 16657/20010 [13:08:40<2:37:02,  2.81s/batch]Batch 16700/20010 Done, mean position loss: 22.42623685598373\n",
      "Training growing_up:  82%|██████▌ | 16413/20010 [13:09:00<3:11:22,  3.19s/batch]Batch 16400/20010 Done, mean position loss: 22.851683795452118\n",
      "Training growing_up:  83%|██████▋ | 16572/20010 [13:09:30<2:42:23,  2.83s/batch]Batch 16800/20010 Done, mean position loss: 21.27551417350769\n",
      "Training growing_up:  82%|██████▌ | 16320/20010 [13:09:31<3:01:17,  2.95s/batch]Batch 16200/20010 Done, mean position loss: 22.41371717453003\n",
      "Training growing_up:  83%|██████▌ | 16541/20010 [13:09:31<2:42:09,  2.80s/batch]Batch 16600/20010 Done, mean position loss: 21.736618621349336\n",
      "Training growing_up:  83%|██████▌ | 16532/20010 [13:09:31<2:46:15,  2.87s/batch]Batch 16500/20010 Done, mean position loss: 21.941661126613617\n",
      "Training growing_up:  83%|██████▋ | 16600/20010 [13:09:53<2:48:06,  2.96s/batch]Batch 16600/20010 Done, mean position loss: 22.320761971473694\n",
      "Training growing_up:  84%|██████▋ | 16810/20010 [13:09:56<2:38:50,  2.98s/batch]Batch 16600/20010 Done, mean position loss: 22.814346158504485\n",
      "Training growing_up:  81%|██████▍ | 16215/20010 [13:10:14<3:30:01,  3.32s/batch]Batch 16700/20010 Done, mean position loss: 21.76737033128738\n",
      "Training growing_up:  81%|██████▌ | 16275/20010 [13:10:31<3:10:34,  3.06s/batch]Batch 16500/20010 Done, mean position loss: 22.47200924873352\n",
      "Training growing_up:  83%|██████▋ | 16592/20010 [13:10:35<2:45:32,  2.91s/batch]Batch 16600/20010 Done, mean position loss: 23.013797879219055\n",
      "Training growing_up:  82%|██████▌ | 16466/20010 [13:10:49<3:08:54,  3.20s/batch]Batch 16200/20010 Done, mean position loss: 21.73785239458084\n",
      "Training growing_up:  81%|██████▍ | 16201/20010 [13:10:49<3:26:42,  3.26s/batch]Batch 16700/20010 Done, mean position loss: 22.664526557922365\n",
      "Training growing_up:  83%|██████▋ | 16592/20010 [13:10:54<3:23:19,  3.57s/batch]Batch 16300/20010 Done, mean position loss: 22.69099111557007\n",
      "Training growing_up:  82%|██████▌ | 16348/20010 [13:10:55<2:47:15,  2.74s/batch]Batch 16600/20010 Done, mean position loss: 21.815322983264924\n",
      "Training growing_up:  81%|██████▍ | 16230/20010 [13:11:02<3:19:47,  3.17s/batch]Batch 16600/20010 Done, mean position loss: 22.687593379020694\n",
      "Training growing_up:  83%|██████▋ | 16671/20010 [13:11:04<2:41:56,  2.91s/batch]Batch 16500/20010 Done, mean position loss: 22.875989599227907\n",
      "Training growing_up:  83%|██████▌ | 16516/20010 [13:11:19<3:07:17,  3.22s/batch]Batch 16700/20010 Done, mean position loss: 22.303652200698853\n",
      "Training growing_up:  83%|██████▌ | 16567/20010 [13:11:21<3:22:56,  3.54s/batch]Batch 16600/20010 Done, mean position loss: 22.058824560642243\n",
      "Training growing_up:  82%|██████▌ | 16488/20010 [13:11:38<3:16:03,  3.34s/batch]Batch 16400/20010 Done, mean position loss: 21.783485963344575\n",
      "Training growing_up:  83%|██████▋ | 16588/20010 [13:11:40<3:03:30,  3.22s/batch]Batch 16600/20010 Done, mean position loss: 21.75035270690918\n",
      "Training growing_up:  82%|██████▌ | 16500/20010 [13:11:42<3:07:22,  3.20s/batch]Batch 16300/20010 Done, mean position loss: 21.975037713050842\n",
      "Training growing_up:  82%|██████▌ | 16457/20010 [13:11:45<2:41:05,  2.72s/batch]Batch 16500/20010 Done, mean position loss: 21.93883241891861\n",
      "Training growing_up:  82%|██████▌ | 16459/20010 [13:11:51<2:43:00,  2.75s/batch]Batch 16300/20010 Done, mean position loss: 21.5744381070137\n",
      "Training growing_up:  83%|██████▋ | 16652/20010 [13:12:09<2:40:08,  2.86s/batch]Batch 16500/20010 Done, mean position loss: 22.748571400642398\n",
      "Training growing_up:  83%|██████▋ | 16597/20010 [13:12:11<3:11:43,  3.37s/batch]Batch 16600/20010 Done, mean position loss: 22.04487458229065\n",
      "Training growing_up:  83%|██████▋ | 16694/20010 [13:12:17<3:04:01,  3.33s/batch]Batch 16500/20010 Done, mean position loss: 22.801908569335936\n",
      "Training growing_up:  83%|██████▋ | 16615/20010 [13:12:22<2:53:36,  3.07s/batch]Batch 16600/20010 Done, mean position loss: 22.22260261297226\n",
      "Training growing_up:  82%|██████▌ | 16378/20010 [13:12:29<3:05:06,  3.06s/batch]Batch 16500/20010 Done, mean position loss: 22.842440927028655\n",
      "Training growing_up:  83%|██████▋ | 16652/20010 [13:12:33<2:53:11,  3.09s/batch]Batch 16600/20010 Done, mean position loss: 22.475896298885345\n",
      "Training growing_up:  84%|██████▋ | 16727/20010 [13:12:38<2:30:34,  2.75s/batch]Batch 16700/20010 Done, mean position loss: 21.706950466632843\n",
      "Training growing_up:  83%|██████▌ | 16510/20010 [13:12:38<3:09:12,  3.24s/batch]Batch 16500/20010 Done, mean position loss: 22.137340881824493\n",
      "Training growing_up:  84%|██████▋ | 16782/20010 [13:12:42<2:32:19,  2.83s/batch]Batch 16600/20010 Done, mean position loss: 22.818995938301086\n",
      "Training growing_up:  83%|██████▋ | 16633/20010 [13:12:54<2:40:36,  2.85s/batch]Batch 16600/20010 Done, mean position loss: 22.089750423431397\n",
      "Training growing_up:  83%|██████▋ | 16663/20010 [13:13:04<2:32:54,  2.74s/batch]Batch 16600/20010 Done, mean position loss: 22.635610172748564\n",
      "Training growing_up:  82%|██████▌ | 16436/20010 [13:13:26<2:57:14,  2.98s/batch]Batch 16500/20010 Done, mean position loss: 23.031743559837338\n",
      "Training growing_up:  83%|██████▋ | 16613/20010 [13:13:31<2:34:25,  2.73s/batch]Batch 16300/20010 Done, mean position loss: 22.36716504812241\n",
      "Training growing_up:  83%|██████▋ | 16655/20010 [13:13:36<2:37:30,  2.82s/batch]Batch 16500/20010 Done, mean position loss: 22.47850136756897\n",
      "Training growing_up:  81%|██████▌ | 16303/20010 [13:13:38<3:28:32,  3.38s/batch]Batch 16800/20010 Done, mean position loss: 22.379962437152862\n",
      "Training growing_up:  83%|██████▋ | 16584/20010 [13:13:38<2:48:52,  2.96s/batch]Batch 16400/20010 Done, mean position loss: 22.649506592750548\n",
      "Training growing_up:  83%|██████▋ | 16613/20010 [13:13:40<2:35:37,  2.75s/batch]Batch 16500/20010 Done, mean position loss: 22.553615250587463\n",
      "Training growing_up:  83%|██████▋ | 16619/20010 [13:13:58<2:39:12,  2.82s/batch]Batch 16500/20010 Done, mean position loss: 22.758395042419433\n",
      "Training growing_up:  83%|██████▋ | 16692/20010 [13:14:29<2:28:45,  2.69s/batch]Batch 16600/20010 Done, mean position loss: 21.783601884841918\n",
      "Training growing_up:  83%|██████▌ | 16519/20010 [13:14:29<2:55:22,  3.01s/batch]Batch 16900/20010 Done, mean position loss: 21.392673532962796\n",
      "Training growing_up:  83%|██████▋ | 16644/20010 [13:14:38<2:37:03,  2.80s/batch]Batch 16700/20010 Done, mean position loss: 21.680546202659606\n",
      "Training growing_up:  83%|██████▋ | 16651/20010 [13:14:45<2:56:32,  3.15s/batch]Batch 16300/20010 Done, mean position loss: 22.337186162471774\n",
      "Training growing_up:  83%|██████▌ | 16563/20010 [13:14:48<2:48:03,  2.93s/batch]Batch 16700/20010 Done, mean position loss: 22.396049799919126\n",
      "Training growing_up:  81%|██████▌ | 16304/20010 [13:14:54<3:09:36,  3.07s/batch]Batch 16700/20010 Done, mean position loss: 22.929193959236144\n",
      "Training growing_up:  82%|██████▌ | 16432/20010 [13:15:12<3:14:35,  3.26s/batch]Batch 16800/20010 Done, mean position loss: 21.748620574474334\n",
      "Training growing_up:  82%|██████▌ | 16318/20010 [13:15:35<3:00:04,  2.93s/batch]Batch 16700/20010 Done, mean position loss: 22.67568824768066\n",
      "Training growing_up:  85%|██████▊ | 16923/20010 [13:15:37<2:42:38,  3.16s/batch]Batch 16600/20010 Done, mean position loss: 22.486239290237428\n",
      "Training growing_up:  84%|██████▋ | 16719/20010 [13:15:47<2:29:26,  2.72s/batch]Batch 16800/20010 Done, mean position loss: 22.77613999128342\n",
      "Training growing_up:  84%|██████▋ | 16722/20010 [13:15:52<2:43:23,  2.98s/batch]Batch 16700/20010 Done, mean position loss: 21.754763901233673\n",
      "Training growing_up:  83%|██████▋ | 16578/20010 [13:16:05<2:33:03,  2.68s/batch]Batch 16700/20010 Done, mean position loss: 22.73831750392914\n",
      "Training growing_up:  83%|██████▋ | 16611/20010 [13:16:05<2:32:14,  2.69s/batch]Batch 16300/20010 Done, mean position loss: 21.66988241434097\n",
      "Training growing_up:  83%|██████▋ | 16699/20010 [13:16:11<3:04:57,  3.35s/batch]Batch 16600/20010 Done, mean position loss: 22.862670514583588\n",
      "Training growing_up:  83%|██████▋ | 16670/20010 [13:16:12<2:48:16,  3.02s/batch]Batch 16400/20010 Done, mean position loss: 22.70342835426331\n",
      "Training growing_up:  84%|██████▋ | 16735/20010 [13:16:16<2:31:44,  2.78s/batch]Batch 16700/20010 Done, mean position loss: 22.124635446071625\n",
      "Training growing_up:  84%|██████▋ | 16710/20010 [13:16:20<2:55:10,  3.19s/batch]Batch 16800/20010 Done, mean position loss: 22.352101621627806\n",
      "Training growing_up:  83%|██████▋ | 16686/20010 [13:16:34<2:52:52,  3.12s/batch]Batch 16500/20010 Done, mean position loss: 21.6655508351326\n",
      "Training growing_up:  82%|██████▌ | 16363/20010 [13:16:42<3:07:39,  3.09s/batch]Batch 16700/20010 Done, mean position loss: 21.72103924751282\n",
      "Training growing_up:  82%|██████▌ | 16462/20010 [13:16:42<2:52:55,  2.92s/batch]Batch 16600/20010 Done, mean position loss: 22.15897435903549\n",
      "Training growing_up:  82%|██████▌ | 16341/20010 [13:16:44<2:52:09,  2.82s/batch]Batch 16400/20010 Done, mean position loss: 21.545406663417815\n",
      "Training growing_up:  83%|██████▌ | 16560/20010 [13:16:57<2:47:40,  2.92s/batch]Batch 16400/20010 Done, mean position loss: 21.521061966419218\n",
      "Training growing_up:  83%|██████▋ | 16700/20010 [13:17:16<2:51:44,  3.11s/batch]Batch 16600/20010 Done, mean position loss: 22.538694584369658\n",
      "Training growing_up:  83%|██████▋ | 16614/20010 [13:17:19<2:42:38,  2.87s/batch]Batch 16600/20010 Done, mean position loss: 22.75321183204651\n",
      "Training growing_up:  83%|██████▋ | 16699/20010 [13:17:19<2:39:34,  2.89s/batch]Batch 16700/20010 Done, mean position loss: 22.029375557899478\n",
      "Training growing_up:  83%|██████▌ | 16569/20010 [13:17:24<2:38:00,  2.76s/batch]Batch 16700/20010 Done, mean position loss: 22.147577316761016\n",
      "Training growing_up:  83%|██████▋ | 16578/20010 [13:17:31<2:58:34,  3.12s/batch]Batch 16600/20010 Done, mean position loss: 22.684908947944642\n",
      "Training growing_up:  83%|██████▋ | 16704/20010 [13:17:34<2:55:05,  3.18s/batch]Batch 16700/20010 Done, mean position loss: 22.262043743133543\n",
      "Batch 16800/20010 Done, mean position loss: 21.812273705005644\n",
      "Training growing_up:  83%|██████▋ | 16584/20010 [13:17:38<3:07:00,  3.28s/batch]Batch 16600/20010 Done, mean position loss: 22.130745408535002\n",
      "Training growing_up:  84%|██████▋ | 16735/20010 [13:17:44<3:10:00,  3.48s/batch]Batch 16700/20010 Done, mean position loss: 22.73821348905563\n",
      "Training growing_up:  84%|██████▋ | 16808/20010 [13:17:55<2:29:06,  2.79s/batch]Batch 16700/20010 Done, mean position loss: 22.01656732082367\n",
      "Training growing_up:  84%|██████▋ | 16860/20010 [13:18:02<2:11:33,  2.51s/batch]Batch 16700/20010 Done, mean position loss: 22.7192839717865\n",
      "Training growing_up:  84%|██████▋ | 16748/20010 [13:18:24<2:42:53,  3.00s/batch]Batch 16600/20010 Done, mean position loss: 22.855248374938967\n",
      "Training growing_up:  84%|██████▋ | 16711/20010 [13:18:28<2:18:03,  2.51s/batch]Batch 16900/20010 Done, mean position loss: 22.490005829334258\n",
      "Training growing_up:  83%|██████▋ | 16684/20010 [13:18:34<2:17:02,  2.47s/batch]Batch 16600/20010 Done, mean position loss: 22.323066713809965\n",
      "Training growing_up:  84%|██████▋ | 16720/20010 [13:18:36<2:29:35,  2.73s/batch]Batch 16500/20010 Done, mean position loss: 22.433729939460754\n",
      "Batch 16600/20010 Done, mean position loss: 22.43154097795486\n",
      "Batch 16400/20010 Done, mean position loss: 22.64189389228821\n",
      "Training growing_up:  82%|██████▌ | 16507/20010 [13:18:50<2:18:24,  2.37s/batch]Batch 16600/20010 Done, mean position loss: 22.54633249282837\n",
      "Training growing_up:  82%|██████▌ | 16365/20010 [13:19:17<2:49:25,  2.79s/batch]Batch 17000/20010 Done, mean position loss: 21.494576733112336\n",
      "Training growing_up:  84%|██████▋ | 16743/20010 [13:19:19<2:20:46,  2.59s/batch]Batch 16700/20010 Done, mean position loss: 22.097699687480926\n",
      "Training growing_up:  84%|██████▋ | 16876/20010 [13:19:20<2:12:58,  2.55s/batch]Batch 16800/20010 Done, mean position loss: 21.35830825805664\n",
      "Training growing_up:  84%|██████▋ | 16770/20010 [13:19:32<2:15:44,  2.51s/batch]Batch 16800/20010 Done, mean position loss: 22.339300696849826\n",
      "Training growing_up:  84%|██████▋ | 16871/20010 [13:19:35<1:49:30,  2.09s/batch]Batch 16400/20010 Done, mean position loss: 22.23593631029129\n",
      "Training growing_up:  83%|██████▋ | 16649/20010 [13:19:42<2:15:05,  2.41s/batch]Batch 16800/20010 Done, mean position loss: 22.899060823917388\n",
      "Training growing_up:  83%|██████▋ | 16691/20010 [13:19:51<2:30:32,  2.72s/batch]Batch 16900/20010 Done, mean position loss: 21.7049866771698\n",
      "Training growing_up:  84%|██████▋ | 16755/20010 [13:20:16<2:07:23,  2.35s/batch]Batch 16800/20010 Done, mean position loss: 22.350957679748536\n",
      "Training growing_up:  83%|██████▋ | 16643/20010 [13:20:17<2:21:50,  2.53s/batch]Batch 16700/20010 Done, mean position loss: 22.270227694511412\n",
      "Training growing_up:  84%|██████▋ | 16768/20010 [13:20:23<2:23:46,  2.66s/batch]Batch 16900/20010 Done, mean position loss: 22.53691989660263\n",
      "Training growing_up:  84%|██████▋ | 16769/20010 [13:20:28<2:32:15,  2.82s/batch]Batch 16800/20010 Done, mean position loss: 21.8696901011467\n",
      "Training growing_up:  83%|██████▋ | 16668/20010 [13:20:45<2:50:00,  3.05s/batch]Batch 16700/20010 Done, mean position loss: 22.76377131462097\n",
      "Training growing_up:  83%|██████▋ | 16650/20010 [13:20:49<2:37:17,  2.81s/batch]Batch 16800/20010 Done, mean position loss: 22.511040506362917\n",
      "Training growing_up:  82%|██████▌ | 16485/20010 [13:20:58<2:33:03,  2.61s/batch]Batch 16900/20010 Done, mean position loss: 22.344787299633026\n",
      "Training growing_up:  82%|██████▌ | 16492/20010 [13:20:59<2:49:12,  2.89s/batch]Batch 16800/20010 Done, mean position loss: 22.229587450027466\n",
      "Training growing_up:  85%|██████▊ | 16959/20010 [13:21:04<2:22:25,  2.80s/batch]Batch 16500/20010 Done, mean position loss: 22.726538376808165\n",
      "Training growing_up:  83%|██████▋ | 16684/20010 [13:21:06<2:28:11,  2.67s/batch]Batch 16400/20010 Done, mean position loss: 21.68194574594498\n",
      "Training growing_up:  85%|██████▊ | 16916/20010 [13:21:08<2:21:54,  2.75s/batch]Batch 16600/20010 Done, mean position loss: 21.57550822019577\n",
      "Training growing_up:  82%|██████▌ | 16408/20010 [13:21:26<2:52:24,  2.87s/batch]Batch 16700/20010 Done, mean position loss: 21.89820355415344\n",
      "Training growing_up:  82%|██████▌ | 16461/20010 [13:21:28<3:03:16,  3.10s/batch]Batch 16500/20010 Done, mean position loss: 21.95144984006882\n",
      "Training growing_up:  82%|██████▌ | 16409/20010 [13:21:30<3:14:54,  3.25s/batch]Batch 16800/20010 Done, mean position loss: 21.814133484363555\n",
      "Training growing_up:  82%|██████▌ | 16508/20010 [13:21:51<2:59:30,  3.08s/batch]Batch 16500/20010 Done, mean position loss: 21.454323780536654\n",
      "Training growing_up:  83%|██████▋ | 16700/20010 [13:21:59<2:49:39,  3.08s/batch]Batch 16800/20010 Done, mean position loss: 22.03041671037674\n",
      "Training growing_up:  82%|██████▌ | 16450/20010 [13:22:00<3:01:55,  3.07s/batch]Batch 16700/20010 Done, mean position loss: 22.626293621063233\n",
      "Training growing_up:  85%|██████▊ | 17062/20010 [13:22:03<2:18:05,  2.81s/batch]Batch 16700/20010 Done, mean position loss: 22.44742772102356\n",
      "Training growing_up:  84%|██████▋ | 16728/20010 [13:22:05<2:36:13,  2.86s/batch]Batch 16800/20010 Done, mean position loss: 22.208249077796935\n",
      "Training growing_up:  82%|██████▌ | 16423/20010 [13:22:12<2:54:48,  2.92s/batch]Batch 16700/20010 Done, mean position loss: 22.90528917312622\n",
      "Training growing_up:  83%|██████▌ | 16509/20010 [13:22:14<2:50:08,  2.92s/batch]Batch 16900/20010 Done, mean position loss: 21.601772623062132\n",
      "Training growing_up:  84%|██████▋ | 16817/20010 [13:22:19<2:52:01,  3.23s/batch]Batch 16700/20010 Done, mean position loss: 22.1328257894516\n",
      "Training growing_up:  83%|██████▋ | 16686/20010 [13:22:25<2:33:40,  2.77s/batch]Batch 16800/20010 Done, mean position loss: 22.341207337379455\n",
      "Training growing_up:  84%|██████▋ | 16710/20010 [13:22:26<2:40:16,  2.91s/batch]Batch 16800/20010 Done, mean position loss: 22.75393273353577\n",
      "Training growing_up:  85%|██████▊ | 16936/20010 [13:22:35<2:15:27,  2.64s/batch]Batch 16800/20010 Done, mean position loss: 21.983188495635986\n",
      "Training growing_up:  84%|██████▋ | 16805/20010 [13:22:37<2:32:48,  2.86s/batch]Batch 16800/20010 Done, mean position loss: 22.959072742462155\n",
      "Training growing_up:  84%|██████▋ | 16749/20010 [13:23:04<2:38:56,  2.92s/batch]Batch 17000/20010 Done, mean position loss: 22.659544322490692\n",
      "Training growing_up:  84%|██████▋ | 16813/20010 [13:23:09<2:28:34,  2.79s/batch]Batch 16700/20010 Done, mean position loss: 23.245842912197112\n",
      "Training growing_up:  84%|██████▋ | 16753/20010 [13:23:16<2:46:51,  3.07s/batch]Batch 16700/20010 Done, mean position loss: 22.42477499961853\n",
      "Training growing_up:  84%|██████▋ | 16867/20010 [13:23:25<2:46:41,  3.18s/batch]Batch 16700/20010 Done, mean position loss: 22.134707827568054\n",
      "Training growing_up:  83%|██████▋ | 16650/20010 [13:23:30<2:28:04,  2.64s/batch]Batch 16500/20010 Done, mean position loss: 22.279155921936038\n",
      "Training growing_up:  84%|██████▋ | 16729/20010 [13:23:32<2:45:41,  3.03s/batch]Batch 16600/20010 Done, mean position loss: 22.814516439437867\n",
      "Training growing_up:  83%|██████▋ | 16706/20010 [13:23:33<2:55:17,  3.18s/batch]Batch 16700/20010 Done, mean position loss: 22.389144728183744\n",
      "Training growing_up:  82%|██████▌ | 16488/20010 [13:23:53<2:43:58,  2.79s/batch]Batch 17100/20010 Done, mean position loss: 21.53675197839737\n",
      "Training growing_up:  84%|██████▋ | 16744/20010 [13:24:06<2:30:20,  2.76s/batch]Batch 16800/20010 Done, mean position loss: 21.675044085979458\n",
      "Training growing_up:  82%|██████▌ | 16493/20010 [13:24:08<2:47:48,  2.86s/batch]Batch 16900/20010 Done, mean position loss: 21.498965735435483\n",
      "Training growing_up:  84%|██████▋ | 16868/20010 [13:24:17<2:33:36,  2.93s/batch]Batch 16900/20010 Done, mean position loss: 22.246058177948\n",
      "Training growing_up:  85%|██████▊ | 16976/20010 [13:24:31<2:17:30,  2.72s/batch]Batch 16500/20010 Done, mean position loss: 22.347993876934055\n",
      "Training growing_up:  85%|██████▊ | 17033/20010 [13:24:36<2:25:33,  2.93s/batch]Batch 16900/20010 Done, mean position loss: 22.778392236232758\n",
      "Training growing_up:  84%|██████▋ | 16730/20010 [13:24:44<2:48:41,  3.09s/batch]Batch 17000/20010 Done, mean position loss: 21.8132634305954\n",
      "Training growing_up:  84%|██████▋ | 16756/20010 [13:25:01<2:27:22,  2.72s/batch]Batch 16900/20010 Done, mean position loss: 22.449918503761292\n",
      "Training growing_up:  83%|██████▌ | 16516/20010 [13:25:15<3:06:27,  3.20s/batch]Batch 16800/20010 Done, mean position loss: 22.32507117986679\n",
      "Training growing_up:  82%|██████▌ | 16485/20010 [13:25:17<3:00:01,  3.06s/batch]Batch 17000/20010 Done, mean position loss: 22.640737681388856\n",
      "Training growing_up:  84%|██████▋ | 16866/20010 [13:25:23<2:43:57,  3.13s/batch]Batch 16900/20010 Done, mean position loss: 21.65456961631775\n",
      "Training growing_up:  84%|██████▊ | 16886/20010 [13:25:39<2:55:21,  3.37s/batch]Batch 16800/20010 Done, mean position loss: 22.873541076183322\n",
      "Training growing_up:  84%|██████▋ | 16777/20010 [13:25:46<2:41:44,  3.00s/batch]Batch 16900/20010 Done, mean position loss: 22.42674643754959\n",
      "Training growing_up:  84%|██████▋ | 16751/20010 [13:25:48<2:47:19,  3.08s/batch]Batch 17000/20010 Done, mean position loss: 22.097847385406496\n",
      "Training growing_up:  85%|██████▊ | 17059/20010 [13:25:56<2:40:57,  3.27s/batch]Batch 16900/20010 Done, mean position loss: 22.085334997177124\n",
      "Training growing_up:  84%|██████▋ | 16778/20010 [13:26:03<2:37:48,  2.93s/batch]Batch 16700/20010 Done, mean position loss: 21.679500715732573\n",
      "Training growing_up:  84%|██████▊ | 16904/20010 [13:26:05<2:35:32,  3.00s/batch]Batch 16500/20010 Done, mean position loss: 21.795717139244076\n",
      "Training growing_up:  84%|██████▋ | 16880/20010 [13:26:05<2:25:23,  2.79s/batch]Batch 16600/20010 Done, mean position loss: 22.87346715450287\n",
      "Training growing_up:  85%|██████▊ | 16987/20010 [13:26:25<2:38:36,  3.15s/batch]Batch 16900/20010 Done, mean position loss: 21.75268457889557\n",
      "Training growing_up:  84%|██████▊ | 16902/20010 [13:26:28<2:22:35,  2.75s/batch]Batch 16600/20010 Done, mean position loss: 21.757390687465666\n",
      "Training growing_up:  85%|██████▊ | 16946/20010 [13:26:31<2:50:52,  3.35s/batch]Batch 16800/20010 Done, mean position loss: 21.850675382614135\n",
      "Training growing_up:  84%|██████▋ | 16799/20010 [13:26:51<2:28:58,  2.78s/batch]Batch 16600/20010 Done, mean position loss: 21.25756170749664\n",
      "Training growing_up:  84%|██████▋ | 16827/20010 [13:26:57<2:37:03,  2.96s/batch]Batch 16800/20010 Done, mean position loss: 22.430568363666534\n",
      "Training growing_up:  84%|██████▋ | 16796/20010 [13:26:57<2:36:31,  2.92s/batch]Batch 16800/20010 Done, mean position loss: 22.33842679738998\n",
      "Training growing_up:  84%|██████▋ | 16775/20010 [13:27:00<2:41:22,  2.99s/batch]Batch 16900/20010 Done, mean position loss: 21.95993563890457\n",
      "Training growing_up:  84%|██████▋ | 16829/20010 [13:27:04<2:57:55,  3.36s/batch]Batch 17000/20010 Done, mean position loss: 21.573456597328185\n",
      "Training growing_up:  85%|██████▊ | 16959/20010 [13:27:07<2:12:46,  2.61s/batch]Batch 16900/20010 Done, mean position loss: 22.255620343685152\n",
      "Training growing_up:  85%|██████▊ | 17004/20010 [13:27:13<2:25:47,  2.91s/batch]Batch 16800/20010 Done, mean position loss: 22.838374152183533\n",
      "Batch 16900/20010 Done, mean position loss: 22.630145399570466\n",
      "Training growing_up:  84%|██████▋ | 16843/20010 [13:27:19<2:30:39,  2.85s/batch]Batch 16900/20010 Done, mean position loss: 22.654571890830994\n",
      "Training growing_up:  84%|██████▊ | 16896/20010 [13:27:20<2:30:29,  2.90s/batch]Batch 16800/20010 Done, mean position loss: 22.03168502092361\n",
      "Training growing_up:  85%|██████▊ | 17092/20010 [13:27:29<2:20:59,  2.90s/batch]Batch 16900/20010 Done, mean position loss: 22.933744671344755\n",
      "Training growing_up:  84%|██████▋ | 16814/20010 [13:27:35<2:36:46,  2.94s/batch]Batch 16900/20010 Done, mean position loss: 21.989423851966862\n",
      "Training growing_up:  85%|██████▊ | 16910/20010 [13:27:56<2:32:51,  2.96s/batch]Batch 17100/20010 Done, mean position loss: 22.221926095485685\n",
      "Training growing_up:  85%|██████▊ | 16922/20010 [13:28:18<2:26:53,  2.85s/batch]Batch 16800/20010 Done, mean position loss: 22.251906423568727\n",
      "Training growing_up:  85%|██████▊ | 17108/20010 [13:28:19<2:41:33,  3.34s/batch]Batch 16800/20010 Done, mean position loss: 22.98971939086914\n",
      "Training growing_up:  83%|██████▌ | 16546/20010 [13:28:22<2:46:43,  2.89s/batch]Batch 16800/20010 Done, mean position loss: 22.255402233600616\n",
      "Training growing_up:  85%|██████▊ | 16991/20010 [13:28:31<2:23:42,  2.86s/batch]Batch 16800/20010 Done, mean position loss: 22.505280365943907\n",
      "Training growing_up:  84%|██████▋ | 16802/20010 [13:28:33<2:29:39,  2.80s/batch]Batch 16600/20010 Done, mean position loss: 22.56193065881729\n",
      "Training growing_up:  86%|██████▊ | 17195/20010 [13:28:33<2:12:42,  2.83s/batch]Batch 16700/20010 Done, mean position loss: 22.545151958465574\n",
      "Training growing_up:  85%|██████▊ | 16929/20010 [13:28:51<2:38:21,  3.08s/batch]Batch 17200/20010 Done, mean position loss: 21.364168329238893\n",
      "Training growing_up:  85%|██████▊ | 17065/20010 [13:29:00<2:27:33,  3.01s/batch]Batch 17000/20010 Done, mean position loss: 21.577665827274323\n",
      "Training growing_up:  85%|██████▊ | 16981/20010 [13:29:05<2:52:42,  3.42s/batch]Batch 16900/20010 Done, mean position loss: 21.72648277759552\n",
      "Training growing_up:  84%|██████▋ | 16815/20010 [13:29:13<2:35:41,  2.92s/batch]Batch 17000/20010 Done, mean position loss: 22.050830762386322\n",
      "Training growing_up:  83%|██████▋ | 16620/20010 [13:29:33<3:26:47,  3.66s/batch]Batch 17000/20010 Done, mean position loss: 22.845346398353577\n",
      "Training growing_up:  84%|██████▋ | 16848/20010 [13:29:37<2:37:26,  2.99s/batch]Batch 16600/20010 Done, mean position loss: 22.18877125740051\n",
      "Training growing_up:  85%|██████▊ | 17014/20010 [13:29:37<2:19:59,  2.80s/batch]Batch 17100/20010 Done, mean position loss: 21.755781459808347\n",
      "Training growing_up:  83%|██████▋ | 16609/20010 [13:30:02<2:52:05,  3.04s/batch]Batch 17000/20010 Done, mean position loss: 22.440429451465604\n",
      "Training growing_up:  83%|██████▋ | 16582/20010 [13:30:07<2:35:47,  2.73s/batch]Batch 17100/20010 Done, mean position loss: 22.50475729703903\n",
      "Training growing_up:  85%|██████▊ | 16958/20010 [13:30:08<2:33:19,  3.01s/batch]Batch 16900/20010 Done, mean position loss: 22.25352353096008\n",
      "Training growing_up:  85%|██████▊ | 16991/20010 [13:30:12<2:38:18,  3.15s/batch]Batch 17000/20010 Done, mean position loss: 21.717914822101594\n",
      "Training growing_up:  84%|██████▋ | 16792/20010 [13:30:39<2:37:09,  2.93s/batch]Batch 16900/20010 Done, mean position loss: 22.83472585201263\n",
      "Training growing_up:  83%|██████▋ | 16642/20010 [13:30:41<3:11:42,  3.42s/batch]Batch 17000/20010 Done, mean position loss: 22.451360177993774\n",
      "Training growing_up:  85%|██████▊ | 16976/20010 [13:30:44<2:24:09,  2.85s/batch]Batch 17100/20010 Done, mean position loss: 22.02524197101593\n",
      "Training growing_up:  84%|██████▊ | 16906/20010 [13:30:54<2:25:49,  2.82s/batch]Batch 17000/20010 Done, mean position loss: 22.24496977567673\n",
      "Training growing_up:  84%|██████▊ | 16890/20010 [13:31:04<2:53:01,  3.33s/batch]Batch 16600/20010 Done, mean position loss: 21.618737916946415\n",
      "Training growing_up:  83%|██████▋ | 16601/20010 [13:31:05<3:00:00,  3.17s/batch]Batch 16800/20010 Done, mean position loss: 21.741886632442473\n",
      "Training growing_up:  85%|██████▊ | 16922/20010 [13:31:14<2:35:45,  3.03s/batch]Batch 16700/20010 Done, mean position loss: 22.60229773759842\n",
      "Training growing_up:  85%|██████▊ | 17016/20010 [13:31:26<2:22:20,  2.85s/batch]Batch 17000/20010 Done, mean position loss: 21.77960518360138\n",
      "Training growing_up:  84%|██████▋ | 16866/20010 [13:31:37<2:47:31,  3.20s/batch]Batch 16900/20010 Done, mean position loss: 22.033554573059085\n",
      "Training growing_up:  85%|██████▊ | 16993/20010 [13:31:37<2:45:34,  3.29s/batch]Batch 16700/20010 Done, mean position loss: 21.849669225215912\n",
      "Training growing_up:  86%|██████▊ | 17178/20010 [13:31:50<2:13:29,  2.83s/batch]Batch 16900/20010 Done, mean position loss: 22.47744990825653\n",
      "Training growing_up:  84%|██████▋ | 16869/20010 [13:31:51<2:47:55,  3.21s/batch]Batch 16900/20010 Done, mean position loss: 22.590051379203796\n",
      "Training growing_up:  85%|██████▊ | 16993/20010 [13:31:55<2:29:28,  2.97s/batch]Batch 16700/20010 Done, mean position loss: 21.47905670642853\n",
      "Training growing_up:  85%|██████▊ | 17038/20010 [13:32:02<2:21:14,  2.85s/batch]Batch 17000/20010 Done, mean position loss: 21.84482178211212\n",
      "Training growing_up:  85%|██████▊ | 17028/20010 [13:32:03<2:33:28,  3.09s/batch]Batch 17000/20010 Done, mean position loss: 22.242011163234707\n",
      "Training growing_up:  84%|██████▋ | 16710/20010 [13:32:05<2:47:08,  3.04s/batch]Batch 17100/20010 Done, mean position loss: 21.595323576927186\n",
      "Training growing_up:  84%|██████▋ | 16874/20010 [13:32:06<2:21:35,  2.71s/batch]Batch 17000/20010 Done, mean position loss: 22.9146955704689\n",
      "Training growing_up:  85%|██████▊ | 16910/20010 [13:32:16<2:20:51,  2.73s/batch]Batch 16900/20010 Done, mean position loss: 22.115093681812283\n",
      "Batch 16900/20010 Done, mean position loss: 23.069293272495266\n",
      "Training growing_up:  85%|██████▊ | 17048/20010 [13:32:23<2:22:56,  2.90s/batch]Batch 17000/20010 Done, mean position loss: 22.265941240787505\n",
      "Training growing_up:  85%|██████▊ | 17011/20010 [13:32:30<2:14:32,  2.69s/batch]Batch 17000/20010 Done, mean position loss: 22.60183907032013\n",
      "Training growing_up:  85%|██████▊ | 16921/20010 [13:32:38<2:31:40,  2.95s/batch]Batch 17000/20010 Done, mean position loss: 21.892022342681884\n",
      "Training growing_up:  84%|██████▋ | 16729/20010 [13:32:59<2:37:06,  2.87s/batch]Batch 17200/20010 Done, mean position loss: 22.32315934181213\n",
      "Training growing_up:  84%|██████▋ | 16726/20010 [13:33:10<2:44:01,  3.00s/batch]Batch 16900/20010 Done, mean position loss: 22.997959311008454\n",
      "Training growing_up:  86%|██████▊ | 17152/20010 [13:33:20<2:15:13,  2.84s/batch]Batch 16900/20010 Done, mean position loss: 22.325483536720277\n",
      "Training growing_up:  85%|██████▊ | 17066/20010 [13:33:24<2:30:55,  3.08s/batch]Batch 16900/20010 Done, mean position loss: 22.36955346107483\n",
      "Training growing_up:  84%|██████▋ | 16797/20010 [13:33:24<2:42:49,  3.04s/batch]Batch 16900/20010 Done, mean position loss: 22.543030083179474\n",
      "Training growing_up:  85%|██████▊ | 16938/20010 [13:33:34<2:09:13,  2.52s/batch]Batch 17300/20010 Done, mean position loss: 21.722450466156005\n",
      "Training growing_up:  84%|██████▋ | 16850/20010 [13:33:34<2:36:52,  2.98s/batch]Batch 16800/20010 Done, mean position loss: 22.264879870414735\n",
      "Training growing_up:  84%|██████▊ | 16906/20010 [13:33:35<2:01:42,  2.35s/batch]Batch 16700/20010 Done, mean position loss: 22.42902261972427\n",
      "Training growing_up:  85%|██████▊ | 16911/20010 [13:33:48<2:14:28,  2.60s/batch]Batch 17100/20010 Done, mean position loss: 21.760685019493103\n",
      "Training growing_up:  85%|██████▊ | 17035/20010 [13:34:00<2:06:48,  2.56s/batch]Batch 17000/20010 Done, mean position loss: 21.998554482460023\n",
      "Training growing_up:  83%|██████▋ | 16692/20010 [13:34:08<2:46:31,  3.01s/batch]Batch 17100/20010 Done, mean position loss: 22.313152749538425\n",
      "Training growing_up:  85%|██████▊ | 17106/20010 [13:34:23<2:28:39,  3.07s/batch]Batch 17100/20010 Done, mean position loss: 22.72968295812607\n",
      "Training growing_up:  85%|██████▊ | 17090/20010 [13:34:28<2:27:08,  3.02s/batch]Batch 17200/20010 Done, mean position loss: 21.649696934223176\n",
      "Training growing_up:  85%|██████▊ | 16963/20010 [13:34:38<2:23:29,  2.83s/batch]Batch 16700/20010 Done, mean position loss: 22.20092456102371\n",
      "Training growing_up:  85%|██████▊ | 16932/20010 [13:34:53<2:23:51,  2.80s/batch]Batch 17100/20010 Done, mean position loss: 22.391593074798585\n",
      "Training growing_up:  84%|██████▋ | 16831/20010 [13:34:59<2:28:37,  2.81s/batch]Batch 17200/20010 Done, mean position loss: 22.85491405248642\n",
      "Training growing_up:  85%|██████▊ | 17054/20010 [13:35:00<2:16:18,  2.77s/batch]Batch 17100/20010 Done, mean position loss: 21.820299417972564\n",
      "Training growing_up:  86%|██████▊ | 17120/20010 [13:35:03<2:16:31,  2.83s/batch]Batch 17000/20010 Done, mean position loss: 22.248143832683564\n",
      "Training growing_up:  84%|██████▋ | 16780/20010 [13:35:26<2:44:16,  3.05s/batch]Batch 17000/20010 Done, mean position loss: 22.979005224704743\n",
      "Training growing_up:  85%|██████▊ | 16944/20010 [13:35:31<2:30:18,  2.94s/batch]Batch 17100/20010 Done, mean position loss: 22.6611620593071\n",
      "Training growing_up:  85%|██████▊ | 16947/20010 [13:35:40<2:37:48,  3.09s/batch]Batch 17200/20010 Done, mean position loss: 21.860210723876953\n",
      "Training growing_up:  85%|██████▊ | 17077/20010 [13:35:42<2:29:21,  3.06s/batch]Batch 17100/20010 Done, mean position loss: 22.1627654838562\n",
      "Training growing_up:  85%|██████▊ | 16955/20010 [13:36:03<2:21:49,  2.79s/batch]Batch 16900/20010 Done, mean position loss: 21.573381752967837\n",
      "Training growing_up:  85%|██████▊ | 16990/20010 [13:36:03<2:25:50,  2.90s/batch]Batch 16700/20010 Done, mean position loss: 21.60510713338852\n",
      "Training growing_up:  86%|██████▊ | 17185/20010 [13:36:07<2:20:18,  2.98s/batch]Batch 16800/20010 Done, mean position loss: 22.600523753166197\n",
      "Training growing_up:  85%|██████▊ | 17016/20010 [13:36:11<2:31:17,  3.03s/batch]Batch 17100/20010 Done, mean position loss: 21.852242164611816\n",
      "Training growing_up:  86%|██████▊ | 17144/20010 [13:36:27<2:31:43,  3.18s/batch]Batch 16800/20010 Done, mean position loss: 21.797914867401122\n",
      "Training growing_up:  85%|██████▊ | 16910/20010 [13:36:28<2:24:08,  2.79s/batch]Batch 17000/20010 Done, mean position loss: 22.023750243186953\n",
      "Training growing_up:  86%|██████▉ | 17280/20010 [13:36:36<2:13:50,  2.94s/batch]Batch 17000/20010 Done, mean position loss: 22.371791996955874\n",
      "Training growing_up:  84%|██████▋ | 16743/20010 [13:36:38<2:45:47,  3.04s/batch]Batch 17000/20010 Done, mean position loss: 22.678903012275697\n",
      "Training growing_up:  86%|██████▊ | 17113/20010 [13:36:44<2:17:25,  2.85s/batch]Batch 17100/20010 Done, mean position loss: 22.395230479240418\n",
      "Training growing_up:  86%|██████▉ | 17252/20010 [13:36:51<2:08:01,  2.79s/batch]Batch 17100/20010 Done, mean position loss: 22.14516107082367\n",
      "Training growing_up:  84%|██████▋ | 16717/20010 [13:36:53<2:32:40,  2.78s/batch]Batch 17200/20010 Done, mean position loss: 21.864354906082156\n",
      "Training growing_up:  86%|██████▉ | 17227/20010 [13:36:53<2:06:39,  2.73s/batch]Batch 16800/20010 Done, mean position loss: 21.380629136562348\n",
      "Training growing_up:  84%|██████▋ | 16810/20010 [13:36:54<2:32:03,  2.85s/batch]Batch 17100/20010 Done, mean position loss: 22.71829830646515\n",
      "Training growing_up:  85%|██████▊ | 16974/20010 [13:37:03<2:46:48,  3.30s/batch]Batch 17000/20010 Done, mean position loss: 22.868607411384584\n",
      "Training growing_up:  84%|██████▋ | 16772/20010 [13:37:05<2:40:18,  2.97s/batch]Batch 17000/20010 Done, mean position loss: 22.15967437982559\n",
      "Training growing_up:  85%|██████▊ | 17014/20010 [13:37:16<2:44:57,  3.30s/batch]Batch 17100/20010 Done, mean position loss: 23.09877194881439\n",
      "Training growing_up:  85%|██████▊ | 17101/20010 [13:37:16<2:29:29,  3.08s/batch]Batch 17100/20010 Done, mean position loss: 22.396309282779693\n",
      "Training growing_up:  86%|██████▊ | 17113/20010 [13:37:18<2:31:20,  3.13s/batch]Batch 17100/20010 Done, mean position loss: 21.8817826795578\n",
      "Training growing_up:  85%|██████▊ | 16984/20010 [13:37:33<2:32:17,  3.02s/batch]Batch 17300/20010 Done, mean position loss: 22.325286900997163\n",
      "Training growing_up:  86%|██████▊ | 17165/20010 [13:38:04<2:16:45,  2.88s/batch]Batch 17000/20010 Done, mean position loss: 22.872694952487944\n",
      "Training growing_up:  86%|██████▊ | 17155/20010 [13:38:08<2:43:44,  3.44s/batch]Batch 17400/20010 Done, mean position loss: 21.62656133413315\n",
      "Training growing_up:  84%|██████▋ | 16827/20010 [13:38:10<2:36:38,  2.95s/batch]Batch 17000/20010 Done, mean position loss: 22.115947279930115\n",
      "Training growing_up:  86%|██████▊ | 17182/20010 [13:38:21<2:08:32,  2.73s/batch]Batch 17000/20010 Done, mean position loss: 22.38477921485901\n",
      "Training growing_up:  85%|██████▊ | 17005/20010 [13:38:21<2:17:00,  2.74s/batch]Batch 17000/20010 Done, mean position loss: 22.162590820789337\n",
      "Training growing_up:  84%|██████▋ | 16848/20010 [13:38:28<2:31:55,  2.88s/batch]Batch 16900/20010 Done, mean position loss: 22.40434960126877\n",
      "Training growing_up:  85%|██████▊ | 17071/20010 [13:38:32<2:31:13,  3.09s/batch]Batch 16800/20010 Done, mean position loss: 22.42178837299347\n",
      "Training growing_up:  84%|██████▋ | 16785/20010 [13:38:44<2:51:08,  3.18s/batch]Batch 17200/20010 Done, mean position loss: 21.661397047042847\n",
      "Training growing_up:  86%|██████▉ | 17203/20010 [13:38:52<2:47:40,  3.58s/batch]Batch 17100/20010 Done, mean position loss: 22.253652894496916\n",
      "Training growing_up:  86%|██████▊ | 17139/20010 [13:39:08<2:37:02,  3.28s/batch]Batch 17200/20010 Done, mean position loss: 22.159905846118928\n",
      "Training growing_up:  85%|██████▊ | 17022/20010 [13:39:17<2:52:57,  3.47s/batch]Batch 17200/20010 Done, mean position loss: 22.478778126239774\n",
      "Training growing_up:  85%|██████▊ | 17055/20010 [13:39:25<2:46:11,  3.37s/batch]Batch 17300/20010 Done, mean position loss: 21.70796674966812\n",
      "Training growing_up:  86%|██████▉ | 17208/20010 [13:39:40<2:33:55,  3.30s/batch]Batch 16800/20010 Done, mean position loss: 22.256664266586302\n",
      "Training growing_up:  86%|██████▊ | 17161/20010 [13:39:48<2:20:52,  2.97s/batch]Batch 17200/20010 Done, mean position loss: 22.51394439458847\n",
      "Training growing_up:  86%|██████▉ | 17223/20010 [13:39:50<2:05:14,  2.70s/batch]Batch 17200/20010 Done, mean position loss: 21.61694056272507\n",
      "Training growing_up:  84%|██████▋ | 16773/20010 [13:39:52<3:09:35,  3.51s/batch]Batch 17300/20010 Done, mean position loss: 23.049887709617614\n",
      "Training growing_up:  87%|██████▉ | 17314/20010 [13:40:03<2:07:16,  2.83s/batch]Batch 17100/20010 Done, mean position loss: 22.550289266109466\n",
      "Training growing_up:  86%|██████▊ | 17162/20010 [13:40:21<2:19:07,  2.93s/batch]Batch 17100/20010 Done, mean position loss: 22.76678346633911\n",
      "Training growing_up:  86%|██████▉ | 17215/20010 [13:40:28<2:16:55,  2.94s/batch]Batch 17200/20010 Done, mean position loss: 22.7217312669754\n",
      "Training growing_up:  84%|██████▋ | 16883/20010 [13:40:40<2:33:06,  2.94s/batch]Batch 17200/20010 Done, mean position loss: 22.18290729045868\n",
      "Training growing_up:  84%|██████▋ | 16876/20010 [13:40:42<2:40:29,  3.07s/batch]Batch 17300/20010 Done, mean position loss: 22.018483831882477\n",
      "Training growing_up:  85%|██████▊ | 17059/20010 [13:41:06<2:31:57,  3.09s/batch]Batch 17000/20010 Done, mean position loss: 21.749640474319456\n",
      "Training growing_up:  87%|██████▉ | 17338/20010 [13:41:13<2:12:46,  2.98s/batch]Batch 16900/20010 Done, mean position loss: 22.500683002471924\n",
      "Training growing_up:  87%|██████▉ | 17373/20010 [13:41:14<2:05:01,  2.84s/batch]Batch 17200/20010 Done, mean position loss: 21.809895725250243\n",
      "Training growing_up:  85%|██████▊ | 17062/20010 [13:41:16<2:26:14,  2.98s/batch]Batch 16800/20010 Done, mean position loss: 21.532432231903076\n",
      "Training growing_up:  86%|██████▉ | 17246/20010 [13:41:33<2:05:58,  2.73s/batch]Batch 16900/20010 Done, mean position loss: 21.954528517723084\n",
      "Training growing_up:  86%|██████▊ | 17155/20010 [13:41:40<2:45:20,  3.47s/batch]Batch 17100/20010 Done, mean position loss: 21.87931547880173\n",
      "Training growing_up:  87%|██████▉ | 17349/20010 [13:41:44<1:58:08,  2.66s/batch]Batch 17100/20010 Done, mean position loss: 22.490527751445768\n",
      "Training growing_up:  86%|██████▉ | 17253/20010 [13:41:44<2:14:11,  2.92s/batch]Batch 17200/20010 Done, mean position loss: 22.767427821159362\n",
      "Training growing_up:  86%|██████▉ | 17298/20010 [13:41:48<2:15:12,  2.99s/batch]Batch 17100/20010 Done, mean position loss: 22.484140884876247\n",
      "Training growing_up:  85%|██████▊ | 17069/20010 [13:41:52<2:44:33,  3.36s/batch]Batch 17200/20010 Done, mean position loss: 22.06131559848785\n",
      "Training growing_up:  85%|██████▊ | 16915/20010 [13:41:55<2:31:41,  2.94s/batch]Batch 17200/20010 Done, mean position loss: 21.977798149585723\n",
      "Training growing_up:  86%|██████▊ | 17133/20010 [13:41:57<2:05:54,  2.63s/batch]Batch 17300/20010 Done, mean position loss: 21.63657483100891\n",
      "Training growing_up:  86%|██████▉ | 17203/20010 [13:41:57<2:19:27,  2.98s/batch]Batch 16900/20010 Done, mean position loss: 21.23711393594742\n",
      "Training growing_up:  87%|██████▉ | 17357/20010 [13:42:07<2:11:53,  2.98s/batch]Batch 17100/20010 Done, mean position loss: 22.0892005443573\n",
      "Training growing_up:  84%|██████▋ | 16870/20010 [13:42:09<2:48:59,  3.23s/batch]Batch 17100/20010 Done, mean position loss: 22.847301259040833\n",
      "Training growing_up:  86%|██████▉ | 17248/20010 [13:42:15<2:22:04,  3.09s/batch]Batch 17200/20010 Done, mean position loss: 22.34980011701584\n",
      "Training growing_up:  86%|██████▉ | 17252/20010 [13:42:20<2:21:34,  3.08s/batch]Batch 17200/20010 Done, mean position loss: 21.927410802841187\n",
      "Training growing_up:  86%|██████▉ | 17212/20010 [13:42:27<2:20:37,  3.02s/batch]Batch 17200/20010 Done, mean position loss: 22.334356441497803\n",
      "Training growing_up:  86%|██████▉ | 17208/20010 [13:42:40<2:10:33,  2.80s/batch]Batch 17400/20010 Done, mean position loss: 22.14375768661499\n",
      "Training growing_up:  85%|██████▊ | 17100/20010 [13:43:13<2:24:07,  2.97s/batch]Batch 17500/20010 Done, mean position loss: 21.390032052993774\n",
      "Training growing_up:  85%|██████▊ | 17096/20010 [13:43:16<2:33:05,  3.15s/batch]Batch 17100/20010 Done, mean position loss: 22.989479715824128\n",
      "Training growing_up:  85%|██████▊ | 16998/20010 [13:43:17<2:40:52,  3.20s/batch]Batch 17100/20010 Done, mean position loss: 22.394072966575624\n",
      "Training growing_up:  86%|██████▉ | 17298/20010 [13:43:28<2:27:35,  3.27s/batch]Batch 17000/20010 Done, mean position loss: 22.093273556232454\n",
      "Training growing_up:  84%|██████▊ | 16897/20010 [13:43:29<2:37:19,  3.03s/batch]Batch 17100/20010 Done, mean position loss: 22.389916138648985\n",
      "Training growing_up:  84%|██████▋ | 16879/20010 [13:43:32<2:37:48,  3.02s/batch]Batch 17100/20010 Done, mean position loss: 22.256227662563326\n",
      "Training growing_up:  86%|██████▉ | 17263/20010 [13:43:37<2:06:47,  2.77s/batch]Batch 17300/20010 Done, mean position loss: 21.60451821565628\n",
      "Training growing_up:  86%|██████▉ | 17225/20010 [13:43:43<2:30:12,  3.24s/batch]Batch 16900/20010 Done, mean position loss: 22.537036118507388\n",
      "Training growing_up:  87%|██████▉ | 17344/20010 [13:44:00<1:58:23,  2.66s/batch]Batch 17200/20010 Done, mean position loss: 21.759116549491882\n",
      "Training growing_up:  86%|██████▉ | 17247/20010 [13:44:11<2:15:45,  2.95s/batch]Batch 17300/20010 Done, mean position loss: 22.34104729175568\n",
      "Training growing_up:  85%|██████▊ | 16955/20010 [13:44:14<2:43:24,  3.21s/batch]Batch 17300/20010 Done, mean position loss: 22.865761604309082\n",
      "Training growing_up:  86%|██████▊ | 17123/20010 [13:44:21<2:18:18,  2.87s/batch]Batch 17400/20010 Done, mean position loss: 21.724310922622678\n",
      "Training growing_up:  86%|██████▉ | 17256/20010 [13:44:41<2:13:18,  2.90s/batch]Batch 16900/20010 Done, mean position loss: 22.341085650920867\n",
      "Training growing_up:  86%|██████▉ | 17252/20010 [13:44:47<2:15:08,  2.94s/batch]Batch 17300/20010 Done, mean position loss: 22.879207050800325\n",
      "Training growing_up:  86%|██████▊ | 17126/20010 [13:44:51<2:22:27,  2.96s/batch]Batch 17300/20010 Done, mean position loss: 21.819236369132994\n",
      "Training growing_up:  86%|██████▉ | 17284/20010 [13:44:53<2:10:40,  2.88s/batch]Batch 17400/20010 Done, mean position loss: 22.72851370096207\n",
      "Training growing_up:  85%|██████▊ | 16965/20010 [13:45:12<2:32:29,  3.00s/batch]Batch 17200/20010 Done, mean position loss: 22.202555067539215\n",
      "Training growing_up:  86%|██████▊ | 17140/20010 [13:45:23<2:26:16,  3.06s/batch]Batch 17200/20010 Done, mean position loss: 22.627672607898713\n",
      "Training growing_up:  87%|██████▉ | 17462/20010 [13:45:36<2:00:16,  2.83s/batch]Batch 17300/20010 Done, mean position loss: 22.43103928565979\n",
      "Training growing_up:  86%|██████▉ | 17291/20010 [13:45:45<2:15:18,  2.99s/batch]Batch 17300/20010 Done, mean position loss: 22.1766348695755\n",
      "Training growing_up:  86%|██████▊ | 17173/20010 [13:45:49<2:21:53,  3.00s/batch]Batch 17400/20010 Done, mean position loss: 22.059434564113616\n",
      "Training growing_up:  87%|██████▉ | 17406/20010 [13:46:05<2:21:03,  3.25s/batch]Batch 17100/20010 Done, mean position loss: 21.494121298789977\n",
      "Training growing_up:  85%|██████▊ | 17055/20010 [13:46:11<2:35:11,  3.15s/batch]Batch 17000/20010 Done, mean position loss: 22.941477050781252\n",
      "Training growing_up:  88%|███████ | 17562/20010 [13:46:13<1:50:24,  2.71s/batch]Batch 17300/20010 Done, mean position loss: 21.59809915781021\n",
      "Training growing_up:  85%|██████▊ | 17107/20010 [13:46:22<2:25:56,  3.02s/batch]Batch 16900/20010 Done, mean position loss: 21.584874880313873\n",
      "Training growing_up:  86%|██████▊ | 17189/20010 [13:46:32<2:02:00,  2.60s/batch]Batch 17000/20010 Done, mean position loss: 21.575493700504303\n",
      "Training growing_up:  86%|██████▉ | 17299/20010 [13:46:37<2:04:23,  2.75s/batch]Batch 17200/20010 Done, mean position loss: 22.170831959247586\n",
      "Training growing_up:  86%|██████▊ | 17192/20010 [13:46:39<1:55:49,  2.47s/batch]Batch 17200/20010 Done, mean position loss: 22.51391583204269\n",
      "Training growing_up:  87%|██████▉ | 17366/20010 [13:46:41<1:45:41,  2.40s/batch]Batch 17300/20010 Done, mean position loss: 22.879053721427915\n",
      "Training growing_up:  85%|██████▊ | 16942/20010 [13:46:44<2:35:16,  3.04s/batch]Batch 17400/20010 Done, mean position loss: 21.82612850904465\n",
      "Training growing_up:  87%|██████▉ | 17341/20010 [13:46:45<2:00:18,  2.70s/batch]Batch 17300/20010 Done, mean position loss: 22.15417300224304\n",
      "Training growing_up:  86%|██████▉ | 17296/20010 [13:46:51<2:04:34,  2.75s/batch]Batch 17300/20010 Done, mean position loss: 22.060462346076967\n",
      "Batch 17200/20010 Done, mean position loss: 22.493492276668547\n",
      "Training growing_up:  86%|██████▉ | 17235/20010 [13:46:55<2:26:45,  3.17s/batch]Batch 17000/20010 Done, mean position loss: 21.14179058790207\n",
      "Training growing_up:  88%|███████ | 17581/20010 [13:47:03<1:46:18,  2.63s/batch]Batch 17200/20010 Done, mean position loss: 22.072094655036928\n",
      "Training growing_up:  86%|██████▊ | 17172/20010 [13:47:04<1:59:41,  2.53s/batch]Batch 17300/20010 Done, mean position loss: 22.52295407295227\n",
      "Training growing_up:  85%|██████▊ | 17021/20010 [13:47:06<2:07:59,  2.57s/batch]Batch 17200/20010 Done, mean position loss: 22.79266466140747\n",
      "Training growing_up:  86%|██████▉ | 17304/20010 [13:47:13<1:58:31,  2.63s/batch]Batch 17300/20010 Done, mean position loss: 22.199694907665254\n",
      "Training growing_up:  87%|██████▉ | 17368/20010 [13:47:23<1:47:55,  2.45s/batch]Batch 17500/20010 Done, mean position loss: 22.362564973831176\n",
      "Training growing_up:  86%|██████▉ | 17245/20010 [13:47:24<2:02:57,  2.67s/batch]Batch 17300/20010 Done, mean position loss: 22.67703664302826\n",
      "Training growing_up:  86%|██████▉ | 17225/20010 [13:47:59<2:10:52,  2.82s/batch]Batch 17600/20010 Done, mean position loss: 21.64435847759247\n",
      "Training growing_up:  86%|██████▉ | 17197/20010 [13:48:06<2:13:28,  2.85s/batch]Batch 17200/20010 Done, mean position loss: 22.799785432815554\n",
      "Training growing_up:  87%|██████▉ | 17385/20010 [13:48:10<2:17:06,  3.13s/batch]Batch 17200/20010 Done, mean position loss: 22.43732442378998\n",
      "Training growing_up:  87%|██████▉ | 17358/20010 [13:48:18<2:04:45,  2.82s/batch]Batch 17200/20010 Done, mean position loss: 22.28488879442215\n",
      "Training growing_up:  85%|██████▊ | 16942/20010 [13:48:22<2:32:43,  2.99s/batch]Batch 17400/20010 Done, mean position loss: 21.614883008003233\n",
      "Training growing_up:  85%|██████▊ | 16978/20010 [13:48:28<2:30:11,  2.97s/batch]Batch 17100/20010 Done, mean position loss: 22.211635842323304\n",
      "Training growing_up:  87%|██████▉ | 17363/20010 [13:48:33<2:06:44,  2.87s/batch]Batch 17200/20010 Done, mean position loss: 22.349333162307737\n",
      "Training growing_up:  85%|██████▊ | 16948/20010 [13:48:41<2:36:50,  3.07s/batch]Batch 17000/20010 Done, mean position loss: 22.514007809162138\n",
      "Training growing_up:  87%|██████▉ | 17331/20010 [13:48:49<2:16:40,  3.06s/batch]Batch 17300/20010 Done, mean position loss: 21.598880360126493\n",
      "Training growing_up:  85%|██████▊ | 17050/20010 [13:48:53<2:13:31,  2.71s/batch]Batch 17400/20010 Done, mean position loss: 22.698179638385774\n",
      "Training growing_up:  87%|██████▉ | 17360/20010 [13:49:00<2:05:55,  2.85s/batch]Batch 17400/20010 Done, mean position loss: 22.12871076822281\n",
      "Training growing_up:  86%|██████▉ | 17220/20010 [13:49:06<2:12:05,  2.84s/batch]Batch 17500/20010 Done, mean position loss: 21.80233652830124\n",
      "Training growing_up:  85%|██████▊ | 17064/20010 [13:49:33<2:13:34,  2.72s/batch]Batch 17500/20010 Done, mean position loss: 22.465146312713625\n",
      "Training growing_up:  86%|██████▉ | 17291/20010 [13:49:34<2:09:03,  2.85s/batch]Batch 17000/20010 Done, mean position loss: 22.243098411560055\n",
      "Training growing_up:  85%|██████▊ | 17002/20010 [13:49:37<2:18:57,  2.77s/batch]Batch 17400/20010 Done, mean position loss: 21.806926767826084\n",
      "Training growing_up:  86%|██████▊ | 17126/20010 [13:49:41<2:19:55,  2.91s/batch]Batch 17400/20010 Done, mean position loss: 22.56992550611496\n",
      "Training growing_up:  85%|██████▊ | 17031/20010 [13:50:04<2:20:57,  2.84s/batch]Batch 17300/20010 Done, mean position loss: 22.096604199409484\n",
      "Training growing_up:  85%|██████▊ | 17034/20010 [13:50:14<2:41:41,  3.26s/batch]Batch 17300/20010 Done, mean position loss: 22.687353570461276\n",
      "Training growing_up:  86%|██████▉ | 17308/20010 [13:50:24<2:06:41,  2.81s/batch]Batch 17400/20010 Done, mean position loss: 22.453461816310885\n",
      "Training growing_up:  86%|██████▉ | 17252/20010 [13:50:35<2:16:03,  2.96s/batch]Batch 17400/20010 Done, mean position loss: 22.124563145637513\n",
      "Training growing_up:  87%|██████▉ | 17382/20010 [13:50:35<2:13:06,  3.04s/batch]Batch 17500/20010 Done, mean position loss: 22.147382726669314\n",
      "Training growing_up:  87%|██████▉ | 17407/20010 [13:50:53<2:04:22,  2.87s/batch]Batch 17200/20010 Done, mean position loss: 21.630497357845307\n",
      "Training growing_up:  88%|███████ | 17529/20010 [13:50:55<1:50:53,  2.68s/batch]Batch 17400/20010 Done, mean position loss: 21.647598292827606\n",
      "Training growing_up:  85%|██████▊ | 16995/20010 [13:51:01<2:25:02,  2.89s/batch]Batch 17100/20010 Done, mean position loss: 22.587177882194517\n",
      "Training growing_up:  87%|██████▉ | 17384/20010 [13:51:20<2:10:57,  2.99s/batch]Batch 17000/20010 Done, mean position loss: 21.63378831624985\n",
      "Training growing_up:  88%|███████ | 17584/20010 [13:51:21<2:06:23,  3.13s/batch]Batch 17100/20010 Done, mean position loss: 21.67165072441101\n",
      "Training growing_up:  88%|███████ | 17520/20010 [13:51:26<1:52:05,  2.70s/batch]Batch 17300/20010 Done, mean position loss: 22.055240135192868\n",
      "Training growing_up:  85%|██████▊ | 17094/20010 [13:51:30<2:21:41,  2.92s/batch]Batch 17400/20010 Done, mean position loss: 22.256686487197875\n",
      "Training growing_up:  87%|██████▉ | 17400/20010 [13:51:30<2:06:39,  2.91s/batch]Batch 17300/20010 Done, mean position loss: 22.514854514598845\n",
      "Training growing_up:  86%|██████▉ | 17262/20010 [13:51:31<2:07:16,  2.78s/batch]Batch 17400/20010 Done, mean position loss: 22.63012660264969\n",
      "Training growing_up:  87%|██████▉ | 17456/20010 [13:51:33<2:10:44,  3.07s/batch]Batch 17400/20010 Done, mean position loss: 22.265836503505707\n",
      "Batch 17500/20010 Done, mean position loss: 21.676296944618223\n",
      "Training growing_up:  87%|██████▉ | 17332/20010 [13:51:43<2:00:23,  2.70s/batch]Batch 17300/20010 Done, mean position loss: 22.627392044067385\n",
      "Training growing_up:  86%|██████▉ | 17308/20010 [13:51:50<2:12:26,  2.94s/batch]Batch 17100/20010 Done, mean position loss: 21.293756198883056\n",
      "Training growing_up:  86%|██████▉ | 17305/20010 [13:51:55<2:17:36,  3.05s/batch]Batch 17400/20010 Done, mean position loss: 22.4152991771698\n",
      "Training growing_up:  87%|██████▉ | 17429/20010 [13:51:57<2:11:12,  3.05s/batch]Batch 17300/20010 Done, mean position loss: 21.991972670555114\n",
      "Training growing_up:  86%|██████▉ | 17271/20010 [13:52:00<2:20:02,  3.07s/batch]Batch 17300/20010 Done, mean position loss: 23.13282648086548\n",
      "Training growing_up:  87%|██████▉ | 17403/20010 [13:52:00<1:59:39,  2.75s/batch]Batch 17400/20010 Done, mean position loss: 22.22203828573227\n",
      "Training growing_up:  87%|██████▉ | 17454/20010 [13:52:13<2:08:31,  3.02s/batch]Batch 17400/20010 Done, mean position loss: 22.62460995912552\n",
      "Training growing_up:  86%|██████▉ | 17306/20010 [13:52:14<2:08:25,  2.85s/batch]Batch 17600/20010 Done, mean position loss: 22.458481905460356\n",
      "Training growing_up:  87%|██████▉ | 17318/20010 [13:52:47<2:10:06,  2.90s/batch]Batch 17700/20010 Done, mean position loss: 21.31329452753067\n",
      "Training growing_up:  87%|██████▉ | 17387/20010 [13:52:57<2:03:37,  2.83s/batch]Batch 17300/20010 Done, mean position loss: 22.98445564508438\n",
      "Training growing_up:  85%|██████▊ | 17091/20010 [13:53:08<2:49:28,  3.48s/batch]Batch 17300/20010 Done, mean position loss: 22.343483016490936\n",
      "Training growing_up:  87%|██████▉ | 17488/20010 [13:53:09<2:27:40,  3.51s/batch]Batch 17500/20010 Done, mean position loss: 21.54648447751999\n",
      "Training growing_up:  87%|██████▉ | 17326/20010 [13:53:11<2:30:53,  3.37s/batch]Batch 17300/20010 Done, mean position loss: 22.47950303077698\n",
      "Training growing_up:  87%|██████▉ | 17506/20010 [13:53:25<2:18:44,  3.32s/batch]Batch 17200/20010 Done, mean position loss: 22.441607851982116\n",
      "Training growing_up:  85%|██████▊ | 17082/20010 [13:53:34<2:16:20,  2.79s/batch]Batch 17300/20010 Done, mean position loss: 22.1533429813385\n",
      "Training growing_up:  87%|██████▉ | 17444/20010 [13:53:39<1:58:22,  2.77s/batch]Batch 17100/20010 Done, mean position loss: 22.306937980651853\n",
      "Training growing_up:  87%|██████▉ | 17311/20010 [13:53:41<2:30:35,  3.35s/batch]Batch 17400/20010 Done, mean position loss: 21.889881823062897\n",
      "Training growing_up:  87%|██████▉ | 17445/20010 [13:53:46<2:02:48,  2.87s/batch]Batch 17500/20010 Done, mean position loss: 22.70418020963669\n",
      "Training growing_up:  87%|██████▉ | 17438/20010 [13:53:48<2:07:55,  2.98s/batch]Batch 17500/20010 Done, mean position loss: 22.214378387928008\n",
      "Training growing_up:  87%|██████▉ | 17437/20010 [13:54:02<1:57:46,  2.75s/batch]Batch 17600/20010 Done, mean position loss: 21.84052757024765\n",
      "Training growing_up:  89%|███████ | 17735/20010 [13:54:32<2:03:36,  3.26s/batch]Batch 17100/20010 Done, mean position loss: 22.442785913944245\n",
      "Training growing_up:  87%|██████▉ | 17360/20010 [13:54:32<2:15:10,  3.06s/batch]Batch 17500/20010 Done, mean position loss: 22.22253360033035\n",
      "Training growing_up:  87%|██████▉ | 17389/20010 [13:54:33<2:19:06,  3.18s/batch]Batch 17500/20010 Done, mean position loss: 21.735961742401123\n",
      "Training growing_up:  87%|██████▉ | 17474/20010 [13:54:35<2:09:33,  3.07s/batch]Batch 17600/20010 Done, mean position loss: 22.837561452388762\n",
      "Training growing_up:  87%|██████▉ | 17365/20010 [13:55:10<2:07:53,  2.90s/batch]Batch 17400/20010 Done, mean position loss: 22.06952975511551\n",
      "Training growing_up:  88%|███████ | 17573/20010 [13:55:10<2:02:56,  3.03s/batch]Batch 17400/20010 Done, mean position loss: 22.752974240779874\n",
      "Training growing_up:  88%|███████ | 17545/20010 [13:55:22<1:55:29,  2.81s/batch]Batch 17500/20010 Done, mean position loss: 22.302906553745267\n",
      "Training growing_up:  88%|███████ | 17519/20010 [13:55:27<1:59:12,  2.87s/batch]Batch 17600/20010 Done, mean position loss: 21.916948478221894\n",
      "Training growing_up:  86%|██████▊ | 17140/20010 [13:55:40<2:31:38,  3.17s/batch]Batch 17500/20010 Done, mean position loss: 22.149534804821016\n",
      "Training growing_up:  85%|██████▊ | 17089/20010 [13:55:55<2:17:04,  2.82s/batch]Batch 17500/20010 Done, mean position loss: 21.98626699924469\n",
      "Training growing_up:  87%|██████▉ | 17477/20010 [13:55:58<2:24:28,  3.42s/batch]Batch 17300/20010 Done, mean position loss: 21.568722820281984\n",
      "Training growing_up:  88%|███████ | 17531/20010 [13:56:02<2:00:29,  2.92s/batch]Batch 17200/20010 Done, mean position loss: 22.590048966407778\n",
      "Training growing_up:  88%|███████ | 17554/20010 [13:56:23<2:04:04,  3.03s/batch]Batch 17500/20010 Done, mean position loss: 22.23433994293213\n",
      "Training growing_up:  87%|██████▉ | 17366/20010 [13:56:28<2:11:51,  2.99s/batch]Batch 17400/20010 Done, mean position loss: 21.746817083358764\n",
      "Training growing_up:  87%|██████▉ | 17428/20010 [13:56:31<2:04:42,  2.90s/batch]Batch 17100/20010 Done, mean position loss: 21.478043017387392\n",
      "Training growing_up:  88%|███████ | 17684/20010 [13:56:32<1:58:50,  3.07s/batch]Batch 17200/20010 Done, mean position loss: 21.77104862689972\n",
      "Training growing_up:  86%|██████▉ | 17264/20010 [13:56:34<2:13:25,  2.92s/batch]Batch 17600/20010 Done, mean position loss: 21.537839336395265\n",
      "Training growing_up:  87%|██████▉ | 17429/20010 [13:56:35<2:05:03,  2.91s/batch]Batch 17500/20010 Done, mean position loss: 22.83699560880661\n",
      "Training growing_up:  87%|██████▉ | 17460/20010 [13:56:36<1:59:12,  2.80s/batch]Batch 17400/20010 Done, mean position loss: 22.44990778684616\n",
      "Training growing_up:  89%|███████ | 17777/20010 [13:56:36<1:55:34,  3.11s/batch]Batch 17400/20010 Done, mean position loss: 22.520815346240997\n",
      "Training growing_up:  88%|███████ | 17643/20010 [13:56:41<2:13:03,  3.37s/batch]Batch 17500/20010 Done, mean position loss: 21.945664951801298\n",
      "Training growing_up:  86%|██████▊ | 17162/20010 [13:56:51<2:49:15,  3.57s/batch]Batch 17200/20010 Done, mean position loss: 21.13716905117035\n",
      "Training growing_up:  87%|██████▉ | 17466/20010 [13:56:55<2:08:02,  3.02s/batch]Batch 17400/20010 Done, mean position loss: 22.80239019393921\n",
      "Training growing_up:  87%|██████▉ | 17365/20010 [13:56:56<2:21:51,  3.22s/batch]Batch 17500/20010 Done, mean position loss: 22.26214190006256\n",
      "Training growing_up:  88%|███████ | 17694/20010 [13:57:01<1:48:20,  2.81s/batch]Batch 17400/20010 Done, mean position loss: 22.075564665794374\n",
      "Training growing_up:  88%|███████ | 17551/20010 [13:57:03<2:07:25,  3.11s/batch]Batch 17500/20010 Done, mean position loss: 22.047490353584287\n",
      "Training growing_up:  86%|██████▉ | 17275/20010 [13:57:08<2:27:29,  3.24s/batch]Batch 17500/20010 Done, mean position loss: 22.935485484600065\n",
      "Training growing_up:  87%|██████▉ | 17418/20010 [13:57:21<2:18:58,  3.22s/batch]Batch 17700/20010 Done, mean position loss: 22.368443908691408\n",
      "Training growing_up:  87%|██████▉ | 17418/20010 [13:57:50<2:01:12,  2.81s/batch]Batch 17800/20010 Done, mean position loss: 21.519611928462982\n",
      "Training growing_up:  88%|███████ | 17528/20010 [13:57:59<1:54:56,  2.78s/batch]Batch 17400/20010 Done, mean position loss: 22.992117178440097\n",
      "Training growing_up:  86%|██████▉ | 17226/20010 [13:58:05<2:22:00,  3.06s/batch]Batch 17600/20010 Done, mean position loss: 21.50119257211685\n",
      "Training growing_up:  88%|███████ | 17574/20010 [13:58:10<1:51:36,  2.75s/batch]Batch 17400/20010 Done, mean position loss: 22.569921722412108\n",
      "Training growing_up:  86%|██████▉ | 17235/20010 [13:58:14<2:18:24,  2.99s/batch]Batch 17400/20010 Done, mean position loss: 22.203283493518832\n",
      "Training growing_up:  89%|███████ | 17724/20010 [13:58:27<1:46:07,  2.79s/batch]Batch 17300/20010 Done, mean position loss: 22.28858550071716\n",
      "Training growing_up:  88%|███████ | 17694/20010 [13:58:40<2:00:19,  3.12s/batch]Batch 17500/20010 Done, mean position loss: 21.796420331001283\n",
      "Training growing_up:  87%|██████▉ | 17398/20010 [13:58:40<2:18:15,  3.18s/batch]Batch 17600/20010 Done, mean position loss: 22.08858625650406\n",
      "Training growing_up:  88%|███████ | 17538/20010 [13:58:46<1:55:22,  2.80s/batch]Batch 17600/20010 Done, mean position loss: 22.80354608297348\n",
      "Training growing_up:  88%|███████ | 17616/20010 [13:58:50<2:00:55,  3.03s/batch]Batch 17400/20010 Done, mean position loss: 22.19223868608475\n",
      "Training growing_up:  87%|██████▉ | 17448/20010 [13:58:51<2:17:18,  3.22s/batch]Batch 17200/20010 Done, mean position loss: 22.18779091358185\n",
      "Training growing_up:  88%|███████ | 17540/20010 [13:59:01<1:56:32,  2.83s/batch]Batch 17700/20010 Done, mean position loss: 21.832702181339265\n",
      "Training growing_up:  88%|███████ | 17566/20010 [13:59:30<1:53:33,  2.79s/batch]Batch 17600/20010 Done, mean position loss: 22.674609117507934\n",
      "Training growing_up:  88%|███████ | 17616/20010 [13:59:33<2:07:09,  3.19s/batch]Batch 17700/20010 Done, mean position loss: 22.5250302529335\n",
      "Training growing_up:  88%|███████ | 17583/20010 [13:59:33<2:02:22,  3.03s/batch]Batch 17600/20010 Done, mean position loss: 21.740404770374298\n",
      "Training growing_up:  86%|██████▉ | 17275/20010 [13:59:38<2:21:06,  3.10s/batch]Batch 17200/20010 Done, mean position loss: 22.209536757469177\n",
      "Training growing_up:  88%|███████ | 17630/20010 [14:00:06<1:47:26,  2.71s/batch]Batch 17500/20010 Done, mean position loss: 22.899856038093567\n",
      "Training growing_up:  88%|███████ | 17566/20010 [14:00:15<1:46:48,  2.62s/batch]Batch 17500/20010 Done, mean position loss: 22.298410170078277\n",
      "Training growing_up:  88%|███████ | 17620/20010 [14:00:23<1:53:48,  2.86s/batch]Batch 17600/20010 Done, mean position loss: 22.590064828395846\n",
      "Training growing_up:  87%|██████▉ | 17505/20010 [14:00:25<1:51:40,  2.67s/batch]Batch 17700/20010 Done, mean position loss: 21.922479770183564\n",
      "Training growing_up:  88%|███████ | 17539/20010 [14:00:30<1:53:55,  2.77s/batch]Batch 17600/20010 Done, mean position loss: 22.24338726758957\n",
      "Training growing_up:  87%|██████▉ | 17454/20010 [14:00:48<1:50:33,  2.60s/batch]Batch 17600/20010 Done, mean position loss: 21.92018050432205\n",
      "Training growing_up:  87%|██████▉ | 17456/20010 [14:00:48<1:59:44,  2.81s/batch]Batch 17400/20010 Done, mean position loss: 21.65609201669693\n",
      "Training growing_up:  88%|███████ | 17631/20010 [14:00:53<1:54:48,  2.90s/batch]Batch 17300/20010 Done, mean position loss: 22.6161971616745\n",
      "Training growing_up:  88%|███████ | 17652/20010 [14:01:05<1:41:04,  2.57s/batch]Batch 17600/20010 Done, mean position loss: 22.089432823657987\n",
      "Training growing_up:  86%|██████▉ | 17293/20010 [14:01:20<2:04:22,  2.75s/batch]Batch 17500/20010 Done, mean position loss: 21.826143124103545\n",
      "Training growing_up:  88%|███████ | 17658/20010 [14:01:21<1:42:21,  2.61s/batch]Batch 17500/20010 Done, mean position loss: 22.372094261646268\n",
      "Training growing_up:  88%|███████ | 17659/20010 [14:01:23<1:30:51,  2.32s/batch]Batch 17200/20010 Done, mean position loss: 21.279505281448362\n",
      "Training growing_up:  89%|███████ | 17724/20010 [14:01:25<1:46:40,  2.80s/batch]Batch 17600/20010 Done, mean position loss: 22.605292327404023\n",
      "Training growing_up:  88%|███████ | 17590/20010 [14:01:27<1:45:36,  2.62s/batch]Batch 17300/20010 Done, mean position loss: 21.651816301345825\n",
      "Training growing_up:  87%|██████▉ | 17469/20010 [14:01:28<2:03:24,  2.91s/batch]Batch 17700/20010 Done, mean position loss: 21.68630761861801\n",
      "Training growing_up:  88%|███████ | 17591/20010 [14:01:29<1:44:26,  2.59s/batch]Batch 17600/20010 Done, mean position loss: 21.79666090250015\n",
      "Training growing_up:  86%|██████▉ | 17244/20010 [14:01:34<2:00:38,  2.62s/batch]Batch 17500/20010 Done, mean position loss: 22.31749614953995\n",
      "Training growing_up:  87%|██████▉ | 17459/20010 [14:01:39<1:58:06,  2.78s/batch]Batch 17500/20010 Done, mean position loss: 22.741001706123352\n",
      "Training growing_up:  88%|███████ | 17663/20010 [14:01:42<1:45:41,  2.70s/batch]Batch 17300/20010 Done, mean position loss: 21.320827629566192\n",
      "Training growing_up:  88%|███████ | 17651/20010 [14:01:46<1:43:07,  2.62s/batch]Batch 17500/20010 Done, mean position loss: 21.64416505098343\n",
      "Training growing_up:  86%|██████▉ | 17303/20010 [14:01:46<1:53:29,  2.52s/batch]Batch 17600/20010 Done, mean position loss: 22.07522756576538\n",
      "Training growing_up:  87%|██████▉ | 17462/20010 [14:01:47<1:59:52,  2.82s/batch]Batch 17600/20010 Done, mean position loss: 22.228608255386355\n",
      "Training growing_up:  89%|███████ | 17766/20010 [14:01:55<1:36:54,  2.59s/batch]Batch 17600/20010 Done, mean position loss: 22.592780189514162\n",
      "Training growing_up:  88%|███████ | 17511/20010 [14:02:03<2:03:35,  2.97s/batch]Batch 17800/20010 Done, mean position loss: 22.253694384098054\n",
      "Training growing_up:  87%|██████▉ | 17333/20010 [14:02:24<2:25:51,  3.27s/batch]Batch 17900/20010 Done, mean position loss: 21.9436660528183\n",
      "Training growing_up:  88%|███████ | 17651/20010 [14:02:47<1:44:44,  2.66s/batch]Batch 17500/20010 Done, mean position loss: 22.860151302814483\n",
      "Training growing_up:  86%|██████▉ | 17271/20010 [14:02:50<2:08:08,  2.81s/batch]Batch 17700/20010 Done, mean position loss: 21.703603034019473\n",
      "Training growing_up:  88%|███████ | 17529/20010 [14:02:55<2:06:47,  3.07s/batch]Batch 17500/20010 Done, mean position loss: 22.49111913204193\n",
      "Training growing_up:  88%|███████ | 17626/20010 [14:03:02<1:58:40,  2.99s/batch]Batch 17500/20010 Done, mean position loss: 22.28369910001755\n",
      "Training growing_up:  86%|██████▉ | 17279/20010 [14:03:13<2:11:56,  2.90s/batch]Batch 17400/20010 Done, mean position loss: 22.10001643180847\n",
      "Training growing_up:  87%|██████▉ | 17403/20010 [14:03:18<1:55:39,  2.66s/batch]Batch 17700/20010 Done, mean position loss: 22.13226184606552\n",
      "Training growing_up:  88%|███████ | 17701/20010 [14:03:18<1:52:13,  2.92s/batch]Batch 17600/20010 Done, mean position loss: 21.736675171852113\n",
      "Training growing_up:  88%|███████ | 17606/20010 [14:03:31<1:40:19,  2.50s/batch]Batch 17700/20010 Done, mean position loss: 22.517992131710052\n",
      "Training growing_up:  86%|██████▉ | 17247/20010 [14:03:36<2:12:44,  2.88s/batch]Batch 17300/20010 Done, mean position loss: 22.223097734451294\n",
      "Training growing_up:  88%|███████ | 17663/20010 [14:03:38<1:51:08,  2.84s/batch]Batch 17800/20010 Done, mean position loss: 21.783906240463253\n",
      "Training growing_up:  88%|███████ | 17577/20010 [14:03:45<1:55:06,  2.84s/batch]Batch 17500/20010 Done, mean position loss: 22.376096317768095\n",
      "Training growing_up:  88%|███████ | 17524/20010 [14:04:08<2:12:14,  3.19s/batch]Batch 17700/20010 Done, mean position loss: 22.277901015281678\n",
      "Training growing_up:  88%|███████ | 17560/20010 [14:04:11<2:02:13,  2.99s/batch]Batch 17700/20010 Done, mean position loss: 21.810713126659394\n",
      "Training growing_up:  89%|███████ | 17760/20010 [14:04:16<1:48:08,  2.88s/batch]Batch 17800/20010 Done, mean position loss: 22.3201101231575\n",
      "Training growing_up:  88%|███████ | 17512/20010 [14:04:20<2:15:33,  3.26s/batch]Batch 17300/20010 Done, mean position loss: 22.561049938201904\n",
      "Training growing_up:  88%|███████ | 17661/20010 [14:04:53<1:53:40,  2.90s/batch]Batch 17600/20010 Done, mean position loss: 22.69981234550476\n",
      "Training growing_up:  89%|███████▏| 17861/20010 [14:04:57<1:35:13,  2.66s/batch]Batch 17600/20010 Done, mean position loss: 22.168295607566833\n",
      "Training growing_up:  88%|███████ | 17605/20010 [14:05:06<1:48:00,  2.69s/batch]Batch 17700/20010 Done, mean position loss: 22.1018199634552\n",
      "Training growing_up:  88%|███████ | 17577/20010 [14:05:07<2:08:59,  3.18s/batch]Batch 17800/20010 Done, mean position loss: 21.796155195236206\n",
      "Training growing_up:  88%|███████ | 17552/20010 [14:05:14<2:04:20,  3.04s/batch]Batch 17700/20010 Done, mean position loss: 22.67660912036896\n",
      "Training growing_up:  88%|███████ | 17695/20010 [14:05:30<1:47:08,  2.78s/batch]Batch 17700/20010 Done, mean position loss: 21.744444923400877\n",
      "Training growing_up:  89%|███████▏| 17831/20010 [14:05:41<1:56:41,  3.21s/batch]Batch 17400/20010 Done, mean position loss: 22.581472659111025\n",
      "Training growing_up:  88%|███████ | 17690/20010 [14:05:40<1:44:41,  2.71s/batch]Batch 17500/20010 Done, mean position loss: 21.72049883365631\n",
      "Training growing_up:  88%|███████ | 17563/20010 [14:05:48<2:09:27,  3.17s/batch]Batch 17700/20010 Done, mean position loss: 22.186356332302093\n",
      "Training growing_up:  88%|███████ | 17548/20010 [14:06:11<1:55:34,  2.82s/batch]Batch 17700/20010 Done, mean position loss: 21.895426948070526\n",
      "Training growing_up:  88%|███████ | 17569/20010 [14:06:12<2:10:27,  3.21s/batch]Batch 17800/20010 Done, mean position loss: 21.590879395008088\n",
      "Training growing_up:  87%|██████▉ | 17391/20010 [14:06:13<2:11:23,  3.01s/batch]Batch 17700/20010 Done, mean position loss: 22.605110206604003\n",
      "Training growing_up:  89%|███████▏| 17843/20010 [14:06:16<1:51:46,  3.09s/batch]Batch 17600/20010 Done, mean position loss: 22.33554800271988\n",
      "Training growing_up:  89%|███████ | 17717/20010 [14:06:17<1:59:00,  3.11s/batch]Batch 17600/20010 Done, mean position loss: 22.099018948078154\n",
      "Training growing_up:  87%|██████▉ | 17414/20010 [14:06:18<2:03:13,  2.85s/batch]Batch 17300/20010 Done, mean position loss: 21.59020854473114\n",
      "Training growing_up:  88%|███████ | 17602/20010 [14:06:19<2:00:36,  3.01s/batch]Batch 17400/20010 Done, mean position loss: 21.84444048643112\n",
      "Training growing_up:  87%|██████▉ | 17357/20010 [14:06:26<2:15:38,  3.07s/batch]Batch 17600/20010 Done, mean position loss: 22.301362419128417\n",
      "Training growing_up:  88%|███████ | 17708/20010 [14:06:32<2:09:17,  3.37s/batch]Batch 17600/20010 Done, mean position loss: 21.879400296211244\n",
      "Training growing_up:  88%|███████ | 17605/20010 [14:06:36<1:47:52,  2.69s/batch]Batch 17600/20010 Done, mean position loss: 22.79054290533066\n",
      "Training growing_up:  88%|███████ | 17634/20010 [14:06:39<2:11:54,  3.33s/batch]Batch 17700/20010 Done, mean position loss: 21.925386328697204\n",
      "Batch 17700/20010 Done, mean position loss: 22.330029788017274\n",
      "Training growing_up:  89%|███████▏| 17853/20010 [14:06:48<1:45:56,  2.95s/batch]Batch 17400/20010 Done, mean position loss: 21.47935268640518\n",
      "Training growing_up:  89%|███████ | 17785/20010 [14:06:52<1:46:13,  2.86s/batch]Batch 17900/20010 Done, mean position loss: 22.166392204761507\n",
      "Training growing_up:  88%|███████ | 17639/20010 [14:06:54<2:01:00,  3.06s/batch]Batch 17700/20010 Done, mean position loss: 22.40117233514786\n",
      "Training growing_up:  87%|██████▉ | 17430/20010 [14:07:07<1:58:13,  2.75s/batch]Batch 18000/20010 Done, mean position loss: 21.68452543258667\n",
      "Training growing_up:  87%|██████▉ | 17426/20010 [14:07:40<2:31:14,  3.51s/batch]Batch 17800/20010 Done, mean position loss: 21.916011283397673\n",
      "Training growing_up:  88%|███████ | 17579/20010 [14:07:44<1:57:40,  2.90s/batch]Batch 17600/20010 Done, mean position loss: 22.729719789028167\n",
      "Training growing_up:  88%|███████ | 17625/20010 [14:07:51<2:16:36,  3.44s/batch]Batch 17600/20010 Done, mean position loss: 22.64401458978653\n",
      "Training growing_up:  89%|███████▏| 17837/20010 [14:07:59<1:49:56,  3.04s/batch]Batch 17600/20010 Done, mean position loss: 22.132082905769348\n",
      "Training growing_up:  89%|███████ | 17741/20010 [14:08:09<1:39:48,  2.64s/batch]Batch 17500/20010 Done, mean position loss: 22.06444304227829\n",
      "Training growing_up:  89%|███████ | 17742/20010 [14:08:12<1:45:18,  2.79s/batch]Batch 17700/20010 Done, mean position loss: 21.61961548566818\n",
      "Training growing_up:  87%|██████▉ | 17438/20010 [14:08:17<2:09:36,  3.02s/batch]Batch 17800/20010 Done, mean position loss: 21.97440702676773\n",
      "Training growing_up:  89%|███████▏| 17844/20010 [14:08:20<1:41:53,  2.82s/batch]Batch 17800/20010 Done, mean position loss: 22.531474735736847\n",
      "Training growing_up:  88%|███████ | 17509/20010 [14:08:33<2:03:08,  2.95s/batch]Batch 17900/20010 Done, mean position loss: 21.74262682914734\n",
      "Training growing_up:  87%|██████▉ | 17447/20010 [14:08:43<2:07:29,  2.98s/batch]Batch 17400/20010 Done, mean position loss: 22.3394620513916\n",
      "Training growing_up:  88%|███████ | 17619/20010 [14:08:50<2:18:31,  3.48s/batch]Batch 17600/20010 Done, mean position loss: 22.154001524448393\n",
      "Training growing_up:  90%|███████▏| 18039/20010 [14:09:01<1:35:58,  2.92s/batch]Batch 17800/20010 Done, mean position loss: 22.265781526565554\n",
      "Training growing_up:  89%|███████ | 17749/20010 [14:09:07<1:59:38,  3.18s/batch]Batch 17800/20010 Done, mean position loss: 21.574830760955813\n",
      "Training growing_up:  89%|███████ | 17760/20010 [14:09:15<1:56:40,  3.11s/batch]Batch 17900/20010 Done, mean position loss: 22.474860818386077\n",
      "Training growing_up:  89%|███████▏| 17825/20010 [14:09:29<1:40:54,  2.77s/batch]Batch 17400/20010 Done, mean position loss: 22.177508227825165\n",
      "Training growing_up:  90%|███████▏| 17916/20010 [14:09:58<1:37:50,  2.80s/batch]Batch 17700/20010 Done, mean position loss: 22.63587944984436\n",
      "Training growing_up:  87%|██████▉ | 17462/20010 [14:09:57<2:05:24,  2.95s/batch]Batch 17900/20010 Done, mean position loss: 22.00818727731705\n",
      "Training growing_up:  88%|███████ | 17587/20010 [14:10:03<2:01:35,  3.01s/batch]Batch 17700/20010 Done, mean position loss: 22.069340085983278\n",
      "Training growing_up:  87%|██████▉ | 17476/20010 [14:10:11<2:04:53,  2.96s/batch]Batch 17800/20010 Done, mean position loss: 22.02495156288147\n",
      "Training growing_up:  89%|███████ | 17773/20010 [14:10:17<1:41:21,  2.72s/batch]Batch 17800/20010 Done, mean position loss: 22.37237114429474\n",
      "Training growing_up:  90%|███████▏| 17939/20010 [14:10:31<1:44:49,  3.04s/batch]Batch 17800/20010 Done, mean position loss: 21.712418348789214\n",
      "Training growing_up:  89%|███████ | 17778/20010 [14:10:47<1:47:04,  2.88s/batch]Batch 17800/20010 Done, mean position loss: 21.987460026741026\n",
      "Training growing_up:  89%|███████▏| 17867/20010 [14:10:48<1:50:01,  3.08s/batch]Batch 17500/20010 Done, mean position loss: 22.474393899440763\n",
      "Training growing_up:  89%|███████▏| 17894/20010 [14:10:49<1:38:27,  2.79s/batch]Batch 17600/20010 Done, mean position loss: 21.6138645529747\n",
      "Training growing_up:  87%|██████▉ | 17393/20010 [14:11:09<2:12:46,  3.04s/batch]Batch 17800/20010 Done, mean position loss: 21.82946814537048\n",
      "Training growing_up:  88%|███████ | 17561/20010 [14:11:10<1:56:39,  2.86s/batch]Batch 17900/20010 Done, mean position loss: 21.460322315692903\n",
      "Training growing_up:  90%|███████▏| 17985/20010 [14:11:14<1:34:28,  2.80s/batch]Batch 17800/20010 Done, mean position loss: 22.70240842342377\n",
      "Training growing_up:  88%|███████ | 17651/20010 [14:11:25<2:08:00,  3.26s/batch]Batch 17700/20010 Done, mean position loss: 21.97041226387024\n",
      "Training growing_up:  90%|███████▏| 18088/20010 [14:11:25<1:28:39,  2.77s/batch]Batch 17700/20010 Done, mean position loss: 22.530422039031983\n",
      "Training growing_up:  89%|███████ | 17729/20010 [14:11:28<2:18:59,  3.66s/batch]Batch 17500/20010 Done, mean position loss: 21.705829999446866\n",
      "Training growing_up:  88%|███████ | 17652/20010 [14:11:28<2:07:10,  3.24s/batch]Batch 17700/20010 Done, mean position loss: 22.387305080890656\n",
      "Training growing_up:  89%|███████▏| 17828/20010 [14:11:34<1:53:48,  3.13s/batch]Batch 17400/20010 Done, mean position loss: 21.526291129589083\n",
      "Training growing_up:  88%|███████ | 17699/20010 [14:11:37<1:52:33,  2.92s/batch]Batch 17800/20010 Done, mean position loss: 22.51226640224457\n",
      "Training growing_up:  89%|███████▏| 17868/20010 [14:11:40<2:00:59,  3.39s/batch]Batch 17700/20010 Done, mean position loss: 22.929873647689817\n",
      "Training growing_up:  88%|███████ | 17707/20010 [14:11:42<1:55:07,  3.00s/batch]Batch 17700/20010 Done, mean position loss: 21.883049476146695\n",
      "Training growing_up:  90%|███████▏| 17964/20010 [14:11:43<1:30:21,  2.65s/batch]Batch 17800/20010 Done, mean position loss: 21.75960530281067\n",
      "Training growing_up:  89%|███████ | 17710/20010 [14:11:54<1:54:37,  2.99s/batch]Batch 17500/20010 Done, mean position loss: 21.330983257293703\n",
      "Training growing_up:  89%|███████ | 17817/20010 [14:11:55<1:48:45,  2.98s/batch]Batch 17800/20010 Done, mean position loss: 22.4972128534317\n",
      "Training growing_up:  87%|██████▉ | 17503/20010 [14:12:00<2:10:17,  3.12s/batch]Batch 18000/20010 Done, mean position loss: 22.205697860717773\n",
      "Training growing_up:  89%|███████▏| 17861/20010 [14:12:04<1:42:17,  2.86s/batch]Batch 18100/20010 Done, mean position loss: 21.639481356143953\n",
      "Training growing_up:  88%|███████ | 17695/20010 [14:12:30<1:55:26,  2.99s/batch]Batch 17900/20010 Done, mean position loss: 21.705306527614592\n",
      "Training growing_up:  89%|███████ | 17756/20010 [14:12:48<2:06:06,  3.36s/batch]Batch 17700/20010 Done, mean position loss: 22.805143425464628\n",
      "Training growing_up:  91%|███████▏| 18117/20010 [14:12:52<1:31:08,  2.89s/batch]Batch 17700/20010 Done, mean position loss: 22.01633474826813\n",
      "Training growing_up:  89%|███████ | 17732/20010 [14:12:57<1:52:05,  2.95s/batch]Batch 17700/20010 Done, mean position loss: 22.39581007003784\n",
      "Training growing_up:  89%|███████ | 17800/20010 [14:13:11<1:54:19,  3.10s/batch]Batch 17600/20010 Done, mean position loss: 22.278324058055876\n",
      "Training growing_up:  89%|███████▏| 17826/20010 [14:13:14<1:54:47,  3.15s/batch]Batch 17800/20010 Done, mean position loss: 21.652572042942047\n",
      "Training growing_up:  88%|███████ | 17529/20010 [14:13:18<2:09:56,  3.14s/batch]Batch 17900/20010 Done, mean position loss: 22.183415579795838\n",
      "Training growing_up:  89%|███████ | 17805/20010 [14:13:24<1:38:22,  2.68s/batch]Batch 17900/20010 Done, mean position loss: 22.553489334583283\n",
      "Training growing_up:  91%|███████▏| 18132/20010 [14:13:36<1:31:56,  2.94s/batch]Batch 18000/20010 Done, mean position loss: 21.70342693090439\n",
      "Training growing_up:  88%|███████ | 17662/20010 [14:13:53<1:52:10,  2.87s/batch]Batch 17500/20010 Done, mean position loss: 22.360461466312408\n",
      "Training growing_up:  89%|███████ | 17751/20010 [14:13:54<2:00:03,  3.19s/batch]Batch 17900/20010 Done, mean position loss: 22.31258895635605\n",
      "Training growing_up:  89%|███████▏| 17877/20010 [14:13:58<1:45:19,  2.96s/batch]Batch 17700/20010 Done, mean position loss: 22.225214631557467\n",
      "Training growing_up:  88%|███████ | 17568/20010 [14:14:05<1:58:25,  2.91s/batch]Batch 17900/20010 Done, mean position loss: 21.705369720458982\n",
      "Training growing_up:  90%|███████▏| 18043/20010 [14:14:11<1:32:34,  2.82s/batch]Batch 18000/20010 Done, mean position loss: 22.56929556131363\n",
      "Training growing_up:  89%|███████▏| 17860/20010 [14:14:36<1:56:05,  3.24s/batch]Batch 17500/20010 Done, mean position loss: 22.045340559482575\n",
      "Training growing_up:  89%|███████ | 17723/20010 [14:14:55<1:31:37,  2.40s/batch]Batch 18000/20010 Done, mean position loss: 22.015828528404235\n",
      "Training growing_up:  90%|███████▏| 17932/20010 [14:14:57<1:38:34,  2.85s/batch]Batch 17800/20010 Done, mean position loss: 22.890586359500887\n",
      "Training growing_up:  89%|███████▏| 17892/20010 [14:15:06<1:36:18,  2.73s/batch]Batch 17800/20010 Done, mean position loss: 22.10658930063248\n",
      "Training growing_up:  90%|███████▏| 17981/20010 [14:15:08<1:50:16,  3.26s/batch]Batch 17900/20010 Done, mean position loss: 22.063128683567044\n",
      "Training growing_up:  89%|███████▏| 17897/20010 [14:15:20<1:38:14,  2.79s/batch]Batch 17900/20010 Done, mean position loss: 22.361494183540344\n",
      "Training growing_up:  91%|███████▎| 18173/20010 [14:15:29<1:22:34,  2.70s/batch]Batch 17900/20010 Done, mean position loss: 21.97399475336075\n",
      "Training growing_up:  89%|███████▏| 17891/20010 [14:15:37<1:33:51,  2.66s/batch]Batch 17900/20010 Done, mean position loss: 21.85461101055145\n",
      "Training growing_up:  89%|███████▏| 17908/20010 [14:15:39<1:38:22,  2.81s/batch]Batch 17600/20010 Done, mean position loss: 22.411408495903018\n",
      "Training growing_up:  87%|██████▉ | 17483/20010 [14:15:49<2:14:11,  3.19s/batch]Batch 17700/20010 Done, mean position loss: 21.502659533023834\n",
      "Training growing_up:  90%|███████▏| 17972/20010 [14:16:01<1:32:42,  2.73s/batch]Batch 18000/20010 Done, mean position loss: 21.44365730047226\n",
      "Training growing_up:  89%|███████ | 17791/20010 [14:16:06<1:35:26,  2.58s/batch]Batch 17900/20010 Done, mean position loss: 21.794942977428438\n",
      "Training growing_up:  91%|███████▎| 18187/20010 [14:16:07<1:27:57,  2.90s/batch]Batch 17900/20010 Done, mean position loss: 22.659365797042845\n",
      "Training growing_up:  89%|███████ | 17800/20010 [14:16:14<1:49:48,  2.98s/batch]Batch 17800/20010 Done, mean position loss: 21.882373461723326\n",
      "Training growing_up:  89%|███████▏| 17829/20010 [14:16:16<1:40:36,  2.77s/batch]Batch 17800/20010 Done, mean position loss: 22.324041571617126\n",
      "Training growing_up:  89%|███████ | 17801/20010 [14:16:16<1:38:48,  2.68s/batch]Batch 17800/20010 Done, mean position loss: 22.413313059806825\n",
      "Training growing_up:  90%|███████▏| 18047/20010 [14:16:20<1:24:21,  2.58s/batch]Batch 17600/20010 Done, mean position loss: 21.849330980777744\n",
      "Training growing_up:  90%|███████▏| 17969/20010 [14:16:30<1:52:54,  3.32s/batch]Batch 17900/20010 Done, mean position loss: 22.353536624908447\n",
      "Training growing_up:  90%|███████▏| 17927/20010 [14:16:33<1:41:54,  2.94s/batch]Batch 17800/20010 Done, mean position loss: 22.768659353256226\n",
      "Training growing_up:  89%|███████▏| 17902/20010 [14:16:33<1:48:14,  3.08s/batch]Batch 17800/20010 Done, mean position loss: 21.905956137180326\n",
      "Training growing_up:  90%|███████▏| 17931/20010 [14:16:34<1:44:09,  3.01s/batch]Batch 17900/20010 Done, mean position loss: 21.808978035449982\n",
      "Training growing_up:  90%|███████▏| 18067/20010 [14:16:40<1:31:47,  2.83s/batch]Batch 17500/20010 Done, mean position loss: 21.611785440444944\n",
      "Training growing_up:  88%|███████ | 17624/20010 [14:16:45<1:45:37,  2.66s/batch]Batch 17900/20010 Done, mean position loss: 22.60366369962692\n",
      "Training growing_up:  89%|███████▏| 17840/20010 [14:16:47<1:37:01,  2.68s/batch]Batch 18200/20010 Done, mean position loss: 21.32491678953171\n",
      "Training growing_up:  88%|███████ | 17610/20010 [14:16:47<1:53:16,  2.83s/batch]Batch 17600/20010 Done, mean position loss: 21.35476706266403\n",
      "Training growing_up:  89%|███████▏| 17843/20010 [14:16:54<1:31:57,  2.55s/batch]Batch 18100/20010 Done, mean position loss: 22.114938085079196\n",
      "Training growing_up:  91%|███████▏| 18111/20010 [14:17:21<1:26:14,  2.72s/batch]Batch 18000/20010 Done, mean position loss: 21.465133316516877\n",
      "Training growing_up:  88%|███████ | 17578/20010 [14:17:40<2:03:50,  3.06s/batch]Batch 17800/20010 Done, mean position loss: 22.780157759189606\n",
      "Training growing_up:  88%|███████ | 17631/20010 [14:17:45<1:44:40,  2.64s/batch]Batch 17800/20010 Done, mean position loss: 21.965410859584807\n",
      "Training growing_up:  90%|███████▏| 17983/20010 [14:17:48<1:43:15,  3.06s/batch]Batch 17800/20010 Done, mean position loss: 22.597933087348935\n",
      "Training growing_up:  90%|███████▏| 17937/20010 [14:17:52<1:32:33,  2.68s/batch]Batch 17700/20010 Done, mean position loss: 22.29082484483719\n",
      "Training growing_up:  88%|███████ | 17626/20010 [14:18:01<1:56:45,  2.94s/batch]Batch 18000/20010 Done, mean position loss: 22.047589194774627\n",
      "Training growing_up:  90%|███████▏| 18016/20010 [14:18:03<1:35:48,  2.88s/batch]Batch 17900/20010 Done, mean position loss: 21.948959827423096\n",
      "Training growing_up:  89%|███████▏| 17839/20010 [14:18:05<1:49:53,  3.04s/batch]Batch 18000/20010 Done, mean position loss: 22.68127872943878\n",
      "Training growing_up:  90%|███████▏| 17937/20010 [14:18:15<1:38:32,  2.85s/batch]Batch 18100/20010 Done, mean position loss: 21.781172845363617\n",
      "Training growing_up:  89%|███████▏| 17854/20010 [14:18:40<1:50:33,  3.08s/batch]Batch 18000/20010 Done, mean position loss: 22.520147962570192\n",
      "Batch 17800/20010 Done, mean position loss: 22.206938843727112\n",
      "Training growing_up:  90%|███████▏| 17956/20010 [14:18:45<1:49:30,  3.20s/batch]Batch 18000/20010 Done, mean position loss: 21.808237197399137\n",
      "Training growing_up:  89%|███████▏| 17848/20010 [14:18:47<1:46:52,  2.97s/batch]Batch 17600/20010 Done, mean position loss: 22.190921978950502\n",
      "Training growing_up:  90%|███████▏| 17973/20010 [14:18:59<1:47:21,  3.16s/batch]Batch 18100/20010 Done, mean position loss: 22.365034673213962\n",
      "Training growing_up:  90%|███████▏| 17955/20010 [14:19:19<1:45:25,  3.08s/batch]Batch 17600/20010 Done, mean position loss: 22.23359563112259\n",
      "Training growing_up:  89%|███████▏| 17873/20010 [14:19:36<1:42:48,  2.89s/batch]Batch 18100/20010 Done, mean position loss: 21.856618723869325\n",
      "Training growing_up:  89%|███████▏| 17841/20010 [14:19:38<1:50:01,  3.04s/batch]Batch 17900/20010 Done, mean position loss: 22.82187205791473\n",
      "Training growing_up:  89%|███████▏| 17900/20010 [14:19:53<1:47:54,  3.07s/batch]Batch 18000/20010 Done, mean position loss: 21.943803317546845\n",
      "Training growing_up:  88%|███████ | 17674/20010 [14:19:56<2:07:06,  3.26s/batch]Batch 17900/20010 Done, mean position loss: 22.23541166305542\n",
      "Training growing_up:  90%|███████▏| 18029/20010 [14:20:10<1:30:07,  2.73s/batch]Batch 18000/20010 Done, mean position loss: 22.316083860397338\n",
      "Training growing_up:  90%|███████▏| 18035/20010 [14:20:22<1:36:11,  2.92s/batch]Batch 18000/20010 Done, mean position loss: 21.76744971752167\n",
      "Training growing_up:  88%|███████ | 17632/20010 [14:20:24<2:05:49,  3.17s/batch]Batch 18000/20010 Done, mean position loss: 22.048542890548703\n",
      "Training growing_up:  91%|███████▏| 18132/20010 [14:20:32<1:37:46,  3.12s/batch]Batch 17700/20010 Done, mean position loss: 22.645876586437225\n",
      "Training growing_up:  89%|███████▏| 17894/20010 [14:20:46<1:44:28,  2.96s/batch]Batch 17800/20010 Done, mean position loss: 21.666389741897582\n",
      "Training growing_up:  90%|███████▏| 18013/20010 [14:20:46<1:45:46,  3.18s/batch]Batch 18100/20010 Done, mean position loss: 21.52277102231979\n",
      "Training growing_up:  90%|███████▏| 17991/20010 [14:21:01<1:43:34,  3.08s/batch]Batch 18000/20010 Done, mean position loss: 21.767470111846926\n",
      "Training growing_up:  90%|███████▏| 17994/20010 [14:21:04<1:38:09,  2.92s/batch]Batch 18000/20010 Done, mean position loss: 22.390104565620423\n",
      "Training growing_up:  90%|███████▏| 17923/20010 [14:21:04<1:55:02,  3.31s/batch]Batch 17900/20010 Done, mean position loss: 21.791650192737578\n",
      "Training growing_up:  91%|███████▎| 18144/20010 [14:21:09<1:38:37,  3.17s/batch]Batch 17900/20010 Done, mean position loss: 22.3726259803772\n",
      "Training growing_up:  90%|███████▏| 18066/20010 [14:21:12<1:29:36,  2.77s/batch]Batch 17900/20010 Done, mean position loss: 22.27776324272156\n",
      "Training growing_up:  89%|███████▏| 17908/20010 [14:21:24<1:39:18,  2.83s/batch]Batch 18000/20010 Done, mean position loss: 22.33850327730179\n",
      "Batch 17700/20010 Done, mean position loss: 21.553948197364807\n",
      "Training growing_up:  91%|███████▎| 18194/20010 [14:21:26<1:36:34,  3.19s/batch]Batch 17900/20010 Done, mean position loss: 22.68214354753494\n",
      "Training growing_up:  89%|███████▏| 17906/20010 [14:21:30<2:04:58,  3.56s/batch]Batch 17900/20010 Done, mean position loss: 21.820260314941407\n",
      "Training growing_up:  90%|███████▏| 18086/20010 [14:21:32<1:37:35,  3.04s/batch]Batch 18000/20010 Done, mean position loss: 21.91626364946365\n",
      "Training growing_up:  89%|███████▏| 17904/20010 [14:21:38<1:45:22,  3.00s/batch]Batch 18300/20010 Done, mean position loss: 21.455577607154847\n",
      "Training growing_up:  90%|███████▏| 17910/20010 [14:21:41<2:15:22,  3.87s/batch]Batch 18000/20010 Done, mean position loss: 22.70345331430435\n",
      "Training growing_up:  91%|███████▎| 18154/20010 [14:21:44<1:53:50,  3.68s/batch]Batch 17600/20010 Done, mean position loss: 21.63394390106201\n",
      "Training growing_up:  91%|███████▎| 18304/20010 [14:21:48<1:36:10,  3.38s/batch]Batch 18200/20010 Done, mean position loss: 22.105605850219725\n",
      "Training growing_up:  90%|███████▏| 17909/20010 [14:21:53<1:39:33,  2.84s/batch]Batch 17700/20010 Done, mean position loss: 21.329916722774506\n",
      "Training growing_up:  88%|███████ | 17659/20010 [14:22:20<2:17:12,  3.50s/batch]Batch 18100/20010 Done, mean position loss: 21.600777826309205\n",
      "Training growing_up:  90%|███████▏| 18034/20010 [14:22:44<1:40:05,  3.04s/batch]Batch 17900/20010 Done, mean position loss: 22.070013802051545\n",
      "Training growing_up:  89%|███████ | 17727/20010 [14:22:47<1:52:35,  2.96s/batch]Batch 17900/20010 Done, mean position loss: 22.646301217079163\n",
      "Training growing_up:  91%|███████▎| 18164/20010 [14:22:49<1:41:33,  3.30s/batch]Batch 17800/20010 Done, mean position loss: 22.38008707284927\n",
      "Training growing_up:  90%|███████▏| 17993/20010 [14:22:50<1:43:30,  3.08s/batch]Batch 17900/20010 Done, mean position loss: 22.32094722509384\n",
      "Training growing_up:  88%|███████ | 17680/20010 [14:22:59<1:58:38,  3.05s/batch]Batch 18100/20010 Done, mean position loss: 22.10869929552078\n",
      "Training growing_up:  90%|███████▏| 17931/20010 [14:23:01<1:48:03,  3.12s/batch]Batch 18100/20010 Done, mean position loss: 22.60534171104431\n",
      "Training growing_up:  90%|███████▏| 17972/20010 [14:23:13<1:47:04,  3.15s/batch]Batch 18000/20010 Done, mean position loss: 21.662022042274476\n",
      "Training growing_up:  90%|███████▏| 18091/20010 [14:23:21<1:33:54,  2.94s/batch]Batch 18200/20010 Done, mean position loss: 21.77089480638504\n",
      "Training growing_up:  90%|███████▏| 17953/20010 [14:23:45<1:37:22,  2.84s/batch]Batch 17900/20010 Done, mean position loss: 22.16753242492676\n",
      "Training growing_up:  90%|███████▏| 18100/20010 [14:23:48<1:40:21,  3.15s/batch]Batch 18100/20010 Done, mean position loss: 22.419228727817536\n",
      "Training growing_up:  90%|███████▏| 18047/20010 [14:23:52<1:45:25,  3.22s/batch]Batch 18100/20010 Done, mean position loss: 21.903971753120423\n",
      "Training growing_up:  91%|███████▎| 18245/20010 [14:24:03<1:20:49,  2.75s/batch]Batch 18200/20010 Done, mean position loss: 22.695873353481293\n",
      "Training growing_up:  89%|███████▏| 17863/20010 [14:24:06<2:01:41,  3.40s/batch]Batch 17700/20010 Done, mean position loss: 22.17883903980255\n",
      "Training growing_up:  90%|███████▏| 18061/20010 [14:24:28<1:32:34,  2.85s/batch]Batch 17700/20010 Done, mean position loss: 22.257410464286806\n",
      "Training growing_up:  90%|███████▏| 18063/20010 [14:24:39<1:30:03,  2.78s/batch]Batch 18000/20010 Done, mean position loss: 22.71525187253952\n",
      "Training growing_up:  90%|███████▏| 17937/20010 [14:24:39<1:46:39,  3.09s/batch]Batch 18200/20010 Done, mean position loss: 22.042244420051574\n",
      "Training growing_up:  89%|███████ | 17788/20010 [14:25:00<2:04:47,  3.37s/batch]Batch 18100/20010 Done, mean position loss: 22.01992967128754\n",
      "Training growing_up:  92%|███████▎| 18370/20010 [14:25:05<1:23:02,  3.04s/batch]Batch 18000/20010 Done, mean position loss: 22.37495489358902\n",
      "Training growing_up:  89%|███████ | 17768/20010 [14:25:20<1:44:07,  2.79s/batch]Batch 18100/20010 Done, mean position loss: 22.61346968412399\n",
      "Training growing_up:  91%|███████▎| 18165/20010 [14:25:25<1:30:45,  2.95s/batch]Batch 18100/20010 Done, mean position loss: 21.61486674308777\n",
      "Training growing_up:  90%|███████▏| 17956/20010 [14:25:33<1:51:49,  3.27s/batch]Batch 18100/20010 Done, mean position loss: 22.322504932880403\n",
      "Training growing_up:  90%|███████▏| 18012/20010 [14:25:36<1:39:13,  2.98s/batch]Batch 17800/20010 Done, mean position loss: 22.928454535007475\n",
      "Training growing_up:  90%|███████▏| 18108/20010 [14:25:54<1:32:01,  2.90s/batch]Batch 18200/20010 Done, mean position loss: 21.761977331638334\n",
      "Training growing_up:  91%|███████▎| 18254/20010 [14:26:04<1:28:54,  3.04s/batch]Batch 17900/20010 Done, mean position loss: 21.318013179302213\n",
      "Training growing_up:  90%|███████▏| 18092/20010 [14:26:06<1:34:28,  2.96s/batch]Batch 18100/20010 Done, mean position loss: 21.8442947101593\n",
      "Training growing_up:  91%|███████▎| 18231/20010 [14:26:09<1:47:42,  3.63s/batch]Batch 18100/20010 Done, mean position loss: 22.397123188972472\n",
      "Training growing_up:  90%|███████▏| 17992/20010 [14:26:13<1:42:15,  3.04s/batch]Batch 18000/20010 Done, mean position loss: 22.058754234313966\n",
      "Training growing_up:  91%|███████▏| 18115/20010 [14:26:20<1:49:42,  3.47s/batch]Batch 18000/20010 Done, mean position loss: 22.511813910007476\n",
      "Training growing_up:  89%|███████ | 17815/20010 [14:26:21<2:01:24,  3.32s/batch]Batch 18000/20010 Done, mean position loss: 22.362406189441682\n",
      "Training growing_up:  88%|███████ | 17692/20010 [14:26:28<2:05:15,  3.24s/batch]Batch 18100/20010 Done, mean position loss: 22.366681628227234\n",
      "Training growing_up:  89%|███████▏| 17875/20010 [14:26:35<1:53:10,  3.18s/batch]Batch 18100/20010 Done, mean position loss: 21.901727581024172\n",
      "Training growing_up:  91%|███████▎| 18157/20010 [14:26:41<1:39:49,  3.23s/batch]Batch 18100/20010 Done, mean position loss: 22.766121661663057\n",
      "Training growing_up:  91%|███████▎| 18155/20010 [14:26:41<1:31:07,  2.95s/batch]Batch 18000/20010 Done, mean position loss: 21.79690933227539\n",
      "Training growing_up:  91%|███████▎| 18189/20010 [14:26:42<1:27:48,  2.89s/batch]Batch 17800/20010 Done, mean position loss: 21.75317092895508\n",
      "Training growing_up:  89%|███████▏| 17822/20010 [14:26:43<1:49:50,  3.01s/batch]Batch 18000/20010 Done, mean position loss: 22.73233195066452\n",
      "Training growing_up:  91%|███████▎| 18217/20010 [14:26:45<1:35:56,  3.21s/batch]Batch 18400/20010 Done, mean position loss: 21.448472096920014\n",
      "Training growing_up:  89%|███████ | 17753/20010 [14:26:55<2:03:23,  3.28s/batch]Batch 17700/20010 Done, mean position loss: 21.556637420654297\n",
      "Training growing_up:  91%|███████▎| 18259/20010 [14:26:58<1:27:37,  3.00s/batch]Batch 18300/20010 Done, mean position loss: 21.955830965042114\n",
      "Training growing_up:  91%|███████▎| 18142/20010 [14:27:07<1:26:14,  2.77s/batch]Batch 17800/20010 Done, mean position loss: 21.41030455827713\n",
      "Training growing_up:  90%|███████▏| 18045/20010 [14:27:17<1:29:29,  2.73s/batch]Batch 18200/20010 Done, mean position loss: 21.40811853170395\n",
      "Training growing_up:  90%|███████▏| 18000/20010 [14:27:52<1:56:04,  3.46s/batch]Batch 18000/20010 Done, mean position loss: 22.003557543754575\n",
      "Training growing_up:  92%|███████▎| 18424/20010 [14:27:56<1:20:28,  3.04s/batch]Batch 18000/20010 Done, mean position loss: 22.059350771903993\n",
      "Training growing_up:  91%|███████▏| 18129/20010 [14:27:59<1:34:49,  3.02s/batch]Batch 17900/20010 Done, mean position loss: 22.07256096363068\n",
      "Training growing_up:  90%|███████▏| 18034/20010 [14:27:59<1:39:05,  3.01s/batch]Batch 18200/20010 Done, mean position loss: 22.581872069835665\n",
      "Training growing_up:  91%|███████▎| 18139/20010 [14:28:00<1:34:09,  3.02s/batch]Batch 18000/20010 Done, mean position loss: 22.724970433712006\n",
      "Training growing_up:  90%|███████▏| 18027/20010 [14:28:03<1:46:33,  3.22s/batch]Batch 18200/20010 Done, mean position loss: 22.034518308639527\n",
      "Training growing_up:  90%|███████▏| 18077/20010 [14:28:22<1:45:16,  3.27s/batch]Batch 18100/20010 Done, mean position loss: 21.656406700611115\n",
      "Training growing_up:  91%|███████▎| 18161/20010 [14:28:27<1:31:54,  2.98s/batch]Batch 18300/20010 Done, mean position loss: 21.671956894397738\n",
      "Training growing_up:  91%|███████▎| 18153/20010 [14:28:52<1:32:22,  2.98s/batch]Batch 18200/20010 Done, mean position loss: 22.337091174125668\n",
      "Training growing_up:  91%|███████▎| 18219/20010 [14:28:52<1:29:42,  3.01s/batch]Batch 18000/20010 Done, mean position loss: 22.341077826023103\n",
      "Training growing_up:  91%|███████▎| 18285/20010 [14:28:57<1:25:08,  2.96s/batch]Batch 18200/20010 Done, mean position loss: 21.649326293468476\n",
      "Training growing_up:  89%|███████▏| 17869/20010 [14:29:06<1:39:15,  2.78s/batch]Batch 18300/20010 Done, mean position loss: 22.59084095239639\n",
      "Training growing_up:  91%|███████▎| 18165/20010 [14:29:19<1:29:33,  2.91s/batch]Batch 17800/20010 Done, mean position loss: 22.12474472999573\n",
      "Training growing_up:  91%|███████▎| 18297/20010 [14:29:27<1:15:26,  2.64s/batch]Batch 18100/20010 Done, mean position loss: 22.62748593568802\n",
      "Training growing_up:  90%|███████▏| 18039/20010 [14:29:37<1:20:01,  2.44s/batch]Batch 17800/20010 Done, mean position loss: 22.179431762695312\n",
      "Batch 18300/20010 Done, mean position loss: 21.809760801792144\n",
      "Training growing_up:  91%|███████▎| 18172/20010 [14:29:53<1:26:46,  2.83s/batch]Batch 18100/20010 Done, mean position loss: 22.17501171350479\n",
      "Training growing_up:  91%|███████▎| 18193/20010 [14:29:54<1:30:57,  3.00s/batch]Batch 18200/20010 Done, mean position loss: 21.856496968269347\n",
      "Training growing_up:  92%|███████▎| 18340/20010 [14:30:18<1:25:13,  3.06s/batch]Batch 18200/20010 Done, mean position loss: 21.60312024354935\n",
      "Training growing_up:  91%|███████▎| 18263/20010 [14:30:18<1:26:51,  2.98s/batch]Batch 18200/20010 Done, mean position loss: 22.334375088214873\n",
      "Training growing_up:  91%|███████▎| 18189/20010 [14:30:30<1:30:26,  2.98s/batch]Batch 18200/20010 Done, mean position loss: 21.930897171497342\n",
      "Training growing_up:  91%|███████▎| 18207/20010 [14:30:36<1:27:05,  2.90s/batch]Batch 17900/20010 Done, mean position loss: 22.684456861019136\n",
      "Training growing_up:  91%|███████▎| 18239/20010 [14:30:52<1:33:24,  3.16s/batch]Batch 18300/20010 Done, mean position loss: 21.408554894924166\n",
      "Training growing_up:  90%|███████▏| 18062/20010 [14:31:04<1:43:57,  3.20s/batch]Batch 18100/20010 Done, mean position loss: 21.860353636741635\n",
      "Training growing_up:  91%|███████▎| 18136/20010 [14:31:06<1:31:01,  2.91s/batch]Batch 18000/20010 Done, mean position loss: 21.50674968957901\n",
      "Training growing_up:  89%|███████▏| 17831/20010 [14:31:08<1:45:11,  2.90s/batch]Batch 18200/20010 Done, mean position loss: 21.61756560564041\n",
      "Training growing_up:  91%|███████▎| 18245/20010 [14:31:09<1:23:39,  2.84s/batch]Batch 18200/20010 Done, mean position loss: 22.375918099880217\n",
      "Training growing_up:  92%|███████▎| 18336/20010 [14:31:21<1:21:50,  2.93s/batch]Batch 18200/20010 Done, mean position loss: 22.387678697109223\n",
      "Training growing_up:  92%|███████▎| 18362/20010 [14:31:22<1:21:02,  2.95s/batch]Batch 18100/20010 Done, mean position loss: 22.375336813926694\n",
      "Training growing_up:  90%|███████▏| 18074/20010 [14:31:23<1:41:06,  3.13s/batch]Batch 18100/20010 Done, mean position loss: 22.451645460128784\n",
      "Training growing_up:  89%|███████▏| 17897/20010 [14:31:28<1:49:46,  3.12s/batch]Batch 18200/20010 Done, mean position loss: 22.448992137908935\n",
      "Training growing_up:  91%|███████▎| 18209/20010 [14:31:30<1:26:01,  2.87s/batch]Batch 18200/20010 Done, mean position loss: 21.772500751018523\n",
      "Training growing_up:  91%|███████▎| 18205/20010 [14:31:32<1:25:52,  2.85s/batch]Batch 18100/20010 Done, mean position loss: 21.916968488693236\n",
      "Training growing_up:  91%|███████▎| 18204/20010 [14:31:37<1:29:15,  2.97s/batch]Batch 18500/20010 Done, mean position loss: 21.236649742126467\n",
      "Training growing_up:  91%|███████▎| 18292/20010 [14:31:40<1:23:26,  2.91s/batch]Batch 18100/20010 Done, mean position loss: 22.48951322078705\n",
      "Training growing_up:  90%|███████▏| 17924/20010 [14:31:42<1:35:52,  2.76s/batch]Batch 17900/20010 Done, mean position loss: 21.622151980400083\n",
      "Training growing_up:  91%|███████▎| 18231/20010 [14:31:46<1:27:15,  2.94s/batch]Batch 18400/20010 Done, mean position loss: 22.164668152332304\n",
      "Training growing_up:  90%|███████▏| 18082/20010 [14:31:52<1:37:28,  3.03s/batch]Batch 17800/20010 Done, mean position loss: 21.62005402803421\n",
      "Training growing_up:  92%|███████▍| 18509/20010 [14:31:59<1:09:16,  2.77s/batch]Batch 17900/20010 Done, mean position loss: 21.378269410133363\n",
      "Training growing_up:  91%|███████▎| 18269/20010 [14:32:09<1:15:33,  2.60s/batch]Batch 18300/20010 Done, mean position loss: 21.331125950813295\n",
      "Training growing_up:  89%|███████▏| 17862/20010 [14:32:42<1:43:55,  2.90s/batch]Batch 18100/20010 Done, mean position loss: 22.113452813625337\n",
      "Training growing_up:  91%|███████▏| 18130/20010 [14:32:47<1:39:41,  3.18s/batch]Batch 18300/20010 Done, mean position loss: 22.52247152328491\n",
      "Training growing_up:  91%|███████▎| 18253/20010 [14:32:51<1:20:29,  2.75s/batch]Batch 18300/20010 Done, mean position loss: 22.360429878234864\n",
      "Training growing_up:  90%|███████▏| 18100/20010 [14:32:51<1:30:51,  2.85s/batch]Batch 18100/20010 Done, mean position loss: 22.388319039344786\n",
      "Training growing_up:  91%|███████▎| 18235/20010 [14:32:55<1:35:19,  3.22s/batch]Batch 18100/20010 Done, mean position loss: 22.68764083147049\n",
      "Training growing_up:  91%|███████▏| 18134/20010 [14:32:58<1:29:37,  2.87s/batch]Batch 18000/20010 Done, mean position loss: 22.04261676311493\n",
      "Training growing_up:  91%|███████▎| 18269/20010 [14:33:17<1:24:49,  2.92s/batch]Batch 18400/20010 Done, mean position loss: 21.597642056941986\n",
      "Training growing_up:  91%|███████▎| 18238/20010 [14:33:21<1:24:25,  2.86s/batch]Batch 18200/20010 Done, mean position loss: 21.70220786333084\n",
      "Training growing_up:  90%|███████▏| 18054/20010 [14:33:43<1:30:58,  2.79s/batch]Batch 18300/20010 Done, mean position loss: 22.309818367958066\n",
      "Training growing_up:  92%|███████▎| 18412/20010 [14:33:48<1:12:32,  2.72s/batch]Batch 18100/20010 Done, mean position loss: 22.193951127529147\n",
      "Training growing_up:  91%|███████▎| 18248/20010 [14:33:51<1:30:34,  3.08s/batch]Batch 18400/20010 Done, mean position loss: 22.540520622730256\n",
      "Training growing_up:  91%|███████▎| 18274/20010 [14:33:51<1:22:34,  2.85s/batch]Batch 18300/20010 Done, mean position loss: 21.647283263206482\n",
      "Training growing_up:  91%|███████▎| 18160/20010 [14:34:15<1:29:40,  2.91s/batch]Batch 18200/20010 Done, mean position loss: 22.738491287231447\n",
      "Training growing_up:  92%|███████▎| 18372/20010 [14:34:21<1:16:50,  2.81s/batch]Batch 17900/20010 Done, mean position loss: 22.457406198978425\n",
      "Training growing_up:  92%|███████▎| 18336/20010 [14:34:28<1:19:46,  2.86s/batch]Batch 18400/20010 Done, mean position loss: 22.006980023384095\n",
      "Training growing_up:  90%|███████▏| 17962/20010 [14:34:37<1:40:24,  2.94s/batch]Batch 17900/20010 Done, mean position loss: 22.21707698583603\n",
      "Training growing_up:  90%|███████▏| 18038/20010 [14:34:49<1:38:03,  2.98s/batch]Batch 18200/20010 Done, mean position loss: 22.20901854276657\n",
      "Training growing_up:  91%|███████▎| 18141/20010 [14:34:51<1:35:10,  3.06s/batch]Batch 18300/20010 Done, mean position loss: 22.068469610214233\n",
      "Training growing_up:  91%|███████▎| 18179/20010 [14:35:12<1:37:48,  3.21s/batch]Batch 18300/20010 Done, mean position loss: 21.51395402908325\n",
      "Training growing_up:  92%|███████▎| 18329/20010 [14:35:14<1:36:40,  3.45s/batch]Batch 18300/20010 Done, mean position loss: 22.322557747364044\n",
      "Training growing_up:  90%|███████▏| 18000/20010 [14:35:30<1:41:02,  3.02s/batch]Batch 18300/20010 Done, mean position loss: 21.830000700950624\n",
      "Training growing_up:  91%|███████▎| 18226/20010 [14:35:33<1:31:30,  3.08s/batch]Batch 18000/20010 Done, mean position loss: 22.625169179439546\n",
      "Training growing_up:  91%|███████▎| 18191/20010 [14:35:49<1:37:44,  3.22s/batch]Batch 18400/20010 Done, mean position loss: 21.548422148227694\n",
      "Training growing_up:  91%|███████▎| 18298/20010 [14:36:07<1:32:23,  3.24s/batch]Batch 18200/20010 Done, mean position loss: 21.71287055492401\n",
      "Training growing_up:  90%|███████▏| 18013/20010 [14:36:09<1:47:30,  3.23s/batch]Batch 18100/20010 Done, mean position loss: 21.58530824422836\n",
      "Training growing_up:  90%|███████▏| 17939/20010 [14:36:15<1:40:10,  2.90s/batch]Batch 18300/20010 Done, mean position loss: 22.607313060760497\n",
      "Training growing_up:  91%|███████▎| 18200/20010 [14:36:16<1:25:31,  2.84s/batch]Batch 18300/20010 Done, mean position loss: 21.760713720321654\n",
      "Training growing_up:  92%|███████▎| 18323/20010 [14:36:17<1:25:53,  3.05s/batch]Batch 18300/20010 Done, mean position loss: 22.342851061820987\n",
      "Training growing_up:  90%|███████▏| 17933/20010 [14:36:19<1:52:38,  3.25s/batch]Batch 18200/20010 Done, mean position loss: 22.343917970657348\n",
      "Training growing_up:  91%|███████▎| 18206/20010 [14:36:22<1:28:18,  2.94s/batch]Batch 18200/20010 Done, mean position loss: 22.7377986907959\n",
      "Training growing_up:  91%|███████▎| 18198/20010 [14:36:33<1:30:39,  3.00s/batch]Batch 18300/20010 Done, mean position loss: 21.84811113357544\n",
      "Training growing_up:  91%|███████▎| 18206/20010 [14:36:33<1:25:01,  2.83s/batch]Batch 18300/20010 Done, mean position loss: 22.65887158155441\n",
      "Training growing_up:  90%|███████▏| 18071/20010 [14:36:35<1:34:05,  2.91s/batch]Batch 18600/20010 Done, mean position loss: 21.38231509208679\n",
      "Training growing_up:  92%|███████▎| 18445/20010 [14:36:39<1:16:11,  2.92s/batch]Batch 18200/20010 Done, mean position loss: 21.80918109893799\n",
      "Training growing_up:  91%|███████▎| 18304/20010 [14:36:41<1:14:46,  2.63s/batch]Batch 18500/20010 Done, mean position loss: 22.41444722414017\n",
      "Training growing_up:  92%|███████▎| 18310/20010 [14:36:41<1:26:20,  3.05s/batch]Batch 18000/20010 Done, mean position loss: 21.82428173542023\n",
      "Training growing_up:  91%|███████▎| 18175/20010 [14:36:42<1:39:50,  3.26s/batch]Batch 18200/20010 Done, mean position loss: 22.68428126335144\n",
      "Training growing_up:  91%|███████▎| 18208/20010 [14:37:03<1:29:18,  2.97s/batch]Batch 17900/20010 Done, mean position loss: 21.523121378421784\n",
      "Training growing_up:  92%|███████▎| 18311/20010 [14:37:04<1:28:21,  3.12s/batch]Batch 18000/20010 Done, mean position loss: 21.418709597587586\n",
      "Training growing_up:  91%|███████▎| 18184/20010 [14:37:10<1:40:44,  3.31s/batch]Batch 18400/20010 Done, mean position loss: 21.856630523204803\n",
      "Training growing_up:  91%|███████▎| 18197/20010 [14:37:48<1:39:49,  3.30s/batch]Batch 18200/20010 Done, mean position loss: 22.408913481235505\n",
      "Training growing_up:  90%|███████▏| 17962/20010 [14:37:49<1:39:51,  2.93s/batch]Batch 18400/20010 Done, mean position loss: 22.06984473466873\n",
      "Training growing_up:  92%|███████▍| 18482/20010 [14:37:59<1:14:36,  2.93s/batch]Batch 18400/20010 Done, mean position loss: 22.648150708675384\n",
      "Training growing_up:  91%|███████▎| 18237/20010 [14:38:01<1:32:15,  3.12s/batch]Batch 18200/20010 Done, mean position loss: 22.77182844638824\n",
      "Training growing_up:  91%|███████▎| 18139/20010 [14:38:05<1:35:57,  3.08s/batch]Batch 18200/20010 Done, mean position loss: 22.176179847717286\n",
      "Training growing_up:  92%|███████▎| 18339/20010 [14:38:09<1:12:20,  2.60s/batch]Batch 18100/20010 Done, mean position loss: 22.03157295227051\n",
      "Training growing_up:  90%|███████▏| 18054/20010 [14:38:17<2:03:13,  3.78s/batch]Batch 18500/20010 Done, mean position loss: 21.676792368888854\n",
      "Training growing_up:  92%|███████▍| 18504/20010 [14:38:26<1:11:47,  2.86s/batch]Batch 18300/20010 Done, mean position loss: 21.938249466419222\n",
      "Training growing_up:  92%|███████▍| 18489/20010 [14:38:51<1:20:08,  3.16s/batch]Batch 18400/20010 Done, mean position loss: 22.29204844713211\n",
      "Training growing_up:  90%|███████▏| 18067/20010 [14:38:54<1:30:25,  2.79s/batch]Batch 18400/20010 Done, mean position loss: 21.466472544670104\n",
      "Training growing_up:  91%|███████▎| 18246/20010 [14:38:56<1:22:22,  2.80s/batch]Batch 18200/20010 Done, mean position loss: 22.11408721446991\n",
      "Training growing_up:  91%|███████▎| 18283/20010 [14:38:59<1:34:38,  3.29s/batch]Batch 18500/20010 Done, mean position loss: 22.492657136917114\n",
      "Training growing_up:  90%|███████▏| 18076/20010 [14:39:22<1:51:44,  3.47s/batch]Batch 18300/20010 Done, mean position loss: 22.58791682243347\n",
      "Training growing_up:  91%|███████▎| 18302/20010 [14:39:25<1:22:05,  2.88s/batch]Batch 18000/20010 Done, mean position loss: 22.299638888835908\n",
      "Training growing_up:  92%|███████▎| 18378/20010 [14:39:28<1:14:21,  2.73s/batch]Batch 18500/20010 Done, mean position loss: 21.77171263217926\n",
      "Training growing_up:  90%|███████▏| 18010/20010 [14:39:51<1:38:13,  2.95s/batch]Batch 18000/20010 Done, mean position loss: 22.130566468238833\n",
      "Training growing_up:  93%|███████▍| 18519/20010 [14:39:53<1:16:44,  3.09s/batch]Batch 18400/20010 Done, mean position loss: 21.95012209892273\n",
      "Training growing_up:  93%|███████▍| 18521/20010 [14:39:58<1:07:54,  2.74s/batch]Batch 18300/20010 Done, mean position loss: 22.12417827606201\n",
      "Training growing_up:  92%|███████▎| 18319/20010 [14:40:14<1:26:03,  3.05s/batch]Batch 18400/20010 Done, mean position loss: 22.256417574882505\n",
      "Training growing_up:  92%|███████▎| 18430/20010 [14:40:18<1:19:11,  3.01s/batch]Batch 18400/20010 Done, mean position loss: 21.513114676475528\n",
      "Training growing_up:  93%|███████▍| 18579/20010 [14:40:38<1:07:21,  2.82s/batch]Batch 18400/20010 Done, mean position loss: 22.051231770515443\n",
      "Training growing_up:  91%|███████▎| 18190/20010 [14:40:40<1:46:27,  3.51s/batch]Batch 18100/20010 Done, mean position loss: 22.462992739677432\n",
      "Training growing_up:  93%|███████▍| 18581/20010 [14:40:45<1:15:16,  3.16s/batch]Batch 18500/20010 Done, mean position loss: 21.52369431257248\n",
      "Training growing_up:  92%|███████▎| 18355/20010 [14:41:08<1:17:01,  2.79s/batch]Batch 18300/20010 Done, mean position loss: 21.943339257240297\n",
      "Training growing_up:  90%|███████▏| 18089/20010 [14:41:11<1:32:10,  2.88s/batch]Batch 18300/20010 Done, mean position loss: 22.328913402557372\n",
      "Training growing_up:  93%|███████▍| 18546/20010 [14:41:12<1:12:37,  2.98s/batch]Batch 18200/20010 Done, mean position loss: 21.613299067020414\n",
      "Training growing_up:  90%|███████▏| 18029/20010 [14:41:14<1:36:17,  2.92s/batch]Batch 18400/20010 Done, mean position loss: 22.240201549530028\n",
      "Training growing_up:  91%|███████▎| 18264/20010 [14:41:16<1:29:39,  3.08s/batch]Batch 18400/20010 Done, mean position loss: 21.70742980480194\n",
      "Training growing_up:  91%|███████▎| 18294/20010 [14:41:19<1:35:29,  3.34s/batch]Batch 18400/20010 Done, mean position loss: 22.57277998447418\n",
      "Training growing_up:  91%|███████▎| 18267/20010 [14:41:31<1:29:45,  3.09s/batch]Batch 18700/20010 Done, mean position loss: 21.458756976127624\n",
      "Training growing_up:  91%|███████▎| 18255/20010 [14:41:33<1:32:14,  3.15s/batch]Batch 18400/20010 Done, mean position loss: 22.27706907272339\n",
      "Training growing_up:  91%|███████▎| 18167/20010 [14:41:35<1:29:23,  2.91s/batch]Batch 18400/20010 Done, mean position loss: 21.86005697727203\n",
      "Training growing_up:  90%|███████▏| 18044/20010 [14:41:40<1:38:52,  3.02s/batch]Batch 18300/20010 Done, mean position loss: 22.334249436855316\n",
      "Training growing_up:  92%|███████▎| 18349/20010 [14:41:42<1:17:58,  2.82s/batch]Batch 18300/20010 Done, mean position loss: 22.727975800037385\n",
      "Training growing_up:  91%|███████▎| 18302/20010 [14:41:45<1:25:16,  3.00s/batch]Batch 18600/20010 Done, mean position loss: 22.178438045978545\n",
      "Training growing_up:  92%|███████▎| 18423/20010 [14:41:47<1:16:34,  2.89s/batch]Batch 18100/20010 Done, mean position loss: 21.534565041065214\n",
      "Training growing_up:  93%|███████▍| 18523/20010 [14:41:48<1:06:48,  2.70s/batch]Batch 18300/20010 Done, mean position loss: 21.928861038684843\n",
      "Training growing_up:  92%|███████▎| 18358/20010 [14:42:09<1:28:25,  3.21s/batch]Batch 18100/20010 Done, mean position loss: 21.458463480472567\n",
      "Training growing_up:  92%|███████▎| 18417/20010 [14:42:09<1:16:47,  2.89s/batch]Batch 18000/20010 Done, mean position loss: 21.613198606967927\n",
      "Training growing_up:  92%|███████▍| 18489/20010 [14:42:12<1:15:29,  2.98s/batch]Batch 18500/20010 Done, mean position loss: 21.529022397994993\n",
      "Training growing_up:  92%|███████▍| 18453/20010 [14:42:45<1:10:03,  2.70s/batch]Batch 18500/20010 Done, mean position loss: 22.145875878334046\n",
      "Training growing_up:  93%|███████▍| 18544/20010 [14:42:51<1:07:09,  2.75s/batch]Batch 18300/20010 Done, mean position loss: 22.254712491035463\n",
      "Training growing_up:  91%|███████▎| 18298/20010 [14:42:55<1:25:48,  3.01s/batch]Batch 18500/20010 Done, mean position loss: 22.608288836479186\n",
      "Training growing_up:  91%|███████▏| 18126/20010 [14:43:03<1:46:31,  3.39s/batch]Batch 18300/20010 Done, mean position loss: 22.677529039382936\n",
      "Training growing_up:  92%|███████▎| 18332/20010 [14:43:13<1:18:01,  2.79s/batch]Batch 18600/20010 Done, mean position loss: 21.649931116104128\n",
      "Training growing_up:  92%|███████▎| 18441/20010 [14:43:14<1:14:32,  2.85s/batch]Batch 18300/20010 Done, mean position loss: 22.060698914527894\n",
      "Training growing_up:  93%|███████▍| 18631/20010 [14:43:16<1:07:46,  2.95s/batch]Batch 18200/20010 Done, mean position loss: 21.99193452119827\n",
      "Training growing_up:  93%|█████████▎| 18527/20010 [14:43:26<59:11,  2.39s/batch]Batch 18400/20010 Done, mean position loss: 21.743158884048462\n",
      "Training growing_up:  92%|███████▎| 18445/20010 [14:43:45<1:18:04,  2.99s/batch]Batch 18300/20010 Done, mean position loss: 22.00543921947479\n",
      "Training growing_up:  94%|███████▍| 18748/20010 [14:43:47<1:03:14,  3.01s/batch]Batch 18500/20010 Done, mean position loss: 22.279766883850098\n",
      "Training growing_up:  92%|███████▍| 18466/20010 [14:43:50<1:07:47,  2.63s/batch]Batch 18600/20010 Done, mean position loss: 22.237199947834014\n",
      "Training growing_up:  92%|███████▎| 18411/20010 [14:43:51<1:09:03,  2.59s/batch]Batch 18500/20010 Done, mean position loss: 21.582172987461092\n",
      "Training growing_up:  93%|███████▍| 18574/20010 [14:44:16<1:10:56,  2.96s/batch]Batch 18400/20010 Done, mean position loss: 22.48764336824417\n",
      "Training growing_up:  91%|███████▎| 18223/20010 [14:44:22<1:45:34,  3.54s/batch]Batch 18600/20010 Done, mean position loss: 21.85275215148926\n",
      "Training growing_up:  92%|███████▎| 18358/20010 [14:44:26<1:17:28,  2.81s/batch]Batch 18100/20010 Done, mean position loss: 22.26831732034683\n",
      "Training growing_up:  91%|███████▎| 18183/20010 [14:44:47<1:31:28,  3.00s/batch]Batch 18500/20010 Done, mean position loss: 21.783502757549286\n",
      "Training growing_up:  92%|███████▎| 18339/20010 [14:44:52<1:21:38,  2.93s/batch]Batch 18400/20010 Done, mean position loss: 22.024015049934388\n",
      "Training growing_up:  92%|███████▍| 18476/20010 [14:44:55<1:10:55,  2.77s/batch]Batch 18100/20010 Done, mean position loss: 22.160240194797517\n",
      "Training growing_up:  92%|███████▍| 18473/20010 [14:45:06<1:13:06,  2.85s/batch]Batch 18500/20010 Done, mean position loss: 22.533665907382964\n",
      "Training growing_up:  91%|███████▎| 18171/20010 [14:45:14<1:31:06,  2.97s/batch]Batch 18500/20010 Done, mean position loss: 21.605731585025787\n",
      "Training growing_up:  92%|███████▍| 18481/20010 [14:45:35<1:11:41,  2.81s/batch]Batch 18500/20010 Done, mean position loss: 22.123659667968752\n",
      "Training growing_up:  92%|███████▍| 18501/20010 [14:45:36<1:22:09,  3.27s/batch]Batch 18600/20010 Done, mean position loss: 21.351106605529786\n",
      "Training growing_up:  92%|███████▎| 18357/20010 [14:45:43<1:21:20,  2.95s/batch]Batch 18200/20010 Done, mean position loss: 22.352103970050813\n",
      "Training growing_up:  92%|███████▎| 18424/20010 [14:45:59<1:15:16,  2.85s/batch]Batch 18400/20010 Done, mean position loss: 21.78638939380646\n",
      "Training growing_up:  92%|███████▎| 18426/20010 [14:46:03<1:07:43,  2.57s/batch]Batch 18400/20010 Done, mean position loss: 22.275543093681335\n",
      "Training growing_up:  91%|███████▎| 18259/20010 [14:46:05<1:27:49,  3.01s/batch]Batch 18300/20010 Done, mean position loss: 21.663367764949797\n",
      "Training growing_up:  91%|███████▎| 18136/20010 [14:46:09<1:32:07,  2.95s/batch]Batch 18500/20010 Done, mean position loss: 22.219789440631867\n",
      "Training growing_up:  91%|███████▎| 18261/20010 [14:46:12<1:31:27,  3.14s/batch]Batch 18500/20010 Done, mean position loss: 21.658012158870697\n",
      "Training growing_up:  92%|███████▍| 18496/20010 [14:46:18<1:14:39,  2.96s/batch]Batch 18500/20010 Done, mean position loss: 22.554332649707796\n",
      "Training growing_up:  91%|███████▎| 18264/20010 [14:46:20<1:25:59,  2.96s/batch]Batch 18800/20010 Done, mean position loss: 21.392781836986543\n",
      "Training growing_up:  93%|███████▍| 18592/20010 [14:46:32<1:13:13,  3.10s/batch]Batch 18500/20010 Done, mean position loss: 21.60593180656433\n",
      "Training growing_up:  93%|███████▍| 18574/20010 [14:46:34<1:27:27,  3.65s/batch]Batch 18500/20010 Done, mean position loss: 22.46564818620682\n",
      "Training growing_up:  94%|███████▌| 18807/20010 [14:46:38<1:02:24,  3.11s/batch]Batch 18400/20010 Done, mean position loss: 22.830099616050717\n",
      "Training growing_up:  93%|███████▍| 18511/20010 [14:46:40<1:19:02,  3.16s/batch]Batch 18400/20010 Done, mean position loss: 22.507815794944765\n",
      "Training growing_up:  92%|███████▎| 18402/20010 [14:46:42<1:25:00,  3.17s/batch]Batch 18700/20010 Done, mean position loss: 22.372602534294128\n",
      "Training growing_up:  92%|███████▍| 18505/20010 [14:46:45<1:20:15,  3.20s/batch]Batch 18400/20010 Done, mean position loss: 21.98803029060364\n",
      "Training growing_up:  92%|███████▎| 18361/20010 [14:46:46<1:23:13,  3.03s/batch]Batch 18200/20010 Done, mean position loss: 21.923158090114594\n",
      "Training growing_up:  93%|███████▍| 18582/20010 [14:47:00<1:19:44,  3.35s/batch]Batch 18600/20010 Done, mean position loss: 21.557706084251404\n",
      "Training growing_up:  93%|███████▍| 18542/20010 [14:47:10<1:15:58,  3.11s/batch]Batch 18200/20010 Done, mean position loss: 21.449203743934632\n",
      "Training growing_up:  93%|███████▍| 18535/20010 [14:47:19<1:21:38,  3.32s/batch]Batch 18100/20010 Done, mean position loss: 21.847113904953005\n",
      "Training growing_up:  92%|███████▍| 18486/20010 [14:47:41<1:16:44,  3.02s/batch]Batch 18600/20010 Done, mean position loss: 21.889203696250917\n",
      "Training growing_up:  93%|███████▍| 18529/20010 [14:47:56<1:10:36,  2.86s/batch]Batch 18400/20010 Done, mean position loss: 21.901737129688264\n",
      "Training growing_up:  92%|███████▎| 18440/20010 [14:47:56<1:17:51,  2.98s/batch]Batch 18600/20010 Done, mean position loss: 22.71637879371643\n",
      "Training growing_up:  91%|███████▎| 18245/20010 [14:47:58<1:29:04,  3.03s/batch]Batch 18400/20010 Done, mean position loss: 22.6306680727005\n",
      "Training growing_up:  93%|███████▍| 18552/20010 [14:48:15<1:17:31,  3.19s/batch]Batch 18400/20010 Done, mean position loss: 22.05404494047165\n",
      "Training growing_up:  92%|███████▎| 18430/20010 [14:48:16<1:17:10,  2.93s/batch]Batch 18300/20010 Done, mean position loss: 22.290676167011263\n",
      "Training growing_up:  93%|███████▍| 18540/20010 [14:48:16<1:14:39,  3.05s/batch]Batch 18700/20010 Done, mean position loss: 21.566420624256132\n",
      "Training growing_up:  92%|███████▎| 18437/20010 [14:48:25<1:16:08,  2.90s/batch]Batch 18500/20010 Done, mean position loss: 21.536733140945433\n",
      "Training growing_up:  93%|███████▍| 18664/20010 [14:48:49<1:11:52,  3.20s/batch]Batch 18400/20010 Done, mean position loss: 21.90080443620682\n",
      "Training growing_up:  92%|███████▎| 18412/20010 [14:48:49<1:24:31,  3.17s/batch]Batch 18600/20010 Done, mean position loss: 22.091894047260283\n",
      "Training growing_up:  91%|███████▎| 18187/20010 [14:48:52<1:35:58,  3.16s/batch]Batch 18700/20010 Done, mean position loss: 22.337126002311706\n",
      "Training growing_up:  93%|███████▍| 18584/20010 [14:49:03<1:06:50,  2.81s/batch]Batch 18600/20010 Done, mean position loss: 21.552469832897188\n",
      "Training growing_up:  93%|███████▍| 18676/20010 [14:49:23<1:02:44,  2.82s/batch]Batch 18700/20010 Done, mean position loss: 21.827161252498627\n",
      "Training growing_up:  92%|███████▍| 18469/20010 [14:49:28<1:34:56,  3.70s/batch]Batch 18500/20010 Done, mean position loss: 22.734505064487458\n",
      "Training growing_up:  93%|███████▍| 18524/20010 [14:49:36<1:12:42,  2.94s/batch]Batch 18200/20010 Done, mean position loss: 22.060926311016082\n",
      "Training growing_up:  94%|███████▍| 18721/20010 [14:49:57<1:09:01,  3.21s/batch]Batch 18600/20010 Done, mean position loss: 21.939171230793\n",
      "Training growing_up:  92%|███████▎| 18373/20010 [14:49:59<1:30:38,  3.32s/batch]Batch 18500/20010 Done, mean position loss: 22.02210189819336\n",
      "Training growing_up:  91%|███████▎| 18154/20010 [14:50:11<1:49:46,  3.55s/batch]Batch 18600/20010 Done, mean position loss: 22.404595432281496\n",
      "Training growing_up:  93%|███████▍| 18601/20010 [14:50:11<1:12:55,  3.11s/batch]Batch 18200/20010 Done, mean position loss: 22.371419525146486\n",
      "Training growing_up:  92%|███████▍| 18448/20010 [14:50:23<1:18:36,  3.02s/batch]Batch 18600/20010 Done, mean position loss: 21.710541837215423\n",
      "Training growing_up:  91%|███████▎| 18221/20010 [14:50:41<1:44:42,  3.51s/batch]Batch 18700/20010 Done, mean position loss: 21.416816220283508\n",
      "Training growing_up:  93%|███████▍| 18545/20010 [14:50:42<1:12:58,  2.99s/batch]Batch 18600/20010 Done, mean position loss: 21.82382803916931\n",
      "Training growing_up:  94%|███████▍| 18739/20010 [14:50:52<1:04:46,  3.06s/batch]Batch 18300/20010 Done, mean position loss: 22.50257695913315\n",
      "Training growing_up:  92%|███████▍| 18456/20010 [14:51:07<1:27:20,  3.37s/batch]Batch 18500/20010 Done, mean position loss: 21.69112687587738\n",
      "Training growing_up:  93%|███████▍| 18593/20010 [14:51:16<1:13:33,  3.11s/batch]Batch 18500/20010 Done, mean position loss: 22.306093881130217\n",
      "Training growing_up:  92%|███████▍| 18490/20010 [14:51:20<1:21:12,  3.21s/batch]Batch 18600/20010 Done, mean position loss: 22.1752651143074\n",
      "Training growing_up:  93%|███████▍| 18687/20010 [14:51:20<1:07:42,  3.07s/batch]Batch 18600/20010 Done, mean position loss: 21.67886601924896\n",
      "Training growing_up:  92%|███████▎| 18364/20010 [14:51:28<1:29:17,  3.25s/batch]Batch 18600/20010 Done, mean position loss: 22.55276403427124\n",
      "Training growing_up:  93%|███████▍| 18604/20010 [14:51:28<1:08:35,  2.93s/batch]Batch 18400/20010 Done, mean position loss: 21.559145543575287\n",
      "Training growing_up:  93%|███████▍| 18626/20010 [14:51:29<1:16:01,  3.30s/batch]Batch 18900/20010 Done, mean position loss: 21.29141712665558\n",
      "Training growing_up:  93%|███████▍| 18605/20010 [14:51:39<1:09:57,  2.99s/batch]Batch 18600/20010 Done, mean position loss: 21.732162609100342\n",
      "Training growing_up:  93%|███████▍| 18513/20010 [14:51:44<1:11:45,  2.88s/batch]Batch 18600/20010 Done, mean position loss: 23.061735644340516\n",
      "Training growing_up:  94%|███████▌| 18798/20010 [14:51:45<1:00:58,  3.02s/batch]Batch 18500/20010 Done, mean position loss: 22.73321510076523\n",
      "Training growing_up:  93%|███████▍| 18654/20010 [14:51:51<1:02:36,  2.77s/batch]Batch 18500/20010 Done, mean position loss: 22.501306018829347\n",
      "Training growing_up:  93%|███████▍| 18538/20010 [14:51:54<1:17:26,  3.16s/batch]Batch 18800/20010 Done, mean position loss: 22.140491321086884\n",
      "Training growing_up:  93%|███████▍| 18613/20010 [14:51:56<1:15:55,  3.26s/batch]Batch 18500/20010 Done, mean position loss: 21.9350244140625\n",
      "Batch 18300/20010 Done, mean position loss: 21.585827357769013\n",
      "Training growing_up:  93%|███████▍| 18608/20010 [14:52:05<1:14:03,  3.17s/batch]Batch 18700/20010 Done, mean position loss: 21.55106997251511\n",
      "Training growing_up:  94%|███████▌| 18781/20010 [14:52:21<1:05:34,  3.20s/batch]Batch 18300/20010 Done, mean position loss: 21.46907479286194\n",
      "Training growing_up:  93%|███████▍| 18583/20010 [14:52:40<1:09:33,  2.92s/batch]Batch 18200/20010 Done, mean position loss: 21.51655447721481\n",
      "Training growing_up:  92%|███████▎| 18335/20010 [14:52:44<1:23:26,  2.99s/batch]Batch 18700/20010 Done, mean position loss: 22.157166481018066\n",
      "Training growing_up:  95%|█████████▍| 18932/20010 [14:53:01<48:32,  2.70s/batch]Batch 18700/20010 Done, mean position loss: 22.62990882396698\n",
      "Training growing_up:  93%|███████▍| 18524/20010 [14:53:09<1:20:08,  3.24s/batch]Batch 18500/20010 Done, mean position loss: 21.91194351196289\n",
      "Training growing_up:  93%|███████▍| 18637/20010 [14:53:13<1:14:15,  3.25s/batch]Batch 18500/20010 Done, mean position loss: 22.370531556606295\n",
      "Training growing_up:  93%|███████▍| 18528/20010 [14:53:23<1:17:29,  3.14s/batch]Batch 18800/20010 Done, mean position loss: 21.490921778678896\n",
      "Training growing_up:  93%|███████▍| 18658/20010 [14:53:23<1:11:18,  3.16s/batch]Batch 18500/20010 Done, mean position loss: 22.311819674968717\n",
      "Training growing_up:  92%|███████▍| 18504/20010 [14:53:24<1:22:34,  3.29s/batch]Batch 18400/20010 Done, mean position loss: 22.047939050197602\n",
      "Training growing_up:  93%|███████▍| 18548/20010 [14:53:35<1:21:04,  3.33s/batch]Batch 18600/20010 Done, mean position loss: 21.822065103054047\n",
      "Training growing_up:  93%|███████▍| 18652/20010 [14:53:56<1:11:33,  3.16s/batch]Batch 18800/20010 Done, mean position loss: 22.41706482648849\n",
      "Training growing_up:  94%|███████▌| 18840/20010 [14:53:56<1:05:55,  3.38s/batch]Batch 18500/20010 Done, mean position loss: 22.13248951911926\n",
      "Training growing_up:  93%|███████▍| 18652/20010 [14:53:56<1:11:47,  3.17s/batch]Batch 18700/20010 Done, mean position loss: 22.292472386360167\n",
      "Training growing_up:  94%|█████████▍| 18848/20010 [14:54:19<59:21,  3.07s/batch]Batch 18700/20010 Done, mean position loss: 21.536905431747435\n",
      "Training growing_up:  94%|█████████▍| 18823/20010 [14:54:26<51:45,  2.62s/batch]Batch 18800/20010 Done, mean position loss: 21.626140658855437\n",
      "Training growing_up:  93%|███████▍| 18525/20010 [14:54:37<1:16:17,  3.08s/batch]Batch 18600/20010 Done, mean position loss: 22.529721064567568\n",
      "Training growing_up:  94%|███████▌| 18787/20010 [14:54:59<1:02:28,  3.06s/batch]Batch 18300/20010 Done, mean position loss: 22.212908253669738\n",
      "Training growing_up:  95%|█████████▍| 18972/20010 [14:55:02<47:38,  2.75s/batch]Batch 18600/20010 Done, mean position loss: 21.953827645778656\n",
      "Training growing_up:  93%|███████▍| 18667/20010 [14:55:05<1:11:20,  3.19s/batch]Batch 18700/20010 Done, mean position loss: 21.806029365062713\n",
      "Training growing_up:  93%|███████▍| 18580/20010 [14:55:16<1:02:37,  2.63s/batch]Batch 18700/20010 Done, mean position loss: 22.298380386829376\n",
      "Training growing_up:  93%|███████▍| 18543/20010 [14:55:23<1:12:12,  2.95s/batch]Batch 18300/20010 Done, mean position loss: 22.194897072315214\n",
      "Training growing_up:  93%|███████▍| 18548/20010 [14:55:38<1:09:37,  2.86s/batch]Batch 18700/20010 Done, mean position loss: 21.598486711978914\n",
      "Training growing_up:  93%|███████▍| 18588/20010 [14:55:39<1:09:34,  2.94s/batch]Batch 18800/20010 Done, mean position loss: 21.555715458393095\n",
      "Training growing_up:  95%|█████████▍| 18991/20010 [14:55:56<46:04,  2.71s/batch]Batch 18700/20010 Done, mean position loss: 21.842506330013276\n",
      "Training growing_up:  92%|███████▎| 18383/20010 [14:56:08<1:21:30,  3.01s/batch]Batch 18400/20010 Done, mean position loss: 22.33577118873596\n",
      "Training growing_up:  93%|███████▍| 18561/20010 [14:56:18<1:12:08,  2.99s/batch]Batch 18600/20010 Done, mean position loss: 21.68119321346283\n",
      "Training growing_up:  93%|███████▍| 18700/20010 [14:56:23<1:04:46,  2.97s/batch]Batch 18600/20010 Done, mean position loss: 22.176055567264555\n",
      "Training growing_up:  94%|█████████▍| 18784/20010 [14:56:23<58:23,  2.86s/batch]Batch 18700/20010 Done, mean position loss: 21.620008447170257\n",
      "Training growing_up:  92%|███████▎| 18330/20010 [14:56:26<1:16:37,  2.74s/batch]Batch 19000/20010 Done, mean position loss: 21.316421597003938\n",
      "Training growing_up:  92%|███████▍| 18498/20010 [14:56:26<1:21:25,  3.23s/batch]Batch 18700/20010 Done, mean position loss: 22.11635930776596\n",
      "Training growing_up:  93%|███████▍| 18699/20010 [14:56:35<1:01:33,  2.82s/batch]Batch 18700/20010 Done, mean position loss: 22.435369997024537\n",
      "Training growing_up:  94%|███████▍| 18720/20010 [14:56:36<1:07:47,  3.15s/batch]Batch 18500/20010 Done, mean position loss: 21.7303222823143\n",
      "Training growing_up:  93%|█████████▎| 18707/20010 [14:56:40<59:36,  2.74s/batch]Batch 18700/20010 Done, mean position loss: 21.722800731658936\n",
      "Training growing_up:  95%|█████████▍| 19007/20010 [14:56:42<42:38,  2.55s/batch]Batch 18700/20010 Done, mean position loss: 22.87195279121399\n",
      "Training growing_up:  93%|███████▍| 18643/20010 [14:56:45<1:03:49,  2.80s/batch]Batch 18600/20010 Done, mean position loss: 22.752491166591646\n",
      "Training growing_up:  95%|█████████▌| 19012/20010 [14:56:56<45:41,  2.75s/batch]Batch 18400/20010 Done, mean position loss: 21.593461458683013\n",
      "Training growing_up:  94%|█████████▍| 18779/20010 [14:56:57<55:24,  2.70s/batch]Batch 18600/20010 Done, mean position loss: 22.334238595962525\n",
      "Training growing_up:  93%|███████▍| 18606/20010 [14:56:58<1:00:24,  2.58s/batch]Batch 18900/20010 Done, mean position loss: 22.202028007507323\n",
      "Training growing_up:  93%|███████▍| 18617/20010 [14:57:06<1:06:29,  2.86s/batch]Batch 18600/20010 Done, mean position loss: 21.87042223215103\n",
      "Training growing_up:  94%|█████████▍| 18867/20010 [14:57:12<57:48,  3.03s/batch]Batch 18800/20010 Done, mean position loss: 21.38765467643738\n",
      "Training growing_up:  94%|███████▍| 18717/20010 [14:57:22<1:06:58,  3.11s/batch]Batch 18400/20010 Done, mean position loss: 21.391048419475553\n",
      "Training growing_up:  93%|███████▍| 18524/20010 [14:57:38<1:10:37,  2.85s/batch]Batch 18800/20010 Done, mean position loss: 22.00218340873718\n",
      "Training growing_up:  93%|███████▍| 18631/20010 [14:57:48<1:06:18,  2.88s/batch]Batch 18300/20010 Done, mean position loss: 21.394717712402347\n",
      "Training growing_up:  93%|███████▍| 18531/20010 [14:57:58<1:16:20,  3.10s/batch]Batch 18800/20010 Done, mean position loss: 22.710943145751955\n",
      "Training growing_up:  93%|███████▍| 18595/20010 [14:58:06<1:11:58,  3.05s/batch]Batch 18600/20010 Done, mean position loss: 22.38781007051468\n",
      "Training growing_up:  94%|█████████▍| 18887/20010 [14:58:09<54:20,  2.90s/batch]Batch 18900/20010 Done, mean position loss: 21.62595485687256\n",
      "Training growing_up:  92%|███████▎| 18444/20010 [14:58:13<1:16:48,  2.94s/batch]Batch 18600/20010 Done, mean position loss: 22.38879249572754\n",
      "Training growing_up:  92%|███████▎| 18422/20010 [14:58:24<1:13:49,  2.79s/batch]Batch 18600/20010 Done, mean position loss: 22.203555665016175\n",
      "Training growing_up:  94%|███████▌| 18817/20010 [14:58:26<1:00:07,  3.02s/batch]Batch 18500/20010 Done, mean position loss: 22.020475306510924\n",
      "Training growing_up:  92%|███████▎| 18376/20010 [14:58:44<1:27:20,  3.21s/batch]Batch 18800/20010 Done, mean position loss: 22.140607573986053\n",
      "Training growing_up:  94%|███████▌| 18777/20010 [14:58:45<1:00:47,  2.96s/batch]Batch 18700/20010 Done, mean position loss: 21.694707322120667\n",
      "Training growing_up:  94%|███████▍| 18751/20010 [14:58:50<1:05:17,  3.11s/batch]Batch 18900/20010 Done, mean position loss: 22.273291721343995\n",
      "Training growing_up:  93%|███████▍| 18680/20010 [14:58:52<1:10:47,  3.19s/batch]Batch 18600/20010 Done, mean position loss: 22.022338201999666\n",
      "Training growing_up:  94%|███████▍| 18758/20010 [14:59:10<1:03:00,  3.02s/batch]Batch 18900/20010 Done, mean position loss: 21.844964821338657\n",
      "Training growing_up:  93%|███████▍| 18643/20010 [14:59:13<1:13:49,  3.24s/batch]Batch 18800/20010 Done, mean position loss: 21.6439852976799\n",
      "Training growing_up:  92%|███████▍| 18456/20010 [14:59:36<1:14:40,  2.88s/batch]Batch 18700/20010 Done, mean position loss: 22.646428606510163\n",
      "Training growing_up:  94%|███████▌| 18783/20010 [14:59:55<1:00:28,  2.96s/batch]Batch 18800/20010 Done, mean position loss: 21.85987287759781\n",
      "Training growing_up:  92%|███████▎| 18343/20010 [14:59:59<1:19:33,  2.86s/batch]Batch 18700/20010 Done, mean position loss: 22.095107953548435\n",
      "Training growing_up:  93%|███████▍| 18702/20010 [15:00:01<1:03:46,  2.93s/batch]Batch 18400/20010 Done, mean position loss: 22.167742927074432\n",
      "Training growing_up:  93%|███████▍| 18703/20010 [15:00:03<1:00:25,  2.77s/batch]Batch 18800/20010 Done, mean position loss: 22.34111486911774\n",
      "Training growing_up:  95%|█████████▍| 18927/20010 [15:00:22<47:03,  2.61s/batch]Batch 18400/20010 Done, mean position loss: 22.152680900096893\n",
      "Training growing_up:  94%|███████▍| 18718/20010 [15:00:25<1:06:35,  3.09s/batch]Batch 18800/20010 Done, mean position loss: 21.429908888339995\n",
      "Training growing_up:  93%|███████▍| 18653/20010 [15:00:42<1:11:39,  3.17s/batch]Batch 18900/20010 Done, mean position loss: 21.48928587913513\n",
      "Training growing_up:  93%|███████▍| 18649/20010 [15:00:49<1:10:39,  3.11s/batch]Batch 18800/20010 Done, mean position loss: 21.777545816898346\n",
      "Training growing_up:  92%|███████▍| 18476/20010 [15:01:04<1:13:00,  2.86s/batch]Batch 18500/20010 Done, mean position loss: 22.437078042030336\n",
      "Training growing_up:  95%|█████████▍| 18987/20010 [15:01:10<42:52,  2.51s/batch]Batch 18700/20010 Done, mean position loss: 21.76286878347397\n",
      "Training growing_up:  93%|███████▍| 18664/20010 [15:01:13<1:05:08,  2.90s/batch]Batch 19100/20010 Done, mean position loss: 21.443732542991636\n",
      "Training growing_up:  92%|███████▍| 18489/20010 [15:01:14<1:20:20,  3.17s/batch]Batch 18800/20010 Done, mean position loss: 22.231742208004\n",
      "Training growing_up:  94%|█████████▍| 18826/20010 [15:01:16<56:29,  2.86s/batch]Batch 18700/20010 Done, mean position loss: 22.252413556575775\n",
      "Training growing_up:  92%|███████▍| 18491/20010 [15:01:19<1:13:36,  2.91s/batch]Batch 18800/20010 Done, mean position loss: 21.6514258313179\n",
      "Training growing_up:  94%|█████████▍| 18828/20010 [15:01:22<59:05,  3.00s/batch]Batch 18800/20010 Done, mean position loss: 22.466740839481353\n",
      "Training growing_up:  92%|███████▎| 18370/20010 [15:01:24<1:30:39,  3.32s/batch]Batch 18800/20010 Done, mean position loss: 21.748177409172058\n",
      "Training growing_up:  94%|█████████▍| 18887/20010 [15:01:26<55:48,  2.98s/batch]Batch 18800/20010 Done, mean position loss: 22.547301688194274\n",
      "Training growing_up:  95%|█████████▍| 18956/20010 [15:01:29<50:49,  2.89s/batch]Batch 18600/20010 Done, mean position loss: 21.633011519908905\n",
      "Training growing_up:  92%|███████▎| 18422/20010 [15:01:29<1:24:47,  3.20s/batch]Batch 18700/20010 Done, mean position loss: 22.642649955749512\n",
      "Training growing_up:  93%|███████▍| 18694/20010 [15:01:44<1:10:34,  3.22s/batch]Batch 18700/20010 Done, mean position loss: 22.551354966163636\n",
      "Training growing_up:  93%|█████████▎| 18708/20010 [15:01:50<59:32,  2.74s/batch]Batch 18500/20010 Done, mean position loss: 21.619843406677248\n",
      "Training growing_up:  96%|█████████▌| 19115/20010 [15:01:52<44:14,  2.97s/batch]Batch 19000/20010 Done, mean position loss: 22.400947544574738\n",
      "Training growing_up:  93%|███████▍| 18612/20010 [15:02:04<1:09:10,  2.97s/batch]Batch 18700/20010 Done, mean position loss: 21.970854573249817\n",
      "Training growing_up:  94%|███████▍| 18714/20010 [15:02:08<1:02:55,  2.91s/batch]Batch 18900/20010 Done, mean position loss: 21.492028207778933\n",
      "Training growing_up:  92%|███████▎| 18389/20010 [15:02:22<1:27:44,  3.25s/batch]Batch 18500/20010 Done, mean position loss: 21.322749371528623\n",
      "Training growing_up:  94%|███████▌| 18845/20010 [15:02:40<1:04:01,  3.30s/batch]Batch 18900/20010 Done, mean position loss: 21.94905697345734\n",
      "Training growing_up:  94%|█████████▍| 18850/20010 [15:02:54<56:27,  2.92s/batch]Batch 18900/20010 Done, mean position loss: 22.44937953233719\n",
      "Training growing_up:  94%|███████▍| 18729/20010 [15:02:58<1:12:04,  3.38s/batch]Batch 18400/20010 Done, mean position loss: 21.559515364170075\n",
      "Training growing_up:  94%|█████████▎| 18739/20010 [15:03:02<59:59,  2.83s/batch]Batch 19000/20010 Done, mean position loss: 21.48801186323166\n",
      "Training growing_up:  95%|█████████▍| 19002/20010 [15:03:05<52:06,  3.10s/batch]Batch 18700/20010 Done, mean position loss: 21.975398552417754\n",
      "Training growing_up:  94%|█████████▍| 18892/20010 [15:03:13<50:52,  2.73s/batch]Batch 18700/20010 Done, mean position loss: 22.522190771102906\n",
      "Training growing_up:  94%|███████▍| 18738/20010 [15:03:28<1:11:34,  3.38s/batch]Batch 18600/20010 Done, mean position loss: 22.295661983489993\n",
      "Training growing_up:  94%|███████▌| 18775/20010 [15:03:29<1:05:36,  3.19s/batch]Batch 18700/20010 Done, mean position loss: 22.420797193050387\n",
      "Training growing_up:  93%|███████▍| 18552/20010 [15:03:42<1:10:13,  2.89s/batch]Batch 18900/20010 Done, mean position loss: 22.10947547674179\n",
      "Training growing_up:  94%|█████████▍| 18859/20010 [15:03:47<59:13,  3.09s/batch]Batch 19000/20010 Done, mean position loss: 22.22510046720505\n",
      "Training growing_up:  95%|█████████▌| 19037/20010 [15:03:48<49:47,  3.07s/batch]Batch 18800/20010 Done, mean position loss: 21.74261936426163\n",
      "Training growing_up:  93%|███████▍| 18709/20010 [15:03:54<1:05:45,  3.03s/batch]Batch 18700/20010 Done, mean position loss: 21.83891298532486\n",
      "Training growing_up:  93%|███████▍| 18546/20010 [15:04:11<1:18:38,  3.22s/batch]Batch 19000/20010 Done, mean position loss: 21.627725763320925\n",
      "Training growing_up:  95%|█████████▍| 18972/20010 [15:04:18<54:28,  3.15s/batch]Batch 18900/20010 Done, mean position loss: 21.616424233913424\n",
      "Training growing_up:  94%|█████████▍| 18864/20010 [15:04:47<55:15,  2.89s/batch]Batch 18800/20010 Done, mean position loss: 22.480705552101135\n",
      "Training growing_up:  95%|█████████▍| 18986/20010 [15:04:58<48:22,  2.83s/batch]Batch 18800/20010 Done, mean position loss: 22.148062469959257\n",
      "Training growing_up:  94%|███████▍| 18724/20010 [15:05:03<1:04:33,  3.01s/batch]Batch 18900/20010 Done, mean position loss: 21.994291951656344\n",
      "Training growing_up:  95%|█████████▍| 18918/20010 [15:05:11<58:19,  3.20s/batch]Batch 18900/20010 Done, mean position loss: 22.23590494632721\n",
      "Training growing_up:  94%|███████▍| 18734/20010 [15:05:12<1:04:56,  3.05s/batch]Batch 18500/20010 Done, mean position loss: 22.146790897846223\n",
      "Training growing_up:  95%|█████████▍| 18955/20010 [15:05:29<51:03,  2.90s/batch]Batch 18900/20010 Done, mean position loss: 21.499532241821292\n",
      "Training growing_up:  94%|███████▌| 18883/20010 [15:05:35<1:01:04,  3.25s/batch]Batch 18500/20010 Done, mean position loss: 22.296431112289426\n",
      "Training growing_up:  95%|█████████▍| 18912/20010 [15:05:43<55:06,  3.01s/batch]Batch 19000/20010 Done, mean position loss: 21.50459888935089\n",
      "Training growing_up:  95%|█████████▍| 18944/20010 [15:05:51<47:50,  2.69s/batch]Batch 18900/20010 Done, mean position loss: 21.841612381935118\n",
      "Training growing_up:  95%|█████████▍| 18970/20010 [15:06:13<49:48,  2.87s/batch]Batch 18800/20010 Done, mean position loss: 21.757133855819703\n",
      "Training growing_up:  95%|█████████▍| 18982/20010 [15:06:13<47:26,  2.77s/batch]Batch 19200/20010 Done, mean position loss: 21.32601781845093\n",
      "Training growing_up:  95%|█████████▍| 18971/20010 [15:06:15<48:58,  2.83s/batch]Batch 18600/20010 Done, mean position loss: 22.394075396060945\n",
      "Training growing_up:  94%|█████████▍| 18851/20010 [15:06:23<59:36,  3.09s/batch]Batch 18900/20010 Done, mean position loss: 22.213640995025635\n",
      "Training growing_up:  95%|█████████▍| 18957/20010 [15:06:30<57:01,  3.25s/batch]Batch 18800/20010 Done, mean position loss: 22.285462958812715\n",
      "Training growing_up:  94%|█████████▍| 18767/20010 [15:06:31<59:58,  2.90s/batch]Batch 18900/20010 Done, mean position loss: 21.61945193052292\n",
      "Training growing_up:  95%|█████████▍| 18930/20010 [15:06:32<54:01,  3.00s/batch]Batch 18900/20010 Done, mean position loss: 21.67289787054062\n",
      "Training growing_up:  93%|███████▍| 18519/20010 [15:06:32<1:18:14,  3.15s/batch]Batch 18900/20010 Done, mean position loss: 22.219116225242615\n",
      "Training growing_up:  92%|███████▍| 18472/20010 [15:06:40<1:25:37,  3.34s/batch]Batch 18900/20010 Done, mean position loss: 22.447610187530515\n",
      "Training growing_up:  94%|█████████▍| 18904/20010 [15:06:42<58:17,  3.16s/batch]Batch 18700/20010 Done, mean position loss: 21.440332407951352\n",
      "Training growing_up:  94%|███████▌| 18858/20010 [15:06:46<1:03:01,  3.28s/batch]Batch 18800/20010 Done, mean position loss: 22.3414569067955\n",
      "Training growing_up:  93%|███████▍| 18612/20010 [15:06:50<1:12:14,  3.10s/batch]Batch 18800/20010 Done, mean position loss: 22.54979959964752\n",
      "Training growing_up:  95%|█████████▌| 19077/20010 [15:06:55<45:19,  2.91s/batch]Batch 19100/20010 Done, mean position loss: 22.200756237506866\n",
      "Training growing_up:  94%|█████████▍| 18817/20010 [15:06:59<54:12,  2.73s/batch]Batch 18600/20010 Done, mean position loss: 21.61788829565048\n",
      "Training growing_up:  95%|█████████▌| 19082/20010 [15:07:11<44:30,  2.88s/batch]Batch 19000/20010 Done, mean position loss: 21.557875027656557\n",
      "Training growing_up:  95%|█████████▍| 18937/20010 [15:07:18<49:03,  2.74s/batch]Batch 18800/20010 Done, mean position loss: 22.185721292495728\n",
      "Training growing_up:  95%|█████████▌| 19066/20010 [15:07:26<44:53,  2.85s/batch]Batch 18600/20010 Done, mean position loss: 21.337018492221834\n",
      "Training growing_up:  94%|█████████▍| 18811/20010 [15:07:47<58:26,  2.92s/batch]Batch 19000/20010 Done, mean position loss: 22.098849968910216\n",
      "Training growing_up:  95%|█████████▌| 19085/20010 [15:07:54<40:01,  2.60s/batch]Batch 19000/20010 Done, mean position loss: 22.427379953861234\n",
      "Training growing_up:  95%|█████████▍| 18933/20010 [15:08:08<57:49,  3.22s/batch]Batch 18500/20010 Done, mean position loss: 21.612467966079713\n",
      "Training growing_up:  95%|█████████▌| 19090/20010 [15:08:09<42:02,  2.74s/batch]Batch 18800/20010 Done, mean position loss: 22.562489783763887\n",
      "Training growing_up:  95%|█████████▍| 18989/20010 [15:08:09<52:36,  3.09s/batch]Batch 19100/20010 Done, mean position loss: 21.811050760746003\n",
      "Training growing_up:  95%|█████████▍| 18934/20010 [15:08:12<58:45,  3.28s/batch]Batch 18800/20010 Done, mean position loss: 22.104227917194365\n",
      "Training growing_up:  94%|███████▌| 18807/20010 [15:08:33<1:11:19,  3.56s/batch]Batch 18800/20010 Done, mean position loss: 22.376778242588042\n",
      "Training growing_up:  96%|█████████▌| 19135/20010 [15:08:36<42:54,  2.94s/batch]Batch 18700/20010 Done, mean position loss: 22.151373043060303\n",
      "Training growing_up:  94%|█████████▍| 18879/20010 [15:08:42<50:24,  2.67s/batch]Batch 19100/20010 Done, mean position loss: 22.35918396949768\n",
      "Training growing_up:  96%|█████████▌| 19114/20010 [15:08:47<42:46,  2.86s/batch]Batch 19000/20010 Done, mean position loss: 22.411176862716673\n",
      "Training growing_up:  93%|███████▍| 18517/20010 [15:08:56<1:12:55,  2.93s/batch]Batch 18800/20010 Done, mean position loss: 22.13480484724045\n",
      "Training growing_up:  95%|█████████▍| 18955/20010 [15:08:58<48:25,  2.75s/batch]Batch 18900/20010 Done, mean position loss: 21.642526428699494\n",
      "Training growing_up:  93%|███████▍| 18646/20010 [15:09:13<1:12:04,  3.17s/batch]Batch 19100/20010 Done, mean position loss: 21.713938219547273\n",
      "Training growing_up:  94%|███████▌| 18892/20010 [15:09:23<1:01:53,  3.32s/batch]Batch 19000/20010 Done, mean position loss: 21.400957498550415\n",
      "Training growing_up:  95%|█████████▌| 19056/20010 [15:09:51<46:09,  2.90s/batch]Batch 18900/20010 Done, mean position loss: 22.456064391136167\n",
      "Training growing_up:  94%|█████████▍| 18903/20010 [15:09:56<52:34,  2.85s/batch]Batch 18900/20010 Done, mean position loss: 22.089183073043827\n",
      "Training growing_up:  94%|█████████▍| 18838/20010 [15:10:01<53:27,  2.74s/batch]Batch 19000/20010 Done, mean position loss: 21.793076674938202\n",
      "Training growing_up:  94%|█████████▍| 18867/20010 [15:10:06<59:55,  3.15s/batch]Batch 19000/20010 Done, mean position loss: 22.212451231479648\n",
      "Training growing_up:  96%|█████████▋| 19282/20010 [15:10:10<37:02,  3.05s/batch]Batch 18600/20010 Done, mean position loss: 22.091475718021393\n",
      "Training growing_up:  94%|█████████▍| 18878/20010 [15:10:26<52:28,  2.78s/batch]Batch 19000/20010 Done, mean position loss: 21.505690472126005\n",
      "Training growing_up:  96%|█████████▌| 19142/20010 [15:10:40<41:26,  2.86s/batch]Batch 19100/20010 Done, mean position loss: 21.5960315990448\n",
      "Training growing_up:  93%|█████████▎| 18614/20010 [15:10:42<59:22,  2.55s/batch]Batch 18600/20010 Done, mean position loss: 22.11148704767227\n",
      "Training growing_up:  94%|█████████▍| 18784/20010 [15:10:46<55:22,  2.71s/batch]Batch 19000/20010 Done, mean position loss: 21.87994110584259\n",
      "Training growing_up:  94%|█████████▍| 18886/20010 [15:11:01<51:42,  2.76s/batch]Batch 19300/20010 Done, mean position loss: 21.238699758052825\n",
      "Training growing_up:  95%|█████████▍| 18990/20010 [15:11:01<52:31,  3.09s/batch]Batch 18900/20010 Done, mean position loss: 21.782413284778595\n",
      "Training growing_up:  95%|█████████▌| 19069/20010 [15:11:10<45:33,  2.90s/batch]Batch 19000/20010 Done, mean position loss: 22.22776512145996\n",
      "Training growing_up:  95%|█████████▍| 18995/20010 [15:11:19<43:02,  2.54s/batch]Batch 18700/20010 Done, mean position loss: 22.441762714385984\n",
      "Training growing_up:  93%|███████▍| 18570/20010 [15:11:24<1:03:10,  2.63s/batch]Batch 19000/20010 Done, mean position loss: 21.73274153470993\n",
      "Training growing_up:  95%|█████████▍| 18953/20010 [15:11:26<49:02,  2.78s/batch]Batch 19000/20010 Done, mean position loss: 21.63624498128891\n",
      "Training growing_up:  94%|█████████▍| 18887/20010 [15:11:29<46:55,  2.51s/batch]Batch 18900/20010 Done, mean position loss: 22.32934998989105\n",
      "Training growing_up:  96%|█████████▌| 19196/20010 [15:11:30<31:57,  2.36s/batch]Batch 18800/20010 Done, mean position loss: 21.25473443508148\n",
      "Training growing_up:  95%|█████████▍| 19000/20010 [15:11:32<42:39,  2.53s/batch]Batch 19000/20010 Done, mean position loss: 22.42557138442993\n",
      "Training growing_up:  95%|█████████▍| 19004/20010 [15:11:34<44:53,  2.68s/batch]Batch 19000/20010 Done, mean position loss: 22.462412078380584\n",
      "Training growing_up:  93%|█████████▎| 18687/20010 [15:11:37<59:03,  2.68s/batch]Batch 18900/20010 Done, mean position loss: 22.392044177055357\n",
      "Training growing_up:  93%|█████████▎| 18637/20010 [15:11:41<56:27,  2.47s/batch]Batch 18900/20010 Done, mean position loss: 22.37471661567688\n",
      "Training growing_up:  93%|███████▍| 18577/20010 [15:11:43<1:08:05,  2.85s/batch]Batch 19200/20010 Done, mean position loss: 22.171824359893797\n",
      "Training growing_up:  95%|█████████▌| 19040/20010 [15:11:47<40:57,  2.53s/batch]Batch 18700/20010 Done, mean position loss: 21.501796007156372\n",
      "Training growing_up:  94%|█████████▍| 18862/20010 [15:11:51<47:11,  2.47s/batch]Batch 19100/20010 Done, mean position loss: 21.616911289691924\n",
      "Training growing_up:  94%|█████████▍| 18877/20010 [15:12:10<50:26,  2.67s/batch]Batch 18900/20010 Done, mean position loss: 21.78903986930847\n",
      "Training growing_up:  96%|█████████▌| 19111/20010 [15:12:19<50:07,  3.34s/batch]Batch 18700/20010 Done, mean position loss: 21.00257326602936\n",
      "Training growing_up:  96%|█████████▌| 19185/20010 [15:12:36<41:44,  3.04s/batch]Batch 19100/20010 Done, mean position loss: 22.009996190071107\n",
      "Training growing_up:  95%|█████████▌| 19057/20010 [15:12:38<46:48,  2.95s/batch]Batch 19100/20010 Done, mean position loss: 22.36226127386093\n",
      "Training growing_up:  95%|█████████▍| 18928/20010 [15:12:53<51:35,  2.86s/batch]Batch 18600/20010 Done, mean position loss: 21.70530781507492\n",
      "Training growing_up:  96%|█████████▌| 19192/20010 [15:12:58<41:42,  3.06s/batch]Batch 19200/20010 Done, mean position loss: 21.774346919059756\n",
      "Training growing_up:  94%|███████▍| 18725/20010 [15:12:59<1:10:26,  3.29s/batch]Batch 18900/20010 Done, mean position loss: 22.490926737785337\n",
      "Training growing_up:  94%|█████████▍| 18887/20010 [15:13:07<59:21,  3.17s/batch]Batch 18900/20010 Done, mean position loss: 21.942203013896943\n",
      "Training growing_up:  95%|█████████▌| 19042/20010 [15:13:24<47:19,  2.93s/batch]Batch 18900/20010 Done, mean position loss: 22.469136457443238\n",
      "Training growing_up:  94%|█████████▍| 18893/20010 [15:13:25<56:37,  3.04s/batch]Batch 19200/20010 Done, mean position loss: 22.304193904399874\n",
      "Training growing_up:  93%|███████▍| 18613/20010 [15:13:29<1:11:19,  3.06s/batch]Batch 18800/20010 Done, mean position loss: 22.052789130210876\n",
      "Training growing_up:  95%|█████████▍| 18940/20010 [15:13:36<53:31,  3.00s/batch]Batch 19100/20010 Done, mean position loss: 22.52265908718109\n",
      "Training growing_up:  94%|█████████▍| 18807/20010 [15:13:46<59:41,  2.98s/batch]Batch 19000/20010 Done, mean position loss: 21.65081254720688\n",
      "Training growing_up:  94%|███████▍| 18742/20010 [15:13:51<1:03:45,  3.02s/batch]Batch 18900/20010 Done, mean position loss: 21.954068636894224\n",
      "Training growing_up:  96%|█████████▌| 19128/20010 [15:14:03<49:15,  3.35s/batch]Batch 19200/20010 Done, mean position loss: 21.599327557086944\n",
      "Training growing_up:  93%|███████▍| 18668/20010 [15:14:04<1:14:56,  3.35s/batch]Batch 19100/20010 Done, mean position loss: 21.517993347644804\n",
      "Training growing_up:  96%|█████████▌| 19156/20010 [15:14:38<46:39,  3.28s/batch]Batch 19000/20010 Done, mean position loss: 22.52582274675369\n",
      "Training growing_up:  95%|█████████▍| 18967/20010 [15:14:46<50:48,  2.92s/batch]Batch 19000/20010 Done, mean position loss: 21.787270011901853\n",
      "Training growing_up:  96%|█████████▌| 19160/20010 [15:14:51<44:21,  3.13s/batch]Batch 19100/20010 Done, mean position loss: 22.420382251739504\n",
      "Training growing_up:  95%|█████████▍| 19005/20010 [15:14:52<54:15,  3.24s/batch]Batch 19100/20010 Done, mean position loss: 21.90965724945068\n",
      "Training growing_up:  96%|█████████▌| 19148/20010 [15:15:05<38:43,  2.70s/batch]Batch 18700/20010 Done, mean position loss: 22.27679855585098\n",
      "Training growing_up:  96%|█████████▌| 19244/20010 [15:15:15<43:38,  3.42s/batch]Batch 19100/20010 Done, mean position loss: 21.50253190517426\n",
      "Training growing_up:  95%|█████████▌| 19099/20010 [15:15:35<41:59,  2.77s/batch]Batch 19200/20010 Done, mean position loss: 21.323228447437284\n",
      "Training growing_up:  96%|█████████▌| 19111/20010 [15:15:42<41:33,  2.77s/batch]Batch 19100/20010 Done, mean position loss: 21.9514093875885\n",
      "Training growing_up:  95%|█████████▌| 19102/20010 [15:15:44<42:57,  2.84s/batch]Batch 18700/20010 Done, mean position loss: 22.131609423160555\n",
      "Training growing_up:  95%|█████████▍| 18971/20010 [15:15:50<53:20,  3.08s/batch]Batch 19400/20010 Done, mean position loss: 21.44636676311493\n",
      "Training growing_up:  95%|█████████▍| 18956/20010 [15:15:56<50:15,  2.86s/batch]Batch 19000/20010 Done, mean position loss: 21.711726367473602\n",
      "Training growing_up:  95%|█████████▌| 19094/20010 [15:16:03<40:15,  2.64s/batch]Batch 19100/20010 Done, mean position loss: 22.245869913101195\n",
      "Training growing_up:  96%|█████████▋| 19295/20010 [15:16:19<37:59,  3.19s/batch]Batch 19100/20010 Done, mean position loss: 21.51747465133667\n",
      "Training growing_up:  96%|█████████▌| 19131/20010 [15:16:21<49:51,  3.40s/batch]Batch 18800/20010 Done, mean position loss: 22.519588067531586\n",
      "Training growing_up:  94%|███████▌| 18792/20010 [15:16:27<1:00:21,  2.97s/batch]Batch 19100/20010 Done, mean position loss: 22.255759050846102\n",
      "Training growing_up:  96%|█████████▌| 19177/20010 [15:16:28<41:50,  3.01s/batch]Batch 19100/20010 Done, mean position loss: 21.70314575910568\n",
      "Training growing_up:  95%|█████████▌| 19101/20010 [15:16:29<47:53,  3.16s/batch]Batch 18900/20010 Done, mean position loss: 21.54318794965744\n",
      "Training growing_up:  94%|███████▌| 18793/20010 [15:16:31<1:07:12,  3.31s/batch]Batch 19000/20010 Done, mean position loss: 22.342058160305022\n",
      "Training growing_up:  95%|█████████▌| 19040/20010 [15:16:34<46:38,  2.88s/batch]Batch 19100/20010 Done, mean position loss: 22.495377542972562\n",
      "Training growing_up:  94%|███████▍| 18717/20010 [15:16:36<1:11:41,  3.33s/batch]Batch 19000/20010 Done, mean position loss: 22.529107406139374\n",
      "Training growing_up:  94%|█████████▍| 18904/20010 [15:16:37<54:06,  2.94s/batch]Batch 19300/20010 Done, mean position loss: 21.994705972671508\n",
      "Training growing_up:  95%|█████████▍| 18968/20010 [15:16:49<47:55,  2.76s/batch]Batch 19200/20010 Done, mean position loss: 21.267554535865784\n",
      "Training growing_up:  96%|█████████▌| 19226/20010 [15:16:50<36:06,  2.76s/batch]Batch 19000/20010 Done, mean position loss: 22.313105528354644\n",
      "Training growing_up:  96%|█████████▌| 19113/20010 [15:16:54<42:02,  2.81s/batch]Batch 18800/20010 Done, mean position loss: 21.620358877182007\n",
      "Training growing_up:  95%|███████▌| 18987/20010 [15:17:26<1:03:12,  3.71s/batch]Batch 18800/20010 Done, mean position loss: 21.34275063276291\n",
      "Training growing_up:  94%|███████▌| 18811/20010 [15:17:26<1:04:13,  3.21s/batch]Batch 19000/20010 Done, mean position loss: 21.869779427051544\n",
      "Training growing_up:  95%|█████████▌| 19059/20010 [15:17:39<43:35,  2.75s/batch]Batch 19200/20010 Done, mean position loss: 22.050846676826477\n",
      "Training growing_up:  95%|█████████▍| 19005/20010 [15:17:40<53:55,  3.22s/batch]Batch 19200/20010 Done, mean position loss: 22.29810907363892\n",
      "Training growing_up:  96%|█████████▌| 19228/20010 [15:18:10<40:26,  3.10s/batch]Batch 19000/20010 Done, mean position loss: 22.463143632411956\n",
      "Training growing_up:  95%|█████████▌| 19087/20010 [15:18:12<42:50,  2.78s/batch]Batch 18700/20010 Done, mean position loss: 21.40068025112152\n",
      "Training growing_up:  96%|█████████▋| 19297/20010 [15:18:15<34:24,  2.89s/batch]Batch 19300/20010 Done, mean position loss: 21.369831273555754\n",
      "Training growing_up:  97%|█████████▋| 19449/20010 [15:18:16<29:41,  3.18s/batch]Batch 19000/20010 Done, mean position loss: 21.871053524017334\n",
      "Training growing_up:  96%|█████████▌| 19216/20010 [15:18:26<44:32,  3.37s/batch]Batch 19300/20010 Done, mean position loss: 22.380719068050382\n",
      "Training growing_up:  94%|███████▌| 18767/20010 [15:18:31<1:03:53,  3.08s/batch]Batch 18900/20010 Done, mean position loss: 22.148803758621217\n",
      "Training growing_up:  95%|█████████▌| 19079/20010 [15:18:33<47:49,  3.08s/batch]Batch 19000/20010 Done, mean position loss: 22.191352891921994\n",
      "Training growing_up:  96%|█████████▌| 19152/20010 [15:18:38<42:29,  2.97s/batch]Batch 19200/20010 Done, mean position loss: 22.479297807216646\n",
      "Training growing_up:  95%|█████████▍| 19009/20010 [15:18:58<50:10,  3.01s/batch]Batch 19100/20010 Done, mean position loss: 21.644745569229123\n",
      "Training growing_up:  96%|█████████▌| 19148/20010 [15:19:05<45:12,  3.15s/batch]Batch 19300/20010 Done, mean position loss: 21.65892825603485\n",
      "Training growing_up:  96%|█████████▌| 19246/20010 [15:19:06<37:45,  2.97s/batch]Batch 19000/20010 Done, mean position loss: 22.393897638320922\n",
      "Training growing_up:  95%|█████████▌| 19054/20010 [15:19:17<46:09,  2.90s/batch]Batch 19200/20010 Done, mean position loss: 21.541725130081176\n",
      "Training growing_up:  95%|█████████▌| 19011/20010 [15:19:39<57:20,  3.44s/batch]Batch 19100/20010 Done, mean position loss: 22.56835754394531\n",
      "Training growing_up:  96%|█████████▌| 19167/20010 [15:19:46<49:18,  3.51s/batch]Batch 19200/20010 Done, mean position loss: 21.873002479076387\n",
      "Training growing_up:  94%|███████▌| 18846/20010 [15:19:54<1:07:17,  3.47s/batch]Batch 19100/20010 Done, mean position loss: 21.93914745092392\n",
      "Training growing_up:  95%|█████████▌| 19051/20010 [15:20:05<53:33,  3.35s/batch]Batch 19200/20010 Done, mean position loss: 21.999623320102693\n",
      "Training growing_up:  96%|█████████▌| 19177/20010 [15:20:20<45:27,  3.27s/batch]Batch 18800/20010 Done, mean position loss: 22.044732315540315\n",
      "Training growing_up:  96%|█████████▌| 19178/20010 [15:20:22<41:45,  3.01s/batch]Batch 19200/20010 Done, mean position loss: 21.45315519094467\n",
      "Training growing_up:  95%|█████████▌| 19082/20010 [15:20:41<50:13,  3.25s/batch]Batch 19300/20010 Done, mean position loss: 21.40829849720001\n",
      "Training growing_up:  95%|█████████▍| 18985/20010 [15:20:53<54:07,  3.17s/batch]Batch 19200/20010 Done, mean position loss: 21.850499937534334\n",
      "Training growing_up:  96%|█████████▌| 19121/20010 [15:20:56<45:24,  3.07s/batch]Batch 19500/20010 Done, mean position loss: 21.321417648792266\n",
      "Training growing_up:  96%|█████████▌| 19188/20010 [15:20:57<41:32,  3.03s/batch]Batch 19100/20010 Done, mean position loss: 21.658149976730346\n",
      "Training growing_up:  97%|█████████▋| 19310/20010 [15:21:08<33:36,  2.88s/batch]Batch 18800/20010 Done, mean position loss: 22.07846843957901\n",
      "Training growing_up:  97%|█████████▋| 19507/20010 [15:21:14<25:19,  3.02s/batch]Batch 19200/20010 Done, mean position loss: 22.04862412214279\n",
      "Training growing_up:  96%|█████████▌| 19213/20010 [15:21:29<38:18,  2.88s/batch]Batch 19200/20010 Done, mean position loss: 21.582817728519437\n",
      "Training growing_up:  96%|█████████▌| 19138/20010 [15:21:35<43:23,  2.99s/batch]Batch 19200/20010 Done, mean position loss: 21.762853608131408\n",
      "Training growing_up:  95%|█████████▌| 19049/20010 [15:21:36<43:10,  2.70s/batch]Batch 18900/20010 Done, mean position loss: 22.43265106678009\n",
      "Training growing_up:  96%|█████████▌| 19153/20010 [15:21:37<39:15,  2.75s/batch]Batch 19100/20010 Done, mean position loss: 22.216297881603243\n",
      "Training growing_up:  98%|█████████▊| 19516/20010 [15:21:39<23:16,  2.83s/batch]Batch 19200/20010 Done, mean position loss: 22.405961687564847\n",
      "Training growing_up:  97%|█████████▋| 19352/20010 [15:21:41<31:11,  2.84s/batch]Batch 19400/20010 Done, mean position loss: 21.969979140758515\n",
      "Training growing_up:  96%|█████████▌| 19233/20010 [15:21:42<35:54,  2.77s/batch]Batch 19000/20010 Done, mean position loss: 21.39683989286423\n",
      "Training growing_up:  97%|█████████▋| 19323/20010 [15:21:46<35:11,  3.07s/batch]Batch 19100/20010 Done, mean position loss: 22.596388330459597\n",
      "Training growing_up:  94%|███████▌| 18816/20010 [15:21:54<1:02:27,  3.14s/batch]Batch 19200/20010 Done, mean position loss: 22.59114191532135\n",
      "Training growing_up:  97%|█████████▋| 19356/20010 [15:21:54<36:52,  3.38s/batch]Batch 19300/20010 Done, mean position loss: 21.202780754566195\n",
      "Training growing_up:  95%|█████████▌| 19077/20010 [15:22:02<40:34,  2.61s/batch]Batch 19100/20010 Done, mean position loss: 22.59977737188339\n",
      "Training growing_up:  95%|█████████▌| 19067/20010 [15:22:02<51:51,  3.30s/batch]Batch 18900/20010 Done, mean position loss: 21.776957566738126\n",
      "Training growing_up:  95%|█████████▍| 18921/20010 [15:22:37<52:30,  2.89s/batch]Batch 19100/20010 Done, mean position loss: 21.66735287666321\n",
      "Training growing_up:  96%|█████████▌| 19250/20010 [15:22:44<34:39,  2.74s/batch]Batch 19300/20010 Done, mean position loss: 22.023088963031768\n",
      "Training growing_up:  96%|█████████▌| 19259/20010 [15:22:44<35:27,  2.83s/batch]Batch 18900/20010 Done, mean position loss: 21.199860644340514\n",
      "Training growing_up:  96%|█████████▌| 19138/20010 [15:22:49<43:57,  3.03s/batch]Batch 19300/20010 Done, mean position loss: 22.232985875606538\n",
      "Training growing_up:  95%|█████████▍| 18910/20010 [15:23:11<54:55,  3.00s/batch]Batch 19100/20010 Done, mean position loss: 22.32767322778702\n",
      "Training growing_up:  96%|█████████▌| 19238/20010 [15:23:18<35:49,  2.78s/batch]Batch 19400/20010 Done, mean position loss: 21.697721648216245\n",
      "Training growing_up:  95%|█████████▌| 19083/20010 [15:23:23<58:11,  3.77s/batch]Batch 19100/20010 Done, mean position loss: 21.88380264520645\n",
      "Training growing_up:  96%|█████████▌| 19252/20010 [15:23:25<39:07,  3.10s/batch]Batch 18800/20010 Done, mean position loss: 21.295280220508577\n",
      "Training growing_up:  95%|█████████▌| 19102/20010 [15:23:26<47:15,  3.12s/batch]Batch 19400/20010 Done, mean position loss: 22.431630127429962\n",
      "Training growing_up:  96%|█████████▌| 19139/20010 [15:23:41<46:36,  3.21s/batch]Batch 19300/20010 Done, mean position loss: 22.16165456056595\n",
      "Training growing_up:  96%|█████████▌| 19142/20010 [15:23:41<43:59,  3.04s/batch]Batch 19000/20010 Done, mean position loss: 22.133765902519226\n",
      "Training growing_up:  97%|█████████▋| 19321/20010 [15:23:45<34:48,  3.03s/batch]Batch 19100/20010 Done, mean position loss: 22.437033855915068\n",
      "Training growing_up:  97%|█████████▋| 19324/20010 [15:23:59<34:19,  3.00s/batch]Batch 19200/20010 Done, mean position loss: 21.481426827907562\n",
      "Training growing_up:  96%|█████████▌| 19132/20010 [15:24:08<41:57,  2.87s/batch]Batch 19400/20010 Done, mean position loss: 21.728608708381653\n",
      "Training growing_up:  97%|█████████▋| 19418/20010 [15:24:16<26:10,  2.65s/batch]Batch 19100/20010 Done, mean position loss: 22.32187999486923\n",
      "Training growing_up:  97%|█████████▋| 19317/20010 [15:24:27<36:20,  3.15s/batch]Batch 19300/20010 Done, mean position loss: 21.461535856723785\n",
      "Training growing_up:  96%|█████████▋| 19268/20010 [15:24:48<37:36,  3.04s/batch]Batch 19200/20010 Done, mean position loss: 22.417117714881897\n",
      "Training growing_up:  95%|█████████▍| 18942/20010 [15:24:51<50:17,  2.83s/batch]Batch 19300/20010 Done, mean position loss: 21.851271712779997\n",
      "Training growing_up:  97%|█████████▋| 19419/20010 [15:24:59<29:01,  2.95s/batch]Batch 19300/20010 Done, mean position loss: 22.430542681217194\n",
      "Training growing_up:  97%|█████████▋| 19361/20010 [15:24:59<29:05,  2.69s/batch]Batch 19200/20010 Done, mean position loss: 22.114839248657226\n",
      "Training growing_up:  94%|█████████▍| 18899/20010 [15:25:11<50:15,  2.71s/batch]Batch 19300/20010 Done, mean position loss: 21.26800491809845\n",
      "Training growing_up:  96%|█████████▋| 19279/20010 [15:25:17<33:50,  2.78s/batch]Batch 18900/20010 Done, mean position loss: 22.11471937179565\n",
      "Training growing_up:  94%|█████████▍| 18907/20010 [15:25:34<48:59,  2.66s/batch]Batch 19400/20010 Done, mean position loss: 21.498349192142484\n",
      "Training growing_up:  97%|█████████▋| 19448/20010 [15:25:39<31:08,  3.32s/batch]Batch 19600/20010 Done, mean position loss: 21.335878796577454\n",
      "Training growing_up:  94%|█████████▍| 18851/20010 [15:25:43<50:50,  2.63s/batch]Batch 19300/20010 Done, mean position loss: 21.96430224657059\n",
      "Training growing_up:  97%|█████████▋| 19381/20010 [15:25:53<28:55,  2.76s/batch]Batch 19200/20010 Done, mean position loss: 21.546176171302797\n",
      "Training growing_up:  97%|█████████▋| 19324/20010 [15:26:00<33:15,  2.91s/batch]Batch 18900/20010 Done, mean position loss: 22.02441992998123\n",
      "Training growing_up:  96%|█████████▌| 19225/20010 [15:26:04<38:20,  2.93s/batch]Batch 19300/20010 Done, mean position loss: 22.136509721279147\n",
      "Training growing_up:  97%|█████████▋| 19343/20010 [15:26:21<25:21,  2.28s/batch]Batch 19300/20010 Done, mean position loss: 21.674441545009614\n",
      "Training growing_up:  95%|█████████▍| 18924/20010 [15:26:21<46:26,  2.57s/batch]Batch 19000/20010 Done, mean position loss: 22.6142912197113\n",
      "Training growing_up:  97%|█████████▋| 19392/20010 [15:26:23<27:53,  2.71s/batch]Batch 19300/20010 Done, mean position loss: 22.351926176548005\n",
      "Training growing_up:  96%|█████████▌| 19212/20010 [15:26:25<39:27,  2.97s/batch]Batch 19300/20010 Done, mean position loss: 21.709905934333804\n",
      "Training growing_up:  97%|█████████▋| 19329/20010 [15:26:26<27:13,  2.40s/batch]Batch 19100/20010 Done, mean position loss: 21.324208154678345\n",
      "Training growing_up:  97%|█████████▋| 19361/20010 [15:26:28<29:02,  2.69s/batch]Batch 19500/20010 Done, mean position loss: 22.22003628253937\n",
      "Training growing_up:  96%|█████████▌| 19194/20010 [15:26:33<33:56,  2.50s/batch]Batch 19300/20010 Done, mean position loss: 22.476961545944214\n",
      "Training growing_up:  96%|█████████▌| 19216/20010 [15:26:35<35:04,  2.65s/batch]Batch 19200/20010 Done, mean position loss: 22.38478698015213\n",
      "Training growing_up:  96%|█████████▌| 19161/20010 [15:26:36<42:42,  3.02s/batch]Batch 19200/20010 Done, mean position loss: 22.514040398597714\n",
      "Training growing_up:  95%|█████████▌| 19069/20010 [15:26:47<43:22,  2.77s/batch]Batch 19400/20010 Done, mean position loss: 21.48268651008606\n",
      "Training growing_up:  97%|█████████▋| 19340/20010 [15:26:55<28:41,  2.57s/batch]Batch 19200/20010 Done, mean position loss: 22.282767767906186\n",
      "Training growing_up:  96%|█████████▌| 19210/20010 [15:26:59<36:16,  2.72s/batch]Batch 19000/20010 Done, mean position loss: 21.548703022003174\n",
      "Training growing_up:  97%|█████████▋| 19364/20010 [15:27:16<30:14,  2.81s/batch]Batch 19200/20010 Done, mean position loss: 21.795662076473235\n",
      "Training growing_up:  97%|█████████▋| 19471/20010 [15:27:20<23:45,  2.65s/batch]Batch 19400/20010 Done, mean position loss: 21.992626407146453\n",
      "Training growing_up:  97%|█████████▋| 19328/20010 [15:27:38<30:25,  2.68s/batch]Batch 19400/20010 Done, mean position loss: 22.310389556884765\n",
      "Training growing_up:  97%|█████████▋| 19388/20010 [15:27:43<30:31,  2.94s/batch]Batch 19000/20010 Done, mean position loss: 21.25380751132965\n",
      "Training growing_up:  96%|█████████▌| 19215/20010 [15:27:57<37:06,  2.80s/batch]Batch 19200/20010 Done, mean position loss: 22.544765400886536\n",
      "Training growing_up:  97%|█████████▋| 19337/20010 [15:28:03<30:08,  2.69s/batch]Batch 19500/20010 Done, mean position loss: 22.275712029933928\n",
      "Training growing_up:  96%|█████████▌| 19135/20010 [15:28:04<45:32,  3.12s/batch]Batch 19500/20010 Done, mean position loss: 21.706018176078796\n",
      "Training growing_up:  96%|█████████▋| 19276/20010 [15:28:08<31:59,  2.62s/batch]Batch 19200/20010 Done, mean position loss: 22.267790744304655\n",
      "Training growing_up:  95%|█████████▌| 19011/20010 [15:28:08<47:21,  2.84s/batch]Batch 18900/20010 Done, mean position loss: 21.337113630771633\n",
      "Training growing_up:  96%|█████████▋| 19275/20010 [15:28:20<33:04,  2.70s/batch]Batch 19400/20010 Done, mean position loss: 22.11954062461853\n",
      "Training growing_up:  96%|█████████▋| 19276/20010 [15:28:22<31:59,  2.61s/batch]Batch 19100/20010 Done, mean position loss: 22.160503406524658\n",
      "Training growing_up:  97%|█████████▋| 19389/20010 [15:28:29<30:46,  2.97s/batch]Batch 19200/20010 Done, mean position loss: 22.23413303852081\n",
      "Training growing_up:  98%|█████████▊| 19548/20010 [15:28:36<21:01,  2.73s/batch]Batch 19300/20010 Done, mean position loss: 21.538897001743315\n",
      "Training growing_up:  95%|█████████▌| 19038/20010 [15:28:46<46:28,  2.87s/batch]Batch 19500/20010 Done, mean position loss: 21.807654356956483\n",
      "Training growing_up:  97%|█████████▋| 19430/20010 [15:29:00<30:49,  3.19s/batch]Batch 19200/20010 Done, mean position loss: 22.074667079448698\n",
      "Training growing_up:  96%|█████████▌| 19252/20010 [15:29:03<34:05,  2.70s/batch]Batch 19400/20010 Done, mean position loss: 21.396596887111663\n",
      "Training growing_up:  98%|█████████▊| 19513/20010 [15:29:22<25:37,  3.09s/batch]Batch 19300/20010 Done, mean position loss: 22.348837630748747\n",
      "Training growing_up:  96%|█████████▌| 19228/20010 [15:29:26<41:15,  3.17s/batch]Batch 19400/20010 Done, mean position loss: 21.962234201431272\n",
      "Training growing_up:  97%|█████████▋| 19384/20010 [15:29:35<33:17,  3.19s/batch]Batch 19300/20010 Done, mean position loss: 21.807553009986876\n",
      "Training growing_up:  97%|█████████▋| 19378/20010 [15:29:40<32:36,  3.10s/batch]Batch 19400/20010 Done, mean position loss: 22.162638285160064\n",
      "Training growing_up:  97%|█████████▋| 19447/20010 [15:29:50<24:38,  2.63s/batch]Batch 19400/20010 Done, mean position loss: 21.186785135269165\n",
      "Training growing_up:  97%|█████████▋| 19437/20010 [15:30:05<27:21,  2.87s/batch]Batch 19000/20010 Done, mean position loss: 21.985734841823575\n",
      "Training growing_up:  97%|█████████▋| 19399/20010 [15:30:18<31:25,  3.09s/batch]Batch 19500/20010 Done, mean position loss: 21.58325808763504\n",
      "Training growing_up:  97%|█████████▋| 19400/20010 [15:30:21<31:33,  3.10s/batch]Batch 19700/20010 Done, mean position loss: 21.240600912570955\n",
      "Training growing_up:  98%|█████████▊| 19702/20010 [15:30:24<14:08,  2.76s/batch]Batch 19400/20010 Done, mean position loss: 21.736827857494355\n",
      "Training growing_up:  95%|█████████▌| 19076/20010 [15:30:42<48:15,  3.10s/batch]Batch 19300/20010 Done, mean position loss: 21.59532490491867\n",
      "Training growing_up:  97%|█████████▋| 19391/20010 [15:30:46<27:01,  2.62s/batch]Batch 19400/20010 Done, mean position loss: 22.17089225769043\n",
      "Training growing_up:  97%|█████████▋| 19470/20010 [15:30:59<27:06,  3.01s/batch]Batch 19000/20010 Done, mean position loss: 22.209549522399904\n",
      "Training growing_up:  99%|█████████▊| 19715/20010 [15:31:02<15:36,  3.17s/batch]Batch 19400/20010 Done, mean position loss: 21.56695037841797\n",
      "Training growing_up:  96%|█████████▋| 19263/20010 [15:31:04<40:27,  3.25s/batch]Batch 19100/20010 Done, mean position loss: 22.229161229133606\n",
      "Training growing_up:  96%|█████████▌| 19246/20010 [15:31:10<38:15,  3.00s/batch]Batch 19600/20010 Done, mean position loss: 21.97458467721939\n",
      "Training growing_up:  97%|█████████▋| 19447/20010 [15:31:16<27:42,  2.95s/batch]Batch 19400/20010 Done, mean position loss: 22.329526481628417\n",
      "Training growing_up:  97%|█████████▋| 19430/20010 [15:31:18<28:42,  2.97s/batch]Batch 19400/20010 Done, mean position loss: 21.694977824687957\n",
      "Training growing_up:  95%|█████████▍| 18963/20010 [15:31:18<51:06,  2.93s/batch]Batch 19400/20010 Done, mean position loss: 22.34136293411255\n",
      "Training growing_up:  95%|█████████▍| 18964/20010 [15:31:21<51:42,  2.97s/batch]Batch 19200/20010 Done, mean position loss: 21.521754803657533\n",
      "Training growing_up:  97%|█████████▋| 19441/20010 [15:31:24<31:11,  3.29s/batch]Batch 19300/20010 Done, mean position loss: 22.295204586982727\n",
      "Training growing_up:  95%|█████████▌| 19077/20010 [15:31:30<48:19,  3.11s/batch]Batch 19300/20010 Done, mean position loss: 22.40274510383606\n",
      "Training growing_up:  97%|█████████▋| 19413/20010 [15:31:38<27:57,  2.81s/batch]Batch 19500/20010 Done, mean position loss: 21.51654520273209\n",
      "Training growing_up:  95%|█████████▌| 19037/20010 [15:31:50<46:05,  2.84s/batch]Batch 19300/20010 Done, mean position loss: 22.321052169799806\n",
      "Training growing_up:  97%|█████████▋| 19425/20010 [15:31:58<27:16,  2.80s/batch]Batch 19100/20010 Done, mean position loss: 21.597979621887205\n",
      "Training growing_up:  97%|█████████▋| 19457/20010 [15:32:11<28:34,  3.10s/batch]Batch 19500/20010 Done, mean position loss: 21.999024999141692\n",
      "Training growing_up:  97%|█████████▋| 19422/20010 [15:32:16<30:16,  3.09s/batch]Batch 19300/20010 Done, mean position loss: 21.75530066013336\n",
      "Training growing_up:  97%|█████████▋| 19363/20010 [15:32:34<38:33,  3.58s/batch]Batch 19500/20010 Done, mean position loss: 22.38913470029831\n",
      "Training growing_up:  97%|█████████▋| 19448/20010 [15:32:46<33:34,  3.59s/batch]Batch 19100/20010 Done, mean position loss: 21.37035073518753\n",
      "Training growing_up:  98%|█████████▊| 19600/20010 [15:32:58<20:17,  2.97s/batch]Batch 19300/20010 Done, mean position loss: 22.50220419406891\n",
      "Training growing_up:  97%|█████████▋| 19462/20010 [15:33:01<33:01,  3.62s/batch]Batch 19600/20010 Done, mean position loss: 21.630637137889863\n",
      "Training growing_up:  96%|█████████▌| 19138/20010 [15:33:03<47:04,  3.24s/batch]Batch 19600/20010 Done, mean position loss: 22.21042502641678\n",
      "Training growing_up:  96%|█████████▌| 19142/20010 [15:33:16<45:30,  3.15s/batch]Batch 19300/20010 Done, mean position loss: 22.021991312503815\n",
      "Training growing_up:  98%|█████████▊| 19642/20010 [15:33:21<20:32,  3.35s/batch]Batch 19000/20010 Done, mean position loss: 21.221336805820464\n",
      "Training growing_up:  98%|█████████▊| 19517/20010 [15:33:28<25:40,  3.12s/batch]Batch 19200/20010 Done, mean position loss: 22.109195907115936\n",
      "Training growing_up:  97%|█████████▋| 19452/20010 [15:33:28<31:44,  3.41s/batch]Batch 19500/20010 Done, mean position loss: 22.181061835289\n",
      "Training growing_up:  97%|█████████▋| 19357/20010 [15:33:37<30:05,  2.76s/batch]Batch 19300/20010 Done, mean position loss: 22.221635382175446\n",
      "Training growing_up:  95%|█████████▌| 19054/20010 [15:33:44<45:15,  2.84s/batch]Batch 19400/20010 Done, mean position loss: 21.57144447088242\n",
      "Training growing_up:  97%|█████████▋| 19339/20010 [15:33:52<32:14,  2.88s/batch]Batch 19600/20010 Done, mean position loss: 21.726318962574005\n",
      "Training growing_up:  96%|█████████▌| 19124/20010 [15:33:59<44:35,  3.02s/batch]Batch 19500/20010 Done, mean position loss: 21.70011460542679\n",
      "Training growing_up:  97%|█████████▋| 19310/20010 [15:34:04<36:02,  3.09s/batch]Batch 19300/20010 Done, mean position loss: 22.12004847764969\n",
      "Training growing_up:  95%|█████████▌| 19067/20010 [15:34:27<50:10,  3.19s/batch]Batch 19500/20010 Done, mean position loss: 21.73192459821701\n",
      "Training growing_up:  95%|█████████▌| 19024/20010 [15:34:34<49:57,  3.04s/batch]Batch 19400/20010 Done, mean position loss: 22.3511674785614\n",
      "Training growing_up:  97%|█████████▋| 19475/20010 [15:34:41<26:48,  3.01s/batch]Batch 19400/20010 Done, mean position loss: 21.98811285018921\n",
      "Training growing_up:  97%|█████████▋| 19379/20010 [15:34:43<30:04,  2.86s/batch]Batch 19500/20010 Done, mean position loss: 22.440011084079742\n",
      "Training growing_up:  98%|█████████▊| 19596/20010 [15:35:05<22:28,  3.26s/batch]Batch 19500/20010 Done, mean position loss: 21.446484098434446\n",
      "Training growing_up:  98%|█████████▊| 19570/20010 [15:35:12<20:34,  2.81s/batch]Batch 19100/20010 Done, mean position loss: 21.99375625371933\n",
      "Training growing_up:  96%|█████████▌| 19165/20010 [15:35:19<41:06,  2.92s/batch]Batch 19600/20010 Done, mean position loss: 21.421790890693664\n",
      "Training growing_up:  98%|█████████▊| 19652/20010 [15:35:30<15:59,  2.68s/batch]Batch 19800/20010 Done, mean position loss: 21.21608141183853\n",
      "Training growing_up:  98%|█████████▊| 19576/20010 [15:35:32<24:15,  3.35s/batch]Batch 19500/20010 Done, mean position loss: 21.68153778076172\n",
      "Training growing_up:  98%|█████████▊| 19516/20010 [15:35:50<26:09,  3.18s/batch]Batch 19400/20010 Done, mean position loss: 21.666735191345214\n",
      "Training growing_up:  96%|█████████▋| 19290/20010 [15:36:01<34:32,  2.88s/batch]Batch 19500/20010 Done, mean position loss: 22.16322791337967\n",
      "Training growing_up:  98%|█████████▊| 19522/20010 [15:36:08<23:57,  2.95s/batch]Batch 19500/20010 Done, mean position loss: 21.50441123008728\n",
      "Training growing_up:  98%|█████████▊| 19513/20010 [15:36:08<24:31,  2.96s/batch]Batch 19100/20010 Done, mean position loss: 22.073855459690094\n",
      "Training growing_up:  95%|█████████▌| 19103/20010 [15:36:14<44:03,  2.91s/batch]Batch 19200/20010 Done, mean position loss: 22.270969333648683\n",
      "Training growing_up:  97%|█████████▋| 19377/20010 [15:36:18<31:54,  3.02s/batch]Batch 19700/20010 Done, mean position loss: 22.339533264636994\n",
      "Training growing_up:  98%|█████████▊| 19666/20010 [15:36:18<15:59,  2.79s/batch]Batch 19500/20010 Done, mean position loss: 22.33668700933456\n",
      "Training growing_up:  97%|█████████▋| 19362/20010 [15:36:20<32:38,  3.02s/batch]Batch 19500/20010 Done, mean position loss: 21.70485026359558\n",
      "Training growing_up:  98%|█████████▊| 19588/20010 [15:36:30<20:49,  2.96s/batch]Batch 19500/20010 Done, mean position loss: 22.54539702415466\n",
      "Training growing_up:  97%|█████████▋| 19382/20010 [15:36:33<31:23,  3.00s/batch]Batch 19400/20010 Done, mean position loss: 22.254081580638886\n",
      "Training growing_up:  96%|█████████▌| 19189/20010 [15:36:35<40:46,  2.98s/batch]Batch 19300/20010 Done, mean position loss: 21.40098777294159\n",
      "Training growing_up:  98%|█████████▊| 19676/20010 [15:36:39<15:19,  2.75s/batch]Batch 19400/20010 Done, mean position loss: 22.57382312297821\n",
      "Training growing_up:  97%|█████████▋| 19442/20010 [15:36:48<29:29,  3.12s/batch]Batch 19600/20010 Done, mean position loss: 21.390371494293213\n",
      "Training growing_up:  98%|█████████▊| 19519/20010 [15:37:01<24:02,  2.94s/batch]Batch 19400/20010 Done, mean position loss: 22.43473061561584\n",
      "Training growing_up:  97%|█████████▋| 19361/20010 [15:37:09<35:49,  3.31s/batch]Batch 19600/20010 Done, mean position loss: 21.967682387828823\n",
      "Training growing_up:  98%|█████████▊| 19518/20010 [15:37:12<24:48,  3.03s/batch]Batch 19200/20010 Done, mean position loss: 21.48954667329788\n",
      "Training growing_up:  98%|█████████▊| 19527/20010 [15:37:27<25:19,  3.15s/batch]Batch 19400/20010 Done, mean position loss: 21.71546425104141\n",
      "Training growing_up:  95%|█████████▌| 19084/20010 [15:37:43<46:55,  3.04s/batch]Batch 19600/20010 Done, mean position loss: 22.49214147090912\n",
      "Training growing_up:  98%|█████████▊| 19569/20010 [15:37:51<20:34,  2.80s/batch]Batch 19200/20010 Done, mean position loss: 21.27914741039276\n",
      "Training growing_up:  98%|█████████▊| 19593/20010 [15:37:56<21:25,  3.08s/batch]Batch 19700/20010 Done, mean position loss: 22.179664590358733\n",
      "Training growing_up:  98%|█████████▊| 19533/20010 [15:38:03<21:21,  2.69s/batch]Batch 19700/20010 Done, mean position loss: 21.542801251411436\n",
      "Training growing_up:  98%|█████████▊| 19543/20010 [15:38:13<21:25,  2.75s/batch]Batch 19400/20010 Done, mean position loss: 22.370834012031555\n",
      "Training growing_up:  97%|█████████▋| 19434/20010 [15:38:20<28:04,  2.92s/batch]Batch 19600/20010 Done, mean position loss: 22.051494314670563\n",
      "Training growing_up:  98%|█████████▊| 19690/20010 [15:38:23<16:42,  3.13s/batch]Batch 19400/20010 Done, mean position loss: 21.77559197664261\n",
      "Training growing_up:  99%|█████████▊| 19710/20010 [15:38:30<16:16,  3.26s/batch]Batch 19300/20010 Done, mean position loss: 22.037040376663207\n",
      "Training growing_up:  98%|█████████▊| 19619/20010 [15:38:35<19:17,  2.96s/batch]Batch 19100/20010 Done, mean position loss: 21.31748892784119\n",
      "Training growing_up:  95%|█████████▌| 19104/20010 [15:38:44<48:12,  3.19s/batch]Batch 19500/20010 Done, mean position loss: 21.61409177303314\n",
      "Training growing_up:  98%|█████████▊| 19668/20010 [15:38:47<16:52,  2.96s/batch]Batch 19400/20010 Done, mean position loss: 22.276126449108123\n",
      "Training growing_up:  98%|█████████▊| 19552/20010 [15:38:56<22:53,  3.00s/batch]Batch 19700/20010 Done, mean position loss: 21.73813466310501\n",
      "Training growing_up:  97%|█████████▋| 19487/20010 [15:39:01<25:28,  2.92s/batch]Batch 19600/20010 Done, mean position loss: 21.653657190799713\n",
      "Training growing_up:  99%|█████████▉| 19876/20010 [15:39:18<06:55,  3.10s/batch]Batch 19400/20010 Done, mean position loss: 22.152521028518677\n",
      "Training growing_up:  97%|█████████▋| 19425/20010 [15:39:25<29:41,  3.05s/batch]Batch 19600/20010 Done, mean position loss: 21.856516411304476\n",
      "Training growing_up:  97%|█████████▋| 19361/20010 [15:39:43<34:51,  3.22s/batch]Batch 19500/20010 Done, mean position loss: 22.456409215927124\n",
      "Training growing_up:  97%|█████████▋| 19420/20010 [15:39:44<30:42,  3.12s/batch]Batch 19500/20010 Done, mean position loss: 21.96098874807358\n",
      "Training growing_up:  98%|█████████▊| 19632/20010 [15:39:52<19:05,  3.03s/batch]Batch 19600/20010 Done, mean position loss: 22.134473371505738\n",
      "Training growing_up:  99%|█████████▊| 19744/20010 [15:40:07<11:26,  2.58s/batch]Batch 19600/20010 Done, mean position loss: 21.582211878299717\n",
      "Training growing_up:  97%|█████████▋| 19441/20010 [15:40:13<28:33,  3.01s/batch]Batch 19200/20010 Done, mean position loss: 22.347811760902406\n",
      "Training growing_up:  99%|█████████▊| 19751/20010 [15:40:22<11:38,  2.70s/batch]Batch 19700/20010 Done, mean position loss: 21.44291917800903\n",
      "Training growing_up:  98%|█████████▊| 19658/20010 [15:40:27<16:22,  2.79s/batch]Batch 19900/20010 Done, mean position loss: 21.54656586647034\n",
      "Training growing_up:  97%|█████████▋| 19477/20010 [15:40:28<23:27,  2.64s/batch]Batch 19600/20010 Done, mean position loss: 21.953021371364592\n",
      "Training growing_up:  98%|█████████▊| 19676/20010 [15:40:48<14:19,  2.57s/batch]Batch 19500/20010 Done, mean position loss: 21.61560852766037\n",
      "Training growing_up:  98%|█████████▊| 19633/20010 [15:40:57<15:52,  2.53s/batch]Batch 19600/20010 Done, mean position loss: 22.172521584033966\n",
      "Training growing_up:  98%|█████████▊| 19550/20010 [15:41:04<20:37,  2.69s/batch]Batch 19600/20010 Done, mean position loss: 21.408894515037538\n",
      "Training growing_up:  97%|█████████▋| 19389/20010 [15:41:07<27:07,  2.62s/batch]Batch 19200/20010 Done, mean position loss: 22.200889780521393\n",
      "Training growing_up:  98%|█████████▊| 19529/20010 [15:41:09<25:26,  3.17s/batch]Batch 19300/20010 Done, mean position loss: 22.297692160606385\n",
      "Training growing_up:  98%|█████████▊| 19616/20010 [15:41:11<18:12,  2.77s/batch]Batch 19800/20010 Done, mean position loss: 22.14360004901886\n",
      "Training growing_up:  98%|█████████▊| 19639/20010 [15:41:15<19:59,  3.23s/batch]Batch 19600/20010 Done, mean position loss: 21.81683786869049\n",
      "Training growing_up:  96%|█████████▌| 19223/20010 [15:41:17<41:15,  3.15s/batch]Batch 19600/20010 Done, mean position loss: 22.215595831871035\n",
      "Training growing_up:  96%|█████████▌| 19225/20010 [15:41:23<37:44,  2.88s/batch]Batch 19600/20010 Done, mean position loss: 22.477913043498994\n",
      "Training growing_up:  98%|█████████▊| 19558/20010 [15:41:28<22:49,  3.03s/batch]Batch 19500/20010 Done, mean position loss: 22.087801547050475\n",
      "Training growing_up:  99%|█████████▊| 19728/20010 [15:41:39<13:13,  2.81s/batch]Batch 19500/20010 Done, mean position loss: 22.32921854019165\n",
      "Training growing_up:  98%|█████████▊| 19634/20010 [15:41:41<18:15,  2.91s/batch]Batch 19700/20010 Done, mean position loss: 21.38895778656006\n",
      "Training growing_up:  98%|█████████▊| 19564/20010 [15:41:46<21:37,  2.91s/batch]Batch 19400/20010 Done, mean position loss: 21.44515716791153\n",
      "Training growing_up:  98%|█████████▊| 19665/20010 [15:42:01<15:26,  2.69s/batch]Batch 19700/20010 Done, mean position loss: 22.01269486427307\n",
      "Training growing_up:  99%|█████████▊| 19736/20010 [15:42:02<13:03,  2.86s/batch]Batch 19500/20010 Done, mean position loss: 22.4072371172905\n",
      "Training growing_up:  98%|█████████▊| 19554/20010 [15:42:18<20:59,  2.76s/batch]Batch 19300/20010 Done, mean position loss: 21.660303235054016\n",
      "Training growing_up:  98%|█████████▊| 19519/20010 [15:42:21<23:45,  2.90s/batch]Batch 19500/20010 Done, mean position loss: 21.68239397287369\n",
      "Training growing_up:  98%|█████████▊| 19518/20010 [15:42:29<25:39,  3.13s/batch]Batch 19700/20010 Done, mean position loss: 22.529138224124907\n",
      "Training growing_up:  99%|█████████▉| 19800/20010 [15:42:47<09:56,  2.84s/batch]Batch 19800/20010 Done, mean position loss: 22.108528501987458\n",
      "Training growing_up:  99%|█████████▉| 19833/20010 [15:42:49<08:16,  2.81s/batch]Batch 19300/20010 Done, mean position loss: 21.499418911933898\n",
      "Training growing_up:  98%|█████████▊| 19682/20010 [15:42:50<16:33,  3.03s/batch]Batch 19800/20010 Done, mean position loss: 21.46166928768158\n",
      "Training growing_up:  98%|█████████▊| 19637/20010 [15:43:05<17:54,  2.88s/batch]Batch 19500/20010 Done, mean position loss: 22.379079148769378\n",
      "Training growing_up:  96%|█████████▋| 19261/20010 [15:43:10<38:46,  3.11s/batch]Batch 19700/20010 Done, mean position loss: 22.154049212932588\n",
      "Training growing_up:  99%|█████████▊| 19720/20010 [15:43:21<13:52,  2.87s/batch]Batch 19500/20010 Done, mean position loss: 21.944485592842103\n",
      "Training growing_up:  98%|█████████▊| 19554/20010 [15:43:21<21:21,  2.81s/batch]Batch 19400/20010 Done, mean position loss: 21.987220141887665\n",
      "Training growing_up:  99%|█████████▉| 19816/20010 [15:43:32<10:12,  3.16s/batch]Batch 19600/20010 Done, mean position loss: 21.86838703393936\n",
      "Training growing_up:  97%|█████████▋| 19317/20010 [15:43:34<34:27,  2.98s/batch]Batch 19200/20010 Done, mean position loss: 21.322727880477906\n",
      "Training growing_up:  98%|█████████▊| 19699/20010 [15:43:42<15:31,  2.99s/batch]Batch 19500/20010 Done, mean position loss: 22.2300294137001\n",
      "Training growing_up:  98%|█████████▊| 19657/20010 [15:43:43<16:47,  2.85s/batch]Batch 19800/20010 Done, mean position loss: 21.753463699817658\n",
      "Training growing_up:  97%|█████████▋| 19494/20010 [15:43:47<24:06,  2.80s/batch]Batch 19700/20010 Done, mean position loss: 21.698722143173217\n",
      "Training growing_up:  98%|█████████▊| 19658/20010 [15:44:08<17:59,  3.07s/batch]Batch 19500/20010 Done, mean position loss: 21.919377992153166\n",
      "Training growing_up:  99%|█████████▉| 19832/20010 [15:44:20<08:36,  2.90s/batch]Batch 19700/20010 Done, mean position loss: 21.77835735321045\n",
      "Training growing_up:  99%|█████████▉| 19762/20010 [15:44:33<12:30,  3.03s/batch]Batch 19600/20010 Done, mean position loss: 22.416282105445863\n",
      "Training growing_up:  98%|█████████▊| 19511/20010 [15:44:39<26:14,  3.16s/batch]Batch 19600/20010 Done, mean position loss: 22.010931506156922\n",
      "Training growing_up:  98%|█████████▊| 19605/20010 [15:44:45<19:22,  2.87s/batch]Batch 19700/20010 Done, mean position loss: 22.21326011657715\n",
      "Training growing_up:  97%|█████████▋| 19467/20010 [15:44:56<28:36,  3.16s/batch]Batch 19700/20010 Done, mean position loss: 21.375777664184568\n",
      "Training growing_up:  99%|█████████▉| 19849/20010 [15:45:09<07:21,  2.74s/batch]Batch 19800/20010 Done, mean position loss: 21.493661832809448\n",
      "Training growing_up:  98%|█████████▊| 19683/20010 [15:45:10<17:28,  3.21s/batch]Batch 20000/20010 Done, mean position loss: 21.28571848630905\n",
      "Training growing_up:  96%|█████████▌| 19234/20010 [15:45:15<33:21,  2.58s/batch]Batch 19700/20010 Done, mean position loss: 21.84994568824768\n",
      "Training growing_up:  98%|█████████▊| 19560/20010 [15:45:15<22:29,  3.00s/batch]Batch 19300/20010 Done, mean position loss: 22.264968259334566\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:45:36<00:00,  2.84s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19621/20010 [15:45:39<20:38,  3.18s/batch]Batch 19600/20010 Done, mean position loss: 21.633632996082305\n",
      "Training growing_up:  99%|█████████▉| 19865/20010 [15:45:54<06:21,  2.63s/batch]Batch 19700/20010 Done, mean position loss: 22.129358100891114\n",
      "Training growing_up:  98%|█████████▊| 19582/20010 [15:46:04<24:07,  3.38s/batch]Batch 19700/20010 Done, mean position loss: 21.397551090717315\n",
      "Training growing_up:  99%|█████████▉| 19791/20010 [15:46:05<11:10,  3.06s/batch]Batch 19300/20010 Done, mean position loss: 22.088653616905212\n",
      "Training growing_up:  98%|█████████▊| 19593/20010 [15:46:07<22:46,  3.28s/batch]Batch 19400/20010 Done, mean position loss: 22.263595671653746\n",
      "Training growing_up:  99%|█████████▉| 19785/20010 [15:46:11<12:07,  3.23s/batch]Batch 19900/20010 Done, mean position loss: 22.115375385284423\n",
      "Training growing_up:  99%|█████████▊| 19720/20010 [15:46:12<15:12,  3.15s/batch]Batch 19700/20010 Done, mean position loss: 21.665425720214845\n",
      "Training growing_up:  97%|█████████▋| 19368/20010 [15:46:13<34:28,  3.22s/batch]Batch 19700/20010 Done, mean position loss: 22.218448905944825\n",
      "Training growing_up:  99%|█████████▊| 19733/20010 [15:46:16<13:28,  2.92s/batch]Batch 19700/20010 Done, mean position loss: 22.467245655059813\n",
      "Training growing_up:  99%|█████████▉| 19770/20010 [15:46:31<10:38,  2.66s/batch]Batch 19600/20010 Done, mean position loss: 22.265190098285675\n",
      "Training growing_up:  98%|█████████▊| 19709/20010 [15:46:37<15:39,  3.12s/batch]Batch 19800/20010 Done, mean position loss: 21.355273833274843\n",
      "Training growing_up:  98%|█████████▊| 19603/20010 [15:46:38<20:40,  3.05s/batch]Batch 19600/20010 Done, mean position loss: 22.273935625553133\n",
      "Training growing_up:  98%|█████████▊| 19664/20010 [15:46:40<16:54,  2.93s/batch]Batch 19500/20010 Done, mean position loss: 21.296827595233914\n",
      "Training growing_up:  99%|█████████▉| 19868/20010 [15:47:00<07:10,  3.03s/batch]Batch 19800/20010 Done, mean position loss: 22.063159215450288\n",
      "Training growing_up:  97%|█████████▋| 19509/20010 [15:47:04<24:12,  2.90s/batch]Batch 19600/20010 Done, mean position loss: 22.42692831516266\n",
      "Training growing_up:  98%|█████████▊| 19655/20010 [15:47:19<16:45,  2.83s/batch]Batch 19400/20010 Done, mean position loss: 21.491442856788634\n",
      "Training growing_up:  97%|█████████▋| 19390/20010 [15:47:19<31:40,  3.06s/batch]Batch 19600/20010 Done, mean position loss: 21.808001239299774\n",
      "Training growing_up:  99%|█████████▊| 19728/20010 [15:47:23<14:33,  3.10s/batch]Batch 19800/20010 Done, mean position loss: 22.47251347541809\n",
      "Training growing_up:  99%|█████████▉| 19807/20010 [15:47:40<08:55,  2.64s/batch]Batch 19900/20010 Done, mean position loss: 22.254791989326474\n",
      "Training growing_up:  99%|█████████▉| 19782/20010 [15:47:50<10:52,  2.86s/batch]Batch 19900/20010 Done, mean position loss: 21.564423792362213\n",
      "Training growing_up:  99%|█████████▉| 19783/20010 [15:47:52<10:03,  2.66s/batch]Batch 19400/20010 Done, mean position loss: 21.41512132167816\n",
      "Training growing_up:  99%|█████████▉| 19787/20010 [15:48:03<09:48,  2.64s/batch]Batch 19800/20010 Done, mean position loss: 22.06825224876404\n",
      "Training growing_up:  99%|█████████▊| 19738/20010 [15:48:05<13:22,  2.95s/batch]Batch 19600/20010 Done, mean position loss: 22.328989295959474\n",
      "Training growing_up:  99%|█████████▉| 19895/20010 [15:48:16<05:29,  2.86s/batch]Batch 19500/20010 Done, mean position loss: 22.026741745471956\n",
      "Training growing_up:  99%|█████████▉| 19763/20010 [15:48:20<11:31,  2.80s/batch]Batch 19600/20010 Done, mean position loss: 22.18785806417465\n",
      "Training growing_up:  98%|█████████▊| 19640/20010 [15:48:30<17:48,  2.89s/batch]Batch 19700/20010 Done, mean position loss: 21.57543332338333\n",
      "Training growing_up:  99%|█████████▉| 19768/20010 [15:48:33<11:20,  2.81s/batch]Batch 19900/20010 Done, mean position loss: 21.77301622629166\n",
      "Training growing_up:  98%|█████████▊| 19682/20010 [15:48:40<15:12,  2.78s/batch]Batch 19300/20010 Done, mean position loss: 21.506151683330536\n",
      "Training growing_up:  98%|█████████▊| 19683/20010 [15:48:44<16:41,  3.06s/batch]Batch 19800/20010 Done, mean position loss: 21.53809659719467\n",
      "Training growing_up:  98%|█████████▊| 19707/20010 [15:48:47<14:58,  2.97s/batch]Batch 19600/20010 Done, mean position loss: 22.269455270767214\n",
      "Training growing_up: 100%|█████████▉| 19958/20010 [15:49:05<02:32,  2.93s/batch]Batch 19600/20010 Done, mean position loss: 22.35646232843399\n",
      "Training growing_up:  98%|█████████▊| 19645/20010 [15:49:14<19:08,  3.15s/batch]Batch 19800/20010 Done, mean position loss: 21.84968718767166\n",
      "Training growing_up: 100%|█████████▉| 19942/20010 [15:49:37<03:13,  2.85s/batch]Batch 19700/20010 Done, mean position loss: 21.93805792093277\n",
      "Training growing_up:  99%|█████████▉| 19770/20010 [15:49:38<11:26,  2.86s/batch]Batch 19700/20010 Done, mean position loss: 22.270345096588137\n",
      "Training growing_up:  98%|█████████▊| 19702/20010 [15:49:41<14:58,  2.92s/batch]Batch 19800/20010 Done, mean position loss: 22.257987742424014\n",
      "Training growing_up:  98%|█████████▊| 19637/20010 [15:49:53<18:47,  3.02s/batch]Batch 19800/20010 Done, mean position loss: 21.41705237865448\n",
      "Training growing_up:  99%|█████████▉| 19843/20010 [15:50:06<08:18,  2.98s/batch]Batch 19900/20010 Done, mean position loss: 21.5000879406929\n",
      "Training growing_up:  98%|█████████▊| 19638/20010 [15:50:09<17:32,  2.83s/batch]Batch 19800/20010 Done, mean position loss: 21.828817930221554\n",
      "Training growing_up:  98%|█████████▊| 19658/20010 [15:50:16<17:46,  3.03s/batch]Batch 19400/20010 Done, mean position loss: 22.007651696205137\n",
      "Training growing_up:  97%|█████████▋| 19490/20010 [15:50:31<28:29,  3.29s/batch]Batch 19700/20010 Done, mean position loss: 21.658405435085296\n",
      "Training growing_up:  99%|█████████▊| 19726/20010 [15:50:50<13:46,  2.91s/batch]Batch 19800/20010 Done, mean position loss: 22.011055970191954\n",
      "Training growing_up:  97%|█████████▋| 19397/20010 [15:51:00<31:16,  3.06s/batch]Batch 19800/20010 Done, mean position loss: 21.453639245033266\n",
      "Training growing_up:  98%|█████████▊| 19641/20010 [15:51:04<19:57,  3.25s/batch]Batch 19500/20010 Done, mean position loss: 22.3217106795311\n",
      "Training growing_up:  99%|█████████▉| 19864/20010 [15:51:08<07:14,  2.98s/batch]Batch 19800/20010 Done, mean position loss: 21.66950573682785\n",
      "Training growing_up:  99%|█████████▉| 19801/20010 [15:51:08<10:27,  3.00s/batch]Batch 19800/20010 Done, mean position loss: 22.21529112815857\n",
      "Training growing_up:  99%|█████████▉| 19884/20010 [15:51:08<06:21,  3.03s/batch]Batch 20000/20010 Done, mean position loss: 22.073699903488162\n",
      "Training growing_up:  98%|█████████▊| 19651/20010 [15:51:09<15:40,  2.62s/batch]Batch 19800/20010 Done, mean position loss: 22.27556699514389\n",
      "Training growing_up:  99%|█████████▉| 19865/20010 [15:51:12<07:52,  3.26s/batch]Batch 19400/20010 Done, mean position loss: 22.075961039066314\n",
      "Training growing_up: 100%|█████████▉| 20008/20010 [15:51:28<00:05,  2.69s/batch]Batch 19700/20010 Done, mean position loss: 22.28927407503128\n",
      "Training growing_up:  98%|█████████▊| 19702/20010 [15:51:31<14:21,  2.80s/batch]Batch 19900/20010 Done, mean position loss: 21.446977071762085\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:51:33<00:00,  2.85s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19816/20010 [15:51:34<08:54,  2.76s/batch]Batch 19700/20010 Done, mean position loss: 22.56673983097076\n",
      "Training growing_up:  99%|█████████▉| 19810/20010 [15:51:35<09:23,  2.82s/batch]Batch 19600/20010 Done, mean position loss: 21.307109863758086\n",
      "Training growing_up:  99%|█████████▉| 19818/20010 [15:51:59<10:03,  3.14s/batch]Batch 19700/20010 Done, mean position loss: 22.075953044891357\n",
      "Training growing_up:  97%|█████████▋| 19436/20010 [15:52:00<29:41,  3.10s/batch]Batch 19900/20010 Done, mean position loss: 21.90677111387253\n",
      "Training growing_up: 100%|█████████▉| 19998/20010 [15:52:17<00:34,  2.91s/batch]Batch 19900/20010 Done, mean position loss: 22.480900177955625\n",
      "Training growing_up:  98%|█████████▊| 19686/20010 [15:52:18<15:09,  2.81s/batch]Batch 19700/20010 Done, mean position loss: 21.83925587415695\n",
      "Training growing_up: 100%|█████████▉| 19995/20010 [15:52:20<00:41,  2.74s/batch]Batch 19500/20010 Done, mean position loss: 21.482696249485016\n",
      "Training growing_up:  98%|█████████▊| 19529/20010 [15:52:25<24:15,  3.03s/batch]Batch 20000/20010 Done, mean position loss: 22.339912309646607\n",
      "Training growing_up:  99%|█████████▉| 19908/20010 [15:52:37<05:13,  3.08s/batch]Batch 20000/20010 Done, mean position loss: 21.494612040519712\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:52:53<00:00,  2.86s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19596/20010 [15:52:56<19:40,  2.85s/batch]Batch 19500/20010 Done, mean position loss: 21.521063351631163\n",
      "Training growing_up:  99%|█████████▉| 19889/20010 [15:52:57<05:00,  2.49s/batch]Batch 19900/20010 Done, mean position loss: 22.05559397220612\n",
      "Training growing_up:  97%|█████████▋| 19439/20010 [15:52:59<29:09,  3.06s/batch]Batch 19700/20010 Done, mean position loss: 22.566936202049256\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:53:03<00:00,  2.86s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19881/20010 [15:53:09<06:06,  2.84s/batch]Batch 19600/20010 Done, mean position loss: 22.031246101856233\n",
      "Training growing_up: 100%|█████████▉| 19937/20010 [15:53:11<03:29,  2.87s/batch]Batch 19700/20010 Done, mean position loss: 22.16989677190781\n",
      "Training growing_up:  98%|█████████▊| 19547/20010 [15:53:17<22:03,  2.86s/batch]Batch 20000/20010 Done, mean position loss: 21.65128017425537\n",
      "Training growing_up:  99%|█████████▊| 19725/20010 [15:53:23<12:39,  2.66s/batch]Batch 19800/20010 Done, mean position loss: 21.722394721508028\n",
      "Training growing_up:  99%|█████████▉| 19782/20010 [15:53:30<11:37,  3.06s/batch]Batch 19700/20010 Done, mean position loss: 22.20116450071335\n",
      "Training growing_up: 100%|█████████▉| 19945/20010 [15:53:30<02:40,  2.48s/batch]Batch 19900/20010 Done, mean position loss: 21.816389396190644\n",
      "Training growing_up:  99%|█████████▉| 19853/20010 [15:53:31<07:47,  2.98s/batch]Batch 19400/20010 Done, mean position loss: 21.319161152839662\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:53:42<00:00,  2.86s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19938/20010 [15:53:54<02:41,  2.25s/batch]Batch 19700/20010 Done, mean position loss: 22.203852715492253\n",
      "Training growing_up: 100%|█████████▉| 19984/20010 [15:53:58<01:02,  2.39s/batch]Batch 19900/20010 Done, mean position loss: 21.813766129016877\n",
      "Training growing_up:  99%|█████████▉| 19869/20010 [15:54:16<06:23,  2.72s/batch]Batch 19800/20010 Done, mean position loss: 21.967381327152253\n",
      "Training growing_up: 100%|█████████▉| 19922/20010 [15:54:21<03:45,  2.57s/batch]Batch 19800/20010 Done, mean position loss: 22.298912196159364\n",
      "Training growing_up:  97%|█████████▋| 19489/20010 [15:54:22<22:27,  2.59s/batch]Batch 19900/20010 Done, mean position loss: 22.028118793964385\n",
      "Training growing_up:  99%|█████████▉| 19791/20010 [15:54:35<08:05,  2.22s/batch]Batch 19900/20010 Done, mean position loss: 21.420576102733612\n",
      "Training growing_up:  99%|█████████▉| 19878/20010 [15:54:39<05:22,  2.44s/batch]Batch 20000/20010 Done, mean position loss: 21.638374383449552\n",
      "Training growing_up: 100%|█████████▉| 19974/20010 [15:54:44<01:29,  2.49s/batch]Batch 19900/20010 Done, mean position loss: 21.751789441108702\n",
      "Training growing_up:  98%|█████████▊| 19585/20010 [15:54:50<16:05,  2.27s/batch]Batch 19500/20010 Done, mean position loss: 21.82798369407654\n",
      "Training growing_up:  99%|█████████▊| 19745/20010 [15:55:00<11:37,  2.63s/batch]Batch 19800/20010 Done, mean position loss: 21.57583621263504\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:55:03<00:00,  2.86s/batch]\n",
      "Done...\n",
      "Training growing_up:  97%|█████████▋| 19493/20010 [15:55:20<20:48,  2.41s/batch]Batch 19900/20010 Done, mean position loss: 21.94397632598877\n",
      "Training growing_up:  99%|█████████▉| 19811/20010 [15:55:26<08:49,  2.66s/batch]Batch 19900/20010 Done, mean position loss: 21.35176493883133\n",
      "Training growing_up:  98%|█████████▊| 19576/20010 [15:55:29<18:05,  2.50s/batch]Batch 19600/20010 Done, mean position loss: 22.349259824752806\n",
      "Training growing_up:  99%|█████████▉| 19907/20010 [15:55:34<04:12,  2.45s/batch]Batch 19900/20010 Done, mean position loss: 22.365968816280365\n",
      "Training growing_up:  98%|█████████▊| 19564/20010 [15:55:35<17:48,  2.40s/batch]Batch 19900/20010 Done, mean position loss: 21.683153200149537\n",
      "Training growing_up:  99%|█████████▉| 19835/20010 [15:55:38<06:43,  2.31s/batch]Batch 19900/20010 Done, mean position loss: 22.39487850666046\n",
      "Training growing_up: 100%|█████████▉| 19997/20010 [15:55:41<00:31,  2.46s/batch]Batch 19500/20010 Done, mean position loss: 22.02897287130356\n",
      "Training growing_up: 100%|█████████▉| 19937/20010 [15:55:47<02:41,  2.21s/batch]Batch 19800/20010 Done, mean position loss: 22.22778064489365\n",
      "Training growing_up:  99%|█████████▉| 19768/20010 [15:55:51<09:44,  2.42s/batch]Batch 20000/20010 Done, mean position loss: 21.532576663494112\n",
      "Training growing_up: 100%|█████████▉| 19987/20010 [15:55:54<00:55,  2.43s/batch]Batch 19700/20010 Done, mean position loss: 21.21001029253006\n",
      "Training growing_up: 100%|█████████▉| 19930/20010 [15:55:59<03:27,  2.59s/batch]Batch 19800/20010 Done, mean position loss: 22.44813090085983\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:56:11<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▊| 19710/20010 [15:56:13<09:29,  1.90s/batch]Batch 19800/20010 Done, mean position loss: 22.24272749185562\n",
      "Training growing_up: 100%|█████████▉| 19971/20010 [15:56:20<01:32,  2.38s/batch]Batch 20000/20010 Done, mean position loss: 22.030928678512574\n",
      "Training growing_up:  98%|█████████▊| 19541/20010 [15:56:29<18:27,  2.36s/batch]Batch 19600/20010 Done, mean position loss: 21.520431628227232\n",
      "Training growing_up:  98%|█████████▊| 19601/20010 [15:56:29<15:24,  2.26s/batch]Batch 20000/20010 Done, mean position loss: 22.504685332775118\n",
      "Training growing_up: 100%|█████████▉| 20002/20010 [15:56:31<00:19,  2.43s/batch]Batch 19800/20010 Done, mean position loss: 21.746760811805725\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:56:41<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:56:48<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▊| 19730/20010 [15:56:59<09:59,  2.14s/batch]Batch 20000/20010 Done, mean position loss: 22.169486951828002\n",
      "Training growing_up:  99%|█████████▊| 19733/20010 [15:57:05<08:42,  1.89s/batch]Batch 19600/20010 Done, mean position loss: 21.07610555410385\n",
      "Training growing_up: 100%|█████████▉| 19947/20010 [15:57:07<02:09,  2.05s/batch]Batch 19800/20010 Done, mean position loss: 22.501846563816073\n",
      "Training growing_up: 100%|█████████▉| 19973/20010 [15:57:11<01:16,  2.06s/batch]Batch 19800/20010 Done, mean position loss: 21.76889045238495\n",
      "Training growing_up: 100%|█████████▉| 20009/20010 [15:57:19<00:02,  2.44s/batch]Batch 19700/20010 Done, mean position loss: 21.883304936885835\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:57:21<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19949/20010 [15:57:21<01:50,  1.82s/batch]Batch 19900/20010 Done, mean position loss: 21.644999461174013\n",
      "Training growing_up: 100%|█████████▉| 19953/20010 [15:57:27<02:13,  2.34s/batch]Batch 20000/20010 Done, mean position loss: 21.640781152248383\n",
      "Training growing_up:  99%|█████████▉| 19810/20010 [15:57:31<07:20,  2.20s/batch]Batch 19500/20010 Done, mean position loss: 21.306305105686185\n",
      "Training growing_up: 100%|█████████▉| 19955/20010 [15:57:32<02:11,  2.40s/batch]Batch 19800/20010 Done, mean position loss: 22.225767424106596\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:57:46<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19855/20010 [15:57:51<05:43,  2.22s/batch]Batch 19800/20010 Done, mean position loss: 22.062036356925965\n",
      "Training growing_up:  98%|█████████▊| 19641/20010 [15:57:54<13:34,  2.21s/batch]Batch 20000/20010 Done, mean position loss: 21.67308284521103\n",
      "Training growing_up:  98%|█████████▊| 19587/20010 [15:58:08<13:24,  1.90s/batch]Batch 19900/20010 Done, mean position loss: 21.907122509479525\n",
      "Training growing_up:  99%|█████████▉| 19900/20010 [15:58:10<04:01,  2.19s/batch]Batch 20000/20010 Done, mean position loss: 22.109982528686523\n",
      "Training growing_up: 100%|█████████▉| 20002/20010 [15:58:12<00:15,  1.95s/batch]Batch 19900/20010 Done, mean position loss: 22.39245878696442\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:58:14<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19929/20010 [15:58:18<02:46,  2.06s/batch]Batch 20000/20010 Done, mean position loss: 21.519632380008694\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:58:28<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19868/20010 [15:58:33<04:35,  1.94s/batch]Batch 20000/20010 Done, mean position loss: 22.036186234951018\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:58:36<00:00,  2.87s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19877/20010 [15:58:36<04:26,  2.00s/batch]Batch 19600/20010 Done, mean position loss: 21.79819580078125\n",
      "Training growing_up: 100%|█████████▉| 19991/20010 [15:58:43<00:37,  1.96s/batch]Batch 19900/20010 Done, mean position loss: 21.570417568683624\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:58:51<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19856/20010 [15:58:57<04:55,  1.92s/batch]Batch 20000/20010 Done, mean position loss: 22.03705533504486\n",
      "Training growing_up:  99%|█████████▊| 19751/20010 [15:59:01<08:31,  1.97s/batch]Batch 20000/20010 Done, mean position loss: 21.458558971881867\n",
      "Training growing_up: 100%|█████████▉| 19997/20010 [15:59:02<00:24,  1.85s/batch]Batch 19700/20010 Done, mean position loss: 22.315361833572386\n",
      "Training growing_up:  99%|█████████▉| 19847/20010 [15:59:02<05:09,  1.90s/batch]Batch 20000/20010 Done, mean position loss: 22.29726610660553\n",
      "Training growing_up: 100%|█████████▉| 19933/20010 [15:59:08<02:14,  1.75s/batch]Batch 20000/20010 Done, mean position loss: 22.22909719944\n",
      "Training growing_up: 100%|█████████▉| 19931/20010 [15:59:09<02:21,  1.79s/batch]Batch 19600/20010 Done, mean position loss: 22.165579679012296\n",
      "Training growing_up:  98%|█████████▊| 19705/20010 [15:59:10<09:12,  1.81s/batch]Batch 20000/20010 Done, mean position loss: 21.57157011270523\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:59:14<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:59:16<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:59:18<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▊| 19711/20010 [15:59:21<08:49,  1.77s/batch]Batch 19800/20010 Done, mean position loss: 21.25410062551498\n",
      "Training growing_up:  99%|█████████▉| 19884/20010 [15:59:21<03:48,  1.81s/batch]Batch 19900/20010 Done, mean position loss: 22.06249045610428\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:59:24<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [15:59:25<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19563/20010 [15:59:31<12:21,  1.66s/batch]Batch 19900/20010 Done, mean position loss: 22.212287766933443\n",
      "Training growing_up:  99%|█████████▉| 19876/20010 [15:59:32<03:39,  1.64s/batch]Batch 19900/20010 Done, mean position loss: 22.330984134674075\n",
      "Training growing_up:  99%|█████████▉| 19881/20010 [15:59:46<03:05,  1.44s/batch]Batch 19700/20010 Done, mean position loss: 21.53634655714035\n",
      "Training growing_up: 100%|█████████▉| 19952/20010 [15:59:46<01:30,  1.55s/batch]Batch 19900/20010 Done, mean position loss: 21.793222234249114\n",
      "Training growing_up:  99%|█████████▉| 19792/20010 [16:00:09<05:35,  1.54s/batch]Batch 19900/20010 Done, mean position loss: 22.611476325988768\n",
      "Training growing_up:  98%|█████████▊| 19655/20010 [16:00:09<10:15,  1.73s/batch]Batch 19700/20010 Done, mean position loss: 21.22636887073517\n",
      "Training growing_up: 100%|█████████▉| 19972/20010 [16:00:17<00:56,  1.48s/batch]Batch 19900/20010 Done, mean position loss: 21.90336134195328\n",
      "Training growing_up: 100%|█████████▉| 19973/20010 [16:00:19<00:58,  1.58s/batch]Batch 20000/20010 Done, mean position loss: 21.453156409263613\n",
      "Training growing_up: 100%|█████████▉| 19981/20010 [16:00:23<00:42,  1.47s/batch]Batch 19800/20010 Done, mean position loss: 21.941678154468537\n",
      "Training growing_up: 100%|█████████▉| 19929/20010 [16:00:28<02:20,  1.74s/batch]Batch 19900/20010 Done, mean position loss: 22.268443541526793\n",
      "Training growing_up:  98%|█████████▊| 19648/20010 [16:00:30<10:13,  1.69s/batch]Batch 19600/20010 Done, mean position loss: 21.358068625926972\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:00:32<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19609/20010 [16:00:42<10:51,  1.63s/batch]Batch 19900/20010 Done, mean position loss: 21.90492909193039\n",
      "Training growing_up:  99%|█████████▉| 19771/20010 [16:00:55<05:36,  1.41s/batch]Batch 20000/20010 Done, mean position loss: 22.131709468364715\n",
      "Training growing_up:  98%|█████████▊| 19689/20010 [16:01:04<08:23,  1.57s/batch]Batch 20000/20010 Done, mean position loss: 22.337942728996275\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:07<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:15<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19935/20010 [16:01:15<01:03,  1.18batch/s]Batch 19700/20010 Done, mean position loss: 22.104670548439024\n",
      "Training growing_up: 100%|█████████▉| 19925/20010 [16:01:18<01:58,  1.40s/batch]Batch 20000/20010 Done, mean position loss: 21.5060626745224\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:29<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19644/20010 [16:01:35<07:49,  1.28s/batch]Batch 19800/20010 Done, mean position loss: 22.46388938188553\n",
      "Training growing_up: 100%|█████████▉| 19971/20010 [16:01:40<00:36,  1.07batch/s]Batch 20000/20010 Done, mean position loss: 22.258421108722686\n",
      "Training growing_up:  99%|█████████▉| 19862/20010 [16:01:43<01:49,  1.35batch/s]Batch 20000/20010 Done, mean position loss: 22.015903704166412\n",
      "Training growing_up:  98%|█████████▊| 19652/20010 [16:01:45<07:10,  1.20s/batch]Batch 19700/20010 Done, mean position loss: 22.10428268671036\n",
      "Training growing_up:  99%|█████████▉| 19865/20010 [16:01:46<02:19,  1.04batch/s]Batch 19900/20010 Done, mean position loss: 21.43966459035873\n",
      "Training growing_up: 100%|█████████▉| 20008/20010 [16:01:48<00:02,  1.27s/batch]Batch 20000/20010 Done, mean position loss: 22.31803274869919\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:51<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:52<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:01:54<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19999/20010 [16:01:57<00:10,  1.03batch/s]Batch 19800/20010 Done, mean position loss: 21.353281791210172\n",
      "Training growing_up:  99%|█████████▉| 19782/20010 [16:01:59<03:54,  1.03s/batch]Batch 20000/20010 Done, mean position loss: 22.009024732112884\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:02:06<00:00,  2.88s/batch]\n",
      "Done...\n",
      "Training growing_up:  98%|█████████▊| 19674/20010 [16:02:08<05:29,  1.02batch/s]Batch 20000/20010 Done, mean position loss: 22.35212030887604\n",
      "Training growing_up: 100%|█████████▉| 19998/20010 [16:02:16<00:11,  1.08batch/s]Batch 19800/20010 Done, mean position loss: 21.139940078258515\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:02:16<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19760/20010 [16:02:19<03:59,  1.04batch/s]Batch 20000/20010 Done, mean position loss: 21.895715305805204\n",
      "Training growing_up:  98%|█████████▊| 19689/20010 [16:02:20<04:10,  1.28batch/s]Batch 19900/20010 Done, mean position loss: 21.963570079803468\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:02:26<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▉| 19856/20010 [16:02:27<01:26,  1.77batch/s]Batch 20000/20010 Done, mean position loss: 22.372659113407135\n",
      "Training growing_up:  99%|█████████▉| 19818/20010 [16:02:29<02:59,  1.07batch/s]Batch 19700/20010 Done, mean position loss: 21.132380931377412\n",
      "Training growing_up:  99%|█████████▊| 19751/20010 [16:02:33<03:41,  1.17batch/s]Batch 20000/20010 Done, mean position loss: 21.902436437606813\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:02:33<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:02:37<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up:  99%|█████████▊| 19731/20010 [16:02:46<02:03,  2.26batch/s]Batch 19800/20010 Done, mean position loss: 21.99146615743637\n",
      "Training growing_up:  99%|█████████▉| 19787/20010 [16:02:52<01:56,  1.91batch/s]Batch 19900/20010 Done, mean position loss: 22.339166688919068\n",
      "Training growing_up: 100%|█████████▉| 19972/20010 [16:02:58<00:19,  1.96batch/s]Batch 20000/20010 Done, mean position loss: 21.264699501991274\n",
      "Training growing_up: 100%|█████████▉| 19914/20010 [16:02:59<00:48,  1.99batch/s]Batch 19800/20010 Done, mean position loss: 22.202089006900785\n",
      "Training growing_up: 100%|█████████▉| 19982/20010 [16:03:03<00:12,  2.17batch/s]Batch 19900/20010 Done, mean position loss: 21.35815838336945\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:03:03<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19913/20010 [16:03:08<00:38,  2.52batch/s]Batch 19900/20010 Done, mean position loss: 21.100496282577517\n",
      "Training growing_up:  99%|█████████▉| 19855/20010 [16:03:11<01:06,  2.33batch/s]Batch 20000/20010 Done, mean position loss: 21.987977883815766\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:03:15<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19923/20010 [16:03:19<00:34,  2.50batch/s]Batch 19800/20010 Done, mean position loss: 21.27722085237503\n",
      "Training growing_up: 100%|█████████▉| 19953/20010 [16:03:31<00:22,  2.50batch/s]Batch 19900/20010 Done, mean position loss: 22.0096390080452\n",
      "Training growing_up: 100%|█████████▉| 19910/20010 [16:03:35<00:36,  2.76batch/s]Batch 20000/20010 Done, mean position loss: 22.230050139427185\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:03:38<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19987/20010 [16:03:42<00:06,  3.34batch/s]Batch 20000/20010 Done, mean position loss: 21.46847202539444\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:03:46<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19945/20010 [16:03:47<00:20,  3.13batch/s]Batch 19900/20010 Done, mean position loss: 22.22118041753769\n",
      "Training growing_up:  99%|█████████▉| 19901/20010 [16:03:47<00:46,  2.32batch/s]Batch 20000/20010 Done, mean position loss: 21.019536468982697\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:03:50<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19970/20010 [16:03:54<00:11,  3.45batch/s]Batch 19900/20010 Done, mean position loss: 21.268514955043795\n",
      "Training growing_up: 100%|█████████▉| 19954/20010 [16:04:02<00:14,  3.76batch/s]Batch 20000/20010 Done, mean position loss: 21.706667332649232\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:04:06<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 19977/20010 [16:04:14<00:08,  3.89batch/s]Batch 20000/20010 Done, mean position loss: 21.954172687530516\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:04:17<00:00,  2.89s/batch]\n",
      "Done...\n",
      "Training growing_up: 100%|█████████▉| 20000/20010 [16:04:19<00:02,  4.76batch/s]Batch 20000/20010 Done, mean position loss: 21.279905591011048\n",
      "Training growing_up: 100%|██████████| 20010/20010 [16:04:22<00:00,  2.89s/batch]\n",
      "Done...\n",
      "model00...\n",
      "model01...\n",
      "model02...model03...\n",
      "\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...\n",
      "model13...\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training NF1:   1%|▏                    | 97/10001 [03:09<4:44:06,  1.72s/batch]Batch 100/10001 Done, mean position loss: 21.727416737079622\n",
      "Training NF1:   1%|▏                    | 98/10001 [03:10<5:14:16,  1.90s/batch]Batch 100/10001 Done, mean position loss: 21.3804709982872\n",
      "Training NF1:   1%|▏                   | 102/10001 [03:11<4:28:42,  1.63s/batch]Batch 100/10001 Done, mean position loss: 21.879877316951752\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:11<4:27:00,  1.62s/batch]Batch 100/10001 Done, mean position loss: 21.198754496574402\n",
      "Training NF1:   1%|▏                    | 97/10001 [03:11<4:59:50,  1.82s/batch]Batch 100/10001 Done, mean position loss: 21.231174454689025\n",
      "Training NF1:   1%|▏                    | 97/10001 [03:12<5:21:56,  1.95s/batch]Batch 100/10001 Done, mean position loss: 20.73070809841156\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:12<4:48:33,  1.75s/batch]Batch 100/10001 Done, mean position loss: 21.208483369350432\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:12<5:11:59,  1.89s/batch]Batch 100/10001 Done, mean position loss: 21.858874900341036\n",
      "Training NF1:   1%|▏                    | 98/10001 [03:13<5:14:09,  1.90s/batch]Batch 100/10001 Done, mean position loss: 20.930137004852295\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:13<4:59:11,  1.81s/batch]Batch 100/10001 Done, mean position loss: 21.065979356765748\n",
      "Training NF1:   1%|▏                   | 100/10001 [03:13<5:06:22,  1.86s/batch]Batch 100/10001 Done, mean position loss: 21.795249350070954\n",
      "Training NF1:   1%|▏                    | 98/10001 [03:14<5:00:46,  1.82s/batch]Batch 100/10001 Done, mean position loss: 21.236729638576506\n",
      "Training NF1:   1%|▏                   | 103/10001 [03:14<4:23:31,  1.60s/batch]Batch 100/10001 Done, mean position loss: 21.852767758369446\n",
      "Training NF1:   1%|▏                    | 98/10001 [03:15<4:40:09,  1.70s/batch]Batch 100/10001 Done, mean position loss: 21.171171925067902\n",
      "Training NF1:   1%|▏                    | 97/10001 [03:14<4:31:09,  1.64s/batch]Batch 100/10001 Done, mean position loss: 21.649374947547912\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:15<4:57:41,  1.80s/batch]Batch 100/10001 Done, mean position loss: 21.5655007147789\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:15<4:40:29,  1.70s/batch]Batch 100/10001 Done, mean position loss: 21.28898593187332\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:16<4:55:29,  1.79s/batch]Batch 100/10001 Done, mean position loss: 21.73928674221039\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:15<4:33:51,  1.66s/batch]Batch 100/10001 Done, mean position loss: 21.380569562911987\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:15<4:55:01,  1.79s/batch]Batch 100/10001 Done, mean position loss: 21.326937415599822\n",
      "Training NF1:   1%|▏                   | 102/10001 [03:16<4:24:09,  1.60s/batch]Batch 100/10001 Done, mean position loss: 21.403143248558045\n",
      "Training NF1:   1%|▏                   | 103/10001 [03:16<4:40:14,  1.70s/batch]Batch 100/10001 Done, mean position loss: 20.941726548671724\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:16<4:52:54,  1.78s/batch]Batch 100/10001 Done, mean position loss: 21.526771674156187\n",
      "Training NF1:   1%|▏                   | 103/10001 [03:17<5:02:03,  1.83s/batch]Batch 100/10001 Done, mean position loss: 21.501831743717194\n",
      "Training NF1:   1%|▏                    | 97/10001 [03:18<4:56:19,  1.80s/batch]Batch 100/10001 Done, mean position loss: 21.507813684940338\n",
      "Training NF1:   1%|▏                   | 100/10001 [03:17<4:44:15,  1.72s/batch]Batch 100/10001 Done, mean position loss: 21.236201465129852\n",
      "Training NF1:   1%|▏                   | 104/10001 [03:17<4:27:27,  1.62s/batch]Batch 100/10001 Done, mean position loss: 21.555200815200806\n",
      "Training NF1:   1%|▏                    | 99/10001 [03:18<5:25:54,  1.97s/batch]Batch 100/10001 Done, mean position loss: 21.9241770529747\n",
      "Training NF1:   1%|▏                   | 102/10001 [03:18<4:37:54,  1.68s/batch]Batch 100/10001 Done, mean position loss: 21.824923980236054\n",
      "Training NF1:   1%|▏                   | 106/10001 [03:19<4:43:06,  1.72s/batch]Batch 100/10001 Done, mean position loss: 21.566672194004056\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:18<4:31:52,  1.65s/batch]Batch 100/10001 Done, mean position loss: 21.130299484729765\n",
      "Training NF1:   1%|▏                   | 104/10001 [03:20<4:52:56,  1.78s/batch]Batch 100/10001 Done, mean position loss: 21.75348792076111\n",
      "Training NF1:   1%|▏                   | 102/10001 [03:20<4:12:54,  1.53s/batch]Batch 100/10001 Done, mean position loss: 21.936355731487275\n",
      "Training NF1:   1%|▏                   | 105/10001 [03:20<4:44:56,  1.73s/batch]Batch 100/10001 Done, mean position loss: 21.63007530450821\n",
      "Training NF1:   1%|▏                   | 103/10001 [03:20<4:38:28,  1.69s/batch]Batch 100/10001 Done, mean position loss: 21.84726625919342\n",
      "Training NF1:   1%|▏                   | 105/10001 [03:21<4:49:31,  1.76s/batch]Batch 100/10001 Done, mean position loss: 21.298889949321747\n",
      "Training NF1:   1%|▏                   | 108/10001 [03:22<4:17:37,  1.56s/batch]Batch 100/10001 Done, mean position loss: 21.386290378570557\n",
      "Training NF1:   1%|▏                   | 101/10001 [03:21<5:10:11,  1.88s/batch]Batch 100/10001 Done, mean position loss: 20.926285285949707\n",
      "Training NF1:   1%|▏                   | 108/10001 [03:23<4:29:50,  1.64s/batch]Batch 100/10001 Done, mean position loss: 21.609216599464418\n",
      "Training NF1:   1%|▏                   | 110/10001 [03:26<3:56:11,  1.43s/batch]Batch 100/10001 Done, mean position loss: 22.198187346458433\n",
      "Training NF1:   2%|▍                   | 192/10001 [05:49<5:13:08,  1.92s/batch]Batch 200/10001 Done, mean position loss: 21.704972076416013\n",
      "Training NF1:   2%|▍                   | 199/10001 [05:54<3:11:47,  1.17s/batch]Batch 200/10001 Done, mean position loss: 21.18138731956482\n",
      "Training NF1:   2%|▍                   | 196/10001 [05:56<4:13:41,  1.55s/batch]Batch 200/10001 Done, mean position loss: 21.161145911216735\n",
      "Training NF1:   2%|▍                   | 197/10001 [05:57<3:54:40,  1.44s/batch]Batch 200/10001 Done, mean position loss: 20.748177070617675\n",
      "Training NF1:   2%|▍                   | 198/10001 [05:57<4:30:23,  1.65s/batch]Batch 200/10001 Done, mean position loss: 21.61989942789078\n",
      "Training NF1:   2%|▍                   | 195/10001 [05:57<5:10:57,  1.90s/batch]Batch 200/10001 Done, mean position loss: 20.716750197410583\n",
      "Training NF1:   2%|▍                   | 199/10001 [05:59<4:33:31,  1.67s/batch]Batch 200/10001 Done, mean position loss: 21.28509940624237\n",
      "Training NF1:   2%|▍                   | 198/10001 [05:59<4:42:51,  1.73s/batch]Batch 200/10001 Done, mean position loss: 21.14965632677078\n",
      "Training NF1:   2%|▍                   | 199/10001 [05:59<4:46:50,  1.76s/batch]Batch 200/10001 Done, mean position loss: 21.141428449153903\n",
      "Training NF1:   2%|▍                   | 195/10001 [05:59<4:11:31,  1.54s/batch]Batch 200/10001 Done, mean position loss: 21.02402545452118\n",
      "Training NF1:   2%|▍                   | 206/10001 [06:01<3:58:53,  1.46s/batch]Batch 200/10001 Done, mean position loss: 21.74004642009735\n",
      "Training NF1:   2%|▍                   | 200/10001 [06:02<4:10:28,  1.53s/batch]Batch 200/10001 Done, mean position loss: 21.01792551994324\n",
      "Training NF1:   2%|▍                   | 200/10001 [06:02<3:43:02,  1.37s/batch]Batch 200/10001 Done, mean position loss: 21.30757539272308\n",
      "Training NF1:   2%|▍                   | 196/10001 [06:03<5:33:32,  2.04s/batch]Batch 200/10001 Done, mean position loss: 21.16558344125748\n",
      "Training NF1:   2%|▍                   | 205/10001 [06:03<4:44:31,  1.74s/batch]Batch 200/10001 Done, mean position loss: 21.090497097969056\n",
      "Training NF1:   2%|▍                   | 200/10001 [06:03<4:40:37,  1.72s/batch]Batch 200/10001 Done, mean position loss: 20.959382066726683\n",
      "Batch 200/10001 Done, mean position loss: 21.600940060615542\n",
      "Batch 200/10001 Done, mean position loss: 21.472876386642458\n",
      "Training NF1:   2%|▍                   | 192/10001 [06:03<4:45:13,  1.74s/batch]Batch 200/10001 Done, mean position loss: 21.497209117412567\n",
      "Training NF1:   2%|▍                   | 201/10001 [06:03<5:12:59,  1.92s/batch]Batch 200/10001 Done, mean position loss: 21.337405591011045\n",
      "Training NF1:   2%|▍                   | 207/10001 [06:03<4:51:09,  1.78s/batch]Batch 200/10001 Done, mean position loss: 21.19667920589447\n",
      "Training NF1:   2%|▍                   | 202/10001 [06:04<4:20:39,  1.60s/batch]Batch 200/10001 Done, mean position loss: 21.50556156158447\n",
      "Training NF1:   2%|▍                   | 206/10001 [06:04<3:14:40,  1.19s/batch]Batch 200/10001 Done, mean position loss: 21.53410865068436\n",
      "Training NF1:   2%|▍                   | 201/10001 [06:04<4:16:22,  1.57s/batch]Batch 200/10001 Done, mean position loss: 21.69030944108963\n",
      "Training NF1:   2%|▍                   | 203/10001 [06:05<4:34:58,  1.68s/batch]Batch 200/10001 Done, mean position loss: 21.685586256980898\n",
      "Training NF1:   2%|▍                   | 199/10001 [06:04<4:32:20,  1.67s/batch]Batch 200/10001 Done, mean position loss: 21.167860391139982\n",
      "Training NF1:   2%|▍                   | 192/10001 [06:06<5:54:49,  2.17s/batch]Batch 200/10001 Done, mean position loss: 21.576005074977875\n",
      "Training NF1:   2%|▍                   | 207/10001 [06:07<5:07:46,  1.89s/batch]Batch 200/10001 Done, mean position loss: 21.93455016851425\n",
      "Training NF1:   2%|▍                   | 204/10001 [06:07<4:50:14,  1.78s/batch]Batch 200/10001 Done, mean position loss: 21.215017008781437\n",
      "Training NF1:   2%|▍                   | 201/10001 [06:08<4:38:34,  1.71s/batch]Batch 200/10001 Done, mean position loss: 20.861359405517582\n",
      "Training NF1:   2%|▍                   | 202/10001 [06:09<4:58:21,  1.83s/batch]Batch 200/10001 Done, mean position loss: 20.899532957077028\n",
      "Training NF1:   2%|▍                   | 214/10001 [06:09<4:54:32,  1.81s/batch]Batch 200/10001 Done, mean position loss: 21.208639371395112\n",
      "Training NF1:   2%|▍                   | 199/10001 [06:10<6:09:18,  2.26s/batch]Batch 200/10001 Done, mean position loss: 21.64224735498428\n",
      "Training NF1:   2%|▍                   | 204/10001 [06:10<6:32:48,  2.41s/batch]Batch 200/10001 Done, mean position loss: 21.293906824588774\n",
      "Training NF1:   2%|▍                   | 204/10001 [06:11<5:55:06,  2.17s/batch]Batch 200/10001 Done, mean position loss: 21.414239416122435\n",
      "Training NF1:   2%|▍                   | 203/10001 [06:14<5:44:44,  2.11s/batch]Batch 200/10001 Done, mean position loss: 21.672407376766202\n",
      "Training NF1:   2%|▍                   | 206/10001 [06:17<4:05:40,  1.50s/batch]Batch 200/10001 Done, mean position loss: 21.68398415327072\n",
      "Training NF1:   2%|▍                   | 209/10001 [06:19<4:12:26,  1.55s/batch]Batch 200/10001 Done, mean position loss: 22.055725841522214\n",
      "Training NF1:   2%|▍                   | 203/10001 [06:20<4:34:44,  1.68s/batch]Batch 200/10001 Done, mean position loss: 21.317280972003935\n",
      "Training NF1:   2%|▍                   | 208/10001 [06:22<4:48:40,  1.77s/batch]Batch 200/10001 Done, mean position loss: 21.355914030075073\n",
      "Training NF1:   3%|▌                   | 283/10001 [08:30<4:35:58,  1.70s/batch]Batch 300/10001 Done, mean position loss: 21.666468901634214\n",
      "Training NF1:   3%|▌                   | 294/10001 [08:39<5:08:54,  1.91s/batch]Batch 300/10001 Done, mean position loss: 21.106594009399416\n",
      "Training NF1:   3%|▌                   | 293/10001 [08:43<5:02:12,  1.87s/batch]Batch 300/10001 Done, mean position loss: 21.100306165218356\n",
      "Training NF1:   3%|▌                   | 292/10001 [08:44<4:33:34,  1.69s/batch]Batch 300/10001 Done, mean position loss: 20.74350850582123\n",
      "Training NF1:   3%|▌                   | 297/10001 [08:46<4:14:09,  1.57s/batch]Batch 300/10001 Done, mean position loss: 21.459565811157226\n",
      "Training NF1:   3%|▌                   | 301/10001 [08:47<4:51:18,  1.80s/batch]Batch 300/10001 Done, mean position loss: 21.70478641986847\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:47<4:41:46,  1.74s/batch]Batch 300/10001 Done, mean position loss: 20.679580318927766\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:47<4:28:09,  1.66s/batch]Batch 300/10001 Done, mean position loss: 21.61234802246094\n",
      "Training NF1:   3%|▌                   | 287/10001 [08:49<4:35:09,  1.70s/batch]Batch 300/10001 Done, mean position loss: 21.548717036247254\n",
      "Training NF1:   3%|▌                   | 299/10001 [08:48<5:21:52,  1.99s/batch]Batch 300/10001 Done, mean position loss: 21.108734710216524\n",
      "Training NF1:   3%|▌                   | 298/10001 [08:48<4:40:39,  1.74s/batch]Batch 300/10001 Done, mean position loss: 21.62415555715561\n",
      "Training NF1:   3%|▌                   | 297/10001 [08:49<4:44:20,  1.76s/batch]Batch 300/10001 Done, mean position loss: 21.570473339557648\n",
      "Training NF1:   3%|▌                   | 298/10001 [08:51<4:11:39,  1.56s/batch]Batch 300/10001 Done, mean position loss: 21.137845611572267\n",
      "Training NF1:   3%|▌                   | 308/10001 [08:51<3:37:42,  1.35s/batch]Batch 300/10001 Done, mean position loss: 21.05324376583099\n",
      "Training NF1:   3%|▌                   | 301/10001 [08:51<4:40:41,  1.74s/batch]Batch 300/10001 Done, mean position loss: 20.91435821056366\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:51<4:34:13,  1.70s/batch]Batch 300/10001 Done, mean position loss: 21.457297725677492\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:52<4:01:47,  1.50s/batch]Batch 300/10001 Done, mean position loss: 21.15577935934067\n",
      "Training NF1:   3%|▌                   | 301/10001 [08:52<4:37:58,  1.72s/batch]Batch 300/10001 Done, mean position loss: 20.873197569847108\n",
      "Training NF1:   3%|▌                   | 302/10001 [08:52<4:01:23,  1.49s/batch]Batch 300/10001 Done, mean position loss: 21.46318846702576\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:53<4:44:18,  1.76s/batch]Batch 300/10001 Done, mean position loss: 21.23227565050125\n",
      "Training NF1:   3%|▌                   | 300/10001 [08:52<4:48:36,  1.79s/batch]Batch 300/10001 Done, mean position loss: 21.914925899505615\n",
      "Training NF1:   3%|▌                   | 304/10001 [08:53<4:11:25,  1.56s/batch]Batch 300/10001 Done, mean position loss: 21.115731630325318\n",
      "Training NF1:   3%|▌                   | 303/10001 [08:53<3:37:47,  1.35s/batch]Batch 300/10001 Done, mean position loss: 21.33336978197098\n",
      "Training NF1:   3%|▌                   | 301/10001 [08:54<4:26:54,  1.65s/batch]Batch 300/10001 Done, mean position loss: 21.243549292087557\n",
      "Training NF1:   3%|▌                   | 298/10001 [08:54<4:27:59,  1.66s/batch]Batch 300/10001 Done, mean position loss: 20.979555368423462\n",
      "Training NF1:   3%|▌                   | 309/10001 [08:54<4:49:35,  1.79s/batch]Batch 300/10001 Done, mean position loss: 21.114026429653165\n",
      "Training NF1:   3%|▌                   | 305/10001 [08:54<4:17:53,  1.60s/batch]Batch 300/10001 Done, mean position loss: 21.431689488887788\n",
      "Training NF1:   3%|▌                   | 310/10001 [08:55<4:33:05,  1.69s/batch]Batch 300/10001 Done, mean position loss: 20.855344750881194\n",
      "Training NF1:   3%|▌                   | 306/10001 [08:55<4:16:49,  1.59s/batch]Batch 300/10001 Done, mean position loss: 20.976805477142335\n",
      "Training NF1:   3%|▌                   | 306/10001 [08:56<4:17:54,  1.60s/batch]Batch 300/10001 Done, mean position loss: 21.47151168823242\n",
      "Training NF1:   3%|▌                   | 305/10001 [08:56<3:54:17,  1.45s/batch]Batch 300/10001 Done, mean position loss: 21.025112655162815\n",
      "Training NF1:   3%|▌                   | 307/10001 [08:57<4:12:24,  1.56s/batch]Batch 300/10001 Done, mean position loss: 21.265091137886046\n",
      "Training NF1:   3%|▌                   | 308/10001 [08:59<4:28:05,  1.66s/batch]Batch 300/10001 Done, mean position loss: 21.545365076065064\n",
      "Training NF1:   3%|▌                   | 305/10001 [08:59<5:08:13,  1.91s/batch]Batch 300/10001 Done, mean position loss: 21.2780757856369\n",
      "Training NF1:   3%|▋                   | 313/10001 [08:59<4:17:16,  1.59s/batch]Batch 300/10001 Done, mean position loss: 21.539242765903474\n",
      "Training NF1:   3%|▌                   | 299/10001 [09:02<4:34:45,  1.70s/batch]Batch 300/10001 Done, mean position loss: 21.194444608688357\n",
      "Training NF1:   3%|▌                   | 308/10001 [09:05<4:14:17,  1.57s/batch]Batch 300/10001 Done, mean position loss: 21.291659896373748\n",
      "Training NF1:   3%|▌                   | 311/10001 [09:05<4:07:07,  1.53s/batch]Batch 300/10001 Done, mean position loss: 22.058576838970183\n",
      "Training NF1:   3%|▌                   | 309/10001 [09:09<4:37:19,  1.72s/batch]Batch 300/10001 Done, mean position loss: 21.59792540550232\n",
      "Training NF1:   3%|▋                   | 313/10001 [09:15<4:27:47,  1.66s/batch]Batch 300/10001 Done, mean position loss: 21.220344626903533\n",
      "Training NF1:   4%|▊                   | 391/10001 [11:26<5:11:47,  1.95s/batch]Batch 400/10001 Done, mean position loss: 21.633308568000793\n",
      "Training NF1:   4%|▊                   | 392/10001 [11:29<5:28:35,  2.05s/batch]Batch 400/10001 Done, mean position loss: 21.029145214557644\n",
      "Training NF1:   4%|▊                   | 390/10001 [11:29<5:15:02,  1.97s/batch]Batch 400/10001 Done, mean position loss: 20.62958760261536\n",
      "Training NF1:   4%|▊                   | 397/10001 [11:32<4:25:17,  1.66s/batch]Batch 400/10001 Done, mean position loss: 21.051198534965515\n",
      "Training NF1:   4%|▊                   | 405/10001 [11:36<5:12:19,  1.95s/batch]Batch 400/10001 Done, mean position loss: 20.760056071281433\n",
      "Training NF1:   4%|▊                   | 390/10001 [11:37<5:30:34,  2.06s/batch]Batch 400/10001 Done, mean position loss: 21.49272757768631\n",
      "Training NF1:   4%|▊                   | 398/10001 [11:40<4:48:31,  1.80s/batch]Batch 400/10001 Done, mean position loss: 21.122623891830447\n",
      "Training NF1:   4%|▊                   | 403/10001 [11:40<5:09:24,  1.93s/batch]Batch 400/10001 Done, mean position loss: 21.56432490825653\n",
      "Training NF1:   4%|▊                   | 401/10001 [11:40<5:10:01,  1.94s/batch]Batch 400/10001 Done, mean position loss: 20.986889369487763\n",
      "Training NF1:   4%|▊                   | 398/10001 [11:41<4:34:52,  1.72s/batch]Batch 400/10001 Done, mean position loss: 21.064053637981417\n",
      "Batch 400/10001 Done, mean position loss: 21.54476778268814\n",
      "Training NF1:   4%|▊                   | 394/10001 [11:41<4:56:45,  1.85s/batch]Batch 400/10001 Done, mean position loss: 21.17704999923706\n",
      "Training NF1:   4%|▊                   | 409/10001 [11:42<4:11:44,  1.57s/batch]Batch 400/10001 Done, mean position loss: 21.343444285392764\n",
      "Training NF1:   4%|▊                   | 395/10001 [11:43<4:41:53,  1.76s/batch]Batch 400/10001 Done, mean position loss: 21.26272588968277\n",
      "Training NF1:   4%|▊                   | 388/10001 [11:43<5:48:40,  2.18s/batch]Batch 400/10001 Done, mean position loss: 21.457327737808228\n",
      "Training NF1:   4%|▊                   | 401/10001 [11:44<4:40:40,  1.75s/batch]Batch 400/10001 Done, mean position loss: 20.8578169798851\n",
      "Training NF1:   4%|▊                   | 391/10001 [11:45<4:04:12,  1.52s/batch]Batch 400/10001 Done, mean position loss: 21.109270038604734\n",
      "Training NF1:   4%|▊                   | 394/10001 [11:45<5:25:59,  2.04s/batch]Batch 400/10001 Done, mean position loss: 21.50364756822586\n",
      "Training NF1:   4%|▊                   | 400/10001 [11:45<5:04:19,  1.90s/batch]Batch 400/10001 Done, mean position loss: 20.955094101428983\n",
      "Training NF1:   4%|▊                   | 405/10001 [11:45<5:57:29,  2.24s/batch]Batch 400/10001 Done, mean position loss: 21.889515459537506\n",
      "Training NF1:   4%|▊                   | 404/10001 [11:45<4:48:15,  1.80s/batch]Batch 400/10001 Done, mean position loss: 21.41154480218887\n",
      "Training NF1:   4%|▊                   | 411/10001 [11:46<4:35:50,  1.73s/batch]Batch 400/10001 Done, mean position loss: 20.97796410560608\n",
      "Training NF1:   4%|▊                   | 402/10001 [11:47<4:26:31,  1.67s/batch]Batch 400/10001 Done, mean position loss: 20.85363495826721\n",
      "Training NF1:   4%|▊                   | 406/10001 [11:46<5:16:50,  1.98s/batch]Batch 400/10001 Done, mean position loss: 21.414291241168975\n",
      "Training NF1:   4%|▊                   | 405/10001 [11:48<4:17:38,  1.61s/batch]Batch 400/10001 Done, mean position loss: 21.691240625381468\n",
      "Training NF1:   4%|▊                   | 393/10001 [11:47<3:43:09,  1.39s/batch]Batch 400/10001 Done, mean position loss: 21.259919121265412\n",
      "Training NF1:   4%|▊                   | 403/10001 [11:49<4:32:15,  1.70s/batch]Batch 400/10001 Done, mean position loss: 21.05389695882797\n",
      "Training NF1:   4%|▊                   | 392/10001 [11:48<4:28:32,  1.68s/batch]Batch 400/10001 Done, mean position loss: 21.093332519531252\n",
      "Training NF1:   4%|▊                   | 400/10001 [11:48<4:38:23,  1.74s/batch]Batch 400/10001 Done, mean position loss: 21.475595638751983\n",
      "Training NF1:   4%|▊                   | 417/10001 [11:49<3:43:23,  1.40s/batch]Batch 400/10001 Done, mean position loss: 21.420032773017883\n",
      "Training NF1:   4%|▊                   | 407/10001 [11:51<4:20:36,  1.63s/batch]Batch 400/10001 Done, mean position loss: 20.829056963920593\n",
      "Training NF1:   4%|▊                   | 406/10001 [11:53<4:51:43,  1.82s/batch]Batch 400/10001 Done, mean position loss: 21.162128064632416\n",
      "Training NF1:   4%|▊                   | 409/10001 [11:52<4:00:39,  1.51s/batch]Batch 400/10001 Done, mean position loss: 21.47361085653305\n",
      "Training NF1:   4%|▊                   | 407/10001 [11:55<4:22:54,  1.64s/batch]Batch 400/10001 Done, mean position loss: 21.162049717903137\n",
      "Training NF1:   4%|▊                   | 409/10001 [11:58<4:06:09,  1.54s/batch]Batch 400/10001 Done, mean position loss: 21.259423675537107\n",
      "Training NF1:   4%|▊                   | 410/10001 [12:00<4:35:28,  1.72s/batch]Batch 400/10001 Done, mean position loss: 20.95902688264847\n",
      "Training NF1:   4%|▊                   | 415/10001 [12:01<4:24:17,  1.65s/batch]Batch 400/10001 Done, mean position loss: 21.574447977542874\n",
      "Training NF1:   4%|▊                   | 421/10001 [12:04<4:43:43,  1.78s/batch]Batch 400/10001 Done, mean position loss: 21.97795570373535\n",
      "Training NF1:   4%|▊                   | 417/10001 [12:06<4:22:45,  1.65s/batch]Batch 400/10001 Done, mean position loss: 21.23077067375183\n",
      "Training NF1:   4%|▊                   | 404/10001 [12:09<4:16:14,  1.60s/batch]Batch 400/10001 Done, mean position loss: 21.145354664325712\n",
      "Training NF1:   5%|▉                   | 489/10001 [14:16<5:10:48,  1.96s/batch]Batch 500/10001 Done, mean position loss: 21.613110389709473\n",
      "Training NF1:   5%|▉                   | 494/10001 [14:23<4:12:03,  1.59s/batch]Batch 500/10001 Done, mean position loss: 20.61651977539063\n",
      "Training NF1:   5%|▉                   | 497/10001 [14:25<4:43:54,  1.79s/batch]Batch 500/10001 Done, mean position loss: 21.009585733413694\n",
      "Training NF1:   5%|▉                   | 493/10001 [14:25<4:17:08,  1.62s/batch]Batch 500/10001 Done, mean position loss: 21.00058722257614\n",
      "Training NF1:   5%|█                   | 508/10001 [14:26<3:50:31,  1.46s/batch]Batch 500/10001 Done, mean position loss: 20.7439142537117\n",
      "Training NF1:   5%|▉                   | 499/10001 [14:28<4:20:58,  1.65s/batch]Batch 500/10001 Done, mean position loss: 20.933797242641447\n",
      "Training NF1:   5%|▉                   | 489/10001 [14:29<4:33:44,  1.73s/batch]Batch 500/10001 Done, mean position loss: 21.08382015943527\n",
      "Training NF1:   5%|▉                   | 493/10001 [14:30<5:02:10,  1.91s/batch]Batch 500/10001 Done, mean position loss: 21.52825806617737\n",
      "Training NF1:   5%|▉                   | 490/10001 [14:31<4:30:46,  1.71s/batch]Batch 500/10001 Done, mean position loss: 21.394673268795014\n",
      "Training NF1:   5%|▉                   | 498/10001 [14:31<4:23:51,  1.67s/batch]Batch 500/10001 Done, mean position loss: 21.473907639980315\n",
      "Training NF1:   5%|▉                   | 497/10001 [14:33<4:40:58,  1.77s/batch]Batch 500/10001 Done, mean position loss: 20.965287935733798\n",
      "Training NF1:   5%|█                   | 502/10001 [14:34<4:07:20,  1.56s/batch]Batch 500/10001 Done, mean position loss: 21.510649845600128\n",
      "Training NF1:   5%|▉                   | 493/10001 [14:33<4:30:39,  1.71s/batch]Batch 500/10001 Done, mean position loss: 21.098798677921295\n",
      "Training NF1:   5%|▉                   | 496/10001 [14:34<4:14:29,  1.61s/batch]Batch 500/10001 Done, mean position loss: 21.101576981544497\n",
      "Training NF1:   5%|▉                   | 500/10001 [14:35<4:29:37,  1.70s/batch]Batch 500/10001 Done, mean position loss: 21.03129610061645\n",
      "Training NF1:   5%|█                   | 507/10001 [14:35<5:24:10,  2.05s/batch]Batch 500/10001 Done, mean position loss: 21.125286161899567\n",
      "Training NF1:   5%|█                   | 506/10001 [14:35<4:13:44,  1.60s/batch]Batch 500/10001 Done, mean position loss: 21.233156883716582\n",
      "Training NF1:   5%|▉                   | 496/10001 [14:36<4:49:31,  1.83s/batch]Batch 500/10001 Done, mean position loss: 21.007250542640687\n",
      "Training NF1:   5%|▉                   | 494/10001 [14:35<4:44:04,  1.79s/batch]Batch 500/10001 Done, mean position loss: 20.812955303192137\n",
      "Training NF1:   5%|▉                   | 487/10001 [14:36<3:58:05,  1.50s/batch]Batch 500/10001 Done, mean position loss: 21.06796049594879\n",
      "Training NF1:   5%|▉                   | 491/10001 [14:37<4:45:52,  1.80s/batch]Batch 500/10001 Done, mean position loss: 21.84075451374054\n",
      "Training NF1:   5%|▉                   | 498/10001 [14:38<4:36:10,  1.74s/batch]Batch 500/10001 Done, mean position loss: 21.25873483657837\n",
      "Training NF1:   5%|▉                   | 492/10001 [14:38<4:40:54,  1.77s/batch]Batch 500/10001 Done, mean position loss: 21.40121448993683\n",
      "Training NF1:   5%|▉                   | 500/10001 [14:38<4:40:45,  1.77s/batch]Batch 500/10001 Done, mean position loss: 21.436310498714448\n",
      "Training NF1:   5%|█                   | 507/10001 [14:39<4:35:49,  1.74s/batch]Batch 500/10001 Done, mean position loss: 20.833466837406156\n",
      "Training NF1:   5%|█                   | 504/10001 [14:39<4:23:27,  1.66s/batch]Batch 500/10001 Done, mean position loss: 20.800955040454863\n",
      "Training NF1:   5%|█                   | 502/10001 [14:40<4:01:43,  1.53s/batch]Batch 500/10001 Done, mean position loss: 21.371801478862764\n",
      "Training NF1:   5%|█                   | 503/10001 [14:42<4:58:38,  1.89s/batch]Batch 500/10001 Done, mean position loss: 21.365519902706147\n",
      "Training NF1:   5%|█                   | 503/10001 [14:42<4:12:56,  1.60s/batch]Batch 500/10001 Done, mean position loss: 20.920016915798186\n",
      "Training NF1:   5%|█                   | 502/10001 [14:42<5:07:50,  1.94s/batch]Batch 500/10001 Done, mean position loss: 21.431634447574616\n",
      "Training NF1:   5%|█                   | 509/10001 [14:43<4:51:17,  1.84s/batch]Batch 500/10001 Done, mean position loss: 21.251142833232883\n",
      "Training NF1:   5%|▉                   | 498/10001 [14:44<4:09:18,  1.57s/batch]Batch 500/10001 Done, mean position loss: 21.1114794921875\n",
      "Training NF1:   5%|█                   | 506/10001 [14:46<3:40:35,  1.39s/batch]Batch 500/10001 Done, mean position loss: 21.230811452865602\n",
      "Training NF1:   5%|█                   | 510/10001 [14:49<4:08:39,  1.57s/batch]Batch 500/10001 Done, mean position loss: 21.388132944107056\n",
      "Training NF1:   5%|█                   | 504/10001 [14:50<4:55:35,  1.87s/batch]Batch 500/10001 Done, mean position loss: 21.66634699821472\n",
      "Training NF1:   5%|█                   | 510/10001 [14:51<4:25:46,  1.68s/batch]Batch 500/10001 Done, mean position loss: 20.902953367233277\n",
      "Training NF1:   5%|█                   | 513/10001 [14:52<3:59:27,  1.51s/batch]Batch 500/10001 Done, mean position loss: 21.511198706626892\n",
      "Training NF1:   5%|█                   | 513/10001 [14:55<4:25:09,  1.68s/batch]Batch 500/10001 Done, mean position loss: 21.937870681285858\n",
      "Training NF1:   5%|█                   | 507/10001 [15:00<4:23:48,  1.67s/batch]Batch 500/10001 Done, mean position loss: 21.178259325027465\n",
      "Training NF1:   5%|█                   | 522/10001 [15:12<5:07:11,  1.94s/batch]Batch 500/10001 Done, mean position loss: 21.122585699558257\n",
      "Training NF1:   6%|█▏                  | 585/10001 [17:04<4:13:34,  1.62s/batch]Batch 600/10001 Done, mean position loss: 21.524456481933594\n",
      "Training NF1:   6%|█▏                  | 571/10001 [17:11<4:02:42,  1.54s/batch]Batch 600/10001 Done, mean position loss: 20.605982909202574\n",
      "Training NF1:   6%|█▏                  | 596/10001 [17:11<4:48:56,  1.84s/batch]Batch 600/10001 Done, mean position loss: 20.966986029148103\n",
      "Training NF1:   6%|█▏                  | 603/10001 [17:13<4:10:12,  1.60s/batch]Batch 600/10001 Done, mean position loss: 21.43134126663208\n",
      "Training NF1:   6%|█▏                  | 609/10001 [17:17<4:34:44,  1.76s/batch]Batch 600/10001 Done, mean position loss: 20.980947184562684\n",
      "Training NF1:   6%|█▏                  | 584/10001 [17:17<4:56:53,  1.89s/batch]Batch 600/10001 Done, mean position loss: 21.075896706581112\n",
      "Training NF1:   6%|█▏                  | 602/10001 [17:19<4:23:14,  1.68s/batch]Batch 600/10001 Done, mean position loss: 20.89624158859253\n",
      "Training NF1:   6%|█▏                  | 596/10001 [17:19<4:24:45,  1.69s/batch]Batch 600/10001 Done, mean position loss: 20.714282076358796\n",
      "Training NF1:   6%|█▏                  | 594/10001 [17:19<3:33:49,  1.36s/batch]Batch 600/10001 Done, mean position loss: 21.49559460163116\n",
      "Training NF1:   6%|█▏                  | 577/10001 [17:22<4:54:34,  1.88s/batch]Batch 600/10001 Done, mean position loss: 21.059674589633943\n",
      "Training NF1:   6%|█▏                  | 599/10001 [17:23<4:17:06,  1.64s/batch]Batch 600/10001 Done, mean position loss: 21.05356499195099\n",
      "Training NF1:   6%|█▏                  | 600/10001 [17:24<4:30:49,  1.73s/batch]Batch 600/10001 Done, mean position loss: 20.828323423862457\n",
      "Training NF1:   6%|█▏                  | 600/10001 [17:25<4:21:22,  1.67s/batch]Batch 600/10001 Done, mean position loss: 21.01598578214645\n",
      "Training NF1:   6%|█▏                  | 602/10001 [17:25<3:56:31,  1.51s/batch]Batch 600/10001 Done, mean position loss: 21.790427560806272\n",
      "Training NF1:   6%|█▏                  | 598/10001 [17:25<4:02:38,  1.55s/batch]Batch 600/10001 Done, mean position loss: 21.352338294982907\n",
      "Training NF1:   6%|█▏                  | 603/10001 [17:26<4:06:50,  1.58s/batch]Batch 600/10001 Done, mean position loss: 21.37843858957291\n",
      "Training NF1:   6%|█▏                  | 610/10001 [17:26<4:50:42,  1.86s/batch]Batch 600/10001 Done, mean position loss: 20.942487723827362\n",
      "Training NF1:   6%|█▏                  | 615/10001 [17:27<4:25:45,  1.70s/batch]Batch 600/10001 Done, mean position loss: 21.48677148342133\n",
      "Training NF1:   6%|█▏                  | 601/10001 [17:27<4:25:43,  1.70s/batch]Batch 600/10001 Done, mean position loss: 21.106751108169558\n",
      "Training NF1:   6%|█▏                  | 602/10001 [17:27<3:57:27,  1.52s/batch]Batch 600/10001 Done, mean position loss: 20.786975064277648\n",
      "Training NF1:   6%|█▏                  | 591/10001 [17:28<4:08:07,  1.58s/batch]Batch 600/10001 Done, mean position loss: 20.975396327972412\n",
      "Training NF1:   6%|█▏                  | 603/10001 [17:30<3:40:53,  1.41s/batch]Batch 600/10001 Done, mean position loss: 21.243416726589203\n",
      "Training NF1:   6%|█▏                  | 598/10001 [17:30<4:52:45,  1.87s/batch]Batch 600/10001 Done, mean position loss: 21.38821135044098\n",
      "Training NF1:   6%|█▏                  | 604/10001 [17:31<4:55:50,  1.89s/batch]Batch 600/10001 Done, mean position loss: 20.785497322082517\n",
      "Training NF1:   6%|█▏                  | 591/10001 [17:31<4:59:42,  1.91s/batch]Batch 600/10001 Done, mean position loss: 21.19253357410431\n",
      "Training NF1:   6%|█▏                  | 606/10001 [17:32<5:06:51,  1.96s/batch]Batch 600/10001 Done, mean position loss: 21.025230896472934\n",
      "Training NF1:   6%|█▏                  | 602/10001 [17:33<4:21:44,  1.67s/batch]Batch 600/10001 Done, mean position loss: 21.36728211402893\n",
      "Training NF1:   6%|█▏                  | 595/10001 [17:35<4:36:04,  1.76s/batch]Batch 600/10001 Done, mean position loss: 21.215695121288302\n",
      "Training NF1:   6%|█▏                  | 599/10001 [17:36<4:35:17,  1.76s/batch]Batch 600/10001 Done, mean position loss: 21.312072222232818\n",
      "Training NF1:   6%|█▏                  | 613/10001 [17:37<4:12:04,  1.61s/batch]Batch 600/10001 Done, mean position loss: 21.368652188777922\n",
      "Batch 600/10001 Done, mean position loss: 20.901204373836517\n",
      "Training NF1:   6%|█▏                  | 614/10001 [17:39<4:09:03,  1.59s/batch]Batch 600/10001 Done, mean position loss: 21.22913780927658\n",
      "Training NF1:   6%|█▏                  | 612/10001 [17:39<4:22:35,  1.68s/batch]Batch 600/10001 Done, mean position loss: 21.460106792449952\n",
      "Training NF1:   6%|█▏                  | 617/10001 [17:41<4:15:22,  1.63s/batch]Batch 600/10001 Done, mean position loss: 21.05827283382416\n",
      "Training NF1:   6%|█▏                  | 616/10001 [17:43<4:07:33,  1.58s/batch]Batch 600/10001 Done, mean position loss: 21.349273164272308\n",
      "Training NF1:   6%|█▏                  | 612/10001 [17:46<4:52:52,  1.87s/batch]Batch 600/10001 Done, mean position loss: 21.929454135894773\n",
      "Training NF1:   6%|█▏                  | 611/10001 [17:46<4:36:32,  1.77s/batch]Batch 600/10001 Done, mean position loss: 20.86413141727448\n",
      "Training NF1:   6%|█▏                  | 614/10001 [17:47<4:12:12,  1.61s/batch]Batch 600/10001 Done, mean position loss: 21.621782402992245\n",
      "Training NF1:   6%|█▏                  | 622/10001 [17:48<4:27:16,  1.71s/batch]Batch 600/10001 Done, mean position loss: 21.132552967071533\n",
      "Training NF1:   6%|█▏                  | 611/10001 [18:03<4:39:57,  1.79s/batch]Batch 600/10001 Done, mean position loss: 21.11884541511536\n",
      "Training NF1:   7%|█▍                  | 688/10001 [19:49<3:53:45,  1.51s/batch]Batch 700/10001 Done, mean position loss: 20.93641489505768\n",
      "Training NF1:   7%|█▎                  | 670/10001 [19:50<4:48:18,  1.85s/batch]Batch 700/10001 Done, mean position loss: 21.48978909730911\n",
      "Training NF1:   7%|█▍                  | 706/10001 [20:00<4:55:34,  1.91s/batch]Batch 700/10001 Done, mean position loss: 21.064864172935486\n",
      "Training NF1:   7%|█▍                  | 689/10001 [20:01<3:48:15,  1.47s/batch]Batch 700/10001 Done, mean position loss: 21.38948342561722\n",
      "Training NF1:   7%|█▍                  | 688/10001 [20:03<4:27:40,  1.72s/batch]Batch 700/10001 Done, mean position loss: 20.57894980430603\n",
      "Training NF1:   7%|█▍                  | 693/10001 [20:06<4:26:19,  1.72s/batch]Batch 700/10001 Done, mean position loss: 20.706125769615173\n",
      "Training NF1:   7%|█▍                  | 698/10001 [20:06<5:10:59,  2.01s/batch]Batch 700/10001 Done, mean position loss: 20.940548598766327\n",
      "Training NF1:   7%|█▍                  | 692/10001 [20:07<4:49:49,  1.87s/batch]Batch 700/10001 Done, mean position loss: 20.853093240261078\n",
      "Training NF1:   7%|█▍                  | 698/10001 [20:11<5:02:39,  1.95s/batch]Batch 700/10001 Done, mean position loss: 20.959005770683287\n",
      "Training NF1:   7%|█▍                  | 704/10001 [20:11<4:08:35,  1.60s/batch]Batch 700/10001 Done, mean position loss: 21.44414377689362\n",
      "Training NF1:   7%|█▎                  | 686/10001 [20:11<4:06:25,  1.59s/batch]Batch 700/10001 Done, mean position loss: 21.422036855220796\n",
      "Training NF1:   7%|█▍                  | 701/10001 [20:11<4:28:44,  1.73s/batch]Batch 700/10001 Done, mean position loss: 21.000168266296388\n",
      "Training NF1:   7%|█▎                  | 684/10001 [20:15<5:05:02,  1.96s/batch]Batch 700/10001 Done, mean position loss: 21.765422632694246\n",
      "Training NF1:   7%|█▍                  | 698/10001 [20:15<5:10:59,  2.01s/batch]Batch 700/10001 Done, mean position loss: 21.03600790500641\n",
      "Training NF1:   7%|█▍                  | 695/10001 [20:17<4:03:37,  1.57s/batch]Batch 700/10001 Done, mean position loss: 20.807842688560488\n",
      "Training NF1:   7%|█▍                  | 697/10001 [20:16<4:00:38,  1.55s/batch]Batch 700/10001 Done, mean position loss: 21.309180340766908\n",
      "Training NF1:   7%|█▎                  | 685/10001 [20:17<4:36:51,  1.78s/batch]Batch 700/10001 Done, mean position loss: 20.91216754436493\n",
      "Training NF1:   7%|█▍                  | 690/10001 [20:18<4:09:42,  1.61s/batch]Batch 700/10001 Done, mean position loss: 20.75784326314926\n",
      "Batch 700/10001 Done, mean position loss: 21.316910424232482\n",
      "Training NF1:   7%|█▍                  | 702/10001 [20:19<4:11:32,  1.62s/batch]Batch 700/10001 Done, mean position loss: 20.793456733226776\n",
      "Training NF1:   7%|█▍                  | 706/10001 [20:19<4:22:31,  1.69s/batch]Batch 700/10001 Done, mean position loss: 21.004369575977325\n",
      "Training NF1:   7%|█▍                  | 698/10001 [20:21<4:28:15,  1.73s/batch]Batch 700/10001 Done, mean position loss: 21.072486906051637\n",
      "Training NF1:   7%|█▍                  | 706/10001 [20:23<4:28:50,  1.74s/batch]Batch 700/10001 Done, mean position loss: 21.156822957992553\n",
      "Training NF1:   7%|█▍                  | 699/10001 [20:23<4:31:37,  1.75s/batch]Batch 700/10001 Done, mean position loss: 21.256038148403167\n",
      "Batch 700/10001 Done, mean position loss: 20.99753445625305\n",
      "Training NF1:   7%|█▍                  | 717/10001 [20:26<3:57:14,  1.53s/batch]Batch 700/10001 Done, mean position loss: 21.348264360427855\n",
      "Training NF1:   7%|█▍                  | 705/10001 [20:27<4:30:29,  1.75s/batch]Batch 700/10001 Done, mean position loss: 21.158010203838348\n",
      "Training NF1:   7%|█▍                  | 716/10001 [20:27<4:16:39,  1.66s/batch]Batch 700/10001 Done, mean position loss: 21.307408347129822\n",
      "Training NF1:   7%|█▍                  | 710/10001 [20:27<4:24:23,  1.71s/batch]Batch 700/10001 Done, mean position loss: 21.293592104911802\n",
      "Training NF1:   7%|█▍                  | 702/10001 [20:30<5:18:15,  2.05s/batch]Batch 700/10001 Done, mean position loss: 21.88725222349167\n",
      "Training NF1:   7%|█▍                  | 703/10001 [20:31<4:23:53,  1.70s/batch]Batch 700/10001 Done, mean position loss: 20.872920196056366\n",
      "Training NF1:   7%|█▍                  | 717/10001 [20:30<4:30:35,  1.75s/batch]Batch 700/10001 Done, mean position loss: 21.369821023941043\n",
      "Training NF1:   7%|█▍                  | 706/10001 [20:33<4:08:17,  1.60s/batch]Batch 700/10001 Done, mean position loss: 21.091581614017485\n",
      "Training NF1:   7%|█▍                  | 700/10001 [20:33<4:06:38,  1.59s/batch]Batch 700/10001 Done, mean position loss: 21.316832773685455\n",
      "Training NF1:   7%|█▍                  | 715/10001 [20:35<4:30:05,  1.75s/batch]Batch 700/10001 Done, mean position loss: 21.193933906555174\n",
      "Training NF1:   7%|█▍                  | 709/10001 [20:36<4:32:21,  1.76s/batch]Batch 700/10001 Done, mean position loss: 21.453340930938722\n",
      "Training NF1:   7%|█▍                  | 722/10001 [20:36<4:10:42,  1.62s/batch]Batch 700/10001 Done, mean position loss: 21.031427536010742\n",
      "Training NF1:   7%|█▍                  | 715/10001 [20:42<4:33:47,  1.77s/batch]Batch 700/10001 Done, mean position loss: 21.56165360212326\n",
      "Training NF1:   7%|█▍                  | 723/10001 [20:48<3:46:05,  1.46s/batch]Batch 700/10001 Done, mean position loss: 20.840506207942962\n",
      "Training NF1:   7%|█▍                  | 717/10001 [21:01<4:09:39,  1.61s/batch]Batch 700/10001 Done, mean position loss: 21.082851412296296\n",
      "Training NF1:   8%|█▌                  | 757/10001 [22:34<3:48:36,  1.48s/batch]Batch 800/10001 Done, mean position loss: 20.90212165594101\n",
      "Training NF1:   8%|█▌                  | 785/10001 [22:41<4:37:19,  1.81s/batch]Batch 800/10001 Done, mean position loss: 21.444352917671203\n",
      "Training NF1:   8%|█▌                  | 776/10001 [22:43<4:21:03,  1.70s/batch]Batch 800/10001 Done, mean position loss: 20.714341042041777\n",
      "Training NF1:   8%|█▌                  | 796/10001 [22:45<4:04:41,  1.59s/batch]Batch 800/10001 Done, mean position loss: 21.020075843334197\n",
      "Training NF1:   8%|█▌                  | 793/10001 [22:49<4:09:23,  1.63s/batch]Batch 800/10001 Done, mean position loss: 20.830134739875792\n",
      "Training NF1:   8%|█▌                  | 789/10001 [22:50<4:40:35,  1.83s/batch]Batch 800/10001 Done, mean position loss: 21.370969569683073\n",
      "Training NF1:   8%|█▌                  | 794/10001 [22:52<4:02:16,  1.58s/batch]Batch 800/10001 Done, mean position loss: 21.406705548763277\n",
      "Batch 800/10001 Done, mean position loss: 20.567142641544343\n",
      "Training NF1:   8%|█▌                  | 792/10001 [22:53<4:05:17,  1.60s/batch]Batch 800/10001 Done, mean position loss: 20.917852795124055\n",
      "Training NF1:   8%|█▌                  | 785/10001 [22:54<3:59:12,  1.56s/batch]Batch 800/10001 Done, mean position loss: 20.917443695068357\n",
      "Training NF1:   8%|█▌                  | 786/10001 [22:54<4:14:36,  1.66s/batch]Batch 800/10001 Done, mean position loss: 21.379378209114073\n",
      "Training NF1:   8%|█▌                  | 788/10001 [22:56<3:58:48,  1.56s/batch]Batch 800/10001 Done, mean position loss: 20.914140810966494\n",
      "Training NF1:   8%|█▌                  | 806/10001 [23:02<3:59:28,  1.56s/batch]Batch 800/10001 Done, mean position loss: 21.31885740041733\n",
      "Training NF1:   8%|█▌                  | 806/10001 [23:03<4:00:40,  1.57s/batch]Batch 800/10001 Done, mean position loss: 21.276743600368498\n",
      "Training NF1:   8%|█▌                  | 793/10001 [23:04<3:55:00,  1.53s/batch]Batch 800/10001 Done, mean position loss: 20.984452748298644\n",
      "Training NF1:   8%|█▌                  | 806/10001 [23:05<4:32:37,  1.78s/batch]Batch 800/10001 Done, mean position loss: 20.771302332878115\n",
      "Training NF1:   8%|█▋                  | 821/10001 [23:04<3:30:04,  1.37s/batch]Batch 800/10001 Done, mean position loss: 21.708704612255097\n",
      "Training NF1:   8%|█▌                  | 803/10001 [23:07<4:15:12,  1.66s/batch]Batch 800/10001 Done, mean position loss: 20.96067652463913\n",
      "Training NF1:   8%|█▌                  | 792/10001 [23:07<4:02:29,  1.58s/batch]Batch 800/10001 Done, mean position loss: 20.716701543331148\n",
      "Training NF1:   8%|█▌                  | 810/10001 [23:08<3:50:44,  1.51s/batch]Batch 800/10001 Done, mean position loss: 20.961654703617093\n",
      "Training NF1:   8%|█▌                  | 800/10001 [23:10<5:59:43,  2.35s/batch]Batch 800/10001 Done, mean position loss: 21.031857533454897\n",
      "Training NF1:   8%|█▌                  | 811/10001 [23:11<4:17:33,  1.68s/batch]Batch 800/10001 Done, mean position loss: 21.123779785633086\n",
      "Batch 800/10001 Done, mean position loss: 20.982252044677736\n",
      "Training NF1:   8%|█▌                  | 797/10001 [23:11<4:43:47,  1.85s/batch]Batch 800/10001 Done, mean position loss: 21.879631340503693\n",
      "Training NF1:   8%|█▌                  | 795/10001 [23:11<4:56:21,  1.93s/batch]Batch 800/10001 Done, mean position loss: 21.28579810380936\n",
      "Training NF1:   8%|█▌                  | 795/10001 [23:12<4:59:41,  1.95s/batch]Batch 800/10001 Done, mean position loss: 20.76641790866852\n",
      "Training NF1:   8%|█▋                  | 813/10001 [23:13<4:49:29,  1.89s/batch]Batch 800/10001 Done, mean position loss: 21.33691640138626\n",
      "Training NF1:   8%|█▋                  | 817/10001 [23:14<4:37:04,  1.81s/batch]Batch 800/10001 Done, mean position loss: 21.122941558361056\n",
      "Training NF1:   8%|█▌                  | 795/10001 [23:18<4:15:03,  1.66s/batch]Batch 800/10001 Done, mean position loss: 21.409564616680147\n",
      "Training NF1:   8%|█▌                  | 810/10001 [23:19<4:29:59,  1.76s/batch]Batch 800/10001 Done, mean position loss: 20.86519424915314\n",
      "Training NF1:   8%|█▌                  | 802/10001 [23:21<4:02:06,  1.58s/batch]Batch 800/10001 Done, mean position loss: 21.220255472660064\n",
      "Training NF1:   8%|█▋                  | 817/10001 [23:21<4:27:36,  1.75s/batch]Batch 800/10001 Done, mean position loss: 21.30722841501236\n",
      "Training NF1:   8%|█▋                  | 822/10001 [23:22<4:35:48,  1.80s/batch]Batch 800/10001 Done, mean position loss: 21.153503069877623\n",
      "Training NF1:   8%|█▌                  | 812/10001 [23:22<4:12:50,  1.65s/batch]Batch 800/10001 Done, mean position loss: 21.257127275466917\n",
      "Training NF1:   8%|█▌                  | 809/10001 [23:24<3:36:23,  1.41s/batch]Batch 800/10001 Done, mean position loss: 21.29890221118927\n",
      "Training NF1:   8%|█▋                  | 825/10001 [23:25<4:13:02,  1.65s/batch]Batch 800/10001 Done, mean position loss: 21.05486751317978\n",
      "Training NF1:   8%|█▌                  | 803/10001 [23:26<4:19:52,  1.70s/batch]Batch 800/10001 Done, mean position loss: 21.54893316268921\n",
      "Training NF1:   8%|█▋                  | 824/10001 [23:28<4:29:48,  1.76s/batch]Batch 800/10001 Done, mean position loss: 20.98744522571564\n",
      "Training NF1:   8%|█▌                  | 809/10001 [23:30<4:08:55,  1.62s/batch]Batch 800/10001 Done, mean position loss: 20.803927221298217\n",
      "Training NF1:   8%|█▋                  | 835/10001 [23:48<4:26:26,  1.74s/batch]Batch 800/10001 Done, mean position loss: 21.055277686119076\n",
      "Training NF1:   9%|█▊                  | 882/10001 [25:21<3:57:13,  1.56s/batch]Batch 900/10001 Done, mean position loss: 20.869959752559662\n",
      "Training NF1:   9%|█▊                  | 887/10001 [25:29<4:13:48,  1.67s/batch]Batch 900/10001 Done, mean position loss: 21.445988767147064\n",
      "Training NF1:   9%|█▊                  | 900/10001 [25:30<3:32:30,  1.40s/batch]Batch 900/10001 Done, mean position loss: 20.685114250183105\n",
      "Training NF1:   9%|█▊                  | 889/10001 [25:31<3:18:37,  1.31s/batch]Batch 900/10001 Done, mean position loss: 21.0103063082695\n",
      "Training NF1:   9%|█▊                  | 890/10001 [25:35<4:22:10,  1.73s/batch]Batch 900/10001 Done, mean position loss: 20.883878271579743\n",
      "Training NF1:   9%|█▊                  | 883/10001 [25:35<4:21:49,  1.72s/batch]Batch 900/10001 Done, mean position loss: 21.367552607059476\n",
      "Training NF1:   9%|█▊                  | 888/10001 [25:37<5:03:38,  2.00s/batch]Batch 900/10001 Done, mean position loss: 21.38154523611069\n",
      "Training NF1:   9%|█▊                  | 891/10001 [25:38<4:18:41,  1.70s/batch]Batch 900/10001 Done, mean position loss: 20.81192982196808\n",
      "Training NF1:   9%|█▊                  | 885/10001 [25:39<3:59:55,  1.58s/batch]Batch 900/10001 Done, mean position loss: 20.55866708755493\n",
      "Training NF1:   9%|█▊                  | 905/10001 [25:43<4:01:51,  1.60s/batch]Batch 900/10001 Done, mean position loss: 21.337617547512053\n",
      "Training NF1:   9%|█▊                  | 902/10001 [25:44<3:44:10,  1.48s/batch]Batch 900/10001 Done, mean position loss: 20.86887893676758\n",
      "Training NF1:   9%|█▊                  | 897/10001 [25:49<3:58:21,  1.57s/batch]Batch 900/10001 Done, mean position loss: 20.908228037357333\n",
      "Training NF1:   9%|█▊                  | 892/10001 [25:50<4:29:10,  1.77s/batch]Batch 900/10001 Done, mean position loss: 20.981556539535525\n",
      "Training NF1:   9%|█▊                  | 898/10001 [25:51<4:02:34,  1.60s/batch]Batch 900/10001 Done, mean position loss: 20.743036832809448\n",
      "Training NF1:   9%|█▊                  | 908/10001 [25:51<4:17:53,  1.70s/batch]Batch 900/10001 Done, mean position loss: 21.675530118942262\n",
      "Training NF1:   9%|█▊                  | 921/10001 [25:52<4:06:35,  1.63s/batch]Batch 900/10001 Done, mean position loss: 20.698200523853302\n",
      "Training NF1:   9%|█▊                  | 900/10001 [25:53<4:28:43,  1.77s/batch]Batch 900/10001 Done, mean position loss: 20.951596875190734\n",
      "Training NF1:   9%|█▊                  | 914/10001 [25:53<4:22:23,  1.73s/batch]Batch 900/10001 Done, mean position loss: 20.962115931510922\n",
      "Training NF1:   9%|█▊                  | 891/10001 [25:54<4:13:34,  1.67s/batch]Batch 900/10001 Done, mean position loss: 20.931585166454315\n",
      "Training NF1:   9%|█▊                  | 915/10001 [25:54<4:29:54,  1.78s/batch]Batch 900/10001 Done, mean position loss: 21.321013181209565\n",
      "Training NF1:   9%|█▊                  | 904/10001 [25:55<3:44:47,  1.48s/batch]Batch 900/10001 Done, mean position loss: 21.249635689258575\n",
      "Training NF1:   9%|█▊                  | 916/10001 [25:56<3:58:47,  1.58s/batch]Batch 900/10001 Done, mean position loss: 21.235359823703767\n",
      "Training NF1:   9%|█▊                  | 895/10001 [25:56<4:22:32,  1.73s/batch]Batch 900/10001 Done, mean position loss: 21.086832439899446\n",
      "Training NF1:   9%|█▊                  | 900/10001 [25:58<4:50:55,  1.92s/batch]Batch 900/10001 Done, mean position loss: 21.027713720798495\n",
      "Training NF1:   9%|█▊                  | 895/10001 [26:00<3:46:59,  1.50s/batch]Batch 900/10001 Done, mean position loss: 21.320116500854493\n",
      "Training NF1:   9%|█▊                  | 912/10001 [26:02<4:17:28,  1.70s/batch]Batch 900/10001 Done, mean position loss: 20.762574441432953\n",
      "Training NF1:   9%|█▊                  | 897/10001 [26:04<3:40:16,  1.45s/batch]Batch 900/10001 Done, mean position loss: 21.423041117191314\n",
      "Training NF1:   9%|█▊                  | 894/10001 [26:05<3:58:59,  1.57s/batch]Batch 900/10001 Done, mean position loss: 21.210972299575808\n",
      "Training NF1:   9%|█▊                  | 922/10001 [26:05<3:54:44,  1.55s/batch]Batch 900/10001 Done, mean position loss: 21.857145278453828\n",
      "Training NF1:   9%|█▊                  | 910/10001 [26:08<3:43:08,  1.47s/batch]Batch 900/10001 Done, mean position loss: 21.21722723722458\n",
      "Training NF1:   9%|█▊                  | 895/10001 [26:08<4:09:04,  1.64s/batch]Batch 900/10001 Done, mean position loss: 21.13609885454178\n",
      "Training NF1:   9%|█▊                  | 924/10001 [26:10<4:30:27,  1.79s/batch]Batch 900/10001 Done, mean position loss: 21.160703539848328\n",
      "Training NF1:   9%|█▊                  | 903/10001 [26:10<4:08:31,  1.64s/batch]Batch 900/10001 Done, mean position loss: 21.01191775560379\n",
      "Training NF1:   9%|█▊                  | 910/10001 [26:10<4:06:32,  1.63s/batch]Batch 900/10001 Done, mean position loss: 21.274729044437407\n",
      "Training NF1:   9%|█▊                  | 905/10001 [26:10<3:53:32,  1.54s/batch]Batch 900/10001 Done, mean position loss: 20.834928305149077\n",
      "Training NF1:   9%|█▊                  | 936/10001 [26:17<4:06:05,  1.63s/batch]Batch 900/10001 Done, mean position loss: 21.52073745727539\n",
      "Training NF1:   9%|█▊                  | 916/10001 [26:18<3:48:23,  1.51s/batch]Batch 900/10001 Done, mean position loss: 20.94976819515228\n",
      "Batch 900/10001 Done, mean position loss: 20.772408051490785\n",
      "Training NF1:   9%|█▊                  | 908/10001 [26:19<4:54:55,  1.95s/batch]Batch 900/10001 Done, mean position loss: 21.257411427497864\n",
      "Training NF1:   9%|█▊                  | 917/10001 [26:38<4:17:11,  1.70s/batch]Batch 900/10001 Done, mean position loss: 21.04824866771698\n",
      "Training NF1:  10%|█▉                  | 965/10001 [28:04<4:36:38,  1.84s/batch]Batch 1000/10001 Done, mean position loss: 20.85258493900299\n",
      "Training NF1:  10%|█▉                  | 976/10001 [28:14<4:21:42,  1.74s/batch]Batch 1000/10001 Done, mean position loss: 21.346389830112457\n",
      "Training NF1:  10%|█▉                  | 997/10001 [28:19<5:24:34,  2.16s/batch]Batch 1000/10001 Done, mean position loss: 20.97731278181076\n",
      "Training NF1:  10%|█▉                  | 975/10001 [28:21<3:51:40,  1.54s/batch]Batch 1000/10001 Done, mean position loss: 20.67213181972504\n",
      "Training NF1:  10%|█▉                  | 989/10001 [28:21<4:19:46,  1.73s/batch]Batch 1000/10001 Done, mean position loss: 20.78241867542267\n",
      "Training NF1:  10%|█▉                  | 988/10001 [28:24<4:10:25,  1.67s/batch]Batch 1000/10001 Done, mean position loss: 20.842049405574798\n",
      "Training NF1:  10%|█▉                 | 1004/10001 [28:25<3:32:27,  1.42s/batch]Batch 1000/10001 Done, mean position loss: 21.29320424079895\n",
      "Training NF1:  10%|█▉                  | 989/10001 [28:26<4:51:07,  1.94s/batch]Batch 1000/10001 Done, mean position loss: 21.343046760559083\n",
      "Training NF1:  10%|█▉                  | 982/10001 [28:28<4:33:26,  1.82s/batch]Batch 1000/10001 Done, mean position loss: 21.34586569547653\n",
      "Training NF1:  10%|█▉                  | 984/10001 [28:31<4:45:50,  1.90s/batch]Batch 1000/10001 Done, mean position loss: 20.55415858030319\n",
      "Training NF1:  10%|█▉                  | 993/10001 [28:34<5:10:51,  2.07s/batch]Batch 1000/10001 Done, mean position loss: 20.860838124752043\n",
      "Training NF1:  10%|█▉                 | 1008/10001 [28:34<4:08:01,  1.65s/batch]Batch 1000/10001 Done, mean position loss: 20.731586215496062\n",
      "Training NF1:  10%|█▉                 | 1019/10001 [28:35<4:07:23,  1.65s/batch]Batch 1000/10001 Done, mean position loss: 20.934237704277038\n",
      "Training NF1:  10%|█▉                 | 1012/10001 [28:37<3:47:29,  1.52s/batch]Batch 1000/10001 Done, mean position loss: 20.893589141368864\n",
      "Training NF1:  10%|█▉                  | 999/10001 [28:37<3:50:28,  1.54s/batch]Batch 1000/10001 Done, mean position loss: 21.62325283288956\n",
      "Training NF1:  10%|█▉                 | 1012/10001 [28:38<3:39:51,  1.47s/batch]Batch 1000/10001 Done, mean position loss: 20.923353519439697\n",
      "Training NF1:  10%|█▉                 | 1003/10001 [28:38<4:33:30,  1.82s/batch]Batch 1000/10001 Done, mean position loss: 20.953461937904358\n",
      "Training NF1:  10%|█▉                  | 988/10001 [28:38<4:20:53,  1.74s/batch]Batch 1000/10001 Done, mean position loss: 20.893464756011962\n",
      "Training NF1:  10%|█▉                  | 997/10001 [28:42<5:00:25,  2.00s/batch]Batch 1000/10001 Done, mean position loss: 20.66818730831146\n",
      "Training NF1:  10%|█▉                  | 989/10001 [28:42<4:56:29,  1.97s/batch]Batch 1000/10001 Done, mean position loss: 21.27084574699402\n",
      "Training NF1:  10%|█▉                  | 987/10001 [28:43<4:38:48,  1.86s/batch]Batch 1000/10001 Done, mean position loss: 21.200087418556215\n",
      "Training NF1:  10%|█▉                 | 1012/10001 [28:42<4:13:09,  1.69s/batch]Batch 1000/10001 Done, mean position loss: 21.289040703773498\n",
      "Training NF1:  10%|█▉                 | 1003/10001 [28:45<4:12:14,  1.68s/batch]Batch 1000/10001 Done, mean position loss: 21.068293788433074\n",
      "Training NF1:  10%|█▉                  | 991/10001 [28:46<4:14:07,  1.69s/batch]Batch 1000/10001 Done, mean position loss: 21.00304120540619\n",
      "Training NF1:  10%|█▉                 | 1016/10001 [28:49<4:05:35,  1.64s/batch]Batch 1000/10001 Done, mean position loss: 21.216088857650757\n",
      "Training NF1:  10%|█▉                 | 1008/10001 [28:52<5:58:22,  2.39s/batch]Batch 1000/10001 Done, mean position loss: 21.413144798278807\n",
      "Training NF1:  10%|█▉                 | 1022/10001 [28:55<4:08:20,  1.66s/batch]Batch 1000/10001 Done, mean position loss: 21.11846483230591\n",
      "Training NF1:  10%|█▉                  | 981/10001 [28:56<4:34:25,  1.83s/batch]Batch 1000/10001 Done, mean position loss: 21.194675607681276\n",
      "Training NF1:  10%|█▉                 | 1022/10001 [28:58<4:10:10,  1.67s/batch]Batch 1000/10001 Done, mean position loss: 21.163127412796022\n",
      "Training NF1:  10%|█▉                 | 1020/10001 [28:57<4:12:05,  1.68s/batch]Batch 1000/10001 Done, mean position loss: 20.731195385456083\n",
      "Training NF1:  10%|█▉                 | 1016/10001 [28:58<3:21:30,  1.35s/batch]Batch 1000/10001 Done, mean position loss: 21.26669367313385\n",
      "Training NF1:  10%|█▉                 | 1012/10001 [28:59<4:49:17,  1.93s/batch]Batch 1000/10001 Done, mean position loss: 21.18122903585434\n",
      "Training NF1:  10%|█▉                 | 1014/10001 [29:03<4:13:49,  1.69s/batch]Batch 1000/10001 Done, mean position loss: 21.836297566890714\n",
      "Training NF1:  10%|█▉                  | 986/10001 [29:05<4:16:20,  1.71s/batch]Batch 1000/10001 Done, mean position loss: 20.834564318656923\n",
      "Training NF1:  10%|█▉                 | 1003/10001 [29:06<4:00:56,  1.61s/batch]Batch 1000/10001 Done, mean position loss: 20.98157825946808\n",
      "Training NF1:  10%|█▉                 | 1026/10001 [29:07<3:44:09,  1.50s/batch]Batch 1000/10001 Done, mean position loss: 20.738594527244565\n",
      "Training NF1:  10%|█▉                 | 1018/10001 [29:09<4:25:07,  1.77s/batch]Batch 1000/10001 Done, mean position loss: 21.2759660243988\n",
      "Training NF1:  10%|█▉                 | 1031/10001 [29:13<4:58:31,  2.00s/batch]Batch 1000/10001 Done, mean position loss: 21.450251941680907\n",
      "Training NF1:  10%|█▉                 | 1020/10001 [29:17<4:52:54,  1.96s/batch]Batch 1000/10001 Done, mean position loss: 20.941304094791413\n",
      "Training NF1:  10%|█▉                 | 1028/10001 [29:31<4:30:31,  1.81s/batch]Batch 1000/10001 Done, mean position loss: 21.01631857156754\n",
      "Training NF1:  11%|██                 | 1081/10001 [30:58<4:51:30,  1.96s/batch]Batch 1100/10001 Done, mean position loss: 20.82849280357361\n",
      "Training NF1:  11%|██                 | 1098/10001 [31:06<4:15:44,  1.72s/batch]Batch 1100/10001 Done, mean position loss: 21.311377954483035\n",
      "Training NF1:  11%|██                 | 1097/10001 [31:10<4:44:37,  1.92s/batch]Batch 1100/10001 Done, mean position loss: 20.76802334308624\n",
      "Training NF1:  11%|██                 | 1092/10001 [31:10<4:08:44,  1.68s/batch]Batch 1100/10001 Done, mean position loss: 20.960307745933534\n",
      "Training NF1:  11%|██                 | 1093/10001 [31:11<3:47:53,  1.53s/batch]Batch 1100/10001 Done, mean position loss: 20.826495800018307\n",
      "Training NF1:  11%|██                 | 1066/10001 [31:13<5:15:55,  2.12s/batch]Batch 1100/10001 Done, mean position loss: 21.31213775396347\n",
      "Training NF1:  11%|██                 | 1074/10001 [31:15<4:22:05,  1.76s/batch]Batch 1100/10001 Done, mean position loss: 21.34062539577484\n",
      "Training NF1:  11%|██                 | 1078/10001 [31:17<4:30:48,  1.82s/batch]Batch 1100/10001 Done, mean position loss: 20.67138688325882\n",
      "Training NF1:  11%|██                 | 1093/10001 [31:21<3:58:08,  1.60s/batch]Batch 1100/10001 Done, mean position loss: 21.330858504772188\n",
      "Training NF1:  11%|██                 | 1100/10001 [31:23<3:45:05,  1.52s/batch]Batch 1100/10001 Done, mean position loss: 20.722156946659087\n",
      "Training NF1:  11%|██                 | 1083/10001 [31:24<4:07:39,  1.67s/batch]Batch 1100/10001 Done, mean position loss: 20.92657909154892\n",
      "Training NF1:  11%|██                 | 1087/10001 [31:25<3:43:52,  1.51s/batch]Batch 1100/10001 Done, mean position loss: 20.548810930252074\n",
      "Training NF1:  11%|██                 | 1089/10001 [31:27<4:37:51,  1.87s/batch]Batch 1100/10001 Done, mean position loss: 20.856537518501284\n",
      "Training NF1:  11%|██                 | 1086/10001 [31:28<4:14:21,  1.71s/batch]Batch 1100/10001 Done, mean position loss: 20.846402473449707\n",
      "Training NF1:  11%|██                 | 1093/10001 [31:30<4:28:37,  1.81s/batch]Batch 1100/10001 Done, mean position loss: 20.82640251636505\n",
      "Training NF1:  11%|██                 | 1104/10001 [31:33<4:25:29,  1.79s/batch]Batch 1100/10001 Done, mean position loss: 20.665676929950713\n",
      "Batch 1100/10001 Done, mean position loss: 21.237728159427643\n",
      "Training NF1:  11%|██                 | 1103/10001 [31:33<4:03:55,  1.64s/batch]Batch 1100/10001 Done, mean position loss: 21.588528933525087\n",
      "Batch 1100/10001 Done, mean position loss: 20.94167256832123\n",
      "Training NF1:  11%|██                 | 1096/10001 [31:34<3:59:19,  1.61s/batch]Batch 1100/10001 Done, mean position loss: 21.27127753973007\n",
      "Training NF1:  11%|██                 | 1093/10001 [31:34<3:58:44,  1.61s/batch]Batch 1100/10001 Done, mean position loss: 20.989095449447632\n",
      "Training NF1:  11%|██                 | 1089/10001 [31:38<4:41:05,  1.89s/batch]Batch 1100/10001 Done, mean position loss: 21.05594455242157\n",
      "Training NF1:  11%|██                 | 1116/10001 [31:38<4:03:53,  1.65s/batch]Batch 1100/10001 Done, mean position loss: 20.93059489250183\n",
      "Training NF1:  11%|██                 | 1110/10001 [31:42<4:03:41,  1.64s/batch]Batch 1100/10001 Done, mean position loss: 21.185023703575133\n",
      "Training NF1:  11%|██▏                | 1121/10001 [31:45<3:49:18,  1.55s/batch]Batch 1100/10001 Done, mean position loss: 21.183302640914917\n",
      "Training NF1:  11%|██▏                | 1122/10001 [31:46<3:43:48,  1.51s/batch]Batch 1100/10001 Done, mean position loss: 21.126194019317623\n",
      "Training NF1:  11%|██                 | 1113/10001 [31:48<4:17:01,  1.74s/batch]Batch 1100/10001 Done, mean position loss: 21.406254415512088\n",
      "Training NF1:  11%|██                 | 1086/10001 [31:49<5:06:53,  2.07s/batch]Batch 1100/10001 Done, mean position loss: 21.177416303157806\n",
      "Training NF1:  11%|██                 | 1111/10001 [31:50<3:59:16,  1.61s/batch]Batch 1100/10001 Done, mean position loss: 21.17705302476883\n",
      "Training NF1:  11%|██                 | 1111/10001 [31:51<4:29:23,  1.82s/batch]Batch 1100/10001 Done, mean position loss: 20.732725317478177\n",
      "Training NF1:  11%|██                 | 1105/10001 [31:52<4:55:37,  1.99s/batch]Batch 1100/10001 Done, mean position loss: 21.172416377067567\n",
      "Training NF1:  11%|██▏                | 1126/10001 [31:52<4:16:48,  1.74s/batch]Batch 1100/10001 Done, mean position loss: 21.234810967445373\n",
      "Training NF1:  11%|██                 | 1114/10001 [31:55<4:00:17,  1.62s/batch]Batch 1100/10001 Done, mean position loss: 21.81799469947815\n",
      "Training NF1:  11%|██                 | 1117/10001 [31:56<4:47:07,  1.94s/batch]Batch 1100/10001 Done, mean position loss: 21.23950900554657\n",
      "Training NF1:  11%|██▏                | 1122/10001 [31:57<3:51:57,  1.57s/batch]Batch 1100/10001 Done, mean position loss: 20.82809443235397\n",
      "Training NF1:  11%|██                 | 1117/10001 [32:01<4:49:38,  1.96s/batch]Batch 1100/10001 Done, mean position loss: 20.97363559961319\n",
      "Training NF1:  11%|██                 | 1112/10001 [32:02<3:52:18,  1.57s/batch]Batch 1100/10001 Done, mean position loss: 20.715667190551756\n",
      "Training NF1:  11%|██                 | 1117/10001 [32:03<4:40:31,  1.89s/batch]Batch 1100/10001 Done, mean position loss: 21.42887089729309\n",
      "Training NF1:  11%|██▏                | 1133/10001 [32:14<3:47:44,  1.54s/batch]Batch 1100/10001 Done, mean position loss: 20.919712171554565\n",
      "Training NF1:  11%|██▏                | 1143/10001 [32:25<4:16:34,  1.74s/batch]Batch 1100/10001 Done, mean position loss: 21.013135459423065\n",
      "Training NF1:  12%|██▏                | 1170/10001 [33:49<3:57:19,  1.61s/batch]Batch 1200/10001 Done, mean position loss: 20.805719406604766\n",
      "Training NF1:  12%|██▏                | 1171/10001 [33:49<3:58:04,  1.62s/batch]Batch 1200/10001 Done, mean position loss: 21.302363300323485\n",
      "Training NF1:  12%|██▏                | 1175/10001 [33:56<4:06:28,  1.68s/batch]Batch 1200/10001 Done, mean position loss: 20.75432960987091\n",
      "Training NF1:  12%|██▎                | 1202/10001 [33:58<3:49:54,  1.57s/batch]Batch 1200/10001 Done, mean position loss: 21.274235458374022\n",
      "Training NF1:  12%|██▎                | 1196/10001 [34:00<4:23:27,  1.80s/batch]Batch 1200/10001 Done, mean position loss: 20.79888573884964\n",
      "Training NF1:  12%|██▎                | 1191/10001 [34:04<4:04:30,  1.67s/batch]Batch 1200/10001 Done, mean position loss: 21.2802415728569\n",
      "Training NF1:  12%|██▏                | 1178/10001 [34:05<4:12:44,  1.72s/batch]Batch 1200/10001 Done, mean position loss: 20.93652839899063\n",
      "Training NF1:  12%|██▎                | 1202/10001 [34:06<3:36:11,  1.47s/batch]Batch 1200/10001 Done, mean position loss: 20.719496955871584\n",
      "Training NF1:  12%|██▎                | 1193/10001 [34:10<4:40:25,  1.91s/batch]Batch 1200/10001 Done, mean position loss: 20.652859268188475\n",
      "Training NF1:  12%|██▎                | 1193/10001 [34:16<4:07:04,  1.68s/batch]Batch 1200/10001 Done, mean position loss: 20.855064337253573\n",
      "Training NF1:  12%|██▎                | 1189/10001 [34:17<3:47:10,  1.55s/batch]Batch 1200/10001 Done, mean position loss: 21.31344758749008\n",
      "Training NF1:  12%|██▎                | 1202/10001 [34:17<3:56:42,  1.61s/batch]Batch 1200/10001 Done, mean position loss: 21.234150736331937\n",
      "Training NF1:  12%|██▎                | 1211/10001 [34:18<4:38:09,  1.90s/batch]Batch 1200/10001 Done, mean position loss: 20.530070886611938\n",
      "Training NF1:  12%|██▏                | 1179/10001 [34:18<4:40:37,  1.91s/batch]Batch 1200/10001 Done, mean position loss: 20.81245470046997\n",
      "Training NF1:  12%|██▎                | 1211/10001 [34:21<3:48:21,  1.56s/batch]Batch 1200/10001 Done, mean position loss: 20.837360031604767\n",
      "Training NF1:  12%|██▎                | 1208/10001 [34:21<4:26:00,  1.82s/batch]Batch 1200/10001 Done, mean position loss: 20.91451968431473\n",
      "Training NF1:  12%|██▏                | 1175/10001 [34:22<4:31:53,  1.85s/batch]Batch 1200/10001 Done, mean position loss: 20.973474817276\n",
      "Training NF1:  12%|██▎                | 1212/10001 [34:23<3:54:26,  1.60s/batch]Batch 1200/10001 Done, mean position loss: 21.242083277702335\n",
      "Training NF1:  12%|██▎                | 1217/10001 [34:23<4:00:43,  1.64s/batch]Batch 1200/10001 Done, mean position loss: 20.930308983325958\n",
      "Training NF1:  12%|██▎                | 1220/10001 [34:23<4:42:34,  1.93s/batch]Batch 1200/10001 Done, mean position loss: 20.897102143764496\n",
      "Training NF1:  12%|██▎                | 1205/10001 [34:25<3:53:28,  1.59s/batch]Batch 1200/10001 Done, mean position loss: 20.64451636314392\n",
      "Training NF1:  12%|██▎                | 1217/10001 [34:29<4:25:17,  1.81s/batch]Batch 1200/10001 Done, mean position loss: 20.9930721282959\n",
      "Training NF1:  12%|██▎                | 1196/10001 [34:31<4:13:48,  1.73s/batch]Batch 1200/10001 Done, mean position loss: 21.55497657060623\n",
      "Training NF1:  12%|██▎                | 1212/10001 [34:35<4:09:28,  1.70s/batch]Batch 1200/10001 Done, mean position loss: 21.14869990587235\n",
      "Training NF1:  12%|██▎                | 1199/10001 [34:37<4:14:37,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 21.08097766637802\n",
      "Training NF1:  12%|██▎                | 1226/10001 [34:38<4:18:57,  1.77s/batch]Batch 1200/10001 Done, mean position loss: 21.145264468193055\n",
      "Training NF1:  12%|██▎                | 1210/10001 [34:39<3:59:34,  1.64s/batch]Batch 1200/10001 Done, mean position loss: 21.148799457550048\n",
      "Training NF1:  12%|██▎                | 1207/10001 [34:39<3:51:07,  1.58s/batch]Batch 1200/10001 Done, mean position loss: 21.136417026519776\n",
      "Training NF1:  12%|██▎                | 1224/10001 [34:41<4:16:15,  1.75s/batch]Batch 1200/10001 Done, mean position loss: 21.363944163322447\n",
      "Training NF1:  12%|██▎                | 1201/10001 [34:41<4:22:01,  1.79s/batch]Batch 1200/10001 Done, mean position loss: 21.228653514385222\n",
      "Training NF1:  12%|██▎                | 1231/10001 [34:43<4:11:56,  1.72s/batch]Batch 1200/10001 Done, mean position loss: 21.147826840877535\n",
      "Training NF1:  12%|██▎                | 1198/10001 [34:43<4:06:21,  1.68s/batch]Batch 1200/10001 Done, mean position loss: 21.23068721294403\n",
      "Training NF1:  12%|██▎                | 1205/10001 [34:44<4:04:12,  1.67s/batch]Batch 1200/10001 Done, mean position loss: 21.80259070634842\n",
      "Training NF1:  12%|██▎                | 1215/10001 [34:48<4:14:06,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 20.71283492088318\n",
      "Training NF1:  12%|██▎                | 1232/10001 [34:49<4:21:43,  1.79s/batch]Batch 1200/10001 Done, mean position loss: 20.932030460834504\n",
      "Training NF1:  12%|██▎                | 1205/10001 [34:49<3:53:48,  1.59s/batch]Batch 1200/10001 Done, mean position loss: 20.79507009267807\n",
      "Training NF1:  12%|██▎                | 1219/10001 [34:58<4:14:36,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 20.69561430454254\n",
      "Training NF1:  12%|██▎                | 1193/10001 [35:03<4:49:59,  1.98s/batch]Batch 1200/10001 Done, mean position loss: 21.391070544719696\n",
      "Training NF1:  12%|██▎                | 1216/10001 [35:06<4:15:27,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 20.89518527507782\n",
      "Training NF1:  12%|██▎                | 1244/10001 [35:15<4:42:39,  1.94s/batch]Batch 1200/10001 Done, mean position loss: 20.9985684299469\n",
      "Training NF1:  13%|██▍                | 1284/10001 [36:41<3:43:12,  1.54s/batch]Batch 1300/10001 Done, mean position loss: 20.79206954717636\n",
      "Training NF1:  13%|██▍                | 1294/10001 [36:41<4:37:03,  1.91s/batch]Batch 1300/10001 Done, mean position loss: 20.742622904777527\n",
      "Training NF1:  13%|██▍                | 1266/10001 [36:46<4:06:25,  1.69s/batch]Batch 1300/10001 Done, mean position loss: 21.283451051712035\n",
      "Training NF1:  13%|██▍                | 1300/10001 [36:47<3:21:17,  1.39s/batch]Batch 1300/10001 Done, mean position loss: 20.792550213336945\n",
      "Training NF1:  13%|██▍                | 1257/10001 [36:49<4:46:37,  1.97s/batch]Batch 1300/10001 Done, mean position loss: 21.26264261007309\n",
      "Training NF1:  13%|██▍                | 1308/10001 [36:53<3:47:58,  1.57s/batch]Batch 1300/10001 Done, mean position loss: 21.24513346672058\n",
      "Training NF1:  13%|██▍                | 1309/10001 [36:53<4:28:19,  1.85s/batch]Batch 1300/10001 Done, mean position loss: 20.706191408634183\n",
      "Training NF1:  13%|██▍                | 1302/10001 [36:56<4:10:17,  1.73s/batch]Batch 1300/10001 Done, mean position loss: 20.922696340084073\n",
      "Training NF1:  13%|██▍                | 1295/10001 [37:01<3:45:56,  1.56s/batch]Batch 1300/10001 Done, mean position loss: 20.64502967596054\n",
      "Training NF1:  13%|██▌                | 1316/10001 [37:08<4:18:47,  1.79s/batch]Batch 1300/10001 Done, mean position loss: 21.24657816171646\n",
      "Training NF1:  13%|██▍                | 1269/10001 [37:09<4:03:18,  1.67s/batch]Batch 1300/10001 Done, mean position loss: 20.800270953178405\n",
      "Training NF1:  13%|██▍                | 1291/10001 [37:09<4:03:38,  1.68s/batch]Batch 1300/10001 Done, mean position loss: 21.27546969175339\n",
      "Training NF1:  13%|██▍                | 1300/10001 [37:10<3:55:48,  1.63s/batch]Batch 1300/10001 Done, mean position loss: 20.847670323848725\n",
      "Training NF1:  13%|██▍                | 1301/10001 [37:10<4:08:10,  1.71s/batch]Batch 1300/10001 Done, mean position loss: 20.818419523239136\n",
      "Training NF1:  13%|██▍                | 1301/10001 [37:11<4:11:54,  1.74s/batch]Batch 1300/10001 Done, mean position loss: 21.190540502071382\n",
      "Training NF1:  13%|██▍                | 1303/10001 [37:11<3:59:20,  1.65s/batch]Batch 1300/10001 Done, mean position loss: 20.901919417381286\n",
      "Training NF1:  13%|██▍                | 1296/10001 [37:12<4:13:11,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 20.891505053043367\n",
      "Training NF1:  13%|██▍                | 1303/10001 [37:12<4:31:04,  1.87s/batch]Batch 1300/10001 Done, mean position loss: 20.912643826007844\n",
      "Training NF1:  13%|██▍                | 1292/10001 [37:15<4:00:48,  1.66s/batch]Batch 1300/10001 Done, mean position loss: 20.53206505537033\n",
      "Training NF1:  13%|██▍                | 1305/10001 [37:17<4:08:02,  1.71s/batch]Batch 1300/10001 Done, mean position loss: 20.935007407665253\n",
      "Training NF1:  13%|██▌                | 1321/10001 [37:19<4:06:10,  1.70s/batch]Batch 1300/10001 Done, mean position loss: 20.974050965309143\n",
      "Training NF1:  13%|██▍                | 1296/10001 [37:21<5:01:16,  2.08s/batch]Batch 1300/10001 Done, mean position loss: 20.64037499666214\n",
      "Training NF1:  13%|██▍                | 1290/10001 [37:27<4:09:39,  1.72s/batch]Batch 1300/10001 Done, mean position loss: 21.109864411354064\n",
      "Training NF1:  13%|██▍                | 1294/10001 [37:26<4:20:06,  1.79s/batch]Batch 1300/10001 Done, mean position loss: 21.5391095495224\n",
      "Training NF1:  13%|██▍                | 1299/10001 [37:26<4:03:03,  1.68s/batch]Batch 1300/10001 Done, mean position loss: 21.217922556400296\n",
      "Training NF1:  13%|██▍                | 1307/10001 [37:30<3:48:35,  1.58s/batch]Batch 1300/10001 Done, mean position loss: 21.125205054283143\n",
      "Training NF1:  13%|██▍                | 1292/10001 [37:31<4:51:21,  2.01s/batch]Batch 1300/10001 Done, mean position loss: 21.121660821437835\n",
      "Training NF1:  13%|██▍                | 1315/10001 [37:32<3:46:37,  1.57s/batch]Batch 1300/10001 Done, mean position loss: 21.06312484502792\n",
      "Training NF1:  13%|██▍                | 1315/10001 [37:32<3:55:55,  1.63s/batch]Batch 1300/10001 Done, mean position loss: 21.345262050628662\n",
      "Training NF1:  13%|██▍                | 1298/10001 [37:35<4:20:03,  1.79s/batch]Batch 1300/10001 Done, mean position loss: 21.78557391166687\n",
      "Training NF1:  13%|██▍                | 1304/10001 [37:35<4:06:37,  1.70s/batch]Batch 1300/10001 Done, mean position loss: 21.094230568408967\n",
      "Training NF1:  13%|██▌                | 1317/10001 [37:35<3:57:33,  1.64s/batch]Batch 1300/10001 Done, mean position loss: 21.120354135036468\n",
      "Training NF1:  13%|██▌                | 1327/10001 [37:39<4:05:12,  1.70s/batch]Batch 1300/10001 Done, mean position loss: 20.686758732795717\n",
      "Training NF1:  13%|██▍                | 1298/10001 [37:42<4:24:43,  1.83s/batch]Batch 1300/10001 Done, mean position loss: 20.91332857847214\n",
      "Training NF1:  13%|██▍                | 1304/10001 [37:43<3:57:43,  1.64s/batch]Batch 1300/10001 Done, mean position loss: 21.20770735502243\n",
      "Training NF1:  13%|██▍                | 1307/10001 [37:45<3:38:21,  1.51s/batch]Batch 1300/10001 Done, mean position loss: 20.670770978927614\n",
      "Training NF1:  13%|██▍                | 1307/10001 [37:46<4:23:20,  1.82s/batch]Batch 1300/10001 Done, mean position loss: 20.795521857738493\n",
      "Training NF1:  13%|██▍                | 1293/10001 [37:54<3:56:26,  1.63s/batch]Batch 1300/10001 Done, mean position loss: 21.412874405384063\n",
      "Training NF1:  13%|██▌                | 1327/10001 [38:02<3:51:33,  1.60s/batch]Batch 1300/10001 Done, mean position loss: 20.90308128595352\n",
      "Training NF1:  13%|██▌                | 1326/10001 [38:07<3:24:30,  1.41s/batch]Batch 1300/10001 Done, mean position loss: 20.982158641815186\n",
      "Training NF1:  13%|██▌                | 1348/10001 [39:26<4:37:55,  1.93s/batch]Batch 1400/10001 Done, mean position loss: 20.766392412185667\n",
      "Training NF1:  14%|██▌                | 1380/10001 [39:29<4:39:55,  1.95s/batch]Batch 1400/10001 Done, mean position loss: 20.76929145812988\n",
      "Batch 1400/10001 Done, mean position loss: 20.7283514547348\n",
      "Training NF1:  14%|██▋                | 1397/10001 [39:36<4:01:44,  1.69s/batch]Batch 1400/10001 Done, mean position loss: 21.241576986312865\n",
      "Training NF1:  14%|██▋                | 1388/10001 [39:38<3:37:56,  1.52s/batch]Batch 1400/10001 Done, mean position loss: 21.245058069229124\n",
      "Training NF1:  14%|██▋                | 1388/10001 [39:40<4:11:30,  1.75s/batch]Batch 1400/10001 Done, mean position loss: 21.286543221473693\n",
      "Training NF1:  14%|██▌                | 1376/10001 [39:41<3:46:43,  1.58s/batch]Batch 1400/10001 Done, mean position loss: 20.92323763132095\n",
      "Training NF1:  14%|██▋                | 1399/10001 [39:46<3:38:20,  1.52s/batch]Batch 1400/10001 Done, mean position loss: 20.633894915580747\n",
      "Training NF1:  14%|██▌                | 1380/10001 [39:49<4:18:36,  1.80s/batch]Batch 1400/10001 Done, mean position loss: 20.688497354984285\n",
      "Training NF1:  14%|██▋                | 1414/10001 [39:54<3:28:33,  1.46s/batch]Batch 1400/10001 Done, mean position loss: 21.145954959392547\n",
      "Training NF1:  14%|██▌                | 1378/10001 [39:57<4:23:41,  1.83s/batch]Batch 1400/10001 Done, mean position loss: 20.887796618938445\n",
      "Training NF1:  14%|██▋                | 1420/10001 [39:59<4:39:45,  1.96s/batch]Batch 1400/10001 Done, mean position loss: 20.787971658706667\n",
      "Training NF1:  14%|██▌                | 1381/10001 [40:02<4:04:33,  1.70s/batch]Batch 1400/10001 Done, mean position loss: 20.825230920314787\n",
      "Training NF1:  14%|██▋                | 1406/10001 [40:02<3:56:51,  1.65s/batch]Batch 1400/10001 Done, mean position loss: 21.215599715709686\n",
      "Training NF1:  14%|██▋                | 1384/10001 [40:02<3:52:23,  1.62s/batch]Batch 1400/10001 Done, mean position loss: 20.889645040035248\n",
      "Training NF1:  14%|██▋                | 1385/10001 [40:04<4:03:36,  1.70s/batch]Batch 1400/10001 Done, mean position loss: 20.91967606306076\n",
      "Training NF1:  14%|██▋                | 1383/10001 [40:05<3:44:47,  1.57s/batch]Batch 1400/10001 Done, mean position loss: 21.2169996714592\n",
      "Training NF1:  14%|██▋                | 1392/10001 [40:06<4:18:01,  1.80s/batch]Batch 1400/10001 Done, mean position loss: 20.96157428264618\n",
      "Training NF1:  14%|██▋                | 1383/10001 [40:08<4:17:22,  1.79s/batch]Batch 1400/10001 Done, mean position loss: 20.51712213277817\n",
      "Training NF1:  14%|██▋                | 1415/10001 [40:08<3:53:06,  1.63s/batch]Batch 1400/10001 Done, mean position loss: 20.906474556922912\n",
      "Training NF1:  14%|██▋                | 1382/10001 [40:09<3:48:23,  1.59s/batch]Batch 1400/10001 Done, mean position loss: 20.812590644359588\n",
      "Training NF1:  14%|██▋                | 1387/10001 [40:17<4:21:25,  1.82s/batch]Batch 1400/10001 Done, mean position loss: 21.107252554893495\n",
      "Training NF1:  14%|██▋                | 1431/10001 [40:20<4:14:39,  1.78s/batch]Batch 1400/10001 Done, mean position loss: 21.517938640117645\n",
      "Training NF1:  14%|██▋                | 1410/10001 [40:20<3:48:50,  1.60s/batch]Batch 1400/10001 Done, mean position loss: 21.02330751180649\n",
      "Training NF1:  14%|██▋                | 1399/10001 [40:21<3:53:24,  1.63s/batch]Batch 1400/10001 Done, mean position loss: 21.18908776044846\n",
      "Training NF1:  14%|██▋                | 1394/10001 [40:23<4:14:03,  1.77s/batch]Batch 1400/10001 Done, mean position loss: 21.090397737026215\n",
      "Training NF1:  14%|██▋                | 1412/10001 [40:25<4:24:30,  1.85s/batch]Batch 1400/10001 Done, mean position loss: 20.619382240772246\n",
      "Training NF1:  14%|██▋                | 1412/10001 [40:25<3:59:51,  1.68s/batch]Batch 1400/10001 Done, mean position loss: 21.349749746322633\n",
      "Training NF1:  14%|██▋                | 1404/10001 [40:25<3:44:49,  1.57s/batch]Batch 1400/10001 Done, mean position loss: 21.092805445194244\n",
      "Training NF1:  14%|██▋                | 1438/10001 [40:29<3:40:57,  1.55s/batch]Batch 1400/10001 Done, mean position loss: 21.110180306434632\n",
      "Training NF1:  14%|██▋                | 1425/10001 [40:32<4:44:33,  1.99s/batch]Batch 1400/10001 Done, mean position loss: 21.749298527240754\n",
      "Training NF1:  14%|██▋                | 1399/10001 [40:34<4:18:08,  1.80s/batch]Batch 1400/10001 Done, mean position loss: 21.069620659351347\n",
      "Training NF1:  14%|██▋                | 1410/10001 [40:35<4:13:49,  1.77s/batch]Batch 1400/10001 Done, mean position loss: 20.64108332633972\n",
      "Training NF1:  14%|██▋                | 1411/10001 [40:38<4:11:41,  1.76s/batch]Batch 1400/10001 Done, mean position loss: 21.200281841754915\n",
      "Training NF1:  14%|██▋                | 1401/10001 [40:38<4:22:39,  1.83s/batch]Batch 1400/10001 Done, mean position loss: 20.691757171154023\n",
      "Training NF1:  14%|██▋                | 1406/10001 [40:40<4:07:34,  1.73s/batch]Batch 1400/10001 Done, mean position loss: 20.76801162958145\n",
      "Training NF1:  14%|██▋                | 1411/10001 [40:41<3:59:20,  1.67s/batch]Batch 1400/10001 Done, mean position loss: 20.87597987651825\n",
      "Training NF1:  14%|██▋                | 1412/10001 [40:42<3:44:36,  1.57s/batch]Batch 1400/10001 Done, mean position loss: 21.375017437934876\n",
      "Training NF1:  14%|██▋                | 1415/10001 [41:00<3:25:37,  1.44s/batch]Batch 1400/10001 Done, mean position loss: 20.873316988945007\n",
      "Training NF1:  14%|██▋                | 1416/10001 [41:02<4:16:23,  1.79s/batch]Batch 1400/10001 Done, mean position loss: 20.974400980472566\n",
      "Training NF1:  15%|██▊                | 1463/10001 [42:07<4:05:43,  1.73s/batch]Batch 1500/10001 Done, mean position loss: 20.726403942108156\n",
      "Training NF1:  15%|██▊                | 1465/10001 [42:08<3:40:24,  1.55s/batch]Batch 1500/10001 Done, mean position loss: 20.733008389472964\n",
      "Training NF1:  15%|██▊                | 1463/10001 [42:17<3:59:38,  1.68s/batch]Batch 1500/10001 Done, mean position loss: 20.754268093109133\n",
      "Training NF1:  15%|██▊                | 1484/10001 [42:20<4:12:44,  1.78s/batch]Batch 1500/10001 Done, mean position loss: 21.242100043296816\n",
      "Training NF1:  15%|██▊                | 1473/10001 [42:25<4:19:34,  1.83s/batch]Batch 1500/10001 Done, mean position loss: 21.226949532032016\n",
      "Training NF1:  15%|██▊                | 1495/10001 [42:26<3:50:06,  1.62s/batch]Batch 1500/10001 Done, mean position loss: 21.270284242630005\n",
      "Training NF1:  15%|██▊                | 1474/10001 [42:29<3:55:10,  1.65s/batch]Batch 1500/10001 Done, mean position loss: 20.91925724029541\n",
      "Training NF1:  15%|██▊                | 1477/10001 [42:34<3:50:25,  1.62s/batch]Batch 1500/10001 Done, mean position loss: 20.683386926651\n",
      "Training NF1:  15%|██▊                | 1475/10001 [42:37<3:41:06,  1.56s/batch]Batch 1500/10001 Done, mean position loss: 20.6308025765419\n",
      "Training NF1:  15%|██▊                | 1511/10001 [42:43<3:44:46,  1.59s/batch]Batch 1500/10001 Done, mean position loss: 21.17702534198761\n",
      "Training NF1:  15%|██▊                | 1513/10001 [42:44<3:51:33,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 20.876992394924166\n",
      "Training NF1:  15%|██▊                | 1485/10001 [42:45<4:13:33,  1.79s/batch]Batch 1500/10001 Done, mean position loss: 21.153057613372802\n",
      "Training NF1:  15%|██▊                | 1477/10001 [42:50<4:02:41,  1.71s/batch]Batch 1500/10001 Done, mean position loss: 20.77050900220871\n",
      "Training NF1:  15%|██▊                | 1480/10001 [42:51<4:11:50,  1.77s/batch]Batch 1500/10001 Done, mean position loss: 20.879647529125215\n",
      "Training NF1:  15%|██▉                | 1515/10001 [42:52<3:38:59,  1.55s/batch]Batch 1500/10001 Done, mean position loss: 20.924652128219606\n",
      "Training NF1:  15%|██▊                | 1486/10001 [42:55<3:52:02,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 21.199144365787504\n",
      "Training NF1:  15%|██▊                | 1492/10001 [42:54<4:27:38,  1.89s/batch]Batch 1500/10001 Done, mean position loss: 20.903467502593998\n",
      "Training NF1:  15%|██▊                | 1489/10001 [42:56<4:12:15,  1.78s/batch]Batch 1500/10001 Done, mean position loss: 20.930739877223967\n",
      "Training NF1:  15%|██▊                | 1510/10001 [42:57<3:37:07,  1.53s/batch]Batch 1500/10001 Done, mean position loss: 20.789856445789336\n",
      "Training NF1:  15%|██▉                | 1516/10001 [42:59<3:36:25,  1.53s/batch]Batch 1500/10001 Done, mean position loss: 20.518445193767548\n",
      "Training NF1:  15%|██▊                | 1510/10001 [43:01<4:51:25,  2.06s/batch]Batch 1500/10001 Done, mean position loss: 20.786069822311404\n",
      "Training NF1:  15%|██▊                | 1491/10001 [43:06<4:07:52,  1.75s/batch]Batch 1500/10001 Done, mean position loss: 21.059826159477232\n",
      "Training NF1:  15%|██▊                | 1508/10001 [43:08<4:10:45,  1.77s/batch]Batch 1500/10001 Done, mean position loss: 21.50716344833374\n",
      "Training NF1:  15%|██▊                | 1510/10001 [43:09<3:58:40,  1.69s/batch]Batch 1500/10001 Done, mean position loss: 21.06875423192978\n",
      "Training NF1:  15%|██▊                | 1511/10001 [43:11<3:58:33,  1.69s/batch]Batch 1500/10001 Done, mean position loss: 20.99996761083603\n",
      "Training NF1:  15%|██▊                | 1503/10001 [43:11<4:32:57,  1.93s/batch]Batch 1500/10001 Done, mean position loss: 21.10542982816696\n",
      "Training NF1:  15%|██▉                | 1534/10001 [43:13<3:19:47,  1.42s/batch]Batch 1500/10001 Done, mean position loss: 21.17034379005432\n",
      "Training NF1:  15%|██▊                | 1494/10001 [43:13<3:31:30,  1.49s/batch]Batch 1500/10001 Done, mean position loss: 21.100211422443387\n",
      "Batch 1500/10001 Done, mean position loss: 20.595577445030212\n",
      "Training NF1:  15%|██▉                | 1529/10001 [43:16<3:58:39,  1.69s/batch]Batch 1500/10001 Done, mean position loss: 21.354567511081697\n",
      "Training NF1:  15%|██▉                | 1545/10001 [43:20<3:51:07,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 21.05363268613815\n",
      "Training NF1:  15%|██▊                | 1506/10001 [43:21<3:51:53,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 21.174977619647983\n",
      "Training NF1:  15%|██▊                | 1507/10001 [43:23<4:02:52,  1.72s/batch]Batch 1500/10001 Done, mean position loss: 20.650834944248196\n",
      "Training NF1:  15%|██▉                | 1521/10001 [43:26<4:05:33,  1.74s/batch]Batch 1500/10001 Done, mean position loss: 21.764791882038114\n",
      "Training NF1:  15%|██▉                | 1523/10001 [43:26<3:51:17,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 20.763619408607482\n",
      "Training NF1:  15%|██▉                | 1519/10001 [43:28<3:49:47,  1.63s/batch]Batch 1500/10001 Done, mean position loss: 21.40337821006775\n",
      "Training NF1:  15%|██▊                | 1511/10001 [43:31<3:56:21,  1.67s/batch]Batch 1500/10001 Done, mean position loss: 20.86859147310257\n",
      "Training NF1:  15%|██▉                | 1532/10001 [43:32<4:43:59,  2.01s/batch]Batch 1500/10001 Done, mean position loss: 20.674984436035153\n",
      "Training NF1:  15%|██▉                | 1515/10001 [43:54<3:48:09,  1.61s/batch]Batch 1500/10001 Done, mean position loss: 20.862250497341154\n",
      "Training NF1:  15%|██▉                | 1547/10001 [43:56<3:59:20,  1.70s/batch]Batch 1500/10001 Done, mean position loss: 20.950373537540436\n",
      "Training NF1:  16%|██▉                | 1558/10001 [44:52<3:46:50,  1.61s/batch]Batch 1600/10001 Done, mean position loss: 20.693566348552704\n",
      "Training NF1:  16%|██▉                | 1554/10001 [44:53<4:00:21,  1.71s/batch]Batch 1600/10001 Done, mean position loss: 20.703895504474637\n",
      "Training NF1:  16%|██▉                | 1557/10001 [45:07<4:08:45,  1.77s/batch]Batch 1600/10001 Done, mean position loss: 21.231850984096525\n",
      "Training NF1:  16%|███                | 1590/10001 [45:07<3:56:04,  1.68s/batch]Batch 1600/10001 Done, mean position loss: 20.74163955450058\n",
      "Training NF1:  16%|███                | 1605/10001 [45:12<3:36:10,  1.54s/batch]Batch 1600/10001 Done, mean position loss: 21.213761391639707\n",
      "Training NF1:  16%|███                | 1583/10001 [45:17<3:42:05,  1.58s/batch]Batch 1600/10001 Done, mean position loss: 20.677448787689208\n",
      "Training NF1:  16%|███                | 1583/10001 [45:19<3:46:19,  1.61s/batch]Batch 1600/10001 Done, mean position loss: 21.25831783056259\n",
      "Training NF1:  16%|██▉                | 1569/10001 [45:20<3:42:30,  1.58s/batch]Batch 1600/10001 Done, mean position loss: 20.881290986537934\n",
      "Training NF1:  16%|███                | 1592/10001 [45:26<4:12:15,  1.80s/batch]Batch 1600/10001 Done, mean position loss: 20.631646904945374\n",
      "Training NF1:  16%|███                | 1615/10001 [45:31<4:36:36,  1.98s/batch]Batch 1600/10001 Done, mean position loss: 20.868530757427216\n",
      "Training NF1:  16%|██▉                | 1576/10001 [45:32<3:48:14,  1.63s/batch]Batch 1600/10001 Done, mean position loss: 21.081348557472232\n",
      "Training NF1:  16%|███                | 1580/10001 [45:38<4:12:08,  1.80s/batch]Batch 1600/10001 Done, mean position loss: 20.89067801952362\n",
      "Training NF1:  16%|███                | 1595/10001 [45:39<4:05:48,  1.75s/batch]Batch 1600/10001 Done, mean position loss: 20.765140776634215\n",
      "Training NF1:  16%|███                | 1618/10001 [45:39<4:36:06,  1.98s/batch]Batch 1600/10001 Done, mean position loss: 21.16256382703781\n",
      "Training NF1:  16%|███                | 1596/10001 [45:43<4:20:58,  1.86s/batch]Batch 1600/10001 Done, mean position loss: 20.875316896438598\n",
      "Training NF1:  16%|███                | 1608/10001 [45:43<3:32:25,  1.52s/batch]Batch 1600/10001 Done, mean position loss: 21.183809893131258\n",
      "Training NF1:  16%|███                | 1634/10001 [45:49<4:37:19,  1.99s/batch]Batch 1600/10001 Done, mean position loss: 20.76563952445984\n",
      "Training NF1:  16%|███                | 1590/10001 [45:49<3:47:01,  1.62s/batch]Batch 1600/10001 Done, mean position loss: 20.506876738071444\n",
      "Training NF1:  16%|███                | 1583/10001 [45:50<3:54:18,  1.67s/batch]Batch 1600/10001 Done, mean position loss: 21.03441372871399\n",
      "Training NF1:  16%|███                | 1601/10001 [45:50<4:33:50,  1.96s/batch]Batch 1600/10001 Done, mean position loss: 20.80386375188828\n",
      "Training NF1:  16%|██▉                | 1567/10001 [45:51<4:05:41,  1.75s/batch]Batch 1600/10001 Done, mean position loss: 20.880435769557955\n",
      "Training NF1:  16%|███                | 1628/10001 [45:52<3:26:52,  1.48s/batch]Batch 1600/10001 Done, mean position loss: 20.930091407299038\n",
      "Training NF1:  16%|███                | 1599/10001 [45:57<3:53:25,  1.67s/batch]Batch 1600/10001 Done, mean position loss: 21.0565916800499\n",
      "Training NF1:  16%|███                | 1639/10001 [45:57<3:55:17,  1.69s/batch]Batch 1600/10001 Done, mean position loss: 21.151531479358674\n",
      "Training NF1:  16%|███                | 1624/10001 [45:59<4:15:25,  1.83s/batch]Batch 1600/10001 Done, mean position loss: 20.574936506748198\n",
      "Training NF1:  16%|███                | 1621/10001 [46:00<3:40:53,  1.58s/batch]Batch 1600/10001 Done, mean position loss: 21.5051881814003\n",
      "Training NF1:  16%|███                | 1616/10001 [46:04<3:50:24,  1.65s/batch]Batch 1600/10001 Done, mean position loss: 21.049792766571045\n",
      "Training NF1:  16%|███                | 1629/10001 [46:07<3:50:53,  1.65s/batch]Batch 1600/10001 Done, mean position loss: 21.074894647598267\n",
      "Training NF1:  16%|███                | 1612/10001 [46:09<3:32:03,  1.52s/batch]Batch 1600/10001 Done, mean position loss: 21.351718966960906\n",
      "Training NF1:  16%|███                | 1606/10001 [46:09<4:06:11,  1.76s/batch]Batch 1600/10001 Done, mean position loss: 21.026872549057007\n",
      "Training NF1:  16%|███                | 1632/10001 [46:12<3:39:11,  1.57s/batch]Batch 1600/10001 Done, mean position loss: 21.361289846897126\n",
      "Training NF1:  16%|███                | 1634/10001 [46:13<4:01:59,  1.74s/batch]Batch 1600/10001 Done, mean position loss: 21.15379986524582\n",
      "Training NF1:  16%|███                | 1608/10001 [46:13<3:10:17,  1.36s/batch]Batch 1600/10001 Done, mean position loss: 20.628025596141818\n",
      "Training NF1:  16%|███                | 1631/10001 [46:16<4:45:24,  2.05s/batch]Batch 1600/10001 Done, mean position loss: 21.03153786659241\n",
      "Training NF1:  16%|███                | 1611/10001 [46:18<3:31:04,  1.51s/batch]Batch 1600/10001 Done, mean position loss: 21.753840944767\n",
      "Training NF1:  16%|███                | 1625/10001 [46:20<4:12:54,  1.81s/batch]Batch 1600/10001 Done, mean position loss: 20.67226084947586\n",
      "Training NF1:  16%|███                | 1613/10001 [46:20<3:47:42,  1.63s/batch]Batch 1600/10001 Done, mean position loss: 20.74107812643051\n",
      "Training NF1:  16%|███                | 1632/10001 [46:22<4:03:37,  1.75s/batch]Batch 1600/10001 Done, mean position loss: 20.856970393657683\n",
      "Training NF1:  16%|███                | 1617/10001 [46:49<4:10:57,  1.80s/batch]Batch 1600/10001 Done, mean position loss: 20.918707966804504\n",
      "Training NF1:  16%|███                | 1642/10001 [46:49<3:42:45,  1.60s/batch]Batch 1600/10001 Done, mean position loss: 20.86523380279541\n",
      "Training NF1:  17%|███▏               | 1657/10001 [47:50<3:47:10,  1.63s/batch]Batch 1700/10001 Done, mean position loss: 20.693664767742156\n",
      "Training NF1:  17%|███▏               | 1684/10001 [47:52<4:16:36,  1.85s/batch]Batch 1700/10001 Done, mean position loss: 20.702591631412506\n",
      "Training NF1:  17%|███▏               | 1680/10001 [48:04<3:52:24,  1.68s/batch]Batch 1700/10001 Done, mean position loss: 21.19354229688644\n",
      "Training NF1:  17%|███▏               | 1681/10001 [48:07<3:38:49,  1.58s/batch]Batch 1700/10001 Done, mean position loss: 21.247615685462954\n",
      "Training NF1:  17%|███▏               | 1689/10001 [48:09<3:38:52,  1.58s/batch]Batch 1700/10001 Done, mean position loss: 20.730711379051208\n",
      "Training NF1:  17%|███▏               | 1674/10001 [48:09<4:05:41,  1.77s/batch]Batch 1700/10001 Done, mean position loss: 20.659908268451694\n",
      "Training NF1:  17%|███▏               | 1689/10001 [48:11<3:57:43,  1.72s/batch]Batch 1700/10001 Done, mean position loss: 20.883827247619628\n",
      "Training NF1:  17%|███▏               | 1687/10001 [48:11<3:38:47,  1.58s/batch]Batch 1700/10001 Done, mean position loss: 21.237649431228636\n",
      "Training NF1:  17%|███▏               | 1707/10001 [48:20<3:28:11,  1.51s/batch]Batch 1700/10001 Done, mean position loss: 21.046364314556122\n",
      "Training NF1:  17%|███▏               | 1694/10001 [48:22<3:49:46,  1.66s/batch]Batch 1700/10001 Done, mean position loss: 20.623291399478912\n",
      "Training NF1:  17%|███▏               | 1686/10001 [48:25<3:42:08,  1.60s/batch]Batch 1700/10001 Done, mean position loss: 20.85135041952133\n",
      "Training NF1:  17%|███▏               | 1680/10001 [48:29<3:35:58,  1.56s/batch]Batch 1700/10001 Done, mean position loss: 20.87020775318146\n",
      "Training NF1:  17%|███▏               | 1688/10001 [48:33<4:10:42,  1.81s/batch]Batch 1700/10001 Done, mean position loss: 20.728103446960446\n",
      "Training NF1:  17%|███▎               | 1727/10001 [48:33<3:48:44,  1.66s/batch]Batch 1700/10001 Done, mean position loss: 21.120046420097353\n",
      "Training NF1:  17%|███▏               | 1690/10001 [48:38<4:24:04,  1.91s/batch]Batch 1700/10001 Done, mean position loss: 20.848172907829287\n",
      "Training NF1:  17%|███▎               | 1711/10001 [48:38<3:51:44,  1.68s/batch]Batch 1700/10001 Done, mean position loss: 20.754993381500245\n",
      "Training NF1:  17%|███▏               | 1698/10001 [48:40<3:44:50,  1.62s/batch]Batch 1700/10001 Done, mean position loss: 20.90979829788208\n",
      "Batch 1700/10001 Done, mean position loss: 21.165122759342193\n",
      "Training NF1:  17%|███▎               | 1714/10001 [48:43<3:49:26,  1.66s/batch]Batch 1700/10001 Done, mean position loss: 20.87297613620758\n",
      "Training NF1:  17%|███▏               | 1667/10001 [48:46<4:22:21,  1.89s/batch]Batch 1700/10001 Done, mean position loss: 20.99902819633484\n",
      "Training NF1:  17%|███▏               | 1705/10001 [48:47<4:00:21,  1.74s/batch]Batch 1700/10001 Done, mean position loss: 20.496020359992983\n",
      "Training NF1:  17%|███▏               | 1670/10001 [48:49<3:55:44,  1.70s/batch]Batch 1700/10001 Done, mean position loss: 20.801362857818603\n",
      "Training NF1:  17%|███▏               | 1671/10001 [48:52<3:51:33,  1.67s/batch]Batch 1700/10001 Done, mean position loss: 21.486071536540983\n",
      "Training NF1:  17%|███▏               | 1705/10001 [48:52<3:40:28,  1.59s/batch]Batch 1700/10001 Done, mean position loss: 21.045293922424314\n",
      "Training NF1:  17%|███▏               | 1687/10001 [48:53<4:16:51,  1.85s/batch]Batch 1700/10001 Done, mean position loss: 21.029800198078156\n",
      "Training NF1:  17%|███▏               | 1674/10001 [48:57<4:13:35,  1.83s/batch]Batch 1700/10001 Done, mean position loss: 20.58113182783127\n",
      "Training NF1:  17%|███▎               | 1731/10001 [48:57<4:13:26,  1.84s/batch]Batch 1700/10001 Done, mean position loss: 21.14040711164474\n",
      "Training NF1:  17%|███▏               | 1678/10001 [49:04<4:37:16,  2.00s/batch]Batch 1700/10001 Done, mean position loss: 21.309715757369993\n",
      "Training NF1:  17%|███▎               | 1747/10001 [49:06<3:32:04,  1.54s/batch]Batch 1700/10001 Done, mean position loss: 21.053353571891783\n",
      "Training NF1:  17%|███▎               | 1736/10001 [49:09<3:42:51,  1.62s/batch]Batch 1700/10001 Done, mean position loss: 21.004346237182617\n",
      "Training NF1:  17%|███▎               | 1725/10001 [49:09<4:07:01,  1.79s/batch]Batch 1700/10001 Done, mean position loss: 21.15485435247421\n",
      "Training NF1:  17%|███▏               | 1697/10001 [49:09<3:44:49,  1.62s/batch]Batch 1700/10001 Done, mean position loss: 21.010191721916197\n",
      "Training NF1:  17%|███▏               | 1701/10001 [49:10<3:35:11,  1.56s/batch]Batch 1700/10001 Done, mean position loss: 21.344146995544435\n",
      "Training NF1:  17%|███▎               | 1739/10001 [49:12<4:11:22,  1.83s/batch]Batch 1700/10001 Done, mean position loss: 20.61660413503647\n",
      "Training NF1:  17%|███▏               | 1683/10001 [49:13<4:25:02,  1.91s/batch]Batch 1700/10001 Done, mean position loss: 21.754821162223816\n",
      "Training NF1:  17%|███▏               | 1703/10001 [49:16<4:09:11,  1.80s/batch]Batch 1700/10001 Done, mean position loss: 20.748957209587097\n",
      "Training NF1:  17%|███▎               | 1729/10001 [49:16<4:28:46,  1.95s/batch]Batch 1700/10001 Done, mean position loss: 20.85748218536377\n",
      "Training NF1:  17%|███▏               | 1705/10001 [49:18<4:03:04,  1.76s/batch]Batch 1700/10001 Done, mean position loss: 20.662695648670194\n",
      "Training NF1:  17%|███▎               | 1733/10001 [49:43<3:41:32,  1.61s/batch]Batch 1700/10001 Done, mean position loss: 20.923914291858672\n",
      "Training NF1:  17%|███▎               | 1727/10001 [49:46<4:11:34,  1.82s/batch]Batch 1700/10001 Done, mean position loss: 20.851786735057832\n",
      "Training NF1:  17%|███▎               | 1750/10001 [50:37<3:49:42,  1.67s/batch]Batch 1800/10001 Done, mean position loss: 20.68324011802673\n",
      "Training NF1:  18%|███▎               | 1771/10001 [50:44<4:09:13,  1.82s/batch]Batch 1800/10001 Done, mean position loss: 20.678511152267454\n",
      "Training NF1:  18%|███▍               | 1799/10001 [50:56<3:41:53,  1.62s/batch]Batch 1800/10001 Done, mean position loss: 21.219318075180055\n",
      "Training NF1:  18%|███▍               | 1778/10001 [50:57<4:07:37,  1.81s/batch]Batch 1800/10001 Done, mean position loss: 20.63865102529526\n",
      "Training NF1:  18%|███▎               | 1763/10001 [50:57<3:58:51,  1.74s/batch]Batch 1800/10001 Done, mean position loss: 21.186108589172363\n",
      "Training NF1:  18%|███▎               | 1776/10001 [50:59<3:12:12,  1.40s/batch]Batch 1800/10001 Done, mean position loss: 21.242979919910432\n",
      "Training NF1:  18%|███▍               | 1795/10001 [51:00<4:10:48,  1.83s/batch]Batch 1800/10001 Done, mean position loss: 20.86241541147232\n",
      "Training NF1:  18%|███▎               | 1774/10001 [51:04<3:36:02,  1.58s/batch]Batch 1800/10001 Done, mean position loss: 20.718390901088714\n",
      "Training NF1:  18%|███▍               | 1784/10001 [51:08<4:01:00,  1.76s/batch]Batch 1800/10001 Done, mean position loss: 20.61812733888626\n",
      "Training NF1:  18%|███▎               | 1770/10001 [51:09<4:12:36,  1.84s/batch]Batch 1800/10001 Done, mean position loss: 20.864664821624757\n",
      "Training NF1:  18%|███▍               | 1783/10001 [51:11<4:24:49,  1.93s/batch]Batch 1800/10001 Done, mean position loss: 21.04786784172058\n",
      "Training NF1:  18%|███▍               | 1781/10001 [51:20<3:54:24,  1.71s/batch]Batch 1800/10001 Done, mean position loss: 20.840700783729552\n",
      "Training NF1:  18%|███▎               | 1775/10001 [51:21<4:08:02,  1.81s/batch]Batch 1800/10001 Done, mean position loss: 20.721486563682557\n",
      "Training NF1:  18%|███▍               | 1778/10001 [51:26<4:45:10,  2.08s/batch]Batch 1800/10001 Done, mean position loss: 21.144091155529022\n",
      "Training NF1:  18%|███▍               | 1778/10001 [51:27<4:18:06,  1.88s/batch]Batch 1800/10001 Done, mean position loss: 20.7390918135643\n",
      "Training NF1:  18%|███▍               | 1785/10001 [51:35<3:54:09,  1.71s/batch]Batch 1800/10001 Done, mean position loss: 20.905112440586088\n",
      "Training NF1:  18%|███▍               | 1821/10001 [51:34<4:16:06,  1.88s/batch]Batch 1800/10001 Done, mean position loss: 20.839075291156767\n",
      "Training NF1:  18%|███▍               | 1816/10001 [51:35<3:38:55,  1.60s/batch]Batch 1800/10001 Done, mean position loss: 21.145965015888216\n",
      "Training NF1:  18%|███▍               | 1824/10001 [51:36<3:36:24,  1.59s/batch]Batch 1800/10001 Done, mean position loss: 20.980088779926298\n",
      "Training NF1:  18%|███▍               | 1786/10001 [51:39<4:07:51,  1.81s/batch]Batch 1800/10001 Done, mean position loss: 20.88706833362579\n",
      "Training NF1:  18%|███▍               | 1792/10001 [51:39<3:30:59,  1.54s/batch]Batch 1800/10001 Done, mean position loss: 20.79944584131241\n",
      "Training NF1:  18%|███▍               | 1806/10001 [51:42<3:18:29,  1.45s/batch]Batch 1800/10001 Done, mean position loss: 21.03852662563324\n",
      "Training NF1:  18%|███▍               | 1810/10001 [51:44<4:07:03,  1.81s/batch]Batch 1800/10001 Done, mean position loss: 20.484843237400057\n",
      "Training NF1:  18%|███▍               | 1791/10001 [51:49<3:47:01,  1.66s/batch]Batch 1800/10001 Done, mean position loss: 21.023100085258484\n",
      "Training NF1:  18%|███▍               | 1838/10001 [51:49<3:59:37,  1.76s/batch]Batch 1800/10001 Done, mean position loss: 21.448227486610413\n",
      "Training NF1:  18%|███▍               | 1819/10001 [51:51<4:14:19,  1.87s/batch]Batch 1800/10001 Done, mean position loss: 20.57125967264175\n",
      "Training NF1:  18%|███▍               | 1796/10001 [51:54<3:44:14,  1.64s/batch]Batch 1800/10001 Done, mean position loss: 21.125018122196195\n",
      "Training NF1:  18%|███▍               | 1832/10001 [51:55<4:17:04,  1.89s/batch]Batch 1800/10001 Done, mean position loss: 21.284904451370238\n",
      "Training NF1:  18%|███▍               | 1797/10001 [51:57<4:24:35,  1.94s/batch]Batch 1800/10001 Done, mean position loss: 21.041773962974545\n",
      "Training NF1:  18%|███▍               | 1830/10001 [51:59<4:17:47,  1.89s/batch]Batch 1800/10001 Done, mean position loss: 21.00532154083252\n",
      "Training NF1:  18%|███▍               | 1814/10001 [51:59<3:44:42,  1.65s/batch]Batch 1800/10001 Done, mean position loss: 21.139354627132416\n",
      "Training NF1:  18%|███▍               | 1824/10001 [52:02<4:02:04,  1.78s/batch]Batch 1800/10001 Done, mean position loss: 21.3400203704834\n",
      "Training NF1:  18%|███▍               | 1807/10001 [52:04<3:27:44,  1.52s/batch]Batch 1800/10001 Done, mean position loss: 20.74990007638931\n",
      "Training NF1:  18%|███▍               | 1801/10001 [52:05<3:46:03,  1.65s/batch]Batch 1800/10001 Done, mean position loss: 20.99093990564346\n",
      "Training NF1:  18%|███▍               | 1813/10001 [52:04<4:03:39,  1.79s/batch]Batch 1800/10001 Done, mean position loss: 21.727462270259856\n",
      "Training NF1:  18%|███▍               | 1818/10001 [52:06<3:31:54,  1.55s/batch]Batch 1800/10001 Done, mean position loss: 20.586678705215455\n",
      "Training NF1:  18%|███▍               | 1829/10001 [52:08<3:30:59,  1.55s/batch]Batch 1800/10001 Done, mean position loss: 20.842073533535\n",
      "Training NF1:  18%|███▌               | 1844/10001 [52:10<3:34:32,  1.58s/batch]Batch 1800/10001 Done, mean position loss: 20.64742807865143\n",
      "Training NF1:  19%|███▌               | 1858/10001 [52:35<3:36:29,  1.60s/batch]Batch 1800/10001 Done, mean position loss: 20.931986882686616\n",
      "Training NF1:  18%|███▍               | 1832/10001 [52:49<4:04:01,  1.79s/batch]Batch 1800/10001 Done, mean position loss: 20.829778099060057\n",
      "Training NF1:  19%|███▌               | 1873/10001 [53:23<3:59:21,  1.77s/batch]Batch 1900/10001 Done, mean position loss: 20.674414269924164\n",
      "Training NF1:  19%|███▌               | 1891/10001 [53:36<3:29:40,  1.55s/batch]Batch 1900/10001 Done, mean position loss: 20.66716647863388\n",
      "Training NF1:  19%|███▌               | 1861/10001 [53:47<4:15:25,  1.88s/batch]Batch 1900/10001 Done, mean position loss: 20.874408400058748\n",
      "Training NF1:  19%|███▌               | 1862/10001 [53:48<3:42:12,  1.64s/batch]Batch 1900/10001 Done, mean position loss: 21.207694997787474\n",
      "Training NF1:  19%|███▌               | 1865/10001 [53:49<3:11:41,  1.41s/batch]Batch 1900/10001 Done, mean position loss: 21.19819927215576\n",
      "Training NF1:  19%|███▋               | 1919/10001 [53:54<3:49:02,  1.70s/batch]Batch 1900/10001 Done, mean position loss: 21.165922815799714\n",
      "Training NF1:  19%|███▌               | 1880/10001 [53:54<3:41:30,  1.64s/batch]Batch 1900/10001 Done, mean position loss: 20.634782757759094\n",
      "Training NF1:  19%|███▌               | 1906/10001 [53:56<3:56:27,  1.75s/batch]Batch 1900/10001 Done, mean position loss: 20.698893351554872\n",
      "Training NF1:  19%|███▌               | 1880/10001 [54:01<3:58:14,  1.76s/batch]Batch 1900/10001 Done, mean position loss: 20.6209490776062\n",
      "Training NF1:  19%|███▋               | 1909/10001 [54:04<4:43:04,  2.10s/batch]Batch 1900/10001 Done, mean position loss: 21.038333277702332\n",
      "Training NF1:  19%|███▌               | 1881/10001 [54:07<3:59:38,  1.77s/batch]Batch 1900/10001 Done, mean position loss: 20.84356944322586\n",
      "Training NF1:  19%|███▌               | 1878/10001 [54:12<3:45:54,  1.67s/batch]Batch 1900/10001 Done, mean position loss: 20.839801290035247\n",
      "Training NF1:  19%|███▌               | 1898/10001 [54:13<4:17:12,  1.90s/batch]Batch 1900/10001 Done, mean position loss: 20.69129961013794\n",
      "Training NF1:  19%|███▌               | 1878/10001 [54:17<3:55:39,  1.74s/batch]Batch 1900/10001 Done, mean position loss: 20.727696647644045\n",
      "Training NF1:  19%|███▋               | 1911/10001 [54:18<4:04:44,  1.82s/batch]Batch 1900/10001 Done, mean position loss: 21.125412714481353\n",
      "Training NF1:  19%|███▌               | 1883/10001 [54:26<3:59:01,  1.77s/batch]Batch 1900/10001 Done, mean position loss: 20.811234760284425\n",
      "Training NF1:  19%|███▋               | 1919/10001 [54:26<3:47:28,  1.69s/batch]Batch 1900/10001 Done, mean position loss: 21.134057767391205\n",
      "Training NF1:  19%|███▌               | 1896/10001 [54:28<4:26:06,  1.97s/batch]Batch 1900/10001 Done, mean position loss: 20.99279869556427\n",
      "Training NF1:  19%|███▌               | 1882/10001 [54:29<3:36:31,  1.60s/batch]Batch 1900/10001 Done, mean position loss: 20.89881028652191\n",
      "Training NF1:  19%|███▌               | 1907/10001 [54:36<3:00:39,  1.34s/batch]Batch 1900/10001 Done, mean position loss: 20.88701276540756\n",
      "Training NF1:  19%|███▌               | 1892/10001 [54:36<3:49:12,  1.70s/batch]Batch 1900/10001 Done, mean position loss: 21.03599412441254\n",
      "Training NF1:  19%|███▌               | 1888/10001 [54:36<3:45:32,  1.67s/batch]Batch 1900/10001 Done, mean position loss: 20.792081723213194\n",
      "Training NF1:  19%|███▋               | 1927/10001 [54:41<3:46:23,  1.68s/batch]Batch 1900/10001 Done, mean position loss: 20.555535917282107\n",
      "Training NF1:  19%|███▌               | 1862/10001 [54:42<4:15:48,  1.89s/batch]Batch 1900/10001 Done, mean position loss: 20.497565116882328\n",
      "Training NF1:  19%|███▋               | 1927/10001 [54:46<3:51:29,  1.72s/batch]Batch 1900/10001 Done, mean position loss: 21.261879963874815\n",
      "Training NF1:  19%|███▋               | 1909/10001 [54:49<4:13:55,  1.88s/batch]Batch 1900/10001 Done, mean position loss: 21.430263526439667\n",
      "Training NF1:  19%|███▋               | 1923/10001 [54:50<3:54:55,  1.74s/batch]Batch 1900/10001 Done, mean position loss: 21.032434837818144\n",
      "Training NF1:  19%|███▋               | 1933/10001 [54:51<3:35:08,  1.60s/batch]Batch 1900/10001 Done, mean position loss: 21.09869719743729\n",
      "Training NF1:  19%|███▌               | 1900/10001 [54:51<3:51:23,  1.71s/batch]Batch 1900/10001 Done, mean position loss: 21.02154368877411\n",
      "Training NF1:  19%|███▌               | 1898/10001 [54:54<3:44:55,  1.67s/batch]Batch 1900/10001 Done, mean position loss: 21.00785650730133\n",
      "Training NF1:  19%|███▌               | 1902/10001 [54:54<3:21:34,  1.49s/batch]Batch 1900/10001 Done, mean position loss: 21.148658988475802\n",
      "Training NF1:  19%|███▋               | 1937/10001 [54:55<3:42:21,  1.65s/batch]Batch 1900/10001 Done, mean position loss: 20.96741840600967\n",
      "Training NF1:  19%|███▋               | 1939/10001 [54:57<3:50:40,  1.72s/batch]Batch 1900/10001 Done, mean position loss: 21.323407380580903\n",
      "Training NF1:  19%|███▋               | 1940/10001 [54:57<4:02:54,  1.81s/batch]Batch 1900/10001 Done, mean position loss: 20.647596473693845\n",
      "Training NF1:  19%|███▋               | 1920/10001 [54:59<3:37:13,  1.61s/batch]Batch 1900/10001 Done, mean position loss: 20.598719851970674\n",
      "Training NF1:  19%|███▌               | 1903/10001 [55:00<3:29:50,  1.55s/batch]Batch 1900/10001 Done, mean position loss: 20.737742602825165\n",
      "Training NF1:  19%|███▋               | 1917/10001 [55:02<3:42:36,  1.65s/batch]Batch 1900/10001 Done, mean position loss: 21.75630450963974\n",
      "Training NF1:  19%|███▋               | 1927/10001 [55:03<4:13:57,  1.89s/batch]Batch 1900/10001 Done, mean position loss: 20.829688217639927\n",
      "Training NF1:  20%|███▋               | 1954/10001 [55:31<4:40:47,  2.09s/batch]Batch 1900/10001 Done, mean position loss: 20.902664494514468\n",
      "Training NF1:  20%|███▊               | 1980/10001 [55:51<3:01:26,  1.36s/batch]Batch 1900/10001 Done, mean position loss: 20.840363659858703\n",
      "Training NF1:  20%|███▊               | 1978/10001 [56:07<4:00:50,  1.80s/batch]Batch 2000/10001 Done, mean position loss: 20.667326693534854\n",
      "Training NF1:  20%|███▋               | 1972/10001 [56:29<3:14:08,  1.45s/batch]Batch 2000/10001 Done, mean position loss: 20.6717484331131\n",
      "Training NF1:  20%|███▊               | 1993/10001 [56:36<4:11:33,  1.88s/batch]Batch 2000/10001 Done, mean position loss: 20.874979078769684\n",
      "Training NF1:  20%|███▋               | 1971/10001 [56:39<2:31:40,  1.13s/batch]Batch 2000/10001 Done, mean position loss: 21.21368858098984\n",
      "Training NF1:  20%|███▋               | 1963/10001 [56:47<3:33:04,  1.59s/batch]Batch 2000/10001 Done, mean position loss: 20.682201895713806\n",
      "Training NF1:  20%|███▊               | 1982/10001 [56:48<3:00:55,  1.35s/batch]Batch 2000/10001 Done, mean position loss: 21.191536383628844\n",
      "Training NF1:  20%|███▊               | 1984/10001 [56:49<3:22:28,  1.52s/batch]Batch 2000/10001 Done, mean position loss: 20.62077259540558\n",
      "Training NF1:  20%|███▊               | 2001/10001 [56:48<4:21:37,  1.96s/batch]Batch 2000/10001 Done, mean position loss: 21.13324120759964\n",
      "Training NF1:  20%|███▊               | 2003/10001 [56:52<4:02:16,  1.82s/batch]Batch 2000/10001 Done, mean position loss: 21.021724338531495\n",
      "Training NF1:  20%|███▊               | 1983/10001 [56:52<3:42:57,  1.67s/batch]Batch 2000/10001 Done, mean position loss: 20.59926973581314\n",
      "Training NF1:  20%|███▋               | 1971/10001 [56:54<4:16:45,  1.92s/batch]Batch 2000/10001 Done, mean position loss: 20.700582573413847\n",
      "Training NF1:  20%|███▊               | 1984/10001 [57:01<3:38:44,  1.64s/batch]Batch 2000/10001 Done, mean position loss: 20.821434326171875\n",
      "Training NF1:  20%|███▊               | 1994/10001 [57:06<4:03:13,  1.82s/batch]Batch 2000/10001 Done, mean position loss: 20.715313072204587\n",
      "Training NF1:  20%|███▊               | 2005/10001 [57:07<3:50:26,  1.73s/batch]Batch 2000/10001 Done, mean position loss: 21.123160059452054\n",
      "Training NF1:  20%|███▊               | 2010/10001 [57:07<4:01:18,  1.81s/batch]Batch 2000/10001 Done, mean position loss: 20.850978045463563\n",
      "Training NF1:  20%|███▊               | 2021/10001 [57:12<4:09:34,  1.88s/batch]Batch 2000/10001 Done, mean position loss: 20.784911308288574\n",
      "Training NF1:  20%|███▊               | 2015/10001 [57:12<3:30:18,  1.58s/batch]Batch 2000/10001 Done, mean position loss: 20.981354815959932\n",
      "Training NF1:  20%|███▊               | 1981/10001 [57:17<3:30:51,  1.58s/batch]Batch 2000/10001 Done, mean position loss: 20.8014780831337\n",
      "Training NF1:  20%|███▊               | 1990/10001 [57:18<3:34:29,  1.61s/batch]Batch 2000/10001 Done, mean position loss: 21.135605285167692\n",
      "Training NF1:  20%|███▊               | 1994/10001 [57:18<4:03:47,  1.83s/batch]Batch 2000/10001 Done, mean position loss: 20.863963525295254\n",
      "Training NF1:  20%|███▊               | 2011/10001 [57:22<3:19:39,  1.50s/batch]Batch 2000/10001 Done, mean position loss: 20.875431866645812\n",
      "Training NF1:  20%|███▋               | 1958/10001 [57:29<4:13:07,  1.89s/batch]Batch 2000/10001 Done, mean position loss: 21.003156135082243\n",
      "Training NF1:  20%|███▊               | 2002/10001 [57:30<3:04:12,  1.38s/batch]Batch 2000/10001 Done, mean position loss: 21.28115183353424\n",
      "Training NF1:  20%|███▊               | 1989/10001 [57:30<3:27:51,  1.56s/batch]Batch 2000/10001 Done, mean position loss: 20.485939531326295\n",
      "Training NF1:  20%|███▊               | 2028/10001 [57:34<3:41:42,  1.67s/batch]Batch 2000/10001 Done, mean position loss: 21.09104651927948\n",
      "Training NF1:  20%|███▊               | 2005/10001 [57:34<3:20:58,  1.51s/batch]Batch 2000/10001 Done, mean position loss: 20.54286708831787\n",
      "Training NF1:  20%|███▊               | 1976/10001 [57:38<3:46:09,  1.69s/batch]Batch 2000/10001 Done, mean position loss: 21.448660869598392\n",
      "Training NF1:  20%|███▊               | 2012/10001 [57:37<4:23:23,  1.98s/batch]Batch 2000/10001 Done, mean position loss: 21.046721744537354\n",
      "Training NF1:  20%|███▊               | 2033/10001 [57:43<4:03:31,  1.83s/batch]Batch 2000/10001 Done, mean position loss: 21.156203060150148\n",
      "Training NF1:  20%|███▊               | 2010/10001 [57:44<3:38:00,  1.64s/batch]Batch 2000/10001 Done, mean position loss: 21.313317801952362\n",
      "Training NF1:  20%|███▊               | 2002/10001 [57:45<3:46:28,  1.70s/batch]Batch 2000/10001 Done, mean position loss: 21.025384194850922\n",
      "Training NF1:  20%|███▊               | 2016/10001 [57:46<4:11:58,  1.89s/batch]Batch 2000/10001 Done, mean position loss: 20.968929851055144\n",
      "Training NF1:  20%|███▊               | 1999/10001 [57:48<4:05:46,  1.84s/batch]Batch 2000/10001 Done, mean position loss: 20.96782134056091\n",
      "Training NF1:  20%|███▊               | 2001/10001 [57:48<4:06:05,  1.85s/batch]Batch 2000/10001 Done, mean position loss: 20.804668884277344\n",
      "Training NF1:  20%|███▊               | 2032/10001 [57:50<3:40:43,  1.66s/batch]Batch 2000/10001 Done, mean position loss: 20.731059587001802\n",
      "Training NF1:  20%|███▊               | 2024/10001 [57:51<3:43:05,  1.68s/batch]Batch 2000/10001 Done, mean position loss: 21.674535727500917\n",
      "Training NF1:  20%|███▊               | 2038/10001 [57:51<3:36:16,  1.63s/batch]Batch 2000/10001 Done, mean position loss: 20.646781334877012\n",
      "Training NF1:  20%|███▊               | 2039/10001 [57:52<3:50:46,  1.74s/batch]Batch 2000/10001 Done, mean position loss: 20.577479724884032\n",
      "Training NF1:  21%|███▉               | 2060/10001 [58:20<3:42:35,  1.68s/batch]Batch 2000/10001 Done, mean position loss: 20.90299948453903\n",
      "Training NF1:  21%|███▉               | 2068/10001 [58:42<3:29:36,  1.59s/batch]Batch 2000/10001 Done, mean position loss: 20.81817897558212\n",
      "Training NF1:  20%|███▉               | 2048/10001 [58:49<3:16:38,  1.48s/batch]Batch 2100/10001 Done, mean position loss: 20.65576202630997\n",
      "Training NF1:  21%|███▉               | 2085/10001 [59:19<3:45:44,  1.71s/batch]Batch 2100/10001 Done, mean position loss: 21.178783543109894\n",
      "Training NF1:  21%|███▉               | 2066/10001 [59:20<3:41:02,  1.67s/batch]Batch 2100/10001 Done, mean position loss: 20.65481950521469\n",
      "Training NF1:  21%|███▉               | 2079/10001 [59:29<3:39:44,  1.66s/batch]Batch 2100/10001 Done, mean position loss: 20.82630033016205\n",
      "Training NF1:  21%|███▉               | 2083/10001 [59:34<3:35:26,  1.63s/batch]Batch 2100/10001 Done, mean position loss: 20.62237661123276\n",
      "Training NF1:  21%|███▉               | 2068/10001 [59:36<3:34:26,  1.62s/batch]Batch 2100/10001 Done, mean position loss: 20.667591984272\n",
      "Training NF1:  21%|███▉               | 2090/10001 [59:35<3:42:08,  1.68s/batch]Batch 2100/10001 Done, mean position loss: 20.67026596546173\n",
      "Training NF1:  21%|███▉               | 2065/10001 [59:37<3:18:54,  1.50s/batch]Batch 2100/10001 Done, mean position loss: 21.1247270321846\n",
      "Training NF1:  21%|████               | 2131/10001 [59:37<3:37:39,  1.66s/batch]Batch 2100/10001 Done, mean position loss: 21.256195390224455\n",
      "Training NF1:  21%|███▉               | 2076/10001 [59:37<3:30:23,  1.59s/batch]Batch 2100/10001 Done, mean position loss: 20.977732255458832\n",
      "Training NF1:  21%|███▉               | 2105/10001 [59:45<3:55:39,  1.79s/batch]Batch 2100/10001 Done, mean position loss: 20.606522297859193\n",
      "Training NF1:  21%|███▉               | 2075/10001 [59:48<4:03:15,  1.84s/batch]Batch 2100/10001 Done, mean position loss: 20.825557610988618\n",
      "Training NF1:  21%|███▉               | 2093/10001 [59:52<3:53:09,  1.77s/batch]Batch 2100/10001 Done, mean position loss: 20.715059177875517\n",
      "Training NF1:  21%|████               | 2113/10001 [59:54<3:37:44,  1.66s/batch]Batch 2100/10001 Done, mean position loss: 20.81659034967423\n",
      "Training NF1:  21%|███▉               | 2089/10001 [59:56<3:21:23,  1.53s/batch]Batch 2100/10001 Done, mean position loss: 20.972592005729673\n",
      "Training NF1:  21%|███▉               | 2075/10001 [59:57<3:34:54,  1.63s/batch]Batch 2100/10001 Done, mean position loss: 21.116682353019712\n",
      "Training NF1:  21%|███▌             | 2117/10001 [1:00:02<3:46:48,  1.73s/batch]Batch 2100/10001 Done, mean position loss: 20.89009091615677\n",
      "Training NF1:  21%|███▌             | 2084/10001 [1:00:03<3:35:47,  1.64s/batch]\n",
      "Training NF1:  21%|███▌             | 2090/10001 [1:00:05<4:25:51,  2.02s/batch]Batch 2100/10001 Done, mean position loss: 20.75761283159256\n",
      "Training NF1:  21%|███▌             | 2084/10001 [1:00:09<3:56:31,  1.79s/batch]Batch 2100/10001 Done, mean position loss: 21.117589914798735\n",
      "Training NF1:  21%|███▌             | 2086/10001 [1:00:12<4:15:56,  1.94s/batch]Batch 2100/10001 Done, mean position loss: 20.864228765964505\n",
      "Training NF1:  21%|███▌             | 2108/10001 [1:00:16<3:48:21,  1.74s/batch]Batch 2100/10001 Done, mean position loss: 21.093311693668365\n",
      "Training NF1:  21%|███▍             | 2057/10001 [1:00:20<3:56:53,  1.79s/batch]Batch 2100/10001 Done, mean position loss: 20.982440094947812\n",
      "Training NF1:  21%|███▌             | 2127/10001 [1:00:22<4:16:04,  1.95s/batch]Batch 2100/10001 Done, mean position loss: 20.472940781116485\n",
      "Training NF1:  21%|███▌             | 2107/10001 [1:00:22<3:31:29,  1.61s/batch]Batch 2100/10001 Done, mean position loss: 20.539740207195283\n",
      "Training NF1:  21%|███▋             | 2137/10001 [1:00:22<3:20:09,  1.53s/batch]Batch 2100/10001 Done, mean position loss: 21.003860750198363\n",
      "Training NF1:  21%|███▌             | 2123/10001 [1:00:23<3:34:43,  1.64s/batch]Batch 2100/10001 Done, mean position loss: 21.314504947662353\n",
      "Training NF1:  21%|███▋             | 2133/10001 [1:00:27<3:16:00,  1.49s/batch]Batch 2100/10001 Done, mean position loss: 21.405294778347017\n",
      "Training NF1:  21%|███▋             | 2135/10001 [1:00:33<3:18:27,  1.51s/batch]Batch 2100/10001 Done, mean position loss: 20.95653307676315\n",
      "Training NF1:  21%|███▌             | 2096/10001 [1:00:35<3:37:41,  1.65s/batch]Batch 2100/10001 Done, mean position loss: 21.27819076061249\n",
      "Training NF1:  21%|███▌             | 2125/10001 [1:00:36<3:24:11,  1.56s/batch]Batch 2100/10001 Done, mean position loss: 21.145916385650636\n",
      "Training NF1:  21%|███▌             | 2125/10001 [1:00:37<4:09:00,  1.90s/batch]Batch 2100/10001 Done, mean position loss: 20.960968034267424\n",
      "Training NF1:  21%|███▌             | 2098/10001 [1:00:38<3:46:24,  1.72s/batch]Batch 2100/10001 Done, mean position loss: 21.011257457733155\n",
      "Training NF1:  21%|███▌             | 2102/10001 [1:00:38<3:22:15,  1.54s/batch]Batch 2100/10001 Done, mean position loss: 20.788855698108673\n",
      "Training NF1:  21%|███▋             | 2142/10001 [1:00:42<4:06:10,  1.88s/batch]Batch 2100/10001 Done, mean position loss: 20.701539561748504\n",
      "Training NF1:  21%|███▋             | 2150/10001 [1:00:43<3:12:32,  1.47s/batch]Batch 2100/10001 Done, mean position loss: 20.62344415664673\n",
      "Training NF1:  21%|███▌             | 2105/10001 [1:00:43<3:35:24,  1.64s/batch]Batch 2100/10001 Done, mean position loss: 21.65405290365219\n",
      "Training NF1:  21%|███▋             | 2142/10001 [1:00:44<3:51:05,  1.76s/batch]Batch 2100/10001 Done, mean position loss: 20.559260828495027\n",
      "Training NF1:  21%|███▌             | 2132/10001 [1:01:11<4:31:33,  2.07s/batch]Batch 2100/10001 Done, mean position loss: 20.8763627576828\n",
      "Training NF1:  22%|███▋             | 2160/10001 [1:01:35<3:35:38,  1.65s/batch]Batch 2100/10001 Done, mean position loss: 20.822001516819\n",
      "Training NF1:  21%|███▌             | 2116/10001 [1:01:39<3:45:50,  1.72s/batch]Batch 2200/10001 Done, mean position loss: 20.64254059076309\n",
      "Training NF1:  22%|███▋             | 2188/10001 [1:02:04<3:54:24,  1.80s/batch]Batch 2200/10001 Done, mean position loss: 21.16121228456497\n",
      "Training NF1:  22%|███▋             | 2191/10001 [1:02:10<4:11:24,  1.93s/batch]Batch 2200/10001 Done, mean position loss: 20.635195050239563\n",
      "Training NF1:  22%|███▊             | 2209/10001 [1:02:16<3:21:46,  1.55s/batch]Batch 2200/10001 Done, mean position loss: 20.814048326015474\n",
      "Training NF1:  22%|███▋             | 2197/10001 [1:02:21<4:01:56,  1.86s/batch]Batch 2200/10001 Done, mean position loss: 20.658305027484893\n",
      "Training NF1:  22%|███▋             | 2170/10001 [1:02:23<3:56:24,  1.81s/batch]Batch 2200/10001 Done, mean position loss: 21.09588241815567\n",
      "Training NF1:  22%|███▋             | 2206/10001 [1:02:24<3:02:08,  1.40s/batch]Batch 2200/10001 Done, mean position loss: 20.661914672851562\n",
      "Training NF1:  22%|███▋             | 2195/10001 [1:02:25<3:51:34,  1.78s/batch]Batch 2200/10001 Done, mean position loss: 20.607855422496797\n",
      "Training NF1:  22%|███▋             | 2203/10001 [1:02:26<3:04:11,  1.42s/batch]Batch 2200/10001 Done, mean position loss: 20.94676113128662\n",
      "Training NF1:  22%|███▋             | 2173/10001 [1:02:28<3:50:12,  1.76s/batch]Batch 2200/10001 Done, mean position loss: 21.226320860385893\n",
      "Training NF1:  22%|███▋             | 2151/10001 [1:02:37<4:05:47,  1.88s/batch]Batch 2200/10001 Done, mean position loss: 20.837627325057987\n",
      "Training NF1:  22%|███▋             | 2201/10001 [1:02:37<3:57:31,  1.83s/batch]Batch 2200/10001 Done, mean position loss: 20.59143739461899\n",
      "Training NF1:  22%|███▊             | 2218/10001 [1:02:43<3:23:16,  1.57s/batch]Batch 2200/10001 Done, mean position loss: 21.093412780761717\n",
      "Training NF1:  22%|███▋             | 2194/10001 [1:02:44<3:41:59,  1.71s/batch]Batch 2200/10001 Done, mean position loss: 20.81544931411743\n",
      "Training NF1:  22%|███▋             | 2179/10001 [1:02:44<4:43:26,  2.17s/batch]Batch 2200/10001 Done, mean position loss: 20.701635906696318\n",
      "Training NF1:  22%|███▋             | 2175/10001 [1:02:44<3:36:49,  1.66s/batch]Batch 2200/10001 Done, mean position loss: 20.95360702753067\n",
      "Training NF1:  22%|███▋             | 2206/10001 [1:02:53<3:35:36,  1.66s/batch]Batch 2200/10001 Done, mean position loss: 20.884162290096285\n",
      "Training NF1:  22%|███▊             | 2218/10001 [1:02:55<4:15:41,  1.97s/batch]Batch 2200/10001 Done, mean position loss: 20.799215190410614\n",
      "Training NF1:  22%|███▋             | 2193/10001 [1:02:57<3:26:56,  1.59s/batch]Batch 2200/10001 Done, mean position loss: 20.758036468029022\n",
      "Training NF1:  21%|███▋             | 2149/10001 [1:02:59<3:53:42,  1.79s/batch]Batch 2200/10001 Done, mean position loss: 21.133549144268038\n",
      "Training NF1:  22%|███▋             | 2193/10001 [1:03:02<3:12:44,  1.48s/batch]Batch 2200/10001 Done, mean position loss: 20.85105609178543\n",
      "Training NF1:  22%|███▋             | 2187/10001 [1:03:07<4:05:53,  1.89s/batch]Batch 2200/10001 Done, mean position loss: 21.260618760585785\n",
      "Training NF1:  22%|███▊             | 2208/10001 [1:03:10<3:47:57,  1.76s/batch]Batch 2200/10001 Done, mean position loss: 21.072308950424194\n",
      "Training NF1:  22%|███▊             | 2211/10001 [1:03:15<3:44:24,  1.73s/batch]Batch 2200/10001 Done, mean position loss: 20.532830538749693\n",
      "Training NF1:  22%|███▋             | 2204/10001 [1:03:15<3:41:43,  1.71s/batch]Batch 2200/10001 Done, mean position loss: 20.982677400112152\n",
      "Training NF1:  22%|███▋             | 2172/10001 [1:03:16<4:17:27,  1.97s/batch]Batch 2200/10001 Done, mean position loss: 20.470046725273132\n",
      "Training NF1:  22%|███▊             | 2209/10001 [1:03:17<4:07:07,  1.90s/batch]Batch 2200/10001 Done, mean position loss: 21.423751871585843\n",
      "Training NF1:  22%|███▊             | 2232/10001 [1:03:17<4:14:16,  1.96s/batch]Batch 2200/10001 Done, mean position loss: 21.018833293914795\n",
      "Training NF1:  22%|███▋             | 2206/10001 [1:03:24<3:31:33,  1.63s/batch]Batch 2200/10001 Done, mean position loss: 20.952514035701753\n",
      "Training NF1:  22%|███▋             | 2177/10001 [1:03:25<3:39:38,  1.68s/batch]Batch 2200/10001 Done, mean position loss: 20.93918702363968\n",
      "Training NF1:  22%|███▋             | 2205/10001 [1:03:24<3:47:49,  1.75s/batch]Batch 2200/10001 Done, mean position loss: 21.14103363275528\n",
      "Training NF1:  22%|███▊             | 2208/10001 [1:03:30<3:47:16,  1.75s/batch]Batch 2200/10001 Done, mean position loss: 20.805851900577544\n",
      "Training NF1:  22%|███▋             | 2205/10001 [1:03:30<3:20:52,  1.55s/batch]Batch 2200/10001 Done, mean position loss: 21.298693907260894\n",
      "Training NF1:  22%|███▊             | 2209/10001 [1:03:32<4:25:55,  2.05s/batch]Batch 2200/10001 Done, mean position loss: 21.005390617847443\n",
      "Training NF1:  22%|███▊             | 2216/10001 [1:03:35<3:34:10,  1.65s/batch]Batch 2200/10001 Done, mean position loss: 21.657613961696626\n",
      "Training NF1:  22%|███▊             | 2247/10001 [1:03:36<3:50:50,  1.79s/batch]Batch 2200/10001 Done, mean position loss: 20.550408749580384\n",
      "Training NF1:  23%|███▊             | 2253/10001 [1:03:41<3:38:14,  1.69s/batch]Batch 2200/10001 Done, mean position loss: 20.70205373287201\n",
      "Training NF1:  22%|███▊             | 2216/10001 [1:03:41<3:50:34,  1.78s/batch]Batch 2200/10001 Done, mean position loss: 20.61482595682144\n",
      "Training NF1:  22%|███▊             | 2242/10001 [1:04:07<3:28:56,  1.62s/batch]Batch 2200/10001 Done, mean position loss: 20.881958961486816\n",
      "Training NF1:  23%|███▊             | 2269/10001 [1:04:31<4:10:43,  1.95s/batch]Batch 2200/10001 Done, mean position loss: 20.805646052360533\n",
      "Training NF1:  23%|███▊             | 2265/10001 [1:04:32<3:54:32,  1.82s/batch]Batch 2300/10001 Done, mean position loss: 20.637018823623656\n",
      "Training NF1:  23%|███▉             | 2287/10001 [1:04:54<3:29:29,  1.63s/batch]Batch 2300/10001 Done, mean position loss: 21.15709508419037\n",
      "Training NF1:  23%|███▉             | 2288/10001 [1:05:03<3:49:38,  1.79s/batch]Batch 2300/10001 Done, mean position loss: 20.6266889333725\n",
      "Training NF1:  23%|███▊             | 2269/10001 [1:05:07<3:40:02,  1.71s/batch]Batch 2300/10001 Done, mean position loss: 20.809729084968566\n",
      "Training NF1:  23%|███▊             | 2276/10001 [1:05:12<3:20:49,  1.56s/batch]Batch 2300/10001 Done, mean position loss: 20.64162744998932\n",
      "Training NF1:  23%|███▊             | 2259/10001 [1:05:18<3:13:30,  1.50s/batch]Batch 2300/10001 Done, mean position loss: 21.19361937046051\n",
      "Training NF1:  23%|███▊             | 2262/10001 [1:05:18<3:44:34,  1.74s/batch]Batch 2300/10001 Done, mean position loss: 20.66478935480118\n",
      "Training NF1:  23%|███▉             | 2298/10001 [1:05:20<3:33:46,  1.67s/batch]Batch 2300/10001 Done, mean position loss: 21.085742244720457\n",
      "Training NF1:  23%|███▉             | 2294/10001 [1:05:25<3:27:42,  1.62s/batch]Batch 2300/10001 Done, mean position loss: 20.610666790008544\n",
      "Training NF1:  23%|███▊             | 2263/10001 [1:05:25<3:52:55,  1.81s/batch]Batch 2300/10001 Done, mean position loss: 20.91635325908661\n",
      "Training NF1:  23%|███▊             | 2274/10001 [1:05:34<3:21:24,  1.56s/batch]Batch 2300/10001 Done, mean position loss: 20.80211240053177\n",
      "Training NF1:  23%|███▊             | 2279/10001 [1:05:36<4:06:09,  1.91s/batch]Batch 2300/10001 Done, mean position loss: 20.799758069515228\n",
      "Training NF1:  23%|███▊             | 2263/10001 [1:05:37<3:59:56,  1.86s/batch]Batch 2300/10001 Done, mean position loss: 20.568772068023684\n",
      "Training NF1:  23%|███▉             | 2293/10001 [1:05:38<3:25:49,  1.60s/batch]Batch 2300/10001 Done, mean position loss: 20.685252883434295\n",
      "Training NF1:  23%|███▉             | 2309/10001 [1:05:39<3:30:47,  1.64s/batch]Batch 2300/10001 Done, mean position loss: 21.05298818588257\n",
      "Training NF1:  23%|███▉             | 2310/10001 [1:05:41<3:30:46,  1.64s/batch]Batch 2300/10001 Done, mean position loss: 20.9616085934639\n",
      "Training NF1:  23%|███▊             | 2273/10001 [1:05:47<3:59:57,  1.86s/batch]Batch 2300/10001 Done, mean position loss: 20.857419810295106\n",
      "Training NF1:  23%|███▉             | 2321/10001 [1:05:50<3:25:36,  1.61s/batch]Batch 2300/10001 Done, mean position loss: 20.788647935390472\n",
      "Training NF1:  23%|███▉             | 2317/10001 [1:05:51<3:36:58,  1.69s/batch]Batch 2300/10001 Done, mean position loss: 20.751766097545623\n",
      "Training NF1:  23%|███▉             | 2285/10001 [1:05:53<3:43:58,  1.74s/batch]Batch 2300/10001 Done, mean position loss: 21.135100197792053\n",
      "Training NF1:  23%|███▉             | 2313/10001 [1:05:55<4:00:28,  1.88s/batch]Batch 2300/10001 Done, mean position loss: 20.83981837272644\n",
      "Training NF1:  23%|███▉             | 2293/10001 [1:06:02<3:39:26,  1.71s/batch]Batch 2300/10001 Done, mean position loss: 21.251698162555694\n",
      "Training NF1:  23%|███▉             | 2288/10001 [1:06:03<3:53:41,  1.82s/batch]Batch 2300/10001 Done, mean position loss: 20.46663269281387\n",
      "Training NF1:  23%|███▉             | 2327/10001 [1:06:08<3:43:18,  1.75s/batch]Batch 2300/10001 Done, mean position loss: 21.066118333339688\n",
      "Training NF1:  23%|███▉             | 2300/10001 [1:06:12<3:33:28,  1.66s/batch]Batch 2300/10001 Done, mean position loss: 20.51856767177582\n",
      "Training NF1:  23%|███▉             | 2320/10001 [1:06:12<4:24:23,  2.07s/batch]Batch 2300/10001 Done, mean position loss: 20.991218180656432\n",
      "Training NF1:  23%|███▉             | 2337/10001 [1:06:13<3:17:01,  1.54s/batch]Batch 2300/10001 Done, mean position loss: 21.363664989471438\n",
      "Training NF1:  23%|███▉             | 2309/10001 [1:06:14<3:26:49,  1.61s/batch]Batch 2300/10001 Done, mean position loss: 21.122359364032746\n",
      "Training NF1:  23%|███▉             | 2309/10001 [1:06:16<3:29:15,  1.63s/batch]Batch 2300/10001 Done, mean position loss: 20.97625979423523\n",
      "Training NF1:  23%|███▉             | 2335/10001 [1:06:18<4:24:37,  2.07s/batch]Batch 2300/10001 Done, mean position loss: 20.90290411710739\n",
      "Training NF1:  23%|███▉             | 2325/10001 [1:06:19<3:59:48,  1.87s/batch]Batch 2300/10001 Done, mean position loss: 20.949552965164184\n",
      "Training NF1:  23%|███▉             | 2307/10001 [1:06:21<3:21:06,  1.57s/batch]Batch 2300/10001 Done, mean position loss: 20.99218776702881\n",
      "Training NF1:  23%|███▉             | 2330/10001 [1:06:25<3:21:30,  1.58s/batch]Batch 2300/10001 Done, mean position loss: 20.78112506866455\n",
      "Training NF1:  23%|███▉             | 2330/10001 [1:06:28<3:41:09,  1.73s/batch]Batch 2300/10001 Done, mean position loss: 20.533875916004185\n",
      "Training NF1:  23%|███▉             | 2323/10001 [1:06:29<3:53:31,  1.82s/batch]Batch 2300/10001 Done, mean position loss: 21.27653015136719\n",
      "Training NF1:  23%|███▉             | 2320/10001 [1:06:28<3:39:02,  1.71s/batch]Batch 2300/10001 Done, mean position loss: 21.70657172441483\n",
      "Training NF1:  24%|████             | 2356/10001 [1:06:35<3:24:10,  1.60s/batch]Batch 2300/10001 Done, mean position loss: 20.60769154071808\n",
      "Training NF1:  23%|███▉             | 2347/10001 [1:06:42<3:32:23,  1.66s/batch]Batch 2300/10001 Done, mean position loss: 20.700180740356444\n",
      "Training NF1:  24%|████             | 2376/10001 [1:07:01<3:29:18,  1.65s/batch]Batch 2300/10001 Done, mean position loss: 20.863356969356534\n",
      "Training NF1:  23%|███▉             | 2300/10001 [1:07:21<3:43:40,  1.74s/batch]Batch 2400/10001 Done, mean position loss: 20.64616502523422\n",
      "Training NF1:  23%|███▉             | 2344/10001 [1:07:22<3:35:21,  1.69s/batch]Batch 2300/10001 Done, mean position loss: 20.781830022335054\n",
      "Training NF1:  24%|████             | 2376/10001 [1:07:48<3:44:49,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 21.173869705200197\n",
      "Training NF1:  24%|████             | 2393/10001 [1:07:49<3:31:46,  1.67s/batch]Batch 2400/10001 Done, mean position loss: 20.62482394218445\n",
      "Training NF1:  23%|███▉             | 2345/10001 [1:07:59<3:24:21,  1.60s/batch]Batch 2400/10001 Done, mean position loss: 20.6204999256134\n",
      "Training NF1:  24%|████▏            | 2428/10001 [1:08:02<2:45:12,  1.31s/batch]Batch 2400/10001 Done, mean position loss: 20.8141974401474\n",
      "Training NF1:  24%|████             | 2381/10001 [1:08:08<3:35:30,  1.70s/batch]Batch 2400/10001 Done, mean position loss: 20.646002953052523\n",
      "Training NF1:  24%|████             | 2389/10001 [1:08:08<3:17:28,  1.56s/batch]Batch 2400/10001 Done, mean position loss: 20.615558631420136\n",
      "Training NF1:  24%|████             | 2368/10001 [1:08:11<3:10:56,  1.50s/batch]Batch 2400/10001 Done, mean position loss: 20.90636018037796\n",
      "Training NF1:  24%|████             | 2405/10001 [1:08:13<2:59:28,  1.42s/batch]Batch 2400/10001 Done, mean position loss: 21.090622658729554\n",
      "Training NF1:  24%|████             | 2393/10001 [1:08:17<3:30:07,  1.66s/batch]Batch 2400/10001 Done, mean position loss: 21.208841767311096\n",
      "Training NF1:  24%|████             | 2377/10001 [1:08:25<3:28:30,  1.64s/batch]Batch 2400/10001 Done, mean position loss: 20.804188838005068\n",
      "Training NF1:  24%|████             | 2382/10001 [1:08:27<3:58:06,  1.88s/batch]Batch 2400/10001 Done, mean position loss: 20.789452085494993\n",
      "Training NF1:  23%|███▉             | 2350/10001 [1:08:28<3:49:34,  1.80s/batch]Batch 2400/10001 Done, mean position loss: 20.583137836456302\n",
      "Training NF1:  24%|████             | 2419/10001 [1:08:30<4:21:19,  2.07s/batch]Batch 2400/10001 Done, mean position loss: 20.935535044670104\n",
      "Training NF1:  24%|████             | 2370/10001 [1:08:30<4:15:08,  2.01s/batch]Batch 2400/10001 Done, mean position loss: 21.03768280029297\n",
      "Training NF1:  24%|████             | 2382/10001 [1:08:30<3:36:32,  1.71s/batch]Batch 2400/10001 Done, mean position loss: 20.68023074388504\n",
      "Training NF1:  24%|████             | 2422/10001 [1:08:41<2:58:21,  1.41s/batch]Batch 2400/10001 Done, mean position loss: 20.71289478302002\n",
      "Training NF1:  24%|████▏            | 2435/10001 [1:08:44<3:51:57,  1.84s/batch]Batch 2400/10001 Done, mean position loss: 20.835151991844178\n",
      "Training NF1:  24%|████             | 2385/10001 [1:08:49<3:49:15,  1.81s/batch]Batch 2400/10001 Done, mean position loss: 20.829085137844086\n",
      "Training NF1:  24%|████             | 2383/10001 [1:08:51<4:13:43,  2.00s/batch]Batch 2400/10001 Done, mean position loss: 21.110363960266113\n",
      "Training NF1:  24%|███▉             | 2352/10001 [1:08:52<3:48:05,  1.79s/batch]Batch 2400/10001 Done, mean position loss: 20.77853217363357\n",
      "Training NF1:  24%|████             | 2383/10001 [1:08:51<3:32:38,  1.67s/batch]Batch 2400/10001 Done, mean position loss: 21.254138193130494\n",
      "Training NF1:  24%|████             | 2407/10001 [1:09:00<3:44:07,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 20.464602036476137\n",
      "Training NF1:  24%|████▏            | 2438/10001 [1:09:00<3:17:51,  1.57s/batch]Batch 2400/10001 Done, mean position loss: 21.051450526714326\n",
      "Training NF1:  24%|████             | 2396/10001 [1:09:01<3:06:28,  1.47s/batch]Batch 2400/10001 Done, mean position loss: 21.363814833164216\n",
      "Training NF1:  24%|████             | 2409/10001 [1:09:06<3:30:10,  1.66s/batch]Batch 2400/10001 Done, mean position loss: 20.502634556293486\n",
      "Training NF1:  24%|████             | 2415/10001 [1:09:06<3:34:18,  1.70s/batch]Batch 2400/10001 Done, mean position loss: 21.106782660484313\n",
      "Training NF1:  24%|████▏            | 2436/10001 [1:09:07<3:24:11,  1.62s/batch]Batch 2400/10001 Done, mean position loss: 20.89110940694809\n",
      "Training NF1:  24%|████             | 2411/10001 [1:09:09<3:23:00,  1.60s/batch]Batch 2400/10001 Done, mean position loss: 20.98388473033905\n",
      "Training NF1:  24%|████             | 2373/10001 [1:09:09<3:05:44,  1.46s/batch]Batch 2400/10001 Done, mean position loss: 20.764064240455625\n",
      "Training NF1:  24%|████             | 2389/10001 [1:09:14<3:35:46,  1.70s/batch]Batch 2400/10001 Done, mean position loss: 20.940887117385863\n",
      "Training NF1:  24%|████             | 2408/10001 [1:09:16<3:35:45,  1.70s/batch]Batch 2400/10001 Done, mean position loss: 21.000612218379977\n",
      "Training NF1:  24%|████▏            | 2448/10001 [1:09:20<3:07:29,  1.49s/batch]Batch 2400/10001 Done, mean position loss: 20.90851182460785\n",
      "Training NF1:  25%|████▏            | 2476/10001 [1:09:22<3:15:01,  1.55s/batch]Batch 2400/10001 Done, mean position loss: 20.528295204639434\n",
      "Training NF1:  24%|████             | 2425/10001 [1:09:24<3:38:06,  1.73s/batch]Batch 2400/10001 Done, mean position loss: 21.694372758865356\n",
      "Training NF1:  24%|████             | 2405/10001 [1:09:23<3:48:21,  1.80s/batch]Batch 2400/10001 Done, mean position loss: 20.60267831325531\n",
      "Training NF1:  25%|████▏            | 2452/10001 [1:09:23<3:51:58,  1.84s/batch]Batch 2400/10001 Done, mean position loss: 21.2491086769104\n",
      "Training NF1:  24%|████             | 2376/10001 [1:09:32<3:04:48,  1.45s/batch]Batch 2400/10001 Done, mean position loss: 20.701164972782134\n",
      "Training NF1:  25%|████▏            | 2466/10001 [1:09:59<3:24:25,  1.63s/batch]Batch 2400/10001 Done, mean position loss: 20.84681033372879\n",
      "Training NF1:  24%|████             | 2421/10001 [1:10:07<3:53:18,  1.85s/batch]Batch 2500/10001 Done, mean position loss: 20.622868218421935\n",
      "Training NF1:  25%|████▏            | 2465/10001 [1:10:16<3:32:27,  1.69s/batch]Batch 2400/10001 Done, mean position loss: 20.772937982082368\n",
      "Training NF1:  24%|████▏            | 2443/10001 [1:10:36<3:39:36,  1.74s/batch]Batch 2500/10001 Done, mean position loss: 21.143058269023896\n",
      "Training NF1:  25%|████▏            | 2467/10001 [1:10:43<4:27:02,  2.13s/batch]Batch 2500/10001 Done, mean position loss: 20.613320024013518\n",
      "Training NF1:  25%|████▏            | 2462/10001 [1:10:47<3:45:44,  1.80s/batch]Batch 2500/10001 Done, mean position loss: 20.612861819267273\n",
      "Training NF1:  25%|████▏            | 2464/10001 [1:10:53<3:29:52,  1.67s/batch]Batch 2500/10001 Done, mean position loss: 20.8084765291214\n",
      "Training NF1:  25%|████▏            | 2456/10001 [1:10:56<3:23:52,  1.62s/batch]Batch 2500/10001 Done, mean position loss: 20.64034339427948\n",
      "Training NF1:  25%|████▏            | 2491/10001 [1:10:59<3:46:08,  1.81s/batch]Batch 2500/10001 Done, mean position loss: 20.610608537197113\n",
      "Training NF1:  25%|████▏            | 2466/10001 [1:11:00<3:17:26,  1.57s/batch]Batch 2500/10001 Done, mean position loss: 20.874795589447025\n",
      "Training NF1:  25%|████▏            | 2464/10001 [1:11:04<3:26:35,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 21.18819176197052\n",
      "Training NF1:  25%|████▎            | 2508/10001 [1:11:07<3:20:18,  1.60s/batch]Batch 2500/10001 Done, mean position loss: 21.073025608062746\n",
      "Training NF1:  25%|████▏            | 2468/10001 [1:11:16<3:23:23,  1.62s/batch]Batch 2500/10001 Done, mean position loss: 20.78371329307556\n",
      "Training NF1:  25%|████▎            | 2519/10001 [1:11:17<3:55:11,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 20.56406390428543\n",
      "Training NF1:  25%|████▏            | 2499/10001 [1:11:23<3:36:10,  1.73s/batch]Batch 2500/10001 Done, mean position loss: 21.026392662525176\n",
      "Training NF1:  25%|████▎            | 2528/10001 [1:11:23<3:44:34,  1.80s/batch]Batch 2500/10001 Done, mean position loss: 20.791356859207156\n",
      "Training NF1:  25%|████▏            | 2481/10001 [1:11:25<3:41:11,  1.76s/batch]Batch 2500/10001 Done, mean position loss: 20.661292171478273\n",
      "Training NF1:  25%|████▏            | 2473/10001 [1:11:26<3:14:21,  1.55s/batch]Batch 2500/10001 Done, mean position loss: 20.918885056972506\n",
      "Training NF1:  25%|████▏            | 2497/10001 [1:11:37<3:49:11,  1.83s/batch]Batch 2500/10001 Done, mean position loss: 20.843834772109986\n",
      "Training NF1:  25%|████▏            | 2488/10001 [1:11:38<3:36:18,  1.73s/batch]Batch 2500/10001 Done, mean position loss: 20.836743559837338\n",
      "Training NF1:  25%|████▎            | 2509/10001 [1:11:39<3:28:49,  1.67s/batch]Batch 2500/10001 Done, mean position loss: 20.7200511264801\n",
      "Training NF1:  25%|████▎            | 2524/10001 [1:11:42<3:55:35,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 21.120111179351802\n",
      "Training NF1:  25%|████▏            | 2484/10001 [1:11:45<3:56:52,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 20.765934846401215\n",
      "Training NF1:  25%|████▏            | 2490/10001 [1:11:49<4:17:46,  2.06s/batch]Batch 2500/10001 Done, mean position loss: 21.219431347846985\n",
      "Training NF1:  25%|████▎            | 2517/10001 [1:11:53<4:16:10,  2.05s/batch]Batch 2500/10001 Done, mean position loss: 21.049641599655153\n",
      "Training NF1:  25%|████▏            | 2488/10001 [1:11:55<3:07:34,  1.50s/batch]Batch 2500/10001 Done, mean position loss: 20.97975148200989\n",
      "Training NF1:  25%|████▎            | 2532/10001 [1:11:56<3:28:37,  1.68s/batch]Batch 2500/10001 Done, mean position loss: 21.331976060867312\n",
      "Training NF1:  25%|████▎            | 2523/10001 [1:11:56<3:34:10,  1.72s/batch]Batch 2500/10001 Done, mean position loss: 20.499513857364654\n",
      "Training NF1:  25%|████▎            | 2520/10001 [1:11:56<3:38:51,  1.76s/batch]Batch 2500/10001 Done, mean position loss: 20.456972563266753\n",
      "Training NF1:  25%|████▏            | 2491/10001 [1:12:00<3:24:30,  1.63s/batch]Batch 2500/10001 Done, mean position loss: 20.750281593799592\n",
      "Training NF1:  25%|████▎            | 2544/10001 [1:12:02<3:12:09,  1.55s/batch]Batch 2500/10001 Done, mean position loss: 20.842697172164918\n",
      "Training NF1:  25%|████▏            | 2497/10001 [1:12:04<3:25:18,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 20.92291708946228\n",
      "Training NF1:  25%|████▎            | 2531/10001 [1:12:09<3:34:59,  1.73s/batch]Batch 2500/10001 Done, mean position loss: 21.105265822410587\n",
      "Training NF1:  25%|████▎            | 2519/10001 [1:12:11<3:25:38,  1.65s/batch]Batch 2500/10001 Done, mean position loss: 20.986493680477142\n",
      "Training NF1:  26%|████▎            | 2556/10001 [1:12:13<3:37:37,  1.75s/batch]Batch 2500/10001 Done, mean position loss: 21.664979212284088\n",
      "Training NF1:  25%|████▎            | 2514/10001 [1:12:14<4:16:44,  2.06s/batch]Batch 2500/10001 Done, mean position loss: 21.244853632450102\n",
      "Training NF1:  25%|████▎            | 2532/10001 [1:12:14<3:34:20,  1.72s/batch]Batch 2500/10001 Done, mean position loss: 20.575990405082702\n",
      "Training NF1:  25%|████▎            | 2523/10001 [1:12:17<4:11:13,  2.02s/batch]Batch 2500/10001 Done, mean position loss: 20.94382925271988\n",
      "Training NF1:  25%|████▏            | 2473/10001 [1:12:19<3:57:42,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 20.50680776119232\n",
      "Training NF1:  26%|████▎            | 2558/10001 [1:12:25<3:23:17,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 20.69029876470566\n",
      "Training NF1:  26%|████▎            | 2565/10001 [1:12:55<4:05:57,  1.98s/batch]Batch 2500/10001 Done, mean position loss: 20.850671923160554\n",
      "Training NF1:  25%|████▏            | 2494/10001 [1:12:57<3:39:58,  1.76s/batch]Batch 2600/10001 Done, mean position loss: 20.624073786735536\n",
      "Training NF1:  26%|████▎            | 2554/10001 [1:13:10<4:24:38,  2.13s/batch]Batch 2500/10001 Done, mean position loss: 20.77346544265747\n",
      "Training NF1:  26%|████▍            | 2598/10001 [1:13:34<3:17:46,  1.60s/batch]Batch 2600/10001 Done, mean position loss: 21.177599227428434\n",
      "Training NF1:  26%|████▍            | 2587/10001 [1:13:38<3:23:17,  1.65s/batch]Batch 2600/10001 Done, mean position loss: 20.796111040115356\n",
      "Training NF1:  26%|████▍            | 2598/10001 [1:13:39<4:05:16,  1.99s/batch]Batch 2600/10001 Done, mean position loss: 20.58705598115921\n",
      "Training NF1:  25%|████▎            | 2546/10001 [1:13:45<3:42:47,  1.79s/batch]Batch 2600/10001 Done, mean position loss: 20.59351581811905\n",
      "Training NF1:  26%|████▎            | 2562/10001 [1:13:49<3:32:14,  1.71s/batch]Batch 2600/10001 Done, mean position loss: 20.600851516723633\n",
      "Training NF1:  26%|████▎            | 2564/10001 [1:13:48<3:20:49,  1.62s/batch]Batch 2600/10001 Done, mean position loss: 20.635440378189084\n",
      "Training NF1:  26%|████▍            | 2587/10001 [1:13:55<3:14:40,  1.58s/batch]Batch 2600/10001 Done, mean position loss: 20.84069757461548\n",
      "Training NF1:  26%|████▍            | 2612/10001 [1:13:58<3:30:42,  1.71s/batch]Batch 2600/10001 Done, mean position loss: 21.224154374599458\n",
      "Training NF1:  26%|████▍            | 2595/10001 [1:14:01<3:25:37,  1.67s/batch]Batch 2600/10001 Done, mean position loss: 21.07742075443268\n",
      "Training NF1:  26%|████▍            | 2590/10001 [1:14:13<3:12:33,  1.56s/batch]Batch 2600/10001 Done, mean position loss: 20.566582405567168\n",
      "Training NF1:  26%|████▎            | 2565/10001 [1:14:14<3:33:21,  1.72s/batch]Batch 2600/10001 Done, mean position loss: 20.794011664390567\n",
      "Training NF1:  26%|████▎            | 2568/10001 [1:14:14<3:31:59,  1.71s/batch]Batch 2600/10001 Done, mean position loss: 21.040644257068635\n",
      "Training NF1:  26%|████▎            | 2567/10001 [1:14:15<3:43:16,  1.80s/batch]Batch 2600/10001 Done, mean position loss: 20.651523444652554\n",
      "Training NF1:  26%|████▍            | 2624/10001 [1:14:19<3:05:00,  1.50s/batch]Batch 2600/10001 Done, mean position loss: 20.787514255046844\n",
      "Training NF1:  26%|████▍            | 2627/10001 [1:14:19<3:44:39,  1.83s/batch]Batch 2600/10001 Done, mean position loss: 20.912020919322966\n",
      "Training NF1:  26%|████▍            | 2577/10001 [1:14:32<4:03:09,  1.97s/batch]Batch 2600/10001 Done, mean position loss: 21.095215687751768\n",
      "Training NF1:  27%|████▌            | 2654/10001 [1:14:32<3:42:00,  1.81s/batch]Batch 2600/10001 Done, mean position loss: 20.822323324680326\n",
      "Training NF1:  26%|████▍            | 2596/10001 [1:14:38<3:18:31,  1.61s/batch]Batch 2600/10001 Done, mean position loss: 20.70032330274582\n",
      "Training NF1:  26%|████▍            | 2598/10001 [1:14:42<3:23:04,  1.65s/batch]Batch 2600/10001 Done, mean position loss: 20.822858450412753\n",
      "Training NF1:  26%|████▍            | 2641/10001 [1:14:44<3:50:14,  1.88s/batch]Batch 2600/10001 Done, mean position loss: 20.77352967262268\n",
      "Training NF1:  26%|████▍            | 2592/10001 [1:14:47<3:38:46,  1.77s/batch]Batch 2600/10001 Done, mean position loss: 21.04076191902161\n",
      "Batch 2600/10001 Done, mean position loss: 21.23932684421539\n",
      "Training NF1:  26%|████▍            | 2603/10001 [1:14:50<3:23:36,  1.65s/batch]Batch 2600/10001 Done, mean position loss: 20.453945291042327\n",
      "Training NF1:  26%|████▍            | 2621/10001 [1:14:52<3:22:03,  1.64s/batch]Batch 2600/10001 Done, mean position loss: 20.96020290613174\n",
      "Training NF1:  26%|████▎            | 2560/10001 [1:14:53<3:37:21,  1.75s/batch]Batch 2600/10001 Done, mean position loss: 20.507514405250546\n",
      "Training NF1:  26%|████▍            | 2643/10001 [1:14:53<3:52:17,  1.89s/batch]Batch 2600/10001 Done, mean position loss: 21.29939219713211\n",
      "Training NF1:  26%|████▍            | 2635/10001 [1:14:53<3:16:38,  1.60s/batch]Batch 2600/10001 Done, mean position loss: 20.846726951599123\n",
      "Training NF1:  26%|████▍            | 2596/10001 [1:14:55<3:58:28,  1.93s/batch]Batch 2600/10001 Done, mean position loss: 20.739033765792847\n",
      "Training NF1:  26%|████▍            | 2645/10001 [1:15:04<3:47:01,  1.85s/batch]Batch 2600/10001 Done, mean position loss: 20.96786429643631\n",
      "Training NF1:  26%|████▍            | 2613/10001 [1:15:05<3:51:00,  1.88s/batch]Batch 2600/10001 Done, mean position loss: 21.089711089134216\n",
      "Training NF1:  26%|████▍            | 2609/10001 [1:15:07<3:33:29,  1.73s/batch]Batch 2600/10001 Done, mean position loss: 20.921064331531525\n",
      "Training NF1:  26%|████▍            | 2612/10001 [1:15:10<3:04:15,  1.50s/batch]Batch 2600/10001 Done, mean position loss: 21.24683146715164\n",
      "Training NF1:  26%|████▎            | 2572/10001 [1:15:14<3:57:58,  1.92s/batch]Batch 2600/10001 Done, mean position loss: 20.568725414276123\n",
      "Training NF1:  26%|████▍            | 2619/10001 [1:15:16<3:30:47,  1.71s/batch]Batch 2600/10001 Done, mean position loss: 20.924429409503936\n",
      "Training NF1:  26%|████▍            | 2617/10001 [1:15:17<3:05:38,  1.51s/batch]Batch 2600/10001 Done, mean position loss: 20.485053679943086\n",
      "Training NF1:  27%|████▌            | 2654/10001 [1:15:18<3:36:19,  1.77s/batch]Batch 2600/10001 Done, mean position loss: 21.647171878814696\n",
      "Training NF1:  27%|████▌            | 2659/10001 [1:15:18<3:11:02,  1.56s/batch]Batch 2600/10001 Done, mean position loss: 20.691579089164733\n",
      "Training NF1:  27%|████▌            | 2653/10001 [1:15:44<3:24:05,  1.67s/batch]Batch 2600/10001 Done, mean position loss: 20.8232199382782\n",
      "Training NF1:  27%|████▌            | 2673/10001 [1:15:51<3:36:15,  1.77s/batch]Batch 2700/10001 Done, mean position loss: 20.622160050868988\n",
      "Training NF1:  26%|████▍            | 2630/10001 [1:16:01<3:08:09,  1.53s/batch]Batch 2600/10001 Done, mean position loss: 20.777720453739164\n",
      "Training NF1:  27%|████▋            | 2721/10001 [1:16:24<3:39:37,  1.81s/batch]Batch 2700/10001 Done, mean position loss: 20.79516547203064\n",
      "Training NF1:  26%|████▍            | 2645/10001 [1:16:29<3:20:41,  1.64s/batch]Batch 2700/10001 Done, mean position loss: 20.60392721414566\n",
      "Training NF1:  26%|████▍            | 2646/10001 [1:16:30<3:26:01,  1.68s/batch]Batch 2700/10001 Done, mean position loss: 20.593329803943632\n",
      "Training NF1:  26%|████▍            | 2645/10001 [1:16:30<3:34:28,  1.75s/batch]Batch 2700/10001 Done, mean position loss: 21.142349281311038\n",
      "Training NF1:  27%|████▌            | 2663/10001 [1:16:32<3:40:43,  1.80s/batch]Batch 2700/10001 Done, mean position loss: 20.63154177188873\n",
      "Training NF1:  27%|████▌            | 2705/10001 [1:16:36<3:47:22,  1.87s/batch]Batch 2700/10001 Done, mean position loss: 20.61023296117783\n",
      "Training NF1:  27%|████▌            | 2654/10001 [1:16:49<3:14:45,  1.59s/batch]Batch 2700/10001 Done, mean position loss: 20.82714956998825\n",
      "Training NF1:  27%|████▋            | 2738/10001 [1:16:51<3:37:48,  1.80s/batch]Batch 2700/10001 Done, mean position loss: 21.143722860813142\n",
      "Training NF1:  27%|████▌            | 2658/10001 [1:16:53<3:41:36,  1.81s/batch]Batch 2700/10001 Done, mean position loss: 21.050218892097476\n",
      "Training NF1:  27%|████▌            | 2719/10001 [1:16:58<3:13:34,  1.60s/batch]Batch 2700/10001 Done, mean position loss: 20.770789647102355\n",
      "Training NF1:  27%|████▌            | 2678/10001 [1:17:01<3:15:54,  1.61s/batch]Batch 2700/10001 Done, mean position loss: 20.561697707176208\n",
      "Training NF1:  26%|████▌            | 2648/10001 [1:17:03<3:34:31,  1.75s/batch]Batch 2700/10001 Done, mean position loss: 21.008440101146697\n",
      "Training NF1:  27%|████▌            | 2704/10001 [1:17:05<2:44:35,  1.35s/batch]Batch 2700/10001 Done, mean position loss: 20.888580837249755\n",
      "Training NF1:  27%|████▋            | 2750/10001 [1:17:09<3:08:53,  1.56s/batch]Batch 2700/10001 Done, mean position loss: 20.64177706241608\n",
      "Training NF1:  27%|████▌            | 2689/10001 [1:17:15<3:41:20,  1.82s/batch]Batch 2700/10001 Done, mean position loss: 20.841011333465573\n",
      "Training NF1:  27%|████▋            | 2727/10001 [1:17:17<3:25:04,  1.69s/batch]Batch 2700/10001 Done, mean position loss: 20.78198386669159\n",
      "Training NF1:  27%|████▌            | 2694/10001 [1:17:27<3:25:01,  1.68s/batch]Batch 2700/10001 Done, mean position loss: 21.090036587715147\n",
      "Training NF1:  27%|████▌            | 2695/10001 [1:17:30<3:10:31,  1.56s/batch]Batch 2700/10001 Done, mean position loss: 20.774451270103455\n",
      "Training NF1:  27%|████▌            | 2687/10001 [1:17:30<3:24:09,  1.67s/batch]Batch 2700/10001 Done, mean position loss: 20.713114132881167\n",
      "Training NF1:  27%|████▌            | 2678/10001 [1:17:31<3:52:49,  1.91s/batch]Batch 2700/10001 Done, mean position loss: 20.83381291151047\n",
      "Training NF1:  27%|████▌            | 2699/10001 [1:17:36<3:29:51,  1.72s/batch]Batch 2700/10001 Done, mean position loss: 21.007503974437714\n",
      "Training NF1:  27%|████▋            | 2723/10001 [1:17:37<3:27:52,  1.71s/batch]Batch 2700/10001 Done, mean position loss: 20.460484347343446\n",
      "Training NF1:  27%|████▌            | 2702/10001 [1:17:38<3:17:45,  1.63s/batch]Batch 2700/10001 Done, mean position loss: 21.289814250469206\n",
      "Training NF1:  27%|████▌            | 2715/10001 [1:17:39<2:55:33,  1.45s/batch]Batch 2700/10001 Done, mean position loss: 21.207905952930453\n",
      "Training NF1:  27%|████▌            | 2702/10001 [1:17:40<3:23:59,  1.68s/batch]Batch 2700/10001 Done, mean position loss: 20.497637932300567\n",
      "Training NF1:  27%|████▌            | 2707/10001 [1:17:41<3:28:52,  1.72s/batch]Batch 2700/10001 Done, mean position loss: 20.94344264507294\n",
      "Training NF1:  27%|████▌            | 2689/10001 [1:17:45<3:54:34,  1.92s/batch]Batch 2700/10001 Done, mean position loss: 20.744410135746\n",
      "Training NF1:  27%|████▌            | 2711/10001 [1:17:48<3:37:10,  1.79s/batch]Batch 2700/10001 Done, mean position loss: 20.835519542694094\n",
      "Training NF1:  27%|████▋            | 2750/10001 [1:17:54<3:23:37,  1.68s/batch]Batch 2700/10001 Done, mean position loss: 20.91921949863434\n",
      "Batch 2700/10001 Done, mean position loss: 20.963507182598114\n",
      "Training NF1:  27%|████▌            | 2707/10001 [1:17:57<3:26:47,  1.70s/batch]Batch 2700/10001 Done, mean position loss: 21.113303995132448\n",
      "Training NF1:  28%|████▋            | 2755/10001 [1:18:03<3:20:06,  1.66s/batch]Batch 2700/10001 Done, mean position loss: 20.909549424648283\n",
      "Training NF1:  28%|████▋            | 2762/10001 [1:18:05<3:27:58,  1.72s/batch]Batch 2700/10001 Done, mean position loss: 20.687208149433136\n",
      "Training NF1:  27%|████▋            | 2745/10001 [1:18:05<3:39:38,  1.82s/batch]Batch 2700/10001 Done, mean position loss: 21.21092930316925\n",
      "Training NF1:  27%|████▌            | 2701/10001 [1:18:06<3:26:17,  1.70s/batch]Batch 2700/10001 Done, mean position loss: 20.590490930080414\n",
      "Training NF1:  27%|████▌            | 2709/10001 [1:18:06<3:24:50,  1.69s/batch]Batch 2700/10001 Done, mean position loss: 20.494708535671236\n",
      "Training NF1:  27%|████▋            | 2727/10001 [1:18:12<3:14:51,  1.61s/batch]Batch 2700/10001 Done, mean position loss: 21.66450431346893\n",
      "Training NF1:  27%|████▋            | 2747/10001 [1:18:33<3:04:55,  1.53s/batch]Batch 2800/10001 Done, mean position loss: 20.61937072515488\n",
      "Training NF1:  28%|████▋            | 2776/10001 [1:18:37<3:18:50,  1.65s/batch]Batch 2700/10001 Done, mean position loss: 20.834202461242675\n",
      "Training NF1:  27%|████▋            | 2725/10001 [1:18:52<4:30:22,  2.23s/batch]Batch 2700/10001 Done, mean position loss: 20.764670553207395\n",
      "Training NF1:  28%|████▋            | 2753/10001 [1:19:08<4:08:59,  2.06s/batch]Batch 2800/10001 Done, mean position loss: 20.7696990609169\n",
      "Training NF1:  28%|████▋            | 2784/10001 [1:19:19<3:31:57,  1.76s/batch]Batch 2800/10001 Done, mean position loss: 21.124949090480804\n",
      "Batch 2800/10001 Done, mean position loss: 20.582216851711273\n",
      "Training NF1:  28%|████▊            | 2798/10001 [1:19:21<3:15:43,  1.63s/batch]Batch 2800/10001 Done, mean position loss: 20.586127390861513\n",
      "Training NF1:  28%|████▋            | 2767/10001 [1:19:28<3:18:59,  1.65s/batch]Batch 2800/10001 Done, mean position loss: 20.595621135234836\n",
      "Training NF1:  28%|████▋            | 2792/10001 [1:19:30<4:21:24,  2.18s/batch]Batch 2800/10001 Done, mean position loss: 20.62445336818695\n",
      "Training NF1:  28%|████▊            | 2796/10001 [1:19:41<3:15:28,  1.63s/batch]Batch 2800/10001 Done, mean position loss: 20.798162956237793\n",
      "Training NF1:  28%|████▊            | 2814/10001 [1:19:45<3:25:41,  1.72s/batch]Batch 2800/10001 Done, mean position loss: 21.168893642425537\n",
      "Training NF1:  28%|████▋            | 2792/10001 [1:19:49<3:53:43,  1.95s/batch]Batch 2800/10001 Done, mean position loss: 21.04558811187744\n",
      "Training NF1:  27%|████▋            | 2740/10001 [1:19:50<3:51:33,  1.91s/batch]Batch 2800/10001 Done, mean position loss: 20.549758007526396\n",
      "Training NF1:  28%|████▋            | 2776/10001 [1:19:53<3:21:59,  1.68s/batch]Batch 2800/10001 Done, mean position loss: 21.015811421871184\n",
      "Training NF1:  28%|████▊            | 2804/10001 [1:19:55<3:38:13,  1.82s/batch]Batch 2800/10001 Done, mean position loss: 20.783952720165253\n",
      "Training NF1:  28%|████▋            | 2790/10001 [1:20:04<3:33:34,  1.78s/batch]Batch 2800/10001 Done, mean position loss: 20.883708102703096\n",
      "Training NF1:  28%|████▊            | 2819/10001 [1:20:03<3:36:05,  1.81s/batch]Batch 2800/10001 Done, mean position loss: 20.639047930240633\n",
      "Training NF1:  28%|████▋            | 2783/10001 [1:20:09<3:18:24,  1.65s/batch]Batch 2800/10001 Done, mean position loss: 20.82790324687958\n",
      "Training NF1:  28%|████▊            | 2828/10001 [1:20:12<3:15:20,  1.63s/batch]Batch 2800/10001 Done, mean position loss: 20.774537353515626\n",
      "Training NF1:  28%|████▋            | 2785/10001 [1:20:20<3:45:23,  1.87s/batch]Batch 2800/10001 Done, mean position loss: 21.07948038816452\n",
      "Training NF1:  28%|████▊            | 2820/10001 [1:20:28<3:42:18,  1.86s/batch]Batch 2800/10001 Done, mean position loss: 20.75673574924469\n",
      "Training NF1:  28%|████▋            | 2784/10001 [1:20:29<3:23:18,  1.69s/batch]Batch 2800/10001 Done, mean position loss: 20.815467047691342\n",
      "Training NF1:  28%|████▋            | 2777/10001 [1:20:30<3:48:07,  1.89s/batch]Batch 2800/10001 Done, mean position loss: 20.70425054073334\n",
      "Training NF1:  28%|████▋            | 2781/10001 [1:20:31<4:05:04,  2.04s/batch]Batch 2800/10001 Done, mean position loss: 21.25230959177017\n",
      "Training NF1:  28%|████▊            | 2836/10001 [1:20:34<3:34:47,  1.80s/batch]Batch 2800/10001 Done, mean position loss: 20.993170912265775\n",
      "Training NF1:  28%|████▊            | 2843/10001 [1:20:34<3:40:16,  1.85s/batch]Batch 2800/10001 Done, mean position loss: 20.5053751039505\n",
      "Training NF1:  28%|████▊            | 2827/10001 [1:20:35<3:39:18,  1.83s/batch]Batch 2800/10001 Done, mean position loss: 20.919443609714506\n",
      "Training NF1:  28%|████▊            | 2798/10001 [1:20:36<3:42:15,  1.85s/batch]Batch 2800/10001 Done, mean position loss: 20.46054989337921\n",
      "Training NF1:  28%|████▊            | 2822/10001 [1:20:38<3:17:51,  1.65s/batch]Batch 2800/10001 Done, mean position loss: 20.736939375400542\n",
      "Training NF1:  28%|████▊            | 2812/10001 [1:20:40<3:35:34,  1.80s/batch]Batch 2800/10001 Done, mean position loss: 20.842042191028597\n",
      "Training NF1:  28%|████▊            | 2798/10001 [1:20:40<3:05:55,  1.55s/batch]Batch 2800/10001 Done, mean position loss: 21.22885466814041\n",
      "Training NF1:  28%|████▊            | 2804/10001 [1:20:46<3:31:03,  1.76s/batch]Batch 2800/10001 Done, mean position loss: 20.960067734718322\n",
      "Training NF1:  28%|████▊            | 2810/10001 [1:20:48<2:59:59,  1.50s/batch]Batch 2800/10001 Done, mean position loss: 20.8974910736084\n",
      "Training NF1:  28%|████▊            | 2803/10001 [1:20:52<3:48:36,  1.91s/batch]Batch 2800/10001 Done, mean position loss: 21.088621942996976\n",
      "Training NF1:  28%|████▊            | 2842/10001 [1:20:59<3:13:51,  1.62s/batch]Batch 2800/10001 Done, mean position loss: 20.578893377780915\n",
      "Training NF1:  28%|████▊            | 2847/10001 [1:20:59<3:04:25,  1.55s/batch]Batch 2800/10001 Done, mean position loss: 20.894599173069\n",
      "Training NF1:  29%|████▊            | 2855/10001 [1:21:01<3:20:27,  1.68s/batch]Batch 2800/10001 Done, mean position loss: 21.206621658802035\n",
      "Training NF1:  28%|████▊            | 2808/10001 [1:21:03<3:15:19,  1.63s/batch]Batch 2800/10001 Done, mean position loss: 20.678375413417818\n",
      "Training NF1:  28%|████▊            | 2842/10001 [1:21:07<3:28:30,  1.75s/batch]Batch 2800/10001 Done, mean position loss: 20.487838549613954\n",
      "Training NF1:  28%|████▊            | 2847/10001 [1:21:12<3:06:38,  1.57s/batch]Batch 2800/10001 Done, mean position loss: 21.65524843454361\n",
      "Training NF1:  28%|████▊            | 2829/10001 [1:21:25<3:12:49,  1.61s/batch]Batch 2900/10001 Done, mean position loss: 20.611363067626954\n",
      "Training NF1:  29%|████▉            | 2872/10001 [1:21:38<3:52:58,  1.96s/batch]Batch 2800/10001 Done, mean position loss: 20.81956662416458\n",
      "Training NF1:  28%|████▊            | 2834/10001 [1:21:53<3:11:16,  1.60s/batch]Batch 2800/10001 Done, mean position loss: 20.744175119400026\n",
      "Training NF1:  29%|████▉            | 2875/10001 [1:22:03<3:37:09,  1.83s/batch]Batch 2900/10001 Done, mean position loss: 20.762548353672027\n",
      "Training NF1:  29%|████▉            | 2927/10001 [1:22:11<3:15:40,  1.66s/batch]Batch 2900/10001 Done, mean position loss: 20.602722873687746\n",
      "Training NF1:  28%|████▊            | 2840/10001 [1:22:14<3:41:28,  1.86s/batch]Batch 2900/10001 Done, mean position loss: 21.15129987716675\n",
      "Training NF1:  29%|████▊            | 2861/10001 [1:22:14<3:28:50,  1.75s/batch]Batch 2900/10001 Done, mean position loss: 20.563081884384154\n",
      "Training NF1:  29%|████▊            | 2853/10001 [1:22:21<3:43:50,  1.88s/batch]Batch 2900/10001 Done, mean position loss: 20.593135962486265\n",
      "Training NF1:  29%|████▉            | 2868/10001 [1:22:29<3:35:28,  1.81s/batch]Batch 2900/10001 Done, mean position loss: 20.634220230579377\n",
      "Training NF1:  29%|████▊            | 2860/10001 [1:22:32<3:00:02,  1.51s/batch]Batch 2900/10001 Done, mean position loss: 20.771719057559967\n",
      "Training NF1:  29%|████▉            | 2915/10001 [1:22:37<3:37:04,  1.84s/batch]Batch 2900/10001 Done, mean position loss: 21.17586018562317\n",
      "Training NF1:  29%|████▉            | 2878/10001 [1:22:45<3:17:59,  1.67s/batch]Batch 2900/10001 Done, mean position loss: 20.551140763759612\n",
      "Training NF1:  29%|████▉            | 2907/10001 [1:22:47<3:32:49,  1.80s/batch]Batch 2900/10001 Done, mean position loss: 20.77860630750656\n",
      "Training NF1:  29%|████▉            | 2903/10001 [1:22:48<2:48:12,  1.42s/batch]Batch 2900/10001 Done, mean position loss: 21.04430013179779\n",
      "Batch 2900/10001 Done, mean position loss: 20.65023506164551\n",
      "Training NF1:  29%|████▉            | 2887/10001 [1:22:53<3:18:08,  1.67s/batch]Batch 2900/10001 Done, mean position loss: 21.010404798984528\n",
      "Training NF1:  29%|████▊            | 2867/10001 [1:23:02<3:39:11,  1.84s/batch]Batch 2900/10001 Done, mean position loss: 20.88547374725342\n",
      "Training NF1:  29%|████▉            | 2908/10001 [1:23:03<2:51:24,  1.45s/batch]Batch 2900/10001 Done, mean position loss: 20.81901964187622\n",
      "Training NF1:  29%|████▉            | 2931/10001 [1:23:05<3:31:46,  1.80s/batch]Batch 2900/10001 Done, mean position loss: 20.760716037750246\n",
      "Training NF1:  29%|████▉            | 2885/10001 [1:23:15<3:08:30,  1.59s/batch]Batch 2900/10001 Done, mean position loss: 21.092896018028256\n",
      "Training NF1:  29%|████▉            | 2940/10001 [1:23:20<3:18:00,  1.68s/batch]Batch 2900/10001 Done, mean position loss: 20.76708245754242\n",
      "Training NF1:  29%|████▉            | 2920/10001 [1:23:21<3:48:15,  1.93s/batch]Batch 2900/10001 Done, mean position loss: 20.47349203824997\n",
      "Training NF1:  29%|████▉            | 2913/10001 [1:23:23<4:01:15,  2.04s/batch]Batch 2900/10001 Done, mean position loss: 20.73838156700134\n",
      "Training NF1:  29%|████▉            | 2877/10001 [1:23:25<3:01:54,  1.53s/batch]Batch 2900/10001 Done, mean position loss: 20.978070516586307\n",
      "Training NF1:  29%|████▉            | 2928/10001 [1:23:24<3:07:42,  1.59s/batch]Batch 2900/10001 Done, mean position loss: 20.692395193576814\n",
      "Training NF1:  29%|████▊            | 2863/10001 [1:23:28<3:57:24,  2.00s/batch]Batch 2900/10001 Done, mean position loss: 21.25347532749176\n",
      "Training NF1:  29%|████▉            | 2907/10001 [1:23:30<3:05:23,  1.57s/batch]Batch 2900/10001 Done, mean position loss: 20.820361313819888\n",
      "Training NF1:  29%|████▉            | 2928/10001 [1:23:32<3:03:05,  1.55s/batch]Batch 2900/10001 Done, mean position loss: 20.45387406349182\n",
      "Training NF1:  29%|█████            | 2946/10001 [1:23:31<3:48:24,  1.94s/batch]Batch 2900/10001 Done, mean position loss: 21.20819678783417\n",
      "Training NF1:  29%|████▉            | 2909/10001 [1:23:33<3:06:24,  1.58s/batch]Batch 2900/10001 Done, mean position loss: 20.917091181278227\n",
      "Training NF1:  29%|████▉            | 2906/10001 [1:23:34<3:59:16,  2.02s/batch]Batch 2900/10001 Done, mean position loss: 20.838587753772735\n",
      "Training NF1:  29%|████▉            | 2923/10001 [1:23:41<3:09:33,  1.61s/batch]Batch 2900/10001 Done, mean position loss: 20.96398370027542\n",
      "Training NF1:  29%|████▉            | 2909/10001 [1:23:41<3:24:18,  1.73s/batch]Batch 2900/10001 Done, mean position loss: 21.082255437374116\n",
      "Training NF1:  29%|█████            | 2943/10001 [1:23:43<3:26:14,  1.75s/batch]Batch 2900/10001 Done, mean position loss: 20.888520169258115\n",
      "Training NF1:  29%|████▉            | 2897/10001 [1:23:45<3:42:35,  1.88s/batch]Batch 2900/10001 Done, mean position loss: 20.881187875270843\n",
      "Training NF1:  30%|█████            | 2953/10001 [1:23:50<3:28:18,  1.77s/batch]Batch 2900/10001 Done, mean position loss: 21.20288124322891\n",
      "Training NF1:  29%|████▉            | 2919/10001 [1:23:53<3:42:38,  1.89s/batch]Batch 2900/10001 Done, mean position loss: 20.543865916728976\n",
      "Training NF1:  30%|█████            | 2951/10001 [1:23:59<3:26:53,  1.76s/batch]Batch 2900/10001 Done, mean position loss: 20.677821660041808\n",
      "Training NF1:  30%|█████            | 2966/10001 [1:24:03<2:57:46,  1.52s/batch]Batch 2900/10001 Done, mean position loss: 20.4695929145813\n",
      "Training NF1:  29%|████▉            | 2903/10001 [1:24:07<3:36:16,  1.83s/batch]Batch 2900/10001 Done, mean position loss: 21.623641107082364\n",
      "Training NF1:  29%|████▉            | 2926/10001 [1:24:19<3:49:08,  1.94s/batch]Batch 3000/10001 Done, mean position loss: 20.632123334407808\n",
      "Training NF1:  29%|████▉            | 2891/10001 [1:24:34<3:25:44,  1.74s/batch]Batch 2900/10001 Done, mean position loss: 20.836343300342563\n",
      "Training NF1:  30%|█████            | 2992/10001 [1:24:53<3:33:30,  1.83s/batch]Batch 2900/10001 Done, mean position loss: 20.73878061056137\n",
      "Training NF1:  29%|█████            | 2949/10001 [1:25:00<3:22:31,  1.72s/batch]Batch 3000/10001 Done, mean position loss: 20.752618901729583\n",
      "Training NF1:  30%|█████            | 2953/10001 [1:25:07<3:54:15,  1.99s/batch]Batch 3000/10001 Done, mean position loss: 20.566341972351076\n",
      "Training NF1:  30%|█████            | 2986/10001 [1:25:08<3:36:04,  1.85s/batch]Batch 3000/10001 Done, mean position loss: 21.131346812248232\n",
      "Training NF1:  30%|█████            | 2981/10001 [1:25:10<4:12:08,  2.15s/batch]Batch 3000/10001 Done, mean position loss: 20.607652068138123\n",
      "Training NF1:  29%|████▉            | 2940/10001 [1:25:13<3:27:01,  1.76s/batch]Batch 3000/10001 Done, mean position loss: 20.59630630016327\n",
      "Training NF1:  30%|█████            | 2981/10001 [1:25:27<3:39:36,  1.88s/batch]Batch 3000/10001 Done, mean position loss: 20.631064028739928\n",
      "Training NF1:  30%|█████            | 2960/10001 [1:25:29<2:50:55,  1.46s/batch]Batch 3000/10001 Done, mean position loss: 20.800033280849455\n",
      "Training NF1:  30%|█████            | 2970/10001 [1:25:34<4:21:55,  2.24s/batch]Batch 3000/10001 Done, mean position loss: 21.154969220161437\n",
      "Training NF1:  29%|████▉            | 2929/10001 [1:25:41<3:26:39,  1.75s/batch]Batch 3000/10001 Done, mean position loss: 20.542725460529326\n",
      "Training NF1:  30%|█████            | 3001/10001 [1:25:41<3:44:39,  1.93s/batch]Batch 3000/10001 Done, mean position loss: 20.621649231910705\n",
      "Training NF1:  30%|█████            | 2993/10001 [1:25:42<3:06:11,  1.59s/batch]Batch 3000/10001 Done, mean position loss: 20.755230109691617\n",
      "Training NF1:  30%|█████            | 2966/10001 [1:25:44<3:37:23,  1.85s/batch]Batch 3000/10001 Done, mean position loss: 21.0153754401207\n",
      "Training NF1:  30%|█████            | 2974/10001 [1:25:44<3:37:32,  1.86s/batch]Batch 3000/10001 Done, mean position loss: 21.04965796470642\n",
      "Training NF1:  30%|█████            | 2966/10001 [1:25:57<3:29:52,  1.79s/batch]Batch 3000/10001 Done, mean position loss: 20.823286747932432\n",
      "Training NF1:  30%|█████▏           | 3032/10001 [1:26:00<3:20:47,  1.73s/batch]Batch 3000/10001 Done, mean position loss: 20.865094282627105\n",
      "Training NF1:  30%|█████            | 2985/10001 [1:26:02<3:37:58,  1.86s/batch]Batch 3000/10001 Done, mean position loss: 20.76358551740646\n",
      "Training NF1:  30%|█████            | 2987/10001 [1:26:14<3:04:49,  1.58s/batch]Batch 3000/10001 Done, mean position loss: 21.092661464214324\n",
      "Training NF1:  30%|█████            | 2990/10001 [1:26:17<3:16:53,  1.69s/batch]Batch 3000/10001 Done, mean position loss: 20.74063507080078\n",
      "Training NF1:  30%|█████▏           | 3040/10001 [1:26:20<3:39:13,  1.89s/batch]Batch 3000/10001 Done, mean position loss: 20.472955799102785\n",
      "Training NF1:  30%|█████▏           | 3023/10001 [1:26:22<3:05:14,  1.59s/batch]Batch 3000/10001 Done, mean position loss: 20.71495583534241\n",
      "Training NF1:  30%|█████▏           | 3046/10001 [1:26:22<3:11:24,  1.65s/batch]Batch 3000/10001 Done, mean position loss: 21.278384561538694\n",
      "Training NF1:  30%|█████            | 2982/10001 [1:26:24<3:11:24,  1.64s/batch]Batch 3000/10001 Done, mean position loss: 20.721882090568542\n",
      "Training NF1:  30%|█████            | 3000/10001 [1:26:27<3:18:46,  1.70s/batch]Batch 3000/10001 Done, mean position loss: 20.8313272690773\n",
      "Training NF1:  30%|█████▏           | 3028/10001 [1:26:28<3:15:04,  1.68s/batch]Batch 3000/10001 Done, mean position loss: 20.99328496456146\n",
      "Training NF1:  30%|█████            | 2996/10001 [1:26:29<3:30:11,  1.80s/batch]Batch 3000/10001 Done, mean position loss: 20.809499711990355\n",
      "Training NF1:  30%|█████            | 2993/10001 [1:26:31<3:05:01,  1.58s/batch]Batch 3000/10001 Done, mean position loss: 21.220245888233183\n",
      "Training NF1:  30%|█████            | 3013/10001 [1:26:35<3:28:37,  1.79s/batch]Batch 3000/10001 Done, mean position loss: 20.46229754924774\n",
      "Training NF1:  30%|█████            | 2988/10001 [1:26:38<3:32:59,  1.82s/batch]Batch 3000/10001 Done, mean position loss: 20.912326629161836\n",
      "Training NF1:  30%|█████▏           | 3043/10001 [1:26:40<3:34:22,  1.85s/batch]Batch 3000/10001 Done, mean position loss: 21.06633932352066\n",
      "Training NF1:  30%|█████            | 2998/10001 [1:26:40<3:52:45,  1.99s/batch]Batch 3000/10001 Done, mean position loss: 20.86740478038788\n",
      "Training NF1:  30%|█████            | 3011/10001 [1:26:45<3:03:27,  1.57s/batch]Batch 3000/10001 Done, mean position loss: 21.193054049015046\n",
      "Training NF1:  30%|█████            | 2997/10001 [1:26:45<2:44:07,  1.41s/batch]Batch 3000/10001 Done, mean position loss: 20.901886937618258\n",
      "Training NF1:  30%|█████            | 3010/10001 [1:26:46<3:59:51,  2.06s/batch]Batch 3000/10001 Done, mean position loss: 20.930003423690795\n",
      "Training NF1:  30%|█████            | 3013/10001 [1:26:51<3:06:03,  1.60s/batch]Batch 3000/10001 Done, mean position loss: 20.55031234741211\n",
      "Training NF1:  30%|█████▏           | 3021/10001 [1:26:57<3:35:30,  1.85s/batch]Batch 3000/10001 Done, mean position loss: 20.665207819938658\n",
      "Training NF1:  30%|█████▏           | 3020/10001 [1:27:00<3:03:26,  1.58s/batch]Batch 3000/10001 Done, mean position loss: 20.460333361625672\n",
      "Training NF1:  31%|█████▏           | 3072/10001 [1:27:05<3:07:04,  1.62s/batch]Batch 3000/10001 Done, mean position loss: 21.632272322177887\n",
      "Training NF1:  30%|█████            | 3009/10001 [1:27:13<3:06:50,  1.60s/batch]Batch 3100/10001 Done, mean position loss: 20.5980668759346\n",
      "Training NF1:  30%|█████▏           | 3039/10001 [1:27:36<3:33:11,  1.84s/batch]Batch 3000/10001 Done, mean position loss: 20.805993938446044\n",
      "Training NF1:  31%|█████▏           | 3082/10001 [1:27:48<3:14:56,  1.69s/batch]Batch 3000/10001 Done, mean position loss: 20.722812542915342\n",
      "Training NF1:  30%|█████▏           | 3036/10001 [1:27:52<3:15:28,  1.68s/batch]Batch 3100/10001 Done, mean position loss: 20.5468777680397\n",
      "Training NF1:  30%|█████▏           | 3042/10001 [1:27:54<4:06:17,  2.12s/batch]Batch 3100/10001 Done, mean position loss: 20.74804267168045\n",
      "Training NF1:  31%|█████▏           | 3084/10001 [1:28:03<2:49:45,  1.47s/batch]Batch 3100/10001 Done, mean position loss: 21.138881089687345\n",
      "Training NF1:  31%|█████▏           | 3071/10001 [1:28:04<3:50:30,  2.00s/batch]Batch 3100/10001 Done, mean position loss: 20.61050837993622\n",
      "Training NF1:  31%|█████▏           | 3059/10001 [1:28:07<2:57:22,  1.53s/batch]Batch 3100/10001 Done, mean position loss: 20.597665629386903\n",
      "Training NF1:  31%|█████▎           | 3111/10001 [1:28:19<3:00:16,  1.57s/batch]Batch 3100/10001 Done, mean position loss: 20.79697858095169\n",
      "Training NF1:  31%|█████▏           | 3065/10001 [1:28:20<3:32:47,  1.84s/batch]Batch 3100/10001 Done, mean position loss: 20.62280676364899\n",
      "Training NF1:  30%|█████▏           | 3047/10001 [1:28:25<3:26:29,  1.78s/batch]Batch 3100/10001 Done, mean position loss: 21.102307612895963\n",
      "Training NF1:  31%|█████▎           | 3119/10001 [1:28:32<3:00:03,  1.57s/batch]Batch 3100/10001 Done, mean position loss: 20.77815084218979\n",
      "Training NF1:  31%|█████▎           | 3103/10001 [1:28:34<2:38:58,  1.38s/batch]Batch 3100/10001 Done, mean position loss: 20.515422024726867\n",
      "Training NF1:  31%|█████▏           | 3079/10001 [1:28:40<3:41:30,  1.92s/batch]Batch 3100/10001 Done, mean position loss: 21.002899243831635\n",
      "Batch 3100/10001 Done, mean position loss: 20.631655898094177\n",
      "Training NF1:  31%|█████▏           | 3071/10001 [1:28:42<3:51:42,  2.01s/batch]Batch 3100/10001 Done, mean position loss: 21.024538333415983\n",
      "Training NF1:  31%|█████▎           | 3105/10001 [1:28:49<3:28:44,  1.82s/batch]Batch 3100/10001 Done, mean position loss: 20.81435449123383\n",
      "Training NF1:  31%|█████▎           | 3090/10001 [1:28:52<2:34:39,  1.34s/batch]Batch 3100/10001 Done, mean position loss: 20.749350838661194\n",
      "Training NF1:  31%|█████▏           | 3088/10001 [1:28:54<3:19:53,  1.73s/batch]Batch 3100/10001 Done, mean position loss: 20.878280837535858\n",
      "Training NF1:  31%|█████▎           | 3091/10001 [1:29:04<3:37:56,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 20.748694353103637\n",
      "Training NF1:  31%|█████▎           | 3094/10001 [1:29:04<2:52:43,  1.50s/batch]Batch 3100/10001 Done, mean position loss: 21.1017281961441\n",
      "Training NF1:  31%|█████▏           | 3056/10001 [1:29:11<3:06:01,  1.61s/batch]Batch 3100/10001 Done, mean position loss: 21.25482231616974\n",
      "Training NF1:  31%|█████▎           | 3114/10001 [1:29:15<3:30:07,  1.83s/batch]Batch 3100/10001 Done, mean position loss: 20.72623533964157\n",
      "Training NF1:  31%|█████▏           | 3086/10001 [1:29:15<3:36:01,  1.87s/batch]Batch 3100/10001 Done, mean position loss: 20.689004805088047\n",
      "Training NF1:  32%|█████▍           | 3174/10001 [1:29:16<2:54:26,  1.53s/batch]Batch 3100/10001 Done, mean position loss: 20.473323113918305\n",
      "Training NF1:  31%|█████▏           | 3054/10001 [1:29:18<4:01:47,  2.09s/batch]Batch 3100/10001 Done, mean position loss: 20.9762251830101\n",
      "Training NF1:  31%|█████▎           | 3125/10001 [1:29:19<2:59:16,  1.56s/batch]Batch 3100/10001 Done, mean position loss: 20.81625502824783\n",
      "Training NF1:  31%|█████▎           | 3132/10001 [1:29:23<3:11:21,  1.67s/batch]Batch 3100/10001 Done, mean position loss: 20.79815054178238\n",
      "Training NF1:  31%|█████▎           | 3105/10001 [1:29:24<3:15:19,  1.70s/batch]Batch 3100/10001 Done, mean position loss: 21.161741139888765\n",
      "Training NF1:  32%|█████▍           | 3182/10001 [1:29:29<3:01:16,  1.60s/batch]Batch 3100/10001 Done, mean position loss: 20.4525945854187\n",
      "Training NF1:  31%|█████▏           | 3063/10001 [1:29:33<3:11:14,  1.65s/batch]Batch 3100/10001 Done, mean position loss: 20.870346395969392\n",
      "Training NF1:  31%|█████▏           | 3087/10001 [1:29:32<3:08:00,  1.63s/batch]Batch 3100/10001 Done, mean position loss: 20.878770534992217\n",
      "Training NF1:  31%|█████▎           | 3101/10001 [1:29:32<3:16:19,  1.71s/batch]Batch 3100/10001 Done, mean position loss: 20.890763182640075\n",
      "Training NF1:  31%|█████▎           | 3094/10001 [1:29:32<3:03:42,  1.60s/batch]Batch 3100/10001 Done, mean position loss: 21.06206191301346\n",
      "Training NF1:  32%|█████▎           | 3161/10001 [1:29:37<3:24:30,  1.79s/batch]Batch 3100/10001 Done, mean position loss: 20.92184544801712\n",
      "Training NF1:  32%|█████▎           | 3151/10001 [1:29:43<3:27:56,  1.82s/batch]Batch 3100/10001 Done, mean position loss: 21.206322133541107\n",
      "Training NF1:  31%|█████▎           | 3118/10001 [1:29:45<3:00:48,  1.58s/batch]Batch 3100/10001 Done, mean position loss: 20.552773978710174\n",
      "Training NF1:  32%|█████▍           | 3195/10001 [1:29:50<3:15:09,  1.72s/batch]Batch 3100/10001 Done, mean position loss: 20.648381822109222\n",
      "Training NF1:  31%|█████▎           | 3099/10001 [1:30:00<3:19:43,  1.74s/batch]Batch 3100/10001 Done, mean position loss: 20.463776919841766\n",
      "Training NF1:  32%|█████▍           | 3163/10001 [1:30:02<3:02:50,  1.60s/batch]Batch 3200/10001 Done, mean position loss: 20.5781782412529\n",
      "Training NF1:  31%|█████▎           | 3125/10001 [1:30:03<3:48:11,  1.99s/batch]Batch 3100/10001 Done, mean position loss: 21.613698060512544\n",
      "Training NF1:  31%|█████▎           | 3133/10001 [1:30:32<4:05:07,  2.14s/batch]Batch 3100/10001 Done, mean position loss: 20.79161445617676\n",
      "Training NF1:  31%|█████▎           | 3138/10001 [1:30:38<3:35:59,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 20.730507373809814\n",
      "Training NF1:  31%|█████▎           | 3125/10001 [1:30:44<2:56:47,  1.54s/batch]Batch 3200/10001 Done, mean position loss: 20.73994567155838\n",
      "Training NF1:  32%|█████▍           | 3175/10001 [1:30:47<3:26:06,  1.81s/batch]Batch 3200/10001 Done, mean position loss: 20.5662828373909\n",
      "Training NF1:  32%|█████▍           | 3168/10001 [1:30:48<3:03:37,  1.61s/batch]Batch 3200/10001 Done, mean position loss: 20.582185728549955\n",
      "Training NF1:  32%|█████▍           | 3174/10001 [1:30:51<3:14:05,  1.71s/batch]Batch 3200/10001 Done, mean position loss: 21.104247782230377\n",
      "Training NF1:  32%|█████▍           | 3176/10001 [1:31:02<3:33:02,  1.87s/batch]Batch 3200/10001 Done, mean position loss: 20.57318524837494\n",
      "Training NF1:  31%|█████▎           | 3139/10001 [1:31:08<3:19:01,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 21.136892871856688\n",
      "Training NF1:  32%|█████▍           | 3186/10001 [1:31:10<3:13:09,  1.70s/batch]Batch 3200/10001 Done, mean position loss: 20.800521886348722\n",
      "Training NF1:  32%|█████▎           | 3156/10001 [1:31:13<3:31:09,  1.85s/batch]Batch 3200/10001 Done, mean position loss: 20.607020258903503\n",
      "Training NF1:  32%|█████▍           | 3189/10001 [1:31:25<3:09:42,  1.67s/batch]Batch 3200/10001 Done, mean position loss: 20.516148087978365\n",
      "Training NF1:  32%|█████▍           | 3183/10001 [1:31:26<3:16:13,  1.73s/batch]Batch 3200/10001 Done, mean position loss: 20.751393487453463\n",
      "Training NF1:  32%|█████▎           | 3162/10001 [1:31:32<3:12:07,  1.69s/batch]Batch 3200/10001 Done, mean position loss: 20.630466675758363\n",
      "Training NF1:  32%|█████▍           | 3218/10001 [1:31:36<3:11:32,  1.69s/batch]Batch 3200/10001 Done, mean position loss: 21.01993266582489\n",
      "Training NF1:  32%|█████▍           | 3192/10001 [1:31:41<3:25:52,  1.81s/batch]Batch 3200/10001 Done, mean position loss: 21.018220913410186\n",
      "Training NF1:  32%|█████▍           | 3209/10001 [1:31:46<3:22:03,  1.78s/batch]Batch 3200/10001 Done, mean position loss: 20.86035216093063\n",
      "Batch 3200/10001 Done, mean position loss: 20.74028365135193\n",
      "Training NF1:  32%|█████▍           | 3177/10001 [1:31:45<3:15:20,  1.72s/batch]Batch 3200/10001 Done, mean position loss: 20.817944793701173\n",
      "Training NF1:  32%|█████▍           | 3228/10001 [1:31:54<3:25:31,  1.82s/batch]Batch 3200/10001 Done, mean position loss: 21.095687446594237\n",
      "Training NF1:  32%|█████▍           | 3180/10001 [1:31:58<3:47:39,  2.00s/batch]Batch 3200/10001 Done, mean position loss: 20.753591067790985\n",
      "Training NF1:  32%|█████▍           | 3210/10001 [1:32:00<3:15:55,  1.73s/batch]Batch 3200/10001 Done, mean position loss: 21.254740467071535\n",
      "Training NF1:  32%|█████▍           | 3180/10001 [1:32:02<3:13:16,  1.70s/batch]Batch 3200/10001 Done, mean position loss: 20.722311108112336\n",
      "Training NF1:  32%|█████▍           | 3217/10001 [1:32:11<3:15:24,  1.73s/batch]Batch 3200/10001 Done, mean position loss: 20.47147983789444\n",
      "Training NF1:  32%|█████▍           | 3202/10001 [1:32:13<2:45:14,  1.46s/batch]Batch 3200/10001 Done, mean position loss: 20.81810941934586\n",
      "Training NF1:  32%|█████▍           | 3202/10001 [1:32:14<2:46:35,  1.47s/batch]Batch 3200/10001 Done, mean position loss: 20.686770327091217\n",
      "Training NF1:  32%|█████▍           | 3190/10001 [1:32:14<2:50:48,  1.50s/batch]Batch 3200/10001 Done, mean position loss: 20.97482348680496\n",
      "Training NF1:  32%|█████▎           | 3157/10001 [1:32:15<3:14:33,  1.71s/batch]Batch 3200/10001 Done, mean position loss: 20.78705544471741\n",
      "Training NF1:  32%|█████▍           | 3213/10001 [1:32:20<3:18:07,  1.75s/batch]Batch 3200/10001 Done, mean position loss: 21.19204889535904\n",
      "Training NF1:  32%|█████▍           | 3209/10001 [1:32:27<2:57:32,  1.57s/batch]Batch 3200/10001 Done, mean position loss: 20.89648385047913\n",
      "Training NF1:  32%|█████▍           | 3231/10001 [1:32:26<3:16:00,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 21.09303710460663\n",
      "Training NF1:  32%|█████▍           | 3202/10001 [1:32:27<2:42:36,  1.43s/batch]Batch 3200/10001 Done, mean position loss: 20.861762011051177\n",
      "Training NF1:  32%|█████▌           | 3246/10001 [1:32:28<3:12:07,  1.71s/batch]Batch 3200/10001 Done, mean position loss: 20.941146514415742\n",
      "Training NF1:  32%|█████▍           | 3231/10001 [1:32:30<3:29:09,  1.85s/batch]Batch 3200/10001 Done, mean position loss: 20.443379883766177\n",
      "Training NF1:  32%|█████▍           | 3223/10001 [1:32:33<3:43:50,  1.98s/batch]Batch 3200/10001 Done, mean position loss: 20.874914770126345\n",
      "Training NF1:  32%|█████▍           | 3214/10001 [1:32:34<3:20:27,  1.77s/batch]Batch 3200/10001 Done, mean position loss: 20.517891359329223\n",
      "Training NF1:  32%|█████▍           | 3215/10001 [1:32:35<2:53:01,  1.53s/batch]Batch 3200/10001 Done, mean position loss: 20.6663321685791\n",
      "Training NF1:  32%|█████▍           | 3191/10001 [1:32:39<3:23:12,  1.79s/batch]Batch 3200/10001 Done, mean position loss: 21.218281428813935\n",
      "Training NF1:  32%|█████▌           | 3250/10001 [1:32:50<3:11:13,  1.70s/batch]Batch 3200/10001 Done, mean position loss: 20.441314470767978\n",
      "Training NF1:  32%|█████▍           | 3218/10001 [1:32:56<3:22:58,  1.80s/batch]Batch 3200/10001 Done, mean position loss: 21.631677205562593\n",
      "Training NF1:  32%|█████▍           | 3220/10001 [1:32:58<3:20:03,  1.77s/batch]Batch 3300/10001 Done, mean position loss: 20.563090376853943\n",
      "Training NF1:  32%|█████▌           | 3238/10001 [1:33:30<3:22:12,  1.79s/batch]Batch 3300/10001 Done, mean position loss: 20.71267343044281\n",
      "Training NF1:  32%|█████▍           | 3226/10001 [1:33:31<2:55:58,  1.56s/batch]Batch 3300/10001 Done, mean position loss: 20.60336104631424\n",
      "Training NF1:  33%|█████▌           | 3258/10001 [1:33:34<3:52:06,  2.07s/batch]Batch 3200/10001 Done, mean position loss: 20.799052789211274\n",
      "Training NF1:  33%|█████▌           | 3290/10001 [1:33:34<3:07:02,  1.67s/batch]Batch 3200/10001 Done, mean position loss: 20.738332169055937\n",
      "Training NF1:  32%|█████▌           | 3238/10001 [1:33:35<2:40:50,  1.43s/batch]Batch 3300/10001 Done, mean position loss: 20.536647076606748\n",
      "Training NF1:  33%|█████▌           | 3272/10001 [1:33:49<3:37:18,  1.94s/batch]Batch 3300/10001 Done, mean position loss: 21.09765009880066\n",
      "Training NF1:  32%|█████▌           | 3244/10001 [1:33:53<3:02:23,  1.62s/batch]Batch 3300/10001 Done, mean position loss: 21.162332203388214\n",
      "Training NF1:  33%|█████▌           | 3253/10001 [1:33:53<3:46:50,  2.02s/batch]Batch 3300/10001 Done, mean position loss: 20.572535095214842\n",
      "Training NF1:  33%|█████▌           | 3256/10001 [1:34:02<2:14:46,  1.20s/batch]Batch 3300/10001 Done, mean position loss: 20.794398825168614\n",
      "Training NF1:  33%|█████▌           | 3257/10001 [1:34:04<2:24:18,  1.28s/batch]Batch 3300/10001 Done, mean position loss: 20.613931732177733\n",
      "Training NF1:  33%|█████▋           | 3317/10001 [1:34:15<3:04:25,  1.66s/batch]Batch 3300/10001 Done, mean position loss: 20.736121740341186\n",
      "Training NF1:  33%|█████▌           | 3256/10001 [1:34:22<3:18:30,  1.77s/batch]Batch 3300/10001 Done, mean position loss: 20.518405432701112\n",
      "Training NF1:  33%|█████▌           | 3301/10001 [1:34:22<3:33:31,  1.91s/batch]Batch 3300/10001 Done, mean position loss: 20.631142249107363\n",
      "Training NF1:  33%|█████▋           | 3319/10001 [1:34:25<3:16:17,  1.76s/batch]Batch 3300/10001 Done, mean position loss: 21.011878366470338\n",
      "Training NF1:  34%|█████▋           | 3358/10001 [1:34:34<3:22:37,  1.83s/batch]Batch 3300/10001 Done, mean position loss: 20.746717245578765\n",
      "Training NF1:  33%|█████▌           | 3308/10001 [1:34:35<3:23:36,  1.83s/batch]Batch 3300/10001 Done, mean position loss: 21.001293606758118\n",
      "Training NF1:  33%|█████▌           | 3285/10001 [1:34:38<3:21:30,  1.80s/batch]Batch 3300/10001 Done, mean position loss: 20.866989555358884\n",
      "Training NF1:  33%|█████▌           | 3305/10001 [1:34:40<3:04:01,  1.65s/batch]Batch 3300/10001 Done, mean position loss: 20.812776107788086\n",
      "Training NF1:  32%|█████▌           | 3241/10001 [1:34:46<3:23:39,  1.81s/batch]Batch 3300/10001 Done, mean position loss: 20.740271208286288\n",
      "Training NF1:  33%|█████▌           | 3271/10001 [1:34:49<3:28:20,  1.86s/batch]Batch 3300/10001 Done, mean position loss: 21.09980081796646\n",
      "Training NF1:  33%|█████▋           | 3310/10001 [1:34:49<2:35:22,  1.39s/batch]Batch 3300/10001 Done, mean position loss: 21.240786199569705\n",
      "Training NF1:  33%|█████▋           | 3314/10001 [1:34:56<2:53:00,  1.55s/batch]Batch 3300/10001 Done, mean position loss: 20.71057278394699\n",
      "Training NF1:  33%|█████▌           | 3285/10001 [1:35:02<3:51:53,  2.07s/batch]Batch 3300/10001 Done, mean position loss: 20.467510924339294\n",
      "Training NF1:  34%|█████▋           | 3357/10001 [1:35:03<2:51:39,  1.55s/batch]Batch 3300/10001 Done, mean position loss: 20.77419936656952\n",
      "Training NF1:  33%|█████▋           | 3311/10001 [1:35:05<3:03:07,  1.64s/batch]Batch 3300/10001 Done, mean position loss: 20.96658734560013\n",
      "Training NF1:  33%|█████▋           | 3344/10001 [1:35:04<3:40:24,  1.99s/batch]Batch 3300/10001 Done, mean position loss: 20.824345734119415\n",
      "Training NF1:  33%|█████▋           | 3317/10001 [1:35:07<3:17:19,  1.77s/batch]Batch 3300/10001 Done, mean position loss: 20.67387452363968\n",
      "Training NF1:  33%|█████▋           | 3341/10001 [1:35:14<3:21:04,  1.81s/batch]Batch 3300/10001 Done, mean position loss: 21.191589984893803\n",
      "Training NF1:  33%|█████▋           | 3326/10001 [1:35:16<3:06:44,  1.68s/batch]Batch 3300/10001 Done, mean position loss: 20.88718969583511\n",
      "Training NF1:  33%|█████▌           | 3258/10001 [1:35:17<3:06:28,  1.66s/batch]Batch 3300/10001 Done, mean position loss: 20.425336701869966\n",
      "Training NF1:  33%|█████▋           | 3318/10001 [1:35:18<3:40:04,  1.98s/batch]Batch 3300/10001 Done, mean position loss: 20.871791551113127\n",
      "Training NF1:  33%|█████▋           | 3329/10001 [1:35:21<3:10:21,  1.71s/batch]Batch 3300/10001 Done, mean position loss: 21.09903951883316\n",
      "Batch 3300/10001 Done, mean position loss: 20.929551732540133\n",
      "Training NF1:  34%|█████▊           | 3388/10001 [1:35:25<2:58:42,  1.62s/batch]Batch 3300/10001 Done, mean position loss: 20.84968200445175\n",
      "Training NF1:  33%|█████▋           | 3318/10001 [1:35:26<3:25:27,  1.84s/batch]Batch 3300/10001 Done, mean position loss: 20.524690182209014\n",
      "Training NF1:  33%|█████▋           | 3317/10001 [1:35:30<3:12:00,  1.72s/batch]Batch 3300/10001 Done, mean position loss: 21.20100467205048\n",
      "Training NF1:  34%|█████▋           | 3359/10001 [1:35:32<3:39:23,  1.98s/batch]Batch 3300/10001 Done, mean position loss: 20.66027237653732\n",
      "Training NF1:  33%|█████▋           | 3323/10001 [1:35:41<3:05:36,  1.67s/batch]Batch 3300/10001 Done, mean position loss: 20.446915059089662\n",
      "Training NF1:  33%|█████▋           | 3320/10001 [1:35:48<3:02:43,  1.64s/batch]Batch 3300/10001 Done, mean position loss: 21.599549474716184\n",
      "Training NF1:  34%|█████▋           | 3363/10001 [1:35:48<3:19:51,  1.81s/batch]Batch 3400/10001 Done, mean position loss: 20.552134408950806\n",
      "Training NF1:  33%|█████▋           | 3328/10001 [1:36:19<2:55:32,  1.58s/batch]Batch 3400/10001 Done, mean position loss: 20.59078835248947\n",
      "Training NF1:  33%|█████▋           | 3331/10001 [1:36:22<3:52:12,  2.09s/batch]Batch 3400/10001 Done, mean position loss: 20.715669178962706\n",
      "Training NF1:  34%|█████▊           | 3389/10001 [1:36:23<3:13:15,  1.75s/batch]Batch 3300/10001 Done, mean position loss: 20.70496998310089\n",
      "Training NF1:  33%|█████▌           | 3295/10001 [1:36:24<4:07:42,  2.22s/batch]Batch 3400/10001 Done, mean position loss: 20.536718096733093\n",
      "Training NF1:  33%|█████▋           | 3336/10001 [1:36:34<3:58:49,  2.15s/batch]Batch 3300/10001 Done, mean position loss: 20.784206693172457\n",
      "Training NF1:  34%|█████▋           | 3351/10001 [1:36:40<3:01:33,  1.64s/batch]Batch 3400/10001 Done, mean position loss: 20.578958601951598\n",
      "Training NF1:  34%|█████▋           | 3351/10001 [1:36:44<3:25:45,  1.86s/batch]Batch 3400/10001 Done, mean position loss: 21.08842803478241\n",
      "Training NF1:  34%|█████▋           | 3360/10001 [1:36:47<3:12:40,  1.74s/batch]Batch 3400/10001 Done, mean position loss: 21.15160573244095\n",
      "Training NF1:  34%|█████▊           | 3410/10001 [1:36:54<2:43:55,  1.49s/batch]Batch 3400/10001 Done, mean position loss: 20.607176609039307\n",
      "Training NF1:  34%|█████▋           | 3380/10001 [1:36:57<3:03:32,  1.66s/batch]Batch 3400/10001 Done, mean position loss: 20.796281647682193\n",
      "Training NF1:  34%|█████▊           | 3410/10001 [1:37:12<2:47:17,  1.52s/batch]Batch 3400/10001 Done, mean position loss: 20.743506288528444\n",
      "Training NF1:  34%|█████▊           | 3412/10001 [1:37:14<3:26:13,  1.88s/batch]Batch 3400/10001 Done, mean position loss: 20.519681501388554\n",
      "Training NF1:  34%|█████▋           | 3368/10001 [1:37:16<2:59:56,  1.63s/batch]Batch 3400/10001 Done, mean position loss: 20.606539211273194\n",
      "Training NF1:  34%|█████▊           | 3405/10001 [1:37:22<3:46:56,  2.06s/batch]Batch 3400/10001 Done, mean position loss: 20.74069087266922\n",
      "Training NF1:  34%|█████▊           | 3391/10001 [1:37:24<3:08:06,  1.71s/batch]Batch 3400/10001 Done, mean position loss: 21.001032993793487\n",
      "Training NF1:  33%|█████▋           | 3332/10001 [1:37:27<3:15:13,  1.76s/batch]Batch 3400/10001 Done, mean position loss: 20.991119575500488\n",
      "Training NF1:  34%|█████▊           | 3422/10001 [1:37:32<3:27:20,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.803770484924318\n",
      "Training NF1:  34%|█████▊           | 3389/10001 [1:37:35<3:11:49,  1.74s/batch]Batch 3400/10001 Done, mean position loss: 20.83591106414795\n",
      "Training NF1:  33%|█████▋           | 3343/10001 [1:37:41<3:22:17,  1.82s/batch]Batch 3400/10001 Done, mean position loss: 20.764984860420228\n",
      "Training NF1:  34%|█████▋           | 3366/10001 [1:37:41<3:36:49,  1.96s/batch]Batch 3400/10001 Done, mean position loss: 21.129715609550477\n",
      "Training NF1:  34%|█████▊           | 3388/10001 [1:37:49<2:59:44,  1.63s/batch]Batch 3400/10001 Done, mean position loss: 21.25531940698624\n",
      "Training NF1:  34%|█████▊           | 3388/10001 [1:37:51<3:45:35,  2.05s/batch]Batch 3400/10001 Done, mean position loss: 20.70071420907974\n",
      "Training NF1:  34%|█████▊           | 3386/10001 [1:37:59<2:56:40,  1.60s/batch]Batch 3400/10001 Done, mean position loss: 20.45456764936447\n",
      "Training NF1:  34%|█████▊           | 3389/10001 [1:38:02<2:48:23,  1.53s/batch]Batch 3400/10001 Done, mean position loss: 20.83103104352951\n",
      "Training NF1:  34%|█████▊           | 3408/10001 [1:38:02<3:04:55,  1.68s/batch]Batch 3400/10001 Done, mean position loss: 20.76933170080185\n",
      "Training NF1:  35%|█████▉           | 3458/10001 [1:38:02<2:52:39,  1.58s/batch]Batch 3400/10001 Done, mean position loss: 20.66235384464264\n",
      "Training NF1:  34%|█████▊           | 3440/10001 [1:38:04<3:11:54,  1.76s/batch]Batch 3400/10001 Done, mean position loss: 20.946830513477323\n",
      "Training NF1:  34%|█████▊           | 3398/10001 [1:38:09<3:16:55,  1.79s/batch]Batch 3400/10001 Done, mean position loss: 20.848086371421815\n",
      "Training NF1:  34%|█████▊           | 3447/10001 [1:38:09<3:25:00,  1.88s/batch]Batch 3400/10001 Done, mean position loss: 21.239146032333373\n",
      "Training NF1:  34%|█████▊           | 3402/10001 [1:38:10<3:03:32,  1.67s/batch]Batch 3400/10001 Done, mean position loss: 20.423450224399566\n",
      "Training NF1:  34%|█████▊           | 3391/10001 [1:38:15<3:18:02,  1.80s/batch]Batch 3400/10001 Done, mean position loss: 20.938756265640258\n",
      "Training NF1:  34%|█████▊           | 3408/10001 [1:38:16<3:07:28,  1.71s/batch]Batch 3400/10001 Done, mean position loss: 20.849887375831607\n",
      "Training NF1:  34%|█████▊           | 3422/10001 [1:38:19<3:19:14,  1.82s/batch]Batch 3400/10001 Done, mean position loss: 20.840169875621797\n",
      "Training NF1:  34%|█████▊           | 3412/10001 [1:38:22<3:11:44,  1.75s/batch]Batch 3400/10001 Done, mean position loss: 20.650724356174468\n",
      "Training NF1:  34%|█████▊           | 3423/10001 [1:38:22<3:15:37,  1.78s/batch]Batch 3400/10001 Done, mean position loss: 20.524999759197236\n",
      "Training NF1:  34%|█████▊           | 3395/10001 [1:38:27<2:48:59,  1.53s/batch]Batch 3400/10001 Done, mean position loss: 21.066605985164642\n",
      "Training NF1:  34%|█████▊           | 3442/10001 [1:38:31<2:46:17,  1.52s/batch]Batch 3400/10001 Done, mean position loss: 21.178156380653384\n",
      "Training NF1:  34%|█████▊           | 3443/10001 [1:38:38<3:26:33,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.437593812942506\n",
      "Training NF1:  34%|█████▊           | 3416/10001 [1:38:44<3:14:57,  1.78s/batch]Batch 3400/10001 Done, mean position loss: 21.64236974239349\n",
      "Training NF1:  34%|█████▊           | 3419/10001 [1:38:47<3:28:32,  1.90s/batch]Batch 3500/10001 Done, mean position loss: 20.566046264171604\n",
      "Training NF1:  35%|█████▉           | 3464/10001 [1:39:15<3:09:53,  1.74s/batch]Batch 3500/10001 Done, mean position loss: 20.568731470108034\n",
      "Training NF1:  34%|█████▊           | 3440/10001 [1:39:15<3:23:03,  1.86s/batch]Batch 3500/10001 Done, mean position loss: 20.532701349258424\n",
      "Training NF1:  34%|█████▊           | 3429/10001 [1:39:17<3:26:53,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.69508734703064\n",
      "Training NF1:  35%|█████▉           | 3494/10001 [1:39:21<3:08:09,  1.74s/batch]Batch 3500/10001 Done, mean position loss: 20.719576020240783\n",
      "Training NF1:  34%|█████▊           | 3447/10001 [1:39:28<3:24:55,  1.88s/batch]Batch 3400/10001 Done, mean position loss: 20.80266434907913\n",
      "Training NF1:  35%|█████▉           | 3512/10001 [1:39:33<2:55:39,  1.62s/batch]Batch 3500/10001 Done, mean position loss: 20.567313332557678\n",
      "Training NF1:  35%|█████▉           | 3483/10001 [1:39:42<3:23:17,  1.87s/batch]Batch 3500/10001 Done, mean position loss: 21.080632350444795\n",
      "Training NF1:  34%|█████▊           | 3445/10001 [1:39:49<2:52:25,  1.58s/batch]Batch 3500/10001 Done, mean position loss: 21.11584586381912\n",
      "Training NF1:  35%|█████▉           | 3521/10001 [1:39:49<2:52:38,  1.60s/batch]Batch 3500/10001 Done, mean position loss: 20.60932343006134\n",
      "Training NF1:  35%|█████▉           | 3457/10001 [1:39:53<3:09:43,  1.74s/batch]Batch 3500/10001 Done, mean position loss: 20.793107557296754\n",
      "Training NF1:  35%|█████▉           | 3464/10001 [1:40:07<2:39:57,  1.47s/batch]Batch 3500/10001 Done, mean position loss: 20.744673731327058\n",
      "Training NF1:  35%|█████▉           | 3479/10001 [1:40:07<3:08:32,  1.73s/batch]Batch 3500/10001 Done, mean position loss: 20.497748794555665\n",
      "Training NF1:  35%|█████▉           | 3485/10001 [1:40:13<3:07:54,  1.73s/batch]Batch 3500/10001 Done, mean position loss: 20.616095747947693\n",
      "Training NF1:  35%|█████▊           | 3454/10001 [1:40:15<3:11:30,  1.76s/batch]Batch 3500/10001 Done, mean position loss: 20.74215947151184\n",
      "Training NF1:  35%|█████▉           | 3505/10001 [1:40:15<3:32:02,  1.96s/batch]Batch 3500/10001 Done, mean position loss: 21.00420202255249\n",
      "Training NF1:  35%|█████▉           | 3496/10001 [1:40:18<2:54:50,  1.61s/batch]Batch 3500/10001 Done, mean position loss: 20.978716390132902\n",
      "Training NF1:  35%|█████▉           | 3506/10001 [1:40:23<3:27:05,  1.91s/batch]Batch 3500/10001 Done, mean position loss: 20.8024073266983\n",
      "Training NF1:  35%|█████▉           | 3468/10001 [1:40:26<2:50:55,  1.57s/batch]Batch 3500/10001 Done, mean position loss: 20.837201054096223\n",
      "Training NF1:  35%|█████▉           | 3465/10001 [1:40:35<3:48:44,  2.10s/batch]Batch 3500/10001 Done, mean position loss: 20.760634269714355\n",
      "Training NF1:  35%|█████▉           | 3491/10001 [1:40:38<2:54:24,  1.61s/batch]Batch 3500/10001 Done, mean position loss: 21.07768718481064\n",
      "Training NF1:  36%|██████           | 3565/10001 [1:40:40<3:09:08,  1.76s/batch]Batch 3500/10001 Done, mean position loss: 20.712333862781524\n",
      "Training NF1:  35%|█████▉           | 3496/10001 [1:40:47<3:04:18,  1.70s/batch]Batch 3500/10001 Done, mean position loss: 21.236642882823944\n",
      "Training NF1:  35%|██████           | 3540/10001 [1:40:51<3:05:42,  1.72s/batch]Batch 3500/10001 Done, mean position loss: 20.437122950553892\n",
      "Training NF1:  35%|█████▊           | 3455/10001 [1:40:55<3:17:00,  1.81s/batch]Batch 3500/10001 Done, mean position loss: 20.74661988258362\n",
      "Training NF1:  35%|█████▉           | 3520/10001 [1:40:56<3:29:31,  1.94s/batch]Batch 3500/10001 Done, mean position loss: 20.951068863868713\n",
      "Training NF1:  35%|█████▉           | 3502/10001 [1:40:57<3:18:01,  1.83s/batch]Batch 3500/10001 Done, mean position loss: 20.658787627220157\n",
      "Training NF1:  35%|█████▉           | 3494/10001 [1:40:58<3:13:30,  1.78s/batch]Batch 3500/10001 Done, mean position loss: 20.80527893304825\n",
      "Training NF1:  35%|█████▉           | 3511/10001 [1:41:04<3:09:52,  1.76s/batch]Batch 3500/10001 Done, mean position loss: 20.428571643829343\n",
      "Training NF1:  35%|██████           | 3533/10001 [1:41:05<3:21:51,  1.87s/batch]Batch 3500/10001 Done, mean position loss: 21.184538328647612\n",
      "Training NF1:  35%|█████▉           | 3499/10001 [1:41:07<3:01:45,  1.68s/batch]Batch 3500/10001 Done, mean position loss: 20.88492406606674\n",
      "Training NF1:  35%|██████           | 3532/10001 [1:41:10<3:23:24,  1.89s/batch]Batch 3500/10001 Done, mean position loss: 20.85115021467209\n",
      "Training NF1:  35%|█████▉           | 3509/10001 [1:41:10<2:56:11,  1.63s/batch]Batch 3500/10001 Done, mean position loss: 20.638333773612977\n",
      "Training NF1:  35%|█████▉           | 3510/10001 [1:41:12<3:23:48,  1.88s/batch]Batch 3500/10001 Done, mean position loss: 20.931077511310576\n",
      "Training NF1:  35%|█████▉           | 3518/10001 [1:41:16<3:03:29,  1.70s/batch]Batch 3500/10001 Done, mean position loss: 20.818670470714572\n",
      "Training NF1:  36%|██████           | 3554/10001 [1:41:24<2:59:50,  1.67s/batch]Batch 3500/10001 Done, mean position loss: 20.549409673213958\n",
      "Training NF1:  36%|██████           | 3554/10001 [1:41:26<2:53:00,  1.61s/batch]Batch 3500/10001 Done, mean position loss: 21.042681963443755\n",
      "Training NF1:  36%|██████           | 3580/10001 [1:41:29<3:03:21,  1.71s/batch]Batch 3500/10001 Done, mean position loss: 21.162532091140747\n",
      "Training NF1:  35%|█████▉           | 3524/10001 [1:41:34<3:27:24,  1.92s/batch]Batch 3500/10001 Done, mean position loss: 21.621482911109922\n",
      "Training NF1:  35%|█████▉           | 3520/10001 [1:41:40<3:04:31,  1.71s/batch]Batch 3600/10001 Done, mean position loss: 20.552696931362153\n",
      "Training NF1:  35%|██████           | 3543/10001 [1:41:42<3:33:13,  1.98s/batch]Batch 3500/10001 Done, mean position loss: 20.4250931429863\n",
      "Training NF1:  36%|██████           | 3567/10001 [1:42:03<3:00:30,  1.68s/batch]Batch 3600/10001 Done, mean position loss: 20.530172457695006\n",
      "Training NF1:  35%|██████           | 3534/10001 [1:42:08<3:04:11,  1.71s/batch]Batch 3600/10001 Done, mean position loss: 20.710804722309113\n",
      "Training NF1:  36%|██████           | 3569/10001 [1:42:07<3:06:33,  1.74s/batch]Batch 3600/10001 Done, mean position loss: 20.56051224708557\n",
      "Training NF1:  36%|██████▏          | 3607/10001 [1:42:19<3:16:42,  1.85s/batch]Batch 3500/10001 Done, mean position loss: 20.70834677696228\n",
      "Training NF1:  36%|██████           | 3574/10001 [1:42:24<2:53:32,  1.62s/batch]Batch 3500/10001 Done, mean position loss: 20.78379289150238\n",
      "Training NF1:  35%|█████▉           | 3501/10001 [1:42:24<3:22:07,  1.87s/batch]Batch 3600/10001 Done, mean position loss: 20.57984363794327\n",
      "Training NF1:  36%|██████           | 3581/10001 [1:42:36<3:06:26,  1.74s/batch]Batch 3600/10001 Done, mean position loss: 21.077319099903107\n",
      "Training NF1:  35%|█████▉           | 3511/10001 [1:42:41<3:05:37,  1.72s/batch]Batch 3600/10001 Done, mean position loss: 21.11376068353653\n",
      "Training NF1:  36%|██████           | 3592/10001 [1:42:46<3:09:24,  1.77s/batch]Batch 3600/10001 Done, mean position loss: 20.604847667217257\n",
      "Training NF1:  36%|██████           | 3590/10001 [1:42:46<3:01:33,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 20.782272419929505\n",
      "Training NF1:  35%|█████▉           | 3523/10001 [1:43:03<3:20:30,  1.86s/batch]Batch 3600/10001 Done, mean position loss: 20.723898270130157\n",
      "Training NF1:  36%|██████▏          | 3625/10001 [1:43:03<2:39:41,  1.50s/batch]Batch 3600/10001 Done, mean position loss: 20.74790940284729\n",
      "Training NF1:  36%|██████           | 3567/10001 [1:43:04<3:02:59,  1.71s/batch]Batch 3600/10001 Done, mean position loss: 20.498868496417998\n",
      "Training NF1:  36%|██████           | 3569/10001 [1:43:05<2:34:04,  1.44s/batch]Batch 3600/10001 Done, mean position loss: 20.985651173591613\n",
      "Training NF1:  36%|██████           | 3566/10001 [1:43:09<3:13:28,  1.80s/batch]Batch 3600/10001 Done, mean position loss: 20.629361336231234\n",
      "Training NF1:  36%|██████▏          | 3622/10001 [1:43:13<3:05:00,  1.74s/batch]Batch 3600/10001 Done, mean position loss: 20.983835990428922\n",
      "Training NF1:  36%|██████           | 3576/10001 [1:43:16<3:04:37,  1.72s/batch]Batch 3600/10001 Done, mean position loss: 20.791440639495846\n",
      "Training NF1:  36%|██████           | 3582/10001 [1:43:27<3:08:55,  1.77s/batch]Batch 3600/10001 Done, mean position loss: 20.84465407133102\n",
      "Training NF1:  36%|██████▏          | 3640/10001 [1:43:29<2:58:08,  1.68s/batch]Batch 3600/10001 Done, mean position loss: 21.07039502620697\n",
      "Training NF1:  36%|██████           | 3580/10001 [1:43:29<3:24:48,  1.91s/batch]Batch 3600/10001 Done, mean position loss: 20.766944954395292\n",
      "Training NF1:  36%|██████           | 3596/10001 [1:43:36<3:06:09,  1.74s/batch]Batch 3600/10001 Done, mean position loss: 20.695872244834902\n",
      "Training NF1:  36%|██████           | 3599/10001 [1:43:42<2:59:23,  1.68s/batch]Batch 3600/10001 Done, mean position loss: 20.754609689712524\n",
      "Training NF1:  37%|██████▏          | 3662/10001 [1:43:44<2:41:02,  1.52s/batch]Batch 3600/10001 Done, mean position loss: 20.44276762962341\n",
      "Training NF1:  36%|██████           | 3581/10001 [1:43:45<3:01:56,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 21.215614984035494\n",
      "Training NF1:  36%|██████           | 3594/10001 [1:43:45<2:31:46,  1.42s/batch]Batch 3600/10001 Done, mean position loss: 20.659229826927184\n",
      "Training NF1:  36%|██████           | 3591/10001 [1:43:46<2:59:40,  1.68s/batch]Batch 3600/10001 Done, mean position loss: 20.941650357246402\n",
      "Training NF1:  36%|██████           | 3589/10001 [1:43:46<3:01:14,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 20.814215052127835\n",
      "Training NF1:  36%|██████▏          | 3635/10001 [1:43:57<2:55:56,  1.66s/batch]Batch 3600/10001 Done, mean position loss: 20.863966767787932\n",
      "Training NF1:  36%|██████           | 3597/10001 [1:44:01<3:03:30,  1.72s/batch]Batch 3600/10001 Done, mean position loss: 20.656909501552583\n",
      "Training NF1:  36%|██████▏          | 3615/10001 [1:44:02<3:27:53,  1.95s/batch]Batch 3600/10001 Done, mean position loss: 20.43664175271988\n",
      "Training NF1:  36%|██████           | 3603/10001 [1:44:04<2:46:22,  1.56s/batch]Batch 3600/10001 Done, mean position loss: 21.170841376781464\n",
      "Training NF1:  36%|██████           | 3601/10001 [1:44:04<2:51:30,  1.61s/batch]Batch 3600/10001 Done, mean position loss: 20.909449756145477\n",
      "Training NF1:  37%|██████▏          | 3661/10001 [1:44:06<3:05:07,  1.75s/batch]Batch 3600/10001 Done, mean position loss: 20.844483201503756\n",
      "Training NF1:  36%|██████▏          | 3624/10001 [1:44:08<3:05:42,  1.75s/batch]Batch 3600/10001 Done, mean position loss: 20.818563652038574\n",
      "Training NF1:  37%|██████▏          | 3655/10001 [1:44:19<2:49:35,  1.60s/batch]Batch 3600/10001 Done, mean position loss: 20.543375728130343\n",
      "Training NF1:  37%|██████▎          | 3683/10001 [1:44:19<2:52:06,  1.63s/batch]Batch 3600/10001 Done, mean position loss: 21.152435116767883\n",
      "Training NF1:  36%|██████▏          | 3615/10001 [1:44:26<3:03:44,  1.73s/batch]Batch 3600/10001 Done, mean position loss: 21.068566117286686\n",
      "Training NF1:  36%|██████▏          | 3635/10001 [1:44:28<3:09:01,  1.78s/batch]Batch 3600/10001 Done, mean position loss: 21.626765959262848\n",
      "Training NF1:  36%|██████▏          | 3618/10001 [1:44:32<3:14:38,  1.83s/batch]Batch 3700/10001 Done, mean position loss: 20.55345328092575\n",
      "Training NF1:  37%|██████▎          | 3690/10001 [1:44:41<3:01:38,  1.73s/batch]Batch 3600/10001 Done, mean position loss: 20.420981822013857\n",
      "Training NF1:  36%|██████▏          | 3637/10001 [1:44:49<3:11:31,  1.81s/batch]Batch 3700/10001 Done, mean position loss: 20.522045311927794\n",
      "Training NF1:  36%|██████▏          | 3621/10001 [1:45:01<2:50:08,  1.60s/batch]Batch 3700/10001 Done, mean position loss: 20.5670405960083\n",
      "Training NF1:  36%|██████▏          | 3636/10001 [1:45:05<2:45:11,  1.56s/batch]Batch 3700/10001 Done, mean position loss: 20.701820347309113\n",
      "Training NF1:  37%|██████▏          | 3659/10001 [1:45:18<2:54:38,  1.65s/batch]Batch 3600/10001 Done, mean position loss: 20.69813551187515\n",
      "Training NF1:  37%|██████▎          | 3696/10001 [1:45:18<2:52:40,  1.64s/batch]Batch 3700/10001 Done, mean position loss: 20.558275096416473\n",
      "Training NF1:  37%|██████▎          | 3713/10001 [1:45:20<3:13:01,  1.84s/batch]Batch 3600/10001 Done, mean position loss: 20.774425930976868\n",
      "Training NF1:  37%|██████▏          | 3661/10001 [1:45:25<2:20:39,  1.33s/batch]Batch 3700/10001 Done, mean position loss: 21.13917243003845\n",
      "Training NF1:  37%|██████▏          | 3654/10001 [1:45:28<3:16:22,  1.86s/batch]Batch 3700/10001 Done, mean position loss: 21.057331085205078\n",
      "Training NF1:  37%|██████▎          | 3741/10001 [1:45:38<3:04:30,  1.77s/batch]Batch 3700/10001 Done, mean position loss: 20.60289670705795\n",
      "Training NF1:  37%|██████▎          | 3693/10001 [1:45:42<3:02:40,  1.74s/batch]Batch 3700/10001 Done, mean position loss: 20.799669911861418\n",
      "Training NF1:  37%|██████▎          | 3720/10001 [1:45:50<2:55:14,  1.67s/batch]Batch 3700/10001 Done, mean position loss: 20.478295867443084\n",
      "Training NF1:  37%|██████▎          | 3718/10001 [1:45:53<2:55:03,  1.67s/batch]Batch 3700/10001 Done, mean position loss: 20.716916284561158\n",
      "Training NF1:  37%|██████▎          | 3679/10001 [1:45:56<2:39:16,  1.51s/batch]Batch 3700/10001 Done, mean position loss: 20.728057160377503\n",
      "Training NF1:  37%|██████▎          | 3723/10001 [1:46:02<2:52:42,  1.65s/batch]Batch 3700/10001 Done, mean position loss: 21.00320949792862\n",
      "Training NF1:  37%|██████▎          | 3687/10001 [1:46:05<2:51:20,  1.63s/batch]Batch 3700/10001 Done, mean position loss: 20.976276295185087\n",
      "Training NF1:  36%|██████▏          | 3628/10001 [1:46:06<2:55:17,  1.65s/batch]Batch 3700/10001 Done, mean position loss: 20.62543292760849\n",
      "Training NF1:  37%|██████▏          | 3660/10001 [1:46:08<2:44:56,  1.56s/batch]Batch 3700/10001 Done, mean position loss: 20.783974888324735\n",
      "Training NF1:  37%|██████▎          | 3736/10001 [1:46:24<2:43:04,  1.56s/batch]Batch 3700/10001 Done, mean position loss: 20.829505715370182\n",
      "Training NF1:  37%|██████▎          | 3720/10001 [1:46:26<3:15:11,  1.86s/batch]Batch 3700/10001 Done, mean position loss: 20.691619107723238\n",
      "Training NF1:  37%|██████▎          | 3730/10001 [1:46:26<3:00:22,  1.73s/batch]Batch 3700/10001 Done, mean position loss: 21.041327981948854\n",
      "Training NF1:  37%|██████▏          | 3672/10001 [1:46:27<3:06:18,  1.77s/batch]Batch 3700/10001 Done, mean position loss: 20.73577076435089\n",
      "Training NF1:  37%|██████▏          | 3676/10001 [1:46:29<3:18:17,  1.88s/batch]Batch 3700/10001 Done, mean position loss: 20.73801506996155\n",
      "Training NF1:  37%|██████▎          | 3728/10001 [1:46:29<3:17:01,  1.88s/batch]Batch 3700/10001 Done, mean position loss: 20.66016225099564\n",
      "Training NF1:  37%|██████▎          | 3680/10001 [1:46:37<3:15:37,  1.86s/batch]Batch 3700/10001 Done, mean position loss: 20.949531331062317\n",
      "Training NF1:  37%|██████▎          | 3727/10001 [1:46:37<3:01:36,  1.74s/batch]Batch 3700/10001 Done, mean position loss: 20.452781307697293\n",
      "Training NF1:  37%|██████▎          | 3710/10001 [1:46:42<3:03:38,  1.75s/batch]Batch 3700/10001 Done, mean position loss: 20.79011344432831\n",
      "Training NF1:  37%|██████▎          | 3705/10001 [1:46:43<3:14:23,  1.85s/batch]Batch 3700/10001 Done, mean position loss: 21.20945867061615\n",
      "Training NF1:  37%|██████▎          | 3692/10001 [1:46:46<3:09:12,  1.80s/batch]Batch 3700/10001 Done, mean position loss: 20.649436423778532\n",
      "Training NF1:  38%|██████▍          | 3752/10001 [1:46:51<3:09:43,  1.82s/batch]Batch 3700/10001 Done, mean position loss: 20.858407101631165\n",
      "Training NF1:  37%|██████▎          | 3729/10001 [1:46:58<2:57:41,  1.70s/batch]Batch 3700/10001 Done, mean position loss: 20.819430060386658\n",
      "Training NF1:  37%|██████▎          | 3678/10001 [1:46:58<3:00:31,  1.71s/batch]Batch 3700/10001 Done, mean position loss: 20.907514295578004\n",
      "Training NF1:  37%|██████▎          | 3712/10001 [1:47:01<2:57:29,  1.69s/batch]Batch 3700/10001 Done, mean position loss: 20.424177486896514\n",
      "Training NF1:  37%|██████▎          | 3691/10001 [1:47:02<3:18:11,  1.88s/batch]Batch 3700/10001 Done, mean position loss: 20.834034395217895\n",
      "Training NF1:  37%|██████▏          | 3656/10001 [1:47:04<2:49:19,  1.60s/batch]Batch 3700/10001 Done, mean position loss: 21.210743248462677\n",
      "Training NF1:  38%|██████▍          | 3767/10001 [1:47:14<2:59:25,  1.73s/batch]Batch 3700/10001 Done, mean position loss: 21.18278507232666\n",
      "Training NF1:  37%|██████▏          | 3665/10001 [1:47:16<3:01:47,  1.72s/batch]Batch 3700/10001 Done, mean position loss: 20.51608579158783\n",
      "Training NF1:  37%|██████▎          | 3741/10001 [1:47:20<3:00:15,  1.73s/batch]Batch 3700/10001 Done, mean position loss: 21.031254069805144\n",
      "Training NF1:  37%|██████▎          | 3718/10001 [1:47:20<2:59:59,  1.72s/batch]Batch 3700/10001 Done, mean position loss: 21.60895924806595\n",
      "Training NF1:  38%|██████▍          | 3783/10001 [1:47:24<2:50:07,  1.64s/batch]Batch 3800/10001 Done, mean position loss: 20.55828814506531\n",
      "Training NF1:  37%|██████▎          | 3741/10001 [1:47:41<3:41:44,  2.13s/batch]Batch 3700/10001 Done, mean position loss: 20.42334588050842\n",
      "Training NF1:  37%|██████▎          | 3683/10001 [1:47:48<3:25:33,  1.95s/batch]Batch 3800/10001 Done, mean position loss: 20.533750329017643\n",
      "Training NF1:  37%|██████▎          | 3682/10001 [1:47:50<3:06:22,  1.77s/batch]Batch 3800/10001 Done, mean position loss: 20.68006486415863\n",
      "Training NF1:  38%|██████▍          | 3755/10001 [1:47:57<2:38:15,  1.52s/batch]Batch 3800/10001 Done, mean position loss: 20.5557555270195\n",
      "Training NF1:  38%|██████▍          | 3793/10001 [1:48:16<2:42:24,  1.57s/batch]Batch 3800/10001 Done, mean position loss: 20.556116266250612\n",
      "Training NF1:  38%|██████▍          | 3782/10001 [1:48:16<3:01:00,  1.75s/batch]Batch 3800/10001 Done, mean position loss: 21.11633398294449\n",
      "Training NF1:  38%|██████▍          | 3802/10001 [1:48:17<2:47:43,  1.62s/batch]Batch 3800/10001 Done, mean position loss: 21.094392864704133\n",
      "Training NF1:  38%|██████▍          | 3802/10001 [1:48:19<3:09:34,  1.83s/batch]Batch 3700/10001 Done, mean position loss: 20.69053418636322\n",
      "Training NF1:  37%|██████▎          | 3749/10001 [1:48:23<3:07:39,  1.80s/batch]Batch 3700/10001 Done, mean position loss: 20.763077614307406\n",
      "Training NF1:  37%|██████▎          | 3741/10001 [1:48:29<3:05:07,  1.77s/batch]Batch 3800/10001 Done, mean position loss: 20.589941966533658\n",
      "Training NF1:  38%|██████▍          | 3788/10001 [1:48:36<2:47:00,  1.61s/batch]Batch 3800/10001 Done, mean position loss: 20.78983834505081\n",
      "Training NF1:  38%|██████▍          | 3795/10001 [1:48:48<2:54:56,  1.69s/batch]Batch 3800/10001 Done, mean position loss: 20.738209483623503\n",
      "Training NF1:  38%|██████▍          | 3761/10001 [1:48:51<3:11:15,  1.84s/batch]Batch 3800/10001 Done, mean position loss: 20.47491359472275\n",
      "Training NF1:  38%|██████▌          | 3833/10001 [1:48:52<2:45:51,  1.61s/batch]Batch 3800/10001 Done, mean position loss: 20.73529339790344\n",
      "Training NF1:  38%|██████▍          | 3779/10001 [1:48:58<2:39:02,  1.53s/batch]Batch 3800/10001 Done, mean position loss: 20.967706944942474\n",
      "Training NF1:  38%|██████▍          | 3761/10001 [1:48:59<2:44:30,  1.58s/batch]Batch 3800/10001 Done, mean position loss: 20.964899179935458\n",
      "Training NF1:  38%|██████▍          | 3817/10001 [1:49:03<2:44:49,  1.60s/batch]Batch 3800/10001 Done, mean position loss: 20.625267057418824\n",
      "Training NF1:  38%|██████▍          | 3812/10001 [1:49:06<3:03:07,  1.78s/batch]Batch 3800/10001 Done, mean position loss: 20.781174411773684\n",
      "Training NF1:  38%|██████▍          | 3797/10001 [1:49:14<2:52:52,  1.67s/batch]Batch 3800/10001 Done, mean position loss: 20.831943960189818\n",
      "Training NF1:  38%|██████▍          | 3778/10001 [1:49:16<2:44:00,  1.58s/batch]Batch 3800/10001 Done, mean position loss: 21.002334747314453\n",
      "Training NF1:  38%|██████▍          | 3783/10001 [1:49:20<2:48:33,  1.63s/batch]Batch 3800/10001 Done, mean position loss: 20.697619848251342\n",
      "Training NF1:  38%|██████▌          | 3839/10001 [1:49:21<3:12:22,  1.87s/batch]Batch 3800/10001 Done, mean position loss: 20.72799055337906\n",
      "Training NF1:  38%|██████▍          | 3802/10001 [1:49:22<3:22:52,  1.96s/batch]Batch 3800/10001 Done, mean position loss: 20.657466044425966\n",
      "Training NF1:  38%|██████▍          | 3802/10001 [1:49:23<2:48:04,  1.63s/batch]Batch 3800/10001 Done, mean position loss: 20.749680202007294\n",
      "Training NF1:  38%|██████▍          | 3799/10001 [1:49:32<2:43:09,  1.58s/batch]Batch 3800/10001 Done, mean position loss: 20.437510437965393\n",
      "Training NF1:  38%|██████▍          | 3808/10001 [1:49:32<2:49:51,  1.65s/batch]Batch 3800/10001 Done, mean position loss: 20.939898014068604\n",
      "Training NF1:  38%|██████▍          | 3781/10001 [1:49:35<2:52:32,  1.66s/batch]Batch 3800/10001 Done, mean position loss: 20.789524731636046\n",
      "Training NF1:  38%|██████▍          | 3781/10001 [1:49:37<2:55:52,  1.70s/batch]Batch 3800/10001 Done, mean position loss: 21.191743628978728\n",
      "Training NF1:  38%|██████▍          | 3790/10001 [1:49:39<3:00:06,  1.74s/batch]Batch 3800/10001 Done, mean position loss: 20.633062186241148\n",
      "Training NF1:  38%|██████▍          | 3813/10001 [1:49:44<2:58:44,  1.73s/batch]Batch 3800/10001 Done, mean position loss: 20.83295409202576\n",
      "Training NF1:  38%|██████▍          | 3797/10001 [1:49:50<2:47:42,  1.62s/batch]Batch 3800/10001 Done, mean position loss: 20.818987984657287\n",
      "Training NF1:  38%|██████▍          | 3805/10001 [1:49:50<2:53:39,  1.68s/batch]Batch 3800/10001 Done, mean position loss: 20.9122348690033\n",
      "Training NF1:  38%|██████▍          | 3811/10001 [1:49:56<3:10:22,  1.85s/batch]Batch 3800/10001 Done, mean position loss: 21.170189852714536\n",
      "Training NF1:  38%|██████▍          | 3812/10001 [1:49:58<3:18:27,  1.92s/batch]Batch 3800/10001 Done, mean position loss: 20.427304630279544\n",
      "Training NF1:  38%|██████▌          | 3842/10001 [1:49:59<2:32:43,  1.49s/batch]Batch 3800/10001 Done, mean position loss: 20.83431145906448\n",
      "Training NF1:  38%|██████▌          | 3829/10001 [1:50:11<3:11:49,  1.86s/batch]Batch 3800/10001 Done, mean position loss: 21.180611710548398\n",
      "Training NF1:  38%|██████▌          | 3835/10001 [1:50:12<2:37:40,  1.53s/batch]Batch 3800/10001 Done, mean position loss: 21.598681626319884\n",
      "Training NF1:  38%|██████▍          | 3797/10001 [1:50:12<2:56:09,  1.70s/batch]Batch 3800/10001 Done, mean position loss: 20.519318869113924\n",
      "Training NF1:  38%|██████▌          | 3826/10001 [1:50:15<2:50:46,  1.66s/batch]Batch 3900/10001 Done, mean position loss: 20.561753878593443\n",
      "Training NF1:  38%|██████▍          | 3805/10001 [1:50:18<2:49:12,  1.64s/batch]Batch 3800/10001 Done, mean position loss: 21.039439225196837\n",
      "Training NF1:  39%|██████▌          | 3863/10001 [1:50:37<2:34:31,  1.51s/batch]Batch 3800/10001 Done, mean position loss: 20.42889111995697\n",
      "Training NF1:  38%|██████▌          | 3835/10001 [1:50:40<2:49:41,  1.65s/batch]Batch 3900/10001 Done, mean position loss: 20.53032288312912\n",
      "Training NF1:  39%|██████▌          | 3876/10001 [1:50:43<2:45:35,  1.62s/batch]Batch 3900/10001 Done, mean position loss: 20.66858826160431\n",
      "Training NF1:  38%|██████▌          | 3825/10001 [1:50:42<2:57:52,  1.73s/batch]Batch 3900/10001 Done, mean position loss: 20.545725486278535\n",
      "Training NF1:  38%|██████▌          | 3837/10001 [1:51:03<3:01:53,  1.77s/batch]Batch 3900/10001 Done, mean position loss: 21.12956192493439\n",
      "Training NF1:  39%|██████▌          | 3862/10001 [1:51:05<3:30:26,  2.06s/batch]Batch 3900/10001 Done, mean position loss: 20.570090861320498\n",
      "Training NF1:  38%|██████▌          | 3834/10001 [1:51:09<3:01:22,  1.76s/batch]Batch 3900/10001 Done, mean position loss: 21.061995391845706\n",
      "Training NF1:  39%|██████▋          | 3918/10001 [1:51:11<2:47:53,  1.66s/batch]Batch 3800/10001 Done, mean position loss: 20.688469364643097\n",
      "Training NF1:  38%|██████▌          | 3840/10001 [1:51:17<2:57:00,  1.72s/batch]Batch 3800/10001 Done, mean position loss: 20.75172226190567\n",
      "Training NF1:  39%|██████▌          | 3857/10001 [1:51:19<3:10:17,  1.86s/batch]Batch 3900/10001 Done, mean position loss: 20.590421357154845\n",
      "Training NF1:  39%|██████▌          | 3866/10001 [1:51:28<3:20:16,  1.96s/batch]Batch 3900/10001 Done, mean position loss: 20.796846570968626\n",
      "Training NF1:  39%|██████▌          | 3881/10001 [1:51:42<2:57:03,  1.74s/batch]Batch 3900/10001 Done, mean position loss: 20.722077271938325\n",
      "Training NF1:  39%|██████▌          | 3874/10001 [1:51:47<3:04:04,  1.80s/batch]Batch 3900/10001 Done, mean position loss: 20.973603711128234\n",
      "Training NF1:  39%|██████▋          | 3939/10001 [1:51:47<2:38:39,  1.57s/batch]Batch 3900/10001 Done, mean position loss: 20.746740140914916\n",
      "Training NF1:  39%|██████▌          | 3856/10001 [1:51:47<3:08:59,  1.85s/batch]Batch 3900/10001 Done, mean position loss: 20.4767285323143\n",
      "Training NF1:  39%|██████▌          | 3866/10001 [1:51:52<2:54:35,  1.71s/batch]Batch 3900/10001 Done, mean position loss: 20.996674864292146\n",
      "Training NF1:  39%|██████▋          | 3906/10001 [1:51:59<2:38:40,  1.56s/batch]Batch 3900/10001 Done, mean position loss: 20.5991690158844\n",
      "Training NF1:  39%|██████▋          | 3909/10001 [1:52:01<3:13:17,  1.90s/batch]Batch 3900/10001 Done, mean position loss: 20.79301429748535\n",
      "Training NF1:  39%|██████▋          | 3903/10001 [1:52:05<2:51:47,  1.69s/batch]Batch 3900/10001 Done, mean position loss: 20.83725758075714\n",
      "Training NF1:  39%|██████▋          | 3927/10001 [1:52:11<2:48:09,  1.66s/batch]Batch 3900/10001 Done, mean position loss: 20.688852899074554\n",
      "Training NF1:  40%|██████▋          | 3969/10001 [1:52:12<3:01:44,  1.81s/batch]Batch 3900/10001 Done, mean position loss: 20.756921279430387\n",
      "Training NF1:  40%|██████▋          | 3956/10001 [1:52:13<2:50:33,  1.69s/batch]Batch 3900/10001 Done, mean position loss: 20.737491993904115\n",
      "Training NF1:  39%|██████▋          | 3921/10001 [1:52:13<2:25:36,  1.44s/batch]Batch 3900/10001 Done, mean position loss: 21.020648262500764\n",
      "Training NF1:  40%|██████▋          | 3959/10001 [1:52:19<3:31:09,  2.10s/batch]Batch 3900/10001 Done, mean position loss: 20.65337364912033\n",
      "Training NF1:  39%|██████▌          | 3883/10001 [1:52:20<2:58:46,  1.75s/batch]Batch 3900/10001 Done, mean position loss: 20.929237136840822\n",
      "Training NF1:  40%|██████▋          | 3963/10001 [1:52:26<2:48:10,  1.67s/batch]Batch 3900/10001 Done, mean position loss: 20.445540595054627\n",
      "Training NF1:  39%|██████▌          | 3897/10001 [1:52:27<2:43:05,  1.60s/batch]Batch 3900/10001 Done, mean position loss: 20.777214865684506\n",
      "Training NF1:  39%|██████▌          | 3887/10001 [1:52:31<3:00:16,  1.77s/batch]Batch 3900/10001 Done, mean position loss: 20.635927340984345\n",
      "Training NF1:  39%|██████▋          | 3905/10001 [1:52:34<2:40:03,  1.58s/batch]Batch 3900/10001 Done, mean position loss: 21.20306569337845\n",
      "Training NF1:  39%|██████▋          | 3927/10001 [1:52:35<3:00:34,  1.78s/batch]Batch 3900/10001 Done, mean position loss: 20.838623893260955\n",
      "Training NF1:  39%|██████▌          | 3889/10001 [1:52:46<2:50:04,  1.67s/batch]Batch 3900/10001 Done, mean position loss: 20.906127586364747\n",
      "Training NF1:  39%|██████▋          | 3948/10001 [1:52:46<2:50:38,  1.69s/batch]Batch 3900/10001 Done, mean position loss: 20.79664228916168\n",
      "Training NF1:  40%|██████▋          | 3966/10001 [1:52:51<2:49:52,  1.69s/batch]Batch 3900/10001 Done, mean position loss: 21.162337493896487\n",
      "Training NF1:  39%|██████▌          | 3889/10001 [1:52:56<2:40:33,  1.58s/batch]Batch 3900/10001 Done, mean position loss: 20.829235193729403\n",
      "Training NF1:  40%|██████▊          | 3997/10001 [1:53:00<2:41:30,  1.61s/batch]Batch 3900/10001 Done, mean position loss: 20.41525662899017\n",
      "Training NF1:  39%|██████▋          | 3936/10001 [1:53:06<3:07:39,  1.86s/batch]Batch 3900/10001 Done, mean position loss: 21.587245044708254\n",
      "Training NF1:  40%|██████▋          | 3968/10001 [1:53:07<2:50:11,  1.69s/batch]Batch 4000/10001 Done, mean position loss: 20.55308392047882\n",
      "Training NF1:  40%|██████▊          | 4001/10001 [1:53:07<2:52:49,  1.73s/batch]Batch 3900/10001 Done, mean position loss: 21.131027207374572\n",
      "Training NF1:  39%|██████▋          | 3932/10001 [1:53:12<2:38:42,  1.57s/batch]Batch 3900/10001 Done, mean position loss: 20.511352105140688\n",
      "Training NF1:  40%|██████▊          | 3992/10001 [1:53:17<2:45:29,  1.65s/batch]Batch 3900/10001 Done, mean position loss: 21.013025596141816\n",
      "Training NF1:  40%|██████▋          | 3958/10001 [1:53:32<2:39:48,  1.59s/batch]Batch 4000/10001 Done, mean position loss: 20.520243878364564\n",
      "Training NF1:  39%|██████▋          | 3940/10001 [1:53:34<2:47:20,  1.66s/batch]Batch 4000/10001 Done, mean position loss: 20.55887460708618\n",
      "Training NF1:  40%|██████▊          | 3985/10001 [1:53:37<3:06:37,  1.86s/batch]Batch 3900/10001 Done, mean position loss: 20.422361779212952\n",
      "Training NF1:  39%|██████▋          | 3941/10001 [1:53:38<2:52:39,  1.71s/batch]Batch 4000/10001 Done, mean position loss: 20.691537022590637\n",
      "Training NF1:  40%|██████▊          | 3981/10001 [1:53:54<2:39:28,  1.59s/batch]Batch 4000/10001 Done, mean position loss: 20.549473712444303\n",
      "Training NF1:  40%|██████▊          | 4016/10001 [1:53:58<2:19:30,  1.40s/batch]Batch 4000/10001 Done, mean position loss: 21.08975600004196\n",
      "Training NF1:  40%|██████▊          | 4020/10001 [1:54:06<2:47:47,  1.68s/batch]Batch 4000/10001 Done, mean position loss: 20.580603256225587\n",
      "Training NF1:  40%|██████▊          | 4001/10001 [1:54:06<2:40:19,  1.60s/batch]Batch 3900/10001 Done, mean position loss: 20.69248475074768\n",
      "Training NF1:  40%|██████▋          | 3957/10001 [1:54:09<2:46:07,  1.65s/batch]Batch 4000/10001 Done, mean position loss: 21.038343360424044\n",
      "Training NF1:  40%|██████▊          | 3983/10001 [1:54:11<2:27:57,  1.48s/batch]Batch 3900/10001 Done, mean position loss: 20.768207211494445\n",
      "Training NF1:  40%|██████▋          | 3964/10001 [1:54:15<3:05:22,  1.84s/batch]Batch 4000/10001 Done, mean position loss: 20.781330745220185\n",
      "Training NF1:  40%|██████▊          | 4013/10001 [1:54:28<3:00:27,  1.81s/batch]Batch 4000/10001 Done, mean position loss: 20.6972353887558\n",
      "Training NF1:  40%|██████▊          | 3980/10001 [1:54:43<2:51:44,  1.71s/batch]Batch 4000/10001 Done, mean position loss: 20.976617331504823\n",
      "Training NF1:  40%|██████▋          | 3955/10001 [1:54:44<3:19:42,  1.98s/batch]Batch 4000/10001 Done, mean position loss: 20.74204713821411\n",
      "Training NF1:  39%|██████▋          | 3924/10001 [1:54:47<3:07:42,  1.85s/batch]Batch 4000/10001 Done, mean position loss: 20.47888142108917\n",
      "Training NF1:  40%|██████▊          | 3977/10001 [1:54:47<2:52:13,  1.72s/batch]Batch 4000/10001 Done, mean position loss: 20.969017708301543\n",
      "Training NF1:  40%|██████▊          | 4037/10001 [1:55:00<2:51:58,  1.73s/batch]Batch 4000/10001 Done, mean position loss: 20.777105486392976\n",
      "Training NF1:  40%|██████▋          | 3967/10001 [1:55:02<2:49:07,  1.68s/batch]Batch 4000/10001 Done, mean position loss: 20.601159222126007\n",
      "Training NF1:  40%|██████▋          | 3968/10001 [1:55:04<2:55:52,  1.75s/batch]Batch 4000/10001 Done, mean position loss: 20.835689175128937\n",
      "Training NF1:  40%|██████▊          | 4014/10001 [1:55:08<2:47:24,  1.68s/batch]Batch 4000/10001 Done, mean position loss: 20.749264500141145\n",
      "Training NF1:  40%|██████▊          | 3998/10001 [1:55:08<2:39:56,  1.60s/batch]Batch 4000/10001 Done, mean position loss: 20.681101086139677\n",
      "Training NF1:  40%|██████▊          | 3978/10001 [1:55:13<2:56:31,  1.76s/batch]Batch 4000/10001 Done, mean position loss: 20.92805908679962\n",
      "Training NF1:  40%|██████▊          | 4044/10001 [1:55:14<3:47:33,  2.29s/batch]Batch 4000/10001 Done, mean position loss: 20.64844838857651\n",
      "Training NF1:  40%|██████▊          | 4006/10001 [1:55:15<2:33:16,  1.53s/batch]Batch 4000/10001 Done, mean position loss: 20.731034038066866\n",
      "Training NF1:  40%|██████▊          | 3989/10001 [1:55:19<2:37:13,  1.57s/batch]Batch 4000/10001 Done, mean position loss: 20.778761343955995\n",
      "Training NF1:  40%|██████▊          | 4007/10001 [1:55:19<3:27:23,  2.08s/batch]Batch 4000/10001 Done, mean position loss: 20.43054278373718\n",
      "Training NF1:  39%|██████▋          | 3938/10001 [1:55:20<3:19:04,  1.97s/batch]Batch 4000/10001 Done, mean position loss: 21.033035991191863\n",
      "Training NF1:  40%|██████▊          | 3975/10001 [1:55:23<2:51:58,  1.71s/batch]Batch 4000/10001 Done, mean position loss: 20.630422704219818\n",
      "Training NF1:  40%|██████▊          | 4017/10001 [1:55:29<2:55:06,  1.76s/batch]Batch 4000/10001 Done, mean position loss: 20.849117078781127\n",
      "Training NF1:  40%|██████▊          | 4017/10001 [1:55:34<3:01:45,  1.82s/batch]Batch 4000/10001 Done, mean position loss: 21.173215763568876\n",
      "Training NF1:  39%|██████▋          | 3950/10001 [1:55:40<2:33:50,  1.53s/batch]Batch 4000/10001 Done, mean position loss: 20.894228394031522\n",
      "Training NF1:  40%|██████▊          | 4021/10001 [1:55:42<3:05:18,  1.86s/batch]Batch 4000/10001 Done, mean position loss: 20.78854454278946\n",
      "Training NF1:  40%|██████▊          | 4017/10001 [1:55:52<3:00:04,  1.81s/batch]Batch 4000/10001 Done, mean position loss: 20.41802225589752\n",
      "Training NF1:  40%|██████▊          | 4030/10001 [1:55:52<2:50:33,  1.71s/batch]Batch 4000/10001 Done, mean position loss: 21.163817644119263\n",
      "Training NF1:  40%|██████▊          | 4008/10001 [1:55:53<2:41:58,  1.62s/batch]Batch 4000/10001 Done, mean position loss: 20.81637760639191\n",
      "Training NF1:  40%|██████▊          | 4030/10001 [1:55:58<2:59:09,  1.80s/batch]Batch 4100/10001 Done, mean position loss: 20.545481793880462\n",
      "Training NF1:  40%|██████▊          | 3999/10001 [1:55:59<3:16:08,  1.96s/batch]Batch 4000/10001 Done, mean position loss: 21.586693232059478\n",
      "Training NF1:  40%|██████▊          | 4032/10001 [1:56:02<3:22:27,  2.04s/batch]Batch 4000/10001 Done, mean position loss: 20.509182448387147\n",
      "Training NF1:  40%|██████▊          | 4036/10001 [1:56:03<3:07:35,  1.89s/batch]Batch 4000/10001 Done, mean position loss: 21.147509527206424\n",
      "Training NF1:  40%|██████▊          | 4014/10001 [1:56:06<3:01:01,  1.81s/batch]Batch 4000/10001 Done, mean position loss: 21.03368080854416\n",
      "Training NF1:  40%|██████▊          | 4034/10001 [1:56:25<2:44:17,  1.65s/batch]Batch 4100/10001 Done, mean position loss: 20.500047216415403\n",
      "Training NF1:  40%|██████▊          | 4027/10001 [1:56:26<2:53:58,  1.75s/batch]Batch 4100/10001 Done, mean position loss: 20.54506168603897\n",
      "Training NF1:  40%|██████▊          | 4023/10001 [1:56:27<3:01:23,  1.82s/batch]Batch 4100/10001 Done, mean position loss: 20.66500299215317\n",
      "Training NF1:  40%|██████▊          | 4029/10001 [1:56:37<2:46:54,  1.68s/batch]Batch 4000/10001 Done, mean position loss: 20.414970264434814\n",
      "Training NF1:  40%|██████▊          | 4041/10001 [1:56:49<2:45:42,  1.67s/batch]Batch 4100/10001 Done, mean position loss: 20.54546674489975\n",
      "Training NF1:  40%|██████▊          | 4037/10001 [1:56:55<2:44:36,  1.66s/batch]Batch 4100/10001 Done, mean position loss: 21.079082858562472\n",
      "Training NF1:  40%|██████▊          | 4000/10001 [1:56:58<2:57:42,  1.78s/batch]Batch 4100/10001 Done, mean position loss: 21.036426889896394\n",
      "Training NF1:  41%|██████▉          | 4056/10001 [1:56:59<2:51:12,  1.73s/batch]Batch 4000/10001 Done, mean position loss: 20.696249694824218\n",
      "Training NF1:  41%|██████▉          | 4062/10001 [1:57:01<2:49:38,  1.71s/batch]Batch 4100/10001 Done, mean position loss: 20.576132202148436\n",
      "Training NF1:  41%|██████▉          | 4060/10001 [1:57:05<2:45:14,  1.67s/batch]Batch 4100/10001 Done, mean position loss: 20.781596727371216\n",
      "Training NF1:  41%|███████          | 4126/10001 [1:57:08<2:54:14,  1.78s/batch]Batch 4000/10001 Done, mean position loss: 20.755113492012025\n",
      "Training NF1:  41%|███████          | 4150/10001 [1:57:25<2:34:06,  1.58s/batch]Batch 4100/10001 Done, mean position loss: 20.701034109592435\n",
      "Training NF1:  40%|██████▊          | 4032/10001 [1:57:31<2:40:08,  1.61s/batch]Batch 4100/10001 Done, mean position loss: 20.726642968654634\n",
      "Training NF1:  42%|███████          | 4159/10001 [1:57:38<2:27:46,  1.52s/batch]Batch 4100/10001 Done, mean position loss: 20.98463215112686\n",
      "Training NF1:  41%|███████          | 4141/10001 [1:57:39<3:18:58,  2.04s/batch]Batch 4100/10001 Done, mean position loss: 20.97228810787201\n",
      "Training NF1:  41%|██████▉          | 4059/10001 [1:57:41<3:15:39,  1.98s/batch]Batch 4100/10001 Done, mean position loss: 20.471022775173186\n",
      "Training NF1:  41%|██████▉          | 4065/10001 [1:57:53<2:59:03,  1.81s/batch]Batch 4100/10001 Done, mean position loss: 20.59536563158035\n",
      "Training NF1:  42%|███████          | 4151/10001 [1:57:56<2:43:59,  1.68s/batch]Batch 4100/10001 Done, mean position loss: 20.765040216445925\n",
      "Training NF1:  40%|██████▊          | 4035/10001 [1:57:58<3:05:58,  1.87s/batch]Batch 4100/10001 Done, mean position loss: 20.6775065779686\n",
      "Training NF1:  41%|██████▉          | 4092/10001 [1:58:05<3:01:02,  1.84s/batch]Batch 4100/10001 Done, mean position loss: 20.839251878261567\n",
      "Training NF1:  41%|██████▉          | 4116/10001 [1:58:06<2:33:11,  1.56s/batch]Batch 4100/10001 Done, mean position loss: 20.75096844673157\n",
      "Training NF1:  41%|██████▉          | 4088/10001 [1:58:06<3:00:04,  1.83s/batch]Batch 4100/10001 Done, mean position loss: 20.938207333087924\n",
      "Training NF1:  41%|██████▉          | 4082/10001 [1:58:08<2:38:59,  1.61s/batch]Batch 4100/10001 Done, mean position loss: 20.64686621427536\n",
      "Batch 4100/10001 Done, mean position loss: 20.4317351436615\n",
      "Training NF1:  41%|██████▉          | 4052/10001 [1:58:11<3:14:35,  1.96s/batch]Batch 4100/10001 Done, mean position loss: 20.74308756828308\n",
      "Training NF1:  40%|██████▊          | 4043/10001 [1:58:13<3:09:28,  1.91s/batch]Batch 4100/10001 Done, mean position loss: 20.63249394416809\n",
      "Training NF1:  41%|██████▉          | 4080/10001 [1:58:19<2:54:18,  1.77s/batch]Batch 4100/10001 Done, mean position loss: 20.76884953022003\n",
      "Training NF1:  41%|███████          | 4133/10001 [1:58:19<3:12:40,  1.97s/batch]Batch 4100/10001 Done, mean position loss: 21.04733496904373\n",
      "Training NF1:  41%|███████          | 4128/10001 [1:58:25<2:58:56,  1.83s/batch]Batch 4100/10001 Done, mean position loss: 20.827077355384827\n",
      "Training NF1:  41%|██████▉          | 4106/10001 [1:58:28<2:49:36,  1.73s/batch]Batch 4100/10001 Done, mean position loss: 21.17095768928528\n",
      "Training NF1:  41%|██████▉          | 4113/10001 [1:58:35<2:42:40,  1.66s/batch]Batch 4100/10001 Done, mean position loss: 20.885166873931887\n",
      "Training NF1:  41%|███████          | 4120/10001 [1:58:39<2:50:09,  1.74s/batch]Batch 4100/10001 Done, mean position loss: 20.79138729095459\n",
      "Training NF1:  42%|███████          | 4158/10001 [1:58:41<3:06:27,  1.91s/batch]Batch 4100/10001 Done, mean position loss: 21.138777828216554\n",
      "Training NF1:  41%|███████          | 4122/10001 [1:58:43<2:50:16,  1.74s/batch]Batch 4100/10001 Done, mean position loss: 20.39368484735489\n",
      "Training NF1:  42%|███████          | 4167/10001 [1:58:48<2:52:25,  1.77s/batch]Batch 4100/10001 Done, mean position loss: 20.822289083003998\n",
      "Training NF1:  41%|██████▉          | 4097/10001 [1:58:52<3:00:40,  1.84s/batch]Batch 4100/10001 Done, mean position loss: 20.494541060924533\n",
      "Training NF1:  41%|███████          | 4144/10001 [1:58:52<2:43:15,  1.67s/batch]Batch 4200/10001 Done, mean position loss: 20.557464578151702\n",
      "Training NF1:  42%|███████          | 4153/10001 [1:58:55<2:49:17,  1.74s/batch]Batch 4100/10001 Done, mean position loss: 21.567901124954226\n",
      "Training NF1:  41%|███████          | 4131/10001 [1:58:58<2:43:56,  1.68s/batch]Batch 4100/10001 Done, mean position loss: 21.128696293830874\n",
      "Training NF1:  41%|██████▉          | 4114/10001 [1:59:01<3:18:52,  2.03s/batch]Batch 4100/10001 Done, mean position loss: 21.032453904151915\n",
      "Training NF1:  41%|██████▉          | 4111/10001 [1:59:18<2:26:47,  1.50s/batch]Batch 4200/10001 Done, mean position loss: 20.542864677906035\n",
      "Training NF1:  42%|███████          | 4161/10001 [1:59:24<3:01:47,  1.87s/batch]Batch 4200/10001 Done, mean position loss: 20.647737340927122\n",
      "Training NF1:  41%|██████▉          | 4115/10001 [1:59:26<3:02:00,  1.86s/batch]Batch 4200/10001 Done, mean position loss: 20.501350927352906\n",
      "Training NF1:  42%|███████          | 4178/10001 [1:59:37<2:30:40,  1.55s/batch]Batch 4100/10001 Done, mean position loss: 20.407074558734895\n",
      "Training NF1:  42%|███████          | 4189/10001 [1:59:41<2:43:01,  1.68s/batch]Batch 4200/10001 Done, mean position loss: 20.527569701671602\n",
      "Training NF1:  42%|███████          | 4174/10001 [1:59:47<2:50:43,  1.76s/batch]Batch 4200/10001 Done, mean position loss: 21.061767933368685\n",
      "Training NF1:  41%|███████          | 4139/10001 [1:59:51<2:56:45,  1.81s/batch]Batch 4200/10001 Done, mean position loss: 21.094971899986266\n",
      "Training NF1:  42%|███████          | 4165/10001 [1:59:55<2:48:49,  1.74s/batch]Batch 4200/10001 Done, mean position loss: 20.57944918870926\n",
      "Training NF1:  42%|███████          | 4183/10001 [1:59:56<3:01:02,  1.87s/batch]Batch 4100/10001 Done, mean position loss: 20.6954514503479\n",
      "Training NF1:  41%|███████          | 4138/10001 [2:00:00<2:56:45,  1.81s/batch]Batch 4200/10001 Done, mean position loss: 20.801470103263856\n",
      "Training NF1:  41%|███████          | 4137/10001 [2:00:05<2:47:59,  1.72s/batch]Batch 4100/10001 Done, mean position loss: 20.73294226884842\n",
      "Training NF1:  41%|███████          | 4147/10001 [2:00:15<2:46:22,  1.71s/batch]Batch 4200/10001 Done, mean position loss: 20.6941845870018\n",
      "Training NF1:  41%|███████          | 4150/10001 [2:00:28<2:49:27,  1.74s/batch]Batch 4200/10001 Done, mean position loss: 20.735125334262847\n",
      "Training NF1:  42%|███████          | 4188/10001 [2:00:35<2:13:48,  1.38s/batch]Batch 4200/10001 Done, mean position loss: 20.942328145504\n",
      "Training NF1:  42%|███████          | 4189/10001 [2:00:35<2:40:39,  1.66s/batch]Batch 4200/10001 Done, mean position loss: 20.46099526166916\n",
      "Training NF1:  42%|███████          | 4178/10001 [2:00:36<3:31:30,  2.18s/batch]Batch 4200/10001 Done, mean position loss: 20.955734066963196\n",
      "Training NF1:  42%|███████▏         | 4206/10001 [2:00:43<2:44:02,  1.70s/batch]Batch 4200/10001 Done, mean position loss: 20.591654918193818\n",
      "Training NF1:  42%|███████▏         | 4220/10001 [2:00:49<3:02:00,  1.89s/batch]Batch 4200/10001 Done, mean position loss: 20.660770967006684\n",
      "Training NF1:  42%|███████▏         | 4213/10001 [2:00:56<2:47:26,  1.74s/batch]Batch 4200/10001 Done, mean position loss: 20.762447352409364\n",
      "Training NF1:  42%|███████          | 4191/10001 [2:00:58<2:39:16,  1.64s/batch]Batch 4200/10001 Done, mean position loss: 20.831782026290895\n",
      "Training NF1:  42%|███████▏         | 4200/10001 [2:00:59<2:30:29,  1.56s/batch]Batch 4200/10001 Done, mean position loss: 20.923231258392335\n",
      "Training NF1:  41%|███████          | 4137/10001 [2:01:01<3:05:24,  1.90s/batch]Batch 4200/10001 Done, mean position loss: 20.645963928699494\n",
      "Training NF1:  42%|███████▏         | 4227/10001 [2:01:01<2:44:36,  1.71s/batch]Batch 4200/10001 Done, mean position loss: 20.72630388498306\n",
      "Training NF1:  42%|███████          | 4175/10001 [2:01:04<3:18:33,  2.04s/batch]Batch 4200/10001 Done, mean position loss: 20.445396661758423\n",
      "Training NF1:  42%|███████▏         | 4202/10001 [2:01:07<3:28:10,  2.15s/batch]Batch 4200/10001 Done, mean position loss: 20.716493422985078\n",
      "Training NF1:  42%|███████          | 4187/10001 [2:01:13<2:50:15,  1.76s/batch]Batch 4200/10001 Done, mean position loss: 20.774895784854888\n",
      "Training NF1:  43%|███████▎         | 4266/10001 [2:01:13<2:38:14,  1.66s/batch]Batch 4200/10001 Done, mean position loss: 20.63048040151596\n",
      "Training NF1:  42%|███████          | 4181/10001 [2:01:17<2:45:59,  1.71s/batch]Batch 4200/10001 Done, mean position loss: 21.03239327430725\n",
      "Training NF1:  42%|███████▏         | 4203/10001 [2:01:20<2:24:53,  1.50s/batch]Batch 4200/10001 Done, mean position loss: 20.858370108604433\n",
      "Training NF1:  42%|███████▏         | 4209/10001 [2:01:27<2:45:55,  1.72s/batch]Batch 4200/10001 Done, mean position loss: 20.891736574172974\n",
      "Training NF1:  42%|███████          | 4155/10001 [2:01:30<2:25:00,  1.49s/batch]Batch 4200/10001 Done, mean position loss: 20.795248172283173\n",
      "Training NF1:  42%|███████▏         | 4210/10001 [2:01:32<2:57:54,  1.84s/batch]Batch 4200/10001 Done, mean position loss: 21.166181502342226\n",
      "Training NF1:  42%|███████▏         | 4198/10001 [2:01:37<3:10:12,  1.97s/batch]Batch 4200/10001 Done, mean position loss: 21.145976340770723\n",
      "Training NF1:  42%|███████▏         | 4215/10001 [2:01:38<2:59:40,  1.86s/batch]Batch 4200/10001 Done, mean position loss: 20.407053055763246\n",
      "Training NF1:  43%|███████▎         | 4298/10001 [2:01:43<2:53:15,  1.82s/batch]Batch 4200/10001 Done, mean position loss: 20.82016581535339\n",
      "Training NF1:  42%|███████          | 4155/10001 [2:01:49<2:38:03,  1.62s/batch]Batch 4300/10001 Done, mean position loss: 20.54059699058533\n",
      "Training NF1:  43%|███████▎         | 4274/10001 [2:01:49<2:44:19,  1.72s/batch]Batch 4200/10001 Done, mean position loss: 20.48398475408554\n",
      "Training NF1:  43%|███████▎         | 4272/10001 [2:01:53<2:13:36,  1.40s/batch]Batch 4200/10001 Done, mean position loss: 21.589704763889316\n",
      "Training NF1:  42%|███████▏         | 4243/10001 [2:01:57<2:46:27,  1.73s/batch]Batch 4200/10001 Done, mean position loss: 21.131709749698636\n",
      "Training NF1:  42%|███████▏         | 4231/10001 [2:02:00<2:55:58,  1.83s/batch]Batch 4200/10001 Done, mean position loss: 21.00789063692093\n",
      "Training NF1:  42%|███████▏         | 4226/10001 [2:02:11<2:47:31,  1.74s/batch]Batch 4300/10001 Done, mean position loss: 20.559185323715212\n",
      "Training NF1:  43%|███████▎         | 4304/10001 [2:02:16<2:26:54,  1.55s/batch]Batch 4300/10001 Done, mean position loss: 20.49535556316376\n",
      "Training NF1:  42%|███████▏         | 4245/10001 [2:02:19<3:16:53,  2.05s/batch]Batch 4300/10001 Done, mean position loss: 20.650012862682345\n",
      "Training NF1:  43%|███████▎         | 4328/10001 [2:02:37<2:50:22,  1.80s/batch]Batch 4200/10001 Done, mean position loss: 20.391597869396207\n",
      "Training NF1:  43%|███████▏         | 4256/10001 [2:02:38<2:48:30,  1.76s/batch]Batch 4300/10001 Done, mean position loss: 21.07277325630188\n",
      "Training NF1:  42%|███████▏         | 4247/10001 [2:02:39<3:04:09,  1.92s/batch]Batch 4300/10001 Done, mean position loss: 20.51688482046127\n",
      "Training NF1:  42%|███████▏         | 4230/10001 [2:02:45<3:14:17,  2.02s/batch]Batch 4300/10001 Done, mean position loss: 21.064717144966124\n",
      "Training NF1:  43%|███████▏         | 4262/10001 [2:02:47<3:06:41,  1.95s/batch]Batch 4300/10001 Done, mean position loss: 20.567009477615358\n",
      "Training NF1:  43%|███████▏         | 4256/10001 [2:02:54<3:11:56,  2.00s/batch]Batch 4300/10001 Done, mean position loss: 20.80279772758484\n",
      "Training NF1:  43%|███████▎         | 4275/10001 [2:02:56<3:06:17,  1.95s/batch]Batch 4200/10001 Done, mean position loss: 20.672086157798766\n",
      "Training NF1:  42%|███████▏         | 4246/10001 [2:03:10<3:11:39,  2.00s/batch]Batch 4300/10001 Done, mean position loss: 20.717306981086733\n",
      "Training NF1:  43%|███████▎         | 4338/10001 [2:03:17<2:34:28,  1.64s/batch]Batch 4200/10001 Done, mean position loss: 20.745252027511597\n",
      "Training NF1:  43%|███████▎         | 4284/10001 [2:03:29<3:19:24,  2.09s/batch]Batch 4300/10001 Done, mean position loss: 20.71959665775299\n",
      "Training NF1:  43%|███████▎         | 4328/10001 [2:03:29<2:50:32,  1.80s/batch]Batch 4300/10001 Done, mean position loss: 20.934514441490172\n",
      "Training NF1:  43%|███████▏         | 4265/10001 [2:03:30<2:56:28,  1.85s/batch]Batch 4300/10001 Done, mean position loss: 20.94336490869522\n",
      "Training NF1:  43%|███████▏         | 4254/10001 [2:03:37<2:54:10,  1.82s/batch]Batch 4300/10001 Done, mean position loss: 20.448280894756316\n",
      "Training NF1:  43%|███████▎         | 4270/10001 [2:03:39<2:55:21,  1.84s/batch]Batch 4300/10001 Done, mean position loss: 20.656968274116515\n",
      "Training NF1:  43%|███████▎         | 4301/10001 [2:03:39<2:31:45,  1.60s/batch]Batch 4300/10001 Done, mean position loss: 20.58807349920273\n",
      "Training NF1:  43%|███████▍         | 4343/10001 [2:03:49<2:16:34,  1.45s/batch]Batch 4300/10001 Done, mean position loss: 20.92715448617935\n",
      "Batch 4300/10001 Done, mean position loss: 20.635529708862304\n",
      "Training NF1:  44%|███████▍         | 4357/10001 [2:03:50<2:25:14,  1.54s/batch]Batch 4300/10001 Done, mean position loss: 20.820031778812407\n",
      "Training NF1:  43%|███████▎         | 4317/10001 [2:03:54<2:33:50,  1.62s/batch]Batch 4300/10001 Done, mean position loss: 20.719581186771393\n",
      "Training NF1:  43%|███████▎         | 4267/10001 [2:03:56<2:16:58,  1.43s/batch]Batch 4300/10001 Done, mean position loss: 20.767880036830903\n",
      "Training NF1:  44%|███████▍         | 4351/10001 [2:04:02<2:40:55,  1.71s/batch]Batch 4300/10001 Done, mean position loss: 20.41616153240204\n",
      "Training NF1:  43%|███████▎         | 4273/10001 [2:04:04<3:16:10,  2.05s/batch]Batch 4300/10001 Done, mean position loss: 20.72244711637497\n",
      "Training NF1:  43%|███████▎         | 4310/10001 [2:04:04<2:40:35,  1.69s/batch]Batch 4300/10001 Done, mean position loss: 20.626687436103822\n",
      "Training NF1:  43%|███████▎         | 4326/10001 [2:04:12<2:40:22,  1.70s/batch]Batch 4300/10001 Done, mean position loss: 21.06534249305725\n",
      "Training NF1:  44%|███████▍         | 4371/10001 [2:04:15<2:46:08,  1.77s/batch]Batch 4300/10001 Done, mean position loss: 20.79076335906982\n",
      "Training NF1:  43%|███████▎         | 4321/10001 [2:04:16<2:34:43,  1.63s/batch]Batch 4300/10001 Done, mean position loss: 20.85055879831314\n",
      "Training NF1:  44%|███████▍         | 4358/10001 [2:04:23<2:39:31,  1.70s/batch]Batch 4300/10001 Done, mean position loss: 20.88287254333496\n",
      "Training NF1:  43%|███████▎         | 4334/10001 [2:04:28<2:34:24,  1.63s/batch]Batch 4300/10001 Done, mean position loss: 21.167330467700957\n",
      "Training NF1:  44%|███████▍         | 4362/10001 [2:04:30<2:47:11,  1.78s/batch]Batch 4300/10001 Done, mean position loss: 20.800205724239348\n",
      "Training NF1:  43%|███████▎         | 4286/10001 [2:04:31<2:48:15,  1.77s/batch]Batch 4300/10001 Done, mean position loss: 20.41809162139893\n",
      "Training NF1:  43%|███████▎         | 4326/10001 [2:04:38<2:12:08,  1.40s/batch]Batch 4300/10001 Done, mean position loss: 21.183938131332397\n",
      "Training NF1:  43%|███████▎         | 4311/10001 [2:04:44<2:36:15,  1.65s/batch]Batch 4400/10001 Done, mean position loss: 20.53987756729126\n",
      "Training NF1:  43%|███████▎         | 4296/10001 [2:04:48<2:40:24,  1.69s/batch]Batch 4300/10001 Done, mean position loss: 20.80267181158066\n",
      "Training NF1:  43%|███████▎         | 4267/10001 [2:04:52<3:04:24,  1.93s/batch]Batch 4300/10001 Done, mean position loss: 21.57355398416519\n",
      "Training NF1:  44%|███████▍         | 4392/10001 [2:04:52<2:40:03,  1.71s/batch]Batch 4300/10001 Done, mean position loss: 20.487178409099577\n",
      "Training NF1:  44%|███████▍         | 4353/10001 [2:04:57<2:21:28,  1.50s/batch]Batch 4300/10001 Done, mean position loss: 21.12531750679016\n",
      "Training NF1:  43%|███████▏         | 4259/10001 [2:04:58<2:36:26,  1.63s/batch]Batch 4300/10001 Done, mean position loss: 20.994507648944854\n",
      "Training NF1:  43%|███████▎         | 4317/10001 [2:05:02<2:17:36,  1.45s/batch]Batch 4400/10001 Done, mean position loss: 20.55670184373856\n",
      "Training NF1:  44%|███████▍         | 4357/10001 [2:05:05<2:33:56,  1.64s/batch]Batch 4400/10001 Done, mean position loss: 20.640581626892093\n",
      "Training NF1:  44%|███████▍         | 4352/10001 [2:05:05<2:07:46,  1.36s/batch]Batch 4400/10001 Done, mean position loss: 20.50576417207718\n",
      "Training NF1:  43%|███████▍         | 4347/10001 [2:05:24<2:10:31,  1.39s/batch]Batch 4300/10001 Done, mean position loss: 20.39667659044266\n",
      "Training NF1:  44%|███████▍         | 4386/10001 [2:05:25<2:48:33,  1.80s/batch]Batch 4400/10001 Done, mean position loss: 21.05401645421982\n",
      "Training NF1:  44%|███████▍         | 4354/10001 [2:05:35<2:39:30,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 21.092730424404145\n",
      "Training NF1:  44%|███████▍         | 4355/10001 [2:05:37<2:54:35,  1.86s/batch]Batch 4400/10001 Done, mean position loss: 20.52372022151947\n",
      "Training NF1:  44%|███████▍         | 4411/10001 [2:05:41<2:17:45,  1.48s/batch]Batch 4400/10001 Done, mean position loss: 20.56352190732956\n",
      "Training NF1:  43%|███████▎         | 4316/10001 [2:05:50<2:40:41,  1.70s/batch]Batch 4400/10001 Done, mean position loss: 20.801816127300263\n",
      "Training NF1:  44%|███████▍         | 4398/10001 [2:05:52<2:24:34,  1.55s/batch]Batch 4300/10001 Done, mean position loss: 20.661274178028105\n",
      "Training NF1:  44%|███████▌         | 4436/10001 [2:05:57<2:23:11,  1.54s/batch]Batch 4400/10001 Done, mean position loss: 20.69688756465912\n",
      "Training NF1:  44%|███████▍         | 4395/10001 [2:06:09<2:57:19,  1.90s/batch]Batch 4300/10001 Done, mean position loss: 20.754048190116883\n",
      "Training NF1:  44%|███████▍         | 4387/10001 [2:06:14<2:25:26,  1.55s/batch]Batch 4400/10001 Done, mean position loss: 20.950320205688477\n",
      "Training NF1:  44%|███████▌         | 4418/10001 [2:06:17<2:30:59,  1.62s/batch]Batch 4400/10001 Done, mean position loss: 20.919065885543823\n",
      "Training NF1:  44%|███████▍         | 4390/10001 [2:06:22<3:31:05,  2.26s/batch]Batch 4400/10001 Done, mean position loss: 20.71907878637314\n",
      "Training NF1:  44%|███████▍         | 4383/10001 [2:06:25<2:39:27,  1.70s/batch]Batch 4400/10001 Done, mean position loss: 20.45241828918457\n",
      "Training NF1:  44%|███████▍         | 4396/10001 [2:06:34<3:01:52,  1.95s/batch]Batch 4400/10001 Done, mean position loss: 20.66339294433594\n",
      "Training NF1:  44%|███████▌         | 4438/10001 [2:06:39<2:51:57,  1.85s/batch]Batch 4400/10001 Done, mean position loss: 20.93622434616089\n",
      "Training NF1:  45%|███████▌         | 4459/10001 [2:06:39<2:46:36,  1.80s/batch]Batch 4400/10001 Done, mean position loss: 20.8204003739357\n",
      "Training NF1:  44%|███████▍         | 4412/10001 [2:06:41<2:13:43,  1.44s/batch]Batch 4400/10001 Done, mean position loss: 20.646844770908356\n",
      "Training NF1:  44%|███████▍         | 4369/10001 [2:06:44<2:52:49,  1.84s/batch]Batch 4400/10001 Done, mean position loss: 20.722812960147856\n",
      "Training NF1:  44%|███████▍         | 4365/10001 [2:06:45<2:29:19,  1.59s/batch]Batch 4400/10001 Done, mean position loss: 20.583747515678404\n",
      "Training NF1:  44%|███████▍         | 4381/10001 [2:06:48<2:53:32,  1.85s/batch]Batch 4400/10001 Done, mean position loss: 20.764281723499295\n",
      "Training NF1:  44%|███████▌         | 4421/10001 [2:06:57<2:41:11,  1.73s/batch]Batch 4400/10001 Done, mean position loss: 20.623022069931032\n",
      "Training NF1:  44%|███████▍         | 4384/10001 [2:06:57<3:09:05,  2.02s/batch]Batch 4400/10001 Done, mean position loss: 20.41087882757187\n",
      "Training NF1:  44%|███████▍         | 4398/10001 [2:06:59<2:48:19,  1.80s/batch]Batch 4400/10001 Done, mean position loss: 20.73575250148773\n",
      "Training NF1:  44%|███████▍         | 4403/10001 [2:07:01<2:43:50,  1.76s/batch]Batch 4400/10001 Done, mean position loss: 21.01788078069687\n",
      "Training NF1:  44%|███████▍         | 4387/10001 [2:07:05<2:38:19,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 20.84698978662491\n",
      "Training NF1:  44%|███████▍         | 4406/10001 [2:07:07<2:28:20,  1.59s/batch]Batch 4400/10001 Done, mean position loss: 20.79622687578201\n",
      "Training NF1:  44%|███████▌         | 4425/10001 [2:07:20<2:39:14,  1.71s/batch]Batch 4400/10001 Done, mean position loss: 20.409242978096007\n",
      "Training NF1:  44%|███████▍         | 4352/10001 [2:07:24<2:37:14,  1.67s/batch]Batch 4400/10001 Done, mean position loss: 21.16844272136688\n",
      "Training NF1:  44%|███████▍         | 4393/10001 [2:07:24<2:21:24,  1.51s/batch]Batch 4400/10001 Done, mean position loss: 20.800521130561826\n",
      "Training NF1:  45%|███████▌         | 4455/10001 [2:07:25<2:36:13,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 20.88888899087906\n",
      "Training NF1:  45%|███████▌         | 4467/10001 [2:07:28<2:10:24,  1.41s/batch]Batch 4400/10001 Done, mean position loss: 21.15236358165741\n",
      "Training NF1:  44%|███████▍         | 4387/10001 [2:07:29<2:14:54,  1.44s/batch]Batch 4500/10001 Done, mean position loss: 20.53722714662552\n",
      "Training NF1:  44%|███████▍         | 4396/10001 [2:07:38<2:33:58,  1.65s/batch]Batch 4400/10001 Done, mean position loss: 20.79876146793365\n",
      "Training NF1:  44%|███████▌         | 4446/10001 [2:07:40<2:51:39,  1.85s/batch]Batch 4400/10001 Done, mean position loss: 20.487255313396453\n",
      "Training NF1:  44%|███████▌         | 4447/10001 [2:07:43<3:10:44,  2.06s/batch]Batch 4400/10001 Done, mean position loss: 21.552477593421937\n",
      "Training NF1:  44%|███████▌         | 4429/10001 [2:07:48<3:01:30,  1.95s/batch]Batch 4400/10001 Done, mean position loss: 21.11225928544998\n",
      "Batch 4500/10001 Done, mean position loss: 20.495647101402284\n",
      "Training NF1:  44%|███████▍         | 4403/10001 [2:07:51<2:36:29,  1.68s/batch]Batch 4500/10001 Done, mean position loss: 20.559007310867308\n",
      "Training NF1:  44%|███████▌         | 4446/10001 [2:07:51<2:22:07,  1.54s/batch]Batch 4500/10001 Done, mean position loss: 20.65472693681717\n",
      "Training NF1:  44%|███████▌         | 4432/10001 [2:07:54<2:51:56,  1.85s/batch]Batch 4400/10001 Done, mean position loss: 21.00453439712524\n",
      "Training NF1:  45%|███████▋         | 4487/10001 [2:08:18<2:24:24,  1.57s/batch]Batch 4500/10001 Done, mean position loss: 21.090228576660156\n",
      "Training NF1:  44%|███████▍         | 4385/10001 [2:08:20<2:43:53,  1.75s/batch]Batch 4400/10001 Done, mean position loss: 20.396462416648866\n",
      "Training NF1:  45%|███████▌         | 4470/10001 [2:08:27<2:35:53,  1.69s/batch]Batch 4500/10001 Done, mean position loss: 21.062249529361726\n",
      "Training NF1:  44%|███████▌         | 4439/10001 [2:08:28<3:10:32,  2.06s/batch]Batch 4500/10001 Done, mean position loss: 20.531476838588713\n",
      "Training NF1:  45%|███████▋         | 4541/10001 [2:08:36<2:30:50,  1.66s/batch]Batch 4500/10001 Done, mean position loss: 20.581265120506284\n",
      "Training NF1:  44%|███████▌         | 4446/10001 [2:08:44<3:41:39,  2.39s/batch]Batch 4500/10001 Done, mean position loss: 20.796914987564087\n",
      "Training NF1:  45%|███████▌         | 4485/10001 [2:08:46<2:12:38,  1.44s/batch]Batch 4500/10001 Done, mean position loss: 20.69404427528381\n",
      "Training NF1:  45%|███████▋         | 4512/10001 [2:08:47<2:41:35,  1.77s/batch]Batch 4400/10001 Done, mean position loss: 20.671165568828584\n",
      "Training NF1:  45%|███████▌         | 4460/10001 [2:09:05<2:04:45,  1.35s/batch]Batch 4500/10001 Done, mean position loss: 20.94292193174362\n",
      "Training NF1:  45%|███████▌         | 4455/10001 [2:09:12<2:30:31,  1.63s/batch]Batch 4500/10001 Done, mean position loss: 20.93097857713699\n",
      "Training NF1:  45%|███████▋         | 4486/10001 [2:09:15<2:56:22,  1.92s/batch]Batch 4500/10001 Done, mean position loss: 20.71596801519394\n",
      "Training NF1:  45%|███████▋         | 4527/10001 [2:09:14<2:51:05,  1.88s/batch]Batch 4400/10001 Done, mean position loss: 20.741431016921997\n",
      "Training NF1:  44%|███████▌         | 4435/10001 [2:09:20<3:00:51,  1.95s/batch]Batch 4500/10001 Done, mean position loss: 20.462561388015747\n",
      "Training NF1:  45%|███████▋         | 4520/10001 [2:09:20<2:45:55,  1.82s/batch]Batch 4500/10001 Done, mean position loss: 20.647643399238586\n",
      "Training NF1:  45%|███████▋         | 4503/10001 [2:09:23<2:26:33,  1.60s/batch]Batch 4500/10001 Done, mean position loss: 20.836787576675412\n",
      "Training NF1:  46%|███████▋         | 4558/10001 [2:09:29<2:27:47,  1.63s/batch]Batch 4500/10001 Done, mean position loss: 20.635142171382903\n",
      "Training NF1:  46%|███████▊         | 4574/10001 [2:09:33<2:25:22,  1.61s/batch]Batch 4500/10001 Done, mean position loss: 20.576554379463197\n",
      "Training NF1:  45%|███████▌         | 4460/10001 [2:09:34<2:36:49,  1.70s/batch]Batch 4500/10001 Done, mean position loss: 20.907540974617007\n",
      "Training NF1:  45%|███████▌         | 4482/10001 [2:09:39<2:12:58,  1.45s/batch]Batch 4500/10001 Done, mean position loss: 20.751515119075776\n",
      "Training NF1:  45%|███████▋         | 4542/10001 [2:09:42<2:35:27,  1.71s/batch]Batch 4500/10001 Done, mean position loss: 20.72069430112839\n",
      "Training NF1:  46%|███████▊         | 4568/10001 [2:09:44<2:10:24,  1.44s/batch]Batch 4500/10001 Done, mean position loss: 20.624152112007142\n",
      "Training NF1:  45%|███████▋         | 4489/10001 [2:09:48<2:44:28,  1.79s/batch]Batch 4500/10001 Done, mean position loss: 20.41991268634796\n",
      "Training NF1:  45%|███████▋         | 4506/10001 [2:09:53<2:18:15,  1.51s/batch]Batch 4500/10001 Done, mean position loss: 20.72443063259125\n",
      "Training NF1:  45%|███████▋         | 4528/10001 [2:09:56<2:25:10,  1.59s/batch]Batch 4500/10001 Done, mean position loss: 21.02527004003525\n",
      "Training NF1:  46%|███████▊         | 4561/10001 [2:09:59<2:31:44,  1.67s/batch]Batch 4500/10001 Done, mean position loss: 20.808868680000305\n",
      "Batch 4500/10001 Done, mean position loss: 20.77108705997467\n",
      "Training NF1:  45%|███████▋         | 4517/10001 [2:10:09<2:27:04,  1.61s/batch]Batch 4500/10001 Done, mean position loss: 20.404717247486115\n",
      "Training NF1:  46%|███████▋         | 4559/10001 [2:10:11<2:47:38,  1.85s/batch]Batch 4500/10001 Done, mean position loss: 21.176061472892762\n",
      "Training NF1:  45%|███████▌         | 4483/10001 [2:10:13<2:41:48,  1.76s/batch]Batch 4500/10001 Done, mean position loss: 20.886320190429686\n",
      "Training NF1:  45%|███████▋         | 4499/10001 [2:10:15<2:40:42,  1.75s/batch]Batch 4600/10001 Done, mean position loss: 20.51445267677307\n",
      "Training NF1:  45%|███████▋         | 4524/10001 [2:10:19<1:58:40,  1.30s/batch]Batch 4500/10001 Done, mean position loss: 20.79773978948593\n",
      "Training NF1:  45%|███████▋         | 4507/10001 [2:10:22<2:30:49,  1.65s/batch]Batch 4500/10001 Done, mean position loss: 21.127392833232882\n",
      "Training NF1:  46%|███████▊         | 4562/10001 [2:10:28<2:42:53,  1.80s/batch]Batch 4500/10001 Done, mean position loss: 20.797655901908875\n",
      "Training NF1:  46%|███████▋         | 4559/10001 [2:10:28<2:20:26,  1.55s/batch]Batch 4600/10001 Done, mean position loss: 20.48546585559845\n",
      "Training NF1:  45%|███████▋         | 4526/10001 [2:10:30<2:38:47,  1.74s/batch]Batch 4500/10001 Done, mean position loss: 21.59821338415146\n",
      "Training NF1:  46%|███████▊         | 4600/10001 [2:10:35<2:14:29,  1.49s/batch]Batch 4500/10001 Done, mean position loss: 21.100436449050903\n",
      "Training NF1:  45%|███████▋         | 4540/10001 [2:10:37<2:35:13,  1.71s/batch]Batch 4600/10001 Done, mean position loss: 20.654583809375765\n",
      "Training NF1:  45%|███████▋         | 4504/10001 [2:10:39<2:05:11,  1.37s/batch]Batch 4500/10001 Done, mean position loss: 20.487877552509307\n",
      "Training NF1:  45%|███████▋         | 4550/10001 [2:10:44<2:45:33,  1.82s/batch]Batch 4600/10001 Done, mean position loss: 20.559035468101502\n",
      "Training NF1:  46%|███████▊         | 4578/10001 [2:10:44<2:16:04,  1.51s/batch]Batch 4500/10001 Done, mean position loss: 20.99219581365585\n",
      "Training NF1:  46%|███████▊         | 4580/10001 [2:11:05<2:51:34,  1.90s/batch]Batch 4600/10001 Done, mean position loss: 21.05900436878204\n",
      "Training NF1:  46%|███████▊         | 4569/10001 [2:11:08<2:41:17,  1.78s/batch]Batch 4500/10001 Done, mean position loss: 20.390302076339722\n",
      "Training NF1:  45%|███████▋         | 4533/10001 [2:11:17<2:35:25,  1.71s/batch]Batch 4600/10001 Done, mean position loss: 21.083739194869995\n",
      "Training NF1:  46%|███████▊         | 4603/10001 [2:11:20<2:24:43,  1.61s/batch]Batch 4600/10001 Done, mean position loss: 20.57623254060745\n",
      "Training NF1:  45%|███████▋         | 4511/10001 [2:11:24<2:27:13,  1.61s/batch]Batch 4600/10001 Done, mean position loss: 20.50927091121674\n",
      "Training NF1:  47%|███████▉         | 4653/10001 [2:11:40<2:04:00,  1.39s/batch]Batch 4500/10001 Done, mean position loss: 20.6617729640007\n",
      "Training NF1:  46%|███████▊         | 4564/10001 [2:11:40<2:24:59,  1.60s/batch]Batch 4600/10001 Done, mean position loss: 20.800708796977997\n",
      "Training NF1:  46%|███████▊         | 4611/10001 [2:11:41<2:28:09,  1.65s/batch]Batch 4600/10001 Done, mean position loss: 20.689096257686614\n",
      "Training NF1:  46%|███████▊         | 4560/10001 [2:11:58<2:15:08,  1.49s/batch]Batch 4600/10001 Done, mean position loss: 20.706599075794223\n",
      "Training NF1:  46%|███████▊         | 4614/10001 [2:11:59<2:06:55,  1.41s/batch]Batch 4600/10001 Done, mean position loss: 20.92522489786148\n",
      "Training NF1:  46%|███████▊         | 4629/10001 [2:12:01<2:18:38,  1.55s/batch]Batch 4600/10001 Done, mean position loss: 20.92886174917221\n",
      "Training NF1:  46%|███████▊         | 4565/10001 [2:12:07<2:35:48,  1.72s/batch]Batch 4500/10001 Done, mean position loss: 20.741354000568393\n",
      "Training NF1:  46%|███████▊         | 4588/10001 [2:12:09<2:31:28,  1.68s/batch]Batch 4600/10001 Done, mean position loss: 20.66880915403366\n",
      "Training NF1:  46%|███████▊         | 4627/10001 [2:12:09<2:19:06,  1.55s/batch]Batch 4600/10001 Done, mean position loss: 20.45494318008423\n",
      "Training NF1:  47%|███████▉         | 4672/10001 [2:12:12<2:55:05,  1.97s/batch]Batch 4600/10001 Done, mean position loss: 20.83880928993225\n",
      "Training NF1:  46%|███████▊         | 4607/10001 [2:12:17<2:13:45,  1.49s/batch]Batch 4600/10001 Done, mean position loss: 20.914691326618197\n",
      "Training NF1:  46%|███████▊         | 4612/10001 [2:12:19<3:01:56,  2.03s/batch]Batch 4600/10001 Done, mean position loss: 20.631306190490722\n",
      "Training NF1:  45%|███████▋         | 4511/10001 [2:12:25<2:45:22,  1.81s/batch]Batch 4600/10001 Done, mean position loss: 20.58505032300949\n",
      "Training NF1:  46%|███████▊         | 4575/10001 [2:12:25<2:49:22,  1.87s/batch]Batch 4600/10001 Done, mean position loss: 20.62874900817871\n",
      "Training NF1:  46%|███████▊         | 4614/10001 [2:12:28<2:21:02,  1.57s/batch]Batch 4600/10001 Done, mean position loss: 20.769001410007476\n",
      "Training NF1:  46%|███████▊         | 4570/10001 [2:12:30<2:23:04,  1.58s/batch]Batch 4600/10001 Done, mean position loss: 20.42112692594528\n",
      "Batch 4600/10001 Done, mean position loss: 20.691711852550505\n",
      "Training NF1:  46%|███████▊         | 4577/10001 [2:12:42<2:33:04,  1.69s/batch]Batch 4600/10001 Done, mean position loss: 21.02332022190094\n",
      "Training NF1:  46%|███████▊         | 4586/10001 [2:12:42<2:05:12,  1.39s/batch]Batch 4600/10001 Done, mean position loss: 20.735931463241577\n",
      "Training NF1:  45%|███████▋         | 4525/10001 [2:12:50<2:32:26,  1.67s/batch]Batch 4600/10001 Done, mean position loss: 20.81263745546341\n",
      "Training NF1:  47%|███████▉         | 4681/10001 [2:12:56<2:59:04,  2.02s/batch]Batch 4600/10001 Done, mean position loss: 20.771313109397887\n",
      "Training NF1:  47%|███████▉         | 4660/10001 [2:13:05<3:08:34,  2.12s/batch]Batch 4600/10001 Done, mean position loss: 20.87265667438507\n",
      "Training NF1:  45%|███████▋         | 4547/10001 [2:13:06<3:22:42,  2.23s/batch]Batch 4700/10001 Done, mean position loss: 20.519871129989625\n",
      "Training NF1:  45%|███████▋         | 4533/10001 [2:13:09<3:16:56,  2.16s/batch]Batch 4600/10001 Done, mean position loss: 20.79155495405197\n",
      "Training NF1:  47%|███████▉         | 4663/10001 [2:13:10<2:37:58,  1.78s/batch]Batch 4600/10001 Done, mean position loss: 20.4015993642807\n",
      "Training NF1:  46%|███████▉         | 4647/10001 [2:13:15<2:46:08,  1.86s/batch]Batch 4600/10001 Done, mean position loss: 21.15111046791077\n",
      "Training NF1:  46%|███████▊         | 4629/10001 [2:13:16<2:25:49,  1.63s/batch]Batch 4600/10001 Done, mean position loss: 21.154506301879884\n",
      "Training NF1:  46%|███████▉         | 4641/10001 [2:13:22<3:20:44,  2.25s/batch]Batch 4700/10001 Done, mean position loss: 20.49208458185196\n",
      "Training NF1:  46%|███████▊         | 4629/10001 [2:13:23<2:49:08,  1.89s/batch]Batch 4600/10001 Done, mean position loss: 20.78634640932083\n",
      "Training NF1:  46%|███████▉         | 4647/10001 [2:13:24<2:58:08,  2.00s/batch]Batch 4600/10001 Done, mean position loss: 21.117883551120755\n",
      "Training NF1:  46%|███████▋         | 4555/10001 [2:13:25<3:21:32,  2.22s/batch]Batch 4600/10001 Done, mean position loss: 20.48956278562546\n",
      "Training NF1:  46%|███████▉         | 4633/10001 [2:13:28<2:10:28,  1.46s/batch]Batch 4600/10001 Done, mean position loss: 21.572568330764774\n",
      "Training NF1:  46%|███████▊         | 4629/10001 [2:13:35<2:28:44,  1.66s/batch]Batch 4700/10001 Done, mean position loss: 20.638946559429165\n",
      "Training NF1:  47%|███████▉         | 4678/10001 [2:13:38<2:39:18,  1.80s/batch]Batch 4700/10001 Done, mean position loss: 20.545685613155364\n",
      "Training NF1:  47%|███████▉         | 4652/10001 [2:13:42<2:51:36,  1.92s/batch]Batch 4600/10001 Done, mean position loss: 20.993717563152313\n",
      "Training NF1:  47%|███████▉         | 4653/10001 [2:14:02<2:51:00,  1.92s/batch]Batch 4700/10001 Done, mean position loss: 21.01743449449539\n",
      "Training NF1:  47%|███████▉         | 4652/10001 [2:14:01<2:51:49,  1.93s/batch]Batch 4600/10001 Done, mean position loss: 20.41304501771927\n",
      "Training NF1:  46%|███████▊         | 4625/10001 [2:14:09<2:31:06,  1.69s/batch]Batch 4700/10001 Done, mean position loss: 21.087647030353548\n",
      "Training NF1:  46%|███████▊         | 4572/10001 [2:14:17<2:18:57,  1.54s/batch]Batch 4700/10001 Done, mean position loss: 20.515169682502744\n",
      "Training NF1:  46%|███████▉         | 4650/10001 [2:14:18<2:39:08,  1.78s/batch]Batch 4700/10001 Done, mean position loss: 20.56797650814056\n",
      "Training NF1:  47%|███████▉         | 4692/10001 [2:14:34<2:22:05,  1.61s/batch]Batch 4700/10001 Done, mean position loss: 20.797347707748415\n",
      "Training NF1:  47%|███████▉         | 4673/10001 [2:14:37<2:38:51,  1.79s/batch]Batch 4700/10001 Done, mean position loss: 20.684673931598663\n",
      "Training NF1:  46%|███████▊         | 4591/10001 [2:14:50<2:49:02,  1.87s/batch]Batch 4700/10001 Done, mean position loss: 20.915760989189145\n",
      "Training NF1:  46%|███████▊         | 4592/10001 [2:14:51<2:44:56,  1.83s/batch]Batch 4600/10001 Done, mean position loss: 20.667792201042175\n",
      "Training NF1:  47%|███████▉         | 4685/10001 [2:14:59<2:42:35,  1.84s/batch]Batch 4700/10001 Done, mean position loss: 20.70131165266037\n",
      "Training NF1:  47%|███████▉         | 4667/10001 [2:15:01<2:34:34,  1.74s/batch]Batch 4700/10001 Done, mean position loss: 20.931388919353484\n",
      "Training NF1:  47%|███████▉         | 4674/10001 [2:15:03<2:34:12,  1.74s/batch]Batch 4700/10001 Done, mean position loss: 20.663887672424316\n",
      "Training NF1:  47%|███████▉         | 4696/10001 [2:15:08<2:58:59,  2.02s/batch]Batch 4600/10001 Done, mean position loss: 20.729491927623748\n",
      "Training NF1:  47%|███████▉         | 4679/10001 [2:15:13<2:41:08,  1.82s/batch]Batch 4700/10001 Done, mean position loss: 20.444448635578155\n",
      "Training NF1:  47%|████████         | 4745/10001 [2:15:14<2:25:29,  1.66s/batch]Batch 4700/10001 Done, mean position loss: 20.906794073581693\n",
      "Training NF1:  47%|███████▉         | 4695/10001 [2:15:15<2:40:25,  1.81s/batch]Batch 4700/10001 Done, mean position loss: 20.830045204162595\n",
      "Training NF1:  47%|████████         | 4725/10001 [2:15:18<2:25:04,  1.65s/batch]Batch 4700/10001 Done, mean position loss: 20.61518098592758\n",
      "Training NF1:  47%|████████         | 4740/10001 [2:15:27<2:47:49,  1.91s/batch]Batch 4700/10001 Done, mean position loss: 20.679119935035708\n",
      "Batch 4700/10001 Done, mean position loss: 20.616949076652528\n",
      "Training NF1:  47%|███████▉         | 4675/10001 [2:15:28<2:32:33,  1.72s/batch]Batch 4700/10001 Done, mean position loss: 20.57415900707245\n",
      "Training NF1:  47%|████████         | 4707/10001 [2:15:29<2:38:20,  1.79s/batch]Batch 4700/10001 Done, mean position loss: 20.75153816461563\n",
      "Training NF1:  47%|███████▉         | 4691/10001 [2:15:32<2:37:31,  1.78s/batch]Batch 4700/10001 Done, mean position loss: 20.407310721874236\n",
      "Training NF1:  47%|███████▉         | 4698/10001 [2:15:45<2:48:15,  1.90s/batch]Batch 4700/10001 Done, mean position loss: 21.042185420989995\n",
      "Training NF1:  47%|████████         | 4718/10001 [2:15:46<2:37:08,  1.78s/batch]Batch 4700/10001 Done, mean position loss: 20.775136990547182\n",
      "Training NF1:  47%|████████         | 4727/10001 [2:15:49<2:41:41,  1.84s/batch]Batch 4700/10001 Done, mean position loss: 20.718835637569427\n",
      "Training NF1:  47%|███████▉         | 4664/10001 [2:15:50<2:17:01,  1.54s/batch]Batch 4700/10001 Done, mean position loss: 20.81109657049179\n",
      "Training NF1:  47%|████████         | 4722/10001 [2:16:02<2:24:32,  1.64s/batch]Batch 4700/10001 Done, mean position loss: 20.874389560222625\n",
      "Training NF1:  47%|███████▉         | 4693/10001 [2:16:02<2:40:14,  1.81s/batch]Batch 4800/10001 Done, mean position loss: 20.540098433494567\n",
      "Training NF1:  48%|████████         | 4768/10001 [2:16:04<2:20:42,  1.61s/batch]Batch 4700/10001 Done, mean position loss: 20.803959023952483\n",
      "Training NF1:  47%|████████         | 4727/10001 [2:16:09<2:19:24,  1.59s/batch]Batch 4700/10001 Done, mean position loss: 20.40847739934921\n",
      "Training NF1:  48%|████████         | 4756/10001 [2:16:09<2:39:23,  1.82s/batch]Batch 4700/10001 Done, mean position loss: 21.15089182138443\n",
      "Training NF1:  48%|████████▏        | 4780/10001 [2:16:15<2:21:55,  1.63s/batch]Batch 4700/10001 Done, mean position loss: 20.48814307689667\n",
      "Training NF1:  47%|████████         | 4720/10001 [2:16:18<2:44:07,  1.86s/batch]Batch 4700/10001 Done, mean position loss: 21.122987756729124\n",
      "Training NF1:  48%|████████▏        | 4797/10001 [2:16:20<2:47:07,  1.93s/batch]Batch 4700/10001 Done, mean position loss: 20.783145160675048\n",
      "Training NF1:  47%|████████         | 4743/10001 [2:16:20<3:06:34,  2.13s/batch]Batch 4700/10001 Done, mean position loss: 21.119197218418122\n",
      "Training NF1:  48%|████████▏        | 4812/10001 [2:16:21<2:38:13,  1.83s/batch]Batch 4700/10001 Done, mean position loss: 21.5956080698967\n",
      "Training NF1:  47%|████████         | 4737/10001 [2:16:21<2:38:20,  1.80s/batch]Batch 4800/10001 Done, mean position loss: 20.497481360435486\n",
      "Training NF1:  48%|████████         | 4776/10001 [2:16:27<2:25:01,  1.67s/batch]Batch 4800/10001 Done, mean position loss: 20.614159753322603\n",
      "Training NF1:  47%|████████         | 4713/10001 [2:16:40<2:37:26,  1.79s/batch]Batch 4800/10001 Done, mean position loss: 20.544598033428194\n",
      "Training NF1:  47%|████████         | 4715/10001 [2:16:45<2:15:21,  1.54s/batch]Batch 4700/10001 Done, mean position loss: 20.97848562002182\n",
      "Training NF1:  47%|███████▉         | 4659/10001 [2:16:51<2:29:21,  1.68s/batch]Batch 4800/10001 Done, mean position loss: 21.038465728759768\n",
      "Training NF1:  47%|███████▉         | 4663/10001 [2:16:57<2:35:13,  1.74s/batch]Batch 4700/10001 Done, mean position loss: 20.39165344238281\n",
      "Training NF1:  47%|███████▉         | 4677/10001 [2:17:05<2:46:29,  1.88s/batch]Batch 4800/10001 Done, mean position loss: 21.04744167804718\n",
      "Training NF1:  47%|████████         | 4748/10001 [2:17:11<2:32:44,  1.74s/batch]Batch 4800/10001 Done, mean position loss: 20.570065891742708\n",
      "Training NF1:  48%|████████▏        | 4816/10001 [2:17:14<2:26:56,  1.70s/batch]Batch 4800/10001 Done, mean position loss: 20.497686815261844\n",
      "Training NF1:  48%|████████▏        | 4794/10001 [2:17:27<2:22:36,  1.64s/batch]Batch 4800/10001 Done, mean position loss: 20.6826145195961\n",
      "Training NF1:  47%|████████         | 4741/10001 [2:17:30<2:20:15,  1.60s/batch]Batch 4800/10001 Done, mean position loss: 20.77961417913437\n",
      "Training NF1:  48%|████████         | 4754/10001 [2:17:41<2:41:04,  1.84s/batch]Batch 4800/10001 Done, mean position loss: 20.93231711149216\n",
      "Training NF1:  48%|████████▏        | 4823/10001 [2:17:52<2:29:18,  1.73s/batch]Batch 4700/10001 Done, mean position loss: 20.6560676074028\n",
      "Training NF1:  48%|████████         | 4760/10001 [2:17:53<2:13:35,  1.53s/batch]Batch 4800/10001 Done, mean position loss: 20.945048191547393\n",
      "Training NF1:  48%|████████▏        | 4796/10001 [2:17:55<2:25:07,  1.67s/batch]Batch 4800/10001 Done, mean position loss: 20.71514673471451\n",
      "Training NF1:  48%|████████▏        | 4802/10001 [2:17:56<2:01:37,  1.40s/batch]Batch 4800/10001 Done, mean position loss: 20.656766674518586\n",
      "Training NF1:  49%|████████▎        | 4858/10001 [2:18:04<2:41:26,  1.88s/batch]Batch 4800/10001 Done, mean position loss: 20.609349091053012\n",
      "Training NF1:  48%|████████▏        | 4821/10001 [2:18:06<2:14:58,  1.56s/batch]Batch 4700/10001 Done, mean position loss: 20.7288048863411\n",
      "Training NF1:  48%|████████▏        | 4782/10001 [2:18:09<2:40:33,  1.85s/batch]Batch 4800/10001 Done, mean position loss: 20.828450417518617\n",
      "Training NF1:  48%|████████▏        | 4824/10001 [2:18:11<2:17:22,  1.59s/batch]Batch 4800/10001 Done, mean position loss: 20.444531292915343\n",
      "Training NF1:  49%|████████▎        | 4859/10001 [2:18:12<1:59:18,  1.39s/batch]Batch 4800/10001 Done, mean position loss: 20.89825553894043\n",
      "Training NF1:  48%|████████▏        | 4798/10001 [2:18:18<2:30:47,  1.74s/batch]Batch 4800/10001 Done, mean position loss: 20.621344039440153\n",
      "Training NF1:  47%|████████         | 4709/10001 [2:18:20<2:34:19,  1.75s/batch]Batch 4800/10001 Done, mean position loss: 20.577812118530275\n",
      "Training NF1:  48%|████████▏        | 4839/10001 [2:18:19<2:19:54,  1.63s/batch]Batch 4800/10001 Done, mean position loss: 20.690631000995637\n",
      "Training NF1:  48%|████████▏        | 4817/10001 [2:18:23<2:32:37,  1.77s/batch]Batch 4800/10001 Done, mean position loss: 20.411592948436738\n",
      "Training NF1:  49%|████████▎        | 4875/10001 [2:18:28<2:36:53,  1.84s/batch]Batch 4800/10001 Done, mean position loss: 20.742765243053434\n",
      "Training NF1:  48%|████████▏        | 4810/10001 [2:18:36<2:52:17,  1.99s/batch]Batch 4800/10001 Done, mean position loss: 21.001946301460265\n",
      "Training NF1:  48%|████████▏        | 4784/10001 [2:18:38<2:08:48,  1.48s/batch]Batch 4800/10001 Done, mean position loss: 20.722912042140962\n",
      "Training NF1:  48%|████████▏        | 4790/10001 [2:18:43<2:08:03,  1.47s/batch]Batch 4800/10001 Done, mean position loss: 20.763478157520296\n",
      "Training NF1:  48%|████████▏        | 4824/10001 [2:18:44<2:20:33,  1.63s/batch]Batch 4800/10001 Done, mean position loss: 20.804746141433718\n",
      "Training NF1:  49%|████████▎        | 4859/10001 [2:18:47<1:49:32,  1.28s/batch]Batch 4800/10001 Done, mean position loss: 20.852381212711336\n",
      "Training NF1:  48%|████████▏        | 4842/10001 [2:18:51<2:29:04,  1.73s/batch]Batch 4900/10001 Done, mean position loss: 20.51296259880066\n",
      "Training NF1:  48%|████████▏        | 4792/10001 [2:18:59<2:32:27,  1.76s/batch]Batch 4800/10001 Done, mean position loss: 20.772997159957885\n",
      "Training NF1:  48%|████████▏        | 4815/10001 [2:19:02<2:34:55,  1.79s/batch]Batch 4800/10001 Done, mean position loss: 21.131673054695128\n",
      "Training NF1:  48%|████████▏        | 4836/10001 [2:19:03<2:23:55,  1.67s/batch]Batch 4800/10001 Done, mean position loss: 20.471426644325255\n",
      "Training NF1:  48%|████████▏        | 4835/10001 [2:19:06<2:20:36,  1.63s/batch]Batch 4800/10001 Done, mean position loss: 21.10180580377579\n",
      "Training NF1:  48%|████████▏        | 4825/10001 [2:19:08<2:44:48,  1.91s/batch]Batch 4800/10001 Done, mean position loss: 21.174734535217286\n",
      "Training NF1:  47%|████████         | 4740/10001 [2:19:15<2:48:29,  1.92s/batch]Batch 4900/10001 Done, mean position loss: 20.51396347999573\n",
      "Training NF1:  48%|████████▏        | 4800/10001 [2:19:15<2:47:30,  1.93s/batch]Batch 4800/10001 Done, mean position loss: 20.401987748146055\n",
      "Training NF1:  48%|████████▏        | 4840/10001 [2:19:17<2:49:55,  1.98s/batch]Batch 4900/10001 Done, mean position loss: 20.626242485046387\n",
      "Training NF1:  49%|████████▎        | 4901/10001 [2:19:17<2:34:05,  1.81s/batch]Batch 4800/10001 Done, mean position loss: 20.787071120738982\n",
      "Training NF1:  49%|████████▎        | 4899/10001 [2:19:23<3:14:22,  2.29s/batch]Batch 4800/10001 Done, mean position loss: 21.594031817913056\n",
      "Training NF1:  48%|████████▏        | 4792/10001 [2:19:27<2:46:34,  1.92s/batch]Batch 4900/10001 Done, mean position loss: 20.53750632762909\n",
      "Training NF1:  49%|████████▎        | 4861/10001 [2:19:43<2:21:42,  1.65s/batch]Batch 4800/10001 Done, mean position loss: 21.007080514431\n",
      "Training NF1:  48%|████████▏        | 4815/10001 [2:19:43<2:42:48,  1.88s/batch]Batch 4900/10001 Done, mean position loss: 21.022666933536527\n",
      "Training NF1:  48%|████████▏        | 4825/10001 [2:19:48<2:38:48,  1.84s/batch]Batch 4800/10001 Done, mean position loss: 20.390692114830017\n",
      "Training NF1:  49%|████████▎        | 4915/10001 [2:19:50<2:24:08,  1.70s/batch]Batch 4900/10001 Done, mean position loss: 21.059400510787963\n",
      "Training NF1:  48%|████████         | 4773/10001 [2:20:02<2:38:42,  1.82s/batch]Batch 4900/10001 Done, mean position loss: 20.56218624830246\n",
      "Training NF1:  49%|████████▎        | 4891/10001 [2:20:06<2:59:29,  2.11s/batch]Batch 4900/10001 Done, mean position loss: 20.505144097805022\n",
      "Training NF1:  48%|████████▏        | 4818/10001 [2:20:13<2:25:23,  1.68s/batch]Batch 4900/10001 Done, mean position loss: 20.675176696777342\n",
      "Training NF1:  49%|████████▎        | 4879/10001 [2:20:25<2:04:58,  1.46s/batch]Batch 4900/10001 Done, mean position loss: 20.77604292154312\n",
      "Training NF1:  49%|████████▎        | 4886/10001 [2:20:36<2:23:14,  1.68s/batch]Batch 4900/10001 Done, mean position loss: 20.96048698425293\n",
      "Training NF1:  49%|████████▎        | 4884/10001 [2:20:45<2:25:08,  1.70s/batch]Batch 4900/10001 Done, mean position loss: 20.936636924743652\n",
      "Training NF1:  49%|████████▎        | 4855/10001 [2:20:54<2:32:48,  1.78s/batch]Batch 4900/10001 Done, mean position loss: 20.693985435962677\n",
      "Training NF1:  50%|████████▍        | 4971/10001 [2:20:54<2:45:12,  1.97s/batch]Batch 4900/10001 Done, mean position loss: 20.621025514602664\n",
      "Training NF1:  49%|████████▎        | 4881/10001 [2:20:56<2:49:06,  1.98s/batch]Batch 4800/10001 Done, mean position loss: 20.669641373157503\n",
      "Training NF1:  50%|████████▍        | 4957/10001 [2:20:56<2:33:07,  1.82s/batch]Batch 4900/10001 Done, mean position loss: 20.905527422428133\n",
      "Batch 4900/10001 Done, mean position loss: 20.663344554901123\n",
      "Training NF1:  49%|████████▎        | 4887/10001 [2:21:00<2:38:45,  1.86s/batch]Batch 4900/10001 Done, mean position loss: 20.459745700359345\n",
      "Training NF1:  50%|████████▍        | 4961/10001 [2:21:02<2:10:44,  1.56s/batch]Batch 4900/10001 Done, mean position loss: 20.814765417575835\n",
      "Training NF1:  50%|████████▍        | 4980/10001 [2:21:09<2:37:13,  1.88s/batch]Batch 4900/10001 Done, mean position loss: 20.59859488248825\n",
      "Training NF1:  49%|████████▎        | 4872/10001 [2:21:14<2:13:53,  1.57s/batch]Batch 4900/10001 Done, mean position loss: 20.70804015636444\n",
      "Training NF1:  49%|████████▎        | 4873/10001 [2:21:14<2:01:51,  1.43s/batch]Batch 4900/10001 Done, mean position loss: 20.585515403747557\n",
      "Training NF1:  49%|████████▎        | 4902/10001 [2:21:16<2:13:57,  1.58s/batch]Batch 4800/10001 Done, mean position loss: 20.71636328458786\n",
      "Training NF1:  50%|████████▍        | 4966/10001 [2:21:20<2:17:55,  1.64s/batch]Batch 4900/10001 Done, mean position loss: 20.409014353752138\n",
      "Training NF1:  49%|████████▎        | 4877/10001 [2:21:23<2:11:31,  1.54s/batch]Batch 4900/10001 Done, mean position loss: 20.73934510946274\n",
      "Training NF1:  50%|████████▍        | 4991/10001 [2:21:29<2:40:32,  1.92s/batch]Batch 4900/10001 Done, mean position loss: 20.721671361923217\n",
      "Training NF1:  49%|████████▍        | 4946/10001 [2:21:29<2:10:15,  1.55s/batch]Batch 4900/10001 Done, mean position loss: 21.01388892650604\n",
      "Training NF1:  49%|████████▍        | 4945/10001 [2:21:38<2:22:06,  1.69s/batch]Batch 4900/10001 Done, mean position loss: 20.837903401851655\n",
      "Training NF1:  50%|████████▍        | 4982/10001 [2:21:44<2:15:35,  1.62s/batch]Batch 4900/10001 Done, mean position loss: 20.75971101760864\n",
      "Training NF1:  49%|████████▍        | 4949/10001 [2:21:45<2:12:09,  1.57s/batch]Batch 5000/10001 Done, mean position loss: 20.514537642002104\n",
      "Training NF1:  50%|████████▌        | 5001/10001 [2:21:45<2:10:54,  1.57s/batch]Batch 4900/10001 Done, mean position loss: 20.788316130638123\n",
      "Training NF1:  50%|████████▍        | 4996/10001 [2:21:58<2:20:15,  1.68s/batch]Batch 4900/10001 Done, mean position loss: 20.757438397407533\n",
      "Training NF1:  49%|████████▍        | 4939/10001 [2:22:01<2:06:57,  1.50s/batch]Batch 4900/10001 Done, mean position loss: 20.460597908496858\n",
      "Training NF1:  50%|████████▍        | 4952/10001 [2:22:05<2:16:24,  1.62s/batch]Batch 4900/10001 Done, mean position loss: 21.130277845859528\n",
      "Training NF1:  49%|████████▎        | 4899/10001 [2:22:07<2:43:05,  1.92s/batch]Batch 4900/10001 Done, mean position loss: 21.12626800060272\n",
      "Training NF1:  49%|████████▎        | 4923/10001 [2:22:08<2:56:07,  2.08s/batch]Batch 5000/10001 Done, mean position loss: 20.5082855796814\n",
      "Training NF1:  50%|████████▍        | 4973/10001 [2:22:08<2:11:13,  1.57s/batch]Batch 4900/10001 Done, mean position loss: 21.071693692207337\n",
      "Training NF1:  48%|████████▏        | 4842/10001 [2:22:10<2:36:21,  1.82s/batch]Batch 4900/10001 Done, mean position loss: 20.39010903596878\n",
      "Training NF1:  50%|████████▍        | 4997/10001 [2:22:14<2:19:20,  1.67s/batch]Batch 4900/10001 Done, mean position loss: 21.60196410179138\n",
      "Batch 4900/10001 Done, mean position loss: 20.780710158348086\n",
      "Training NF1:  49%|████████▍        | 4937/10001 [2:22:17<2:35:30,  1.84s/batch]Batch 5000/10001 Done, mean position loss: 20.619367291927336\n",
      "Training NF1:  50%|████████▍        | 4979/10001 [2:22:22<2:05:16,  1.50s/batch]Batch 5000/10001 Done, mean position loss: 20.54002425670624\n",
      "Training NF1:  50%|████████▍        | 4966/10001 [2:22:39<2:18:21,  1.65s/batch]Batch 5000/10001 Done, mean position loss: 21.052610352039338\n",
      "Training NF1:  50%|████████▌        | 5016/10001 [2:22:44<2:37:27,  1.90s/batch]Batch 4900/10001 Done, mean position loss: 20.983675940036775\n",
      "Training NF1:  49%|████████▍        | 4927/10001 [2:22:48<2:36:31,  1.85s/batch]Batch 5000/10001 Done, mean position loss: 21.0536475110054\n",
      "Training NF1:  50%|████████▌        | 5007/10001 [2:22:48<2:17:57,  1.66s/batch]Batch 4900/10001 Done, mean position loss: 20.375584661960602\n",
      "Training NF1:  50%|████████▍        | 4977/10001 [2:22:56<2:31:54,  1.81s/batch]Batch 5000/10001 Done, mean position loss: 20.505211482048033\n",
      "Training NF1:  50%|████████▍        | 4977/10001 [2:22:57<2:27:02,  1.76s/batch]Batch 5000/10001 Done, mean position loss: 20.56125417947769\n",
      "Training NF1:  50%|████████▍        | 4974/10001 [2:23:00<2:19:47,  1.67s/batch]Batch 5000/10001 Done, mean position loss: 20.66897260904312\n",
      "Training NF1:  50%|████████▍        | 4987/10001 [2:23:13<2:15:35,  1.62s/batch]Batch 5000/10001 Done, mean position loss: 20.782773208618167\n",
      "Training NF1:  50%|████████▍        | 4981/10001 [2:23:30<2:23:20,  1.71s/batch]Batch 5000/10001 Done, mean position loss: 20.92767797231674\n",
      "Training NF1:  50%|████████▌        | 5043/10001 [2:23:38<2:38:03,  1.91s/batch]Batch 5000/10001 Done, mean position loss: 20.901345913410186\n",
      "Training NF1:  50%|████████▌        | 5016/10001 [2:23:38<2:12:33,  1.60s/batch]Batch 5000/10001 Done, mean position loss: 20.705144217014315\n",
      "Training NF1:  50%|████████▌        | 5049/10001 [2:23:42<2:00:20,  1.46s/batch]Batch 5000/10001 Done, mean position loss: 20.921696202754973\n",
      "Training NF1:  49%|████████▍        | 4935/10001 [2:23:46<2:20:24,  1.66s/batch]Batch 5000/10001 Done, mean position loss: 20.628623411655425\n",
      "Training NF1:  50%|████████▍        | 5000/10001 [2:23:48<2:23:04,  1.72s/batch]Batch 5000/10001 Done, mean position loss: 20.664556906223297\n",
      "Training NF1:  49%|████████▍        | 4937/10001 [2:23:51<2:40:26,  1.90s/batch]Batch 5000/10001 Done, mean position loss: 20.815385661125184\n",
      "Training NF1:  50%|████████▌        | 5003/10001 [2:23:51<2:08:33,  1.54s/batch]Batch 4900/10001 Done, mean position loss: 20.667844042778015\n",
      "Training NF1:  50%|████████▍        | 4989/10001 [2:23:57<2:22:21,  1.70s/batch]Batch 5000/10001 Done, mean position loss: 20.449029195308686\n",
      "Training NF1:  50%|████████▌        | 5001/10001 [2:23:57<2:40:52,  1.93s/batch]Batch 5000/10001 Done, mean position loss: 20.607955679893493\n",
      "Training NF1:  51%|████████▌        | 5071/10001 [2:24:02<2:15:41,  1.65s/batch]Batch 5000/10001 Done, mean position loss: 20.675698306560516\n",
      "Training NF1:  50%|████████▍        | 4998/10001 [2:24:13<2:25:47,  1.75s/batch]Batch 5000/10001 Done, mean position loss: 20.570362408161163\n",
      "Training NF1:  50%|████████▍        | 4999/10001 [2:24:13<2:10:33,  1.57s/batch]Batch 4900/10001 Done, mean position loss: 20.72207967758179\n",
      "Training NF1:  50%|████████▍        | 4969/10001 [2:24:16<2:27:35,  1.76s/batch]Batch 5000/10001 Done, mean position loss: 20.399939053058624\n",
      "Training NF1:  51%|████████▌        | 5066/10001 [2:24:18<2:31:19,  1.84s/batch]Batch 5000/10001 Done, mean position loss: 20.75024363040924\n",
      "Training NF1:  51%|████████▌        | 5060/10001 [2:24:19<2:47:11,  2.03s/batch]Batch 5000/10001 Done, mean position loss: 20.718949580192564\n",
      "Training NF1:  50%|████████▍        | 4981/10001 [2:24:21<2:23:23,  1.71s/batch]Batch 5000/10001 Done, mean position loss: 20.975422830581664\n",
      "Training NF1:  50%|████████▌        | 5008/10001 [2:24:31<2:19:01,  1.67s/batch]Batch 5000/10001 Done, mean position loss: 20.756783483028414\n",
      "Training NF1:  50%|████████▌        | 5034/10001 [2:24:34<2:15:28,  1.64s/batch]Batch 5000/10001 Done, mean position loss: 20.84255551815033\n",
      "Training NF1:  50%|████████▍        | 4989/10001 [2:24:38<2:45:24,  1.98s/batch]Batch 5000/10001 Done, mean position loss: 20.77804724216461\n",
      "Training NF1:  50%|████████▌        | 5036/10001 [2:24:38<2:07:47,  1.54s/batch]Batch 5100/10001 Done, mean position loss: 20.52692041397095\n",
      "Training NF1:  50%|████████▍        | 4995/10001 [2:24:47<2:44:29,  1.97s/batch]Batch 5000/10001 Done, mean position loss: 20.780214955806734\n",
      "Training NF1:  51%|████████▌        | 5073/10001 [2:24:51<2:15:18,  1.65s/batch]Batch 5000/10001 Done, mean position loss: 20.465032198429107\n",
      "Training NF1:  50%|████████▌        | 5046/10001 [2:24:55<2:28:07,  1.79s/batch]Batch 5100/10001 Done, mean position loss: 20.51522351026535\n",
      "Training NF1:  50%|████████▌        | 5037/10001 [2:24:56<2:39:51,  1.93s/batch]Batch 5000/10001 Done, mean position loss: 21.14157028436661\n",
      "Training NF1:  50%|████████▍        | 4981/10001 [2:24:59<2:07:50,  1.53s/batch]Batch 5000/10001 Done, mean position loss: 21.121125485897064\n",
      "Training NF1:  50%|████████▌        | 5003/10001 [2:25:03<2:27:31,  1.77s/batch]Batch 5000/10001 Done, mean position loss: 21.103616070747375\n",
      "Training NF1:  50%|████████▌        | 5047/10001 [2:25:04<2:13:51,  1.62s/batch]Batch 5000/10001 Done, mean position loss: 20.391155993938447\n",
      "Training NF1:  50%|████████▌        | 5003/10001 [2:25:06<2:17:18,  1.65s/batch]Batch 5100/10001 Done, mean position loss: 20.625395560264586\n",
      "Training NF1:  51%|████████▋        | 5084/10001 [2:25:11<2:18:04,  1.68s/batch]Batch 5000/10001 Done, mean position loss: 20.77096863269806\n",
      "Training NF1:  51%|████████▋        | 5090/10001 [2:25:12<2:19:24,  1.70s/batch]Batch 5000/10001 Done, mean position loss: 21.59130692720413\n",
      "Training NF1:  50%|████████▌        | 5033/10001 [2:25:14<2:29:44,  1.81s/batch]Batch 5100/10001 Done, mean position loss: 20.544628167152403\n",
      "Training NF1:  51%|████████▋        | 5095/10001 [2:25:32<2:15:37,  1.66s/batch]Batch 5100/10001 Done, mean position loss: 21.06149892091751\n",
      "Training NF1:  51%|████████▋        | 5087/10001 [2:25:35<2:34:45,  1.89s/batch]Batch 5000/10001 Done, mean position loss: 20.988308589458462\n",
      "Training NF1:  51%|████████▋        | 5090/10001 [2:25:41<2:38:04,  1.93s/batch]Batch 5100/10001 Done, mean position loss: 21.041386921405792\n",
      "Training NF1:  51%|████████▋        | 5107/10001 [2:25:43<2:28:15,  1.82s/batch]Batch 5100/10001 Done, mean position loss: 20.540454416275026\n",
      "Training NF1:  50%|████████▌        | 5037/10001 [2:25:43<2:48:57,  2.04s/batch]Batch 5000/10001 Done, mean position loss: 20.37261313199997\n",
      "Training NF1:  51%|████████▌        | 5071/10001 [2:25:49<2:37:44,  1.92s/batch]Batch 5100/10001 Done, mean position loss: 20.506456351280214\n",
      "Training NF1:  50%|████████▍        | 4961/10001 [2:26:00<2:20:57,  1.68s/batch]Batch 5100/10001 Done, mean position loss: 20.665421073436736\n",
      "Training NF1:  51%|████████▋        | 5087/10001 [2:26:05<2:07:17,  1.55s/batch]Batch 5100/10001 Done, mean position loss: 20.7658589720726\n",
      "Training NF1:  51%|████████▋        | 5096/10001 [2:26:31<2:50:13,  2.08s/batch]Batch 5100/10001 Done, mean position loss: 20.929075376987456\n",
      "Training NF1:  51%|████████▌        | 5054/10001 [2:26:32<2:51:31,  2.08s/batch]Batch 5100/10001 Done, mean position loss: 20.89274494409561\n",
      "Training NF1:  51%|████████▋        | 5094/10001 [2:26:32<2:28:03,  1.81s/batch]Batch 5100/10001 Done, mean position loss: 20.918911521434786\n",
      "Training NF1:  50%|████████▌        | 5032/10001 [2:26:38<2:15:15,  1.63s/batch]Batch 5100/10001 Done, mean position loss: 20.6558664894104\n",
      "Training NF1:  51%|████████▋        | 5101/10001 [2:26:38<2:18:22,  1.69s/batch]Batch 5100/10001 Done, mean position loss: 20.692194595336915\n",
      "Training NF1:  50%|████████▍        | 4983/10001 [2:26:41<2:29:31,  1.79s/batch]Batch 5100/10001 Done, mean position loss: 20.62288749694824\n",
      "Training NF1:  52%|████████▊        | 5163/10001 [2:26:43<2:36:43,  1.94s/batch]Batch 5100/10001 Done, mean position loss: 20.804119989871978\n",
      "Training NF1:  51%|████████▌        | 5060/10001 [2:26:49<2:39:59,  1.94s/batch]Batch 5000/10001 Done, mean position loss: 20.66152863025665\n",
      "Training NF1:  50%|████████▌        | 5045/10001 [2:26:54<2:34:28,  1.87s/batch]Batch 5100/10001 Done, mean position loss: 20.590463123321534\n",
      "Training NF1:  51%|████████▋        | 5144/10001 [2:26:55<2:22:49,  1.76s/batch]Batch 5100/10001 Done, mean position loss: 20.436265997886657\n",
      "Training NF1:  51%|████████▋        | 5101/10001 [2:26:55<2:40:48,  1.97s/batch]Batch 5100/10001 Done, mean position loss: 20.679922385215757\n",
      "Training NF1:  50%|████████▌        | 5047/10001 [2:27:07<2:18:43,  1.68s/batch]Batch 5100/10001 Done, mean position loss: 20.571246690750122\n",
      "Training NF1:  50%|████████▌        | 5011/10001 [2:27:09<2:26:22,  1.76s/batch]Batch 5100/10001 Done, mean position loss: 20.765248823165894\n",
      "Training NF1:  51%|████████▋        | 5138/10001 [2:27:12<2:35:26,  1.92s/batch]Batch 5100/10001 Done, mean position loss: 20.71227427482605\n",
      "Training NF1:  51%|████████▋        | 5105/10001 [2:27:15<2:32:02,  1.86s/batch]Batch 5000/10001 Done, mean position loss: 20.716472427844998\n",
      "Training NF1:  52%|████████▊        | 5196/10001 [2:27:18<2:04:20,  1.55s/batch]Batch 5100/10001 Done, mean position loss: 20.99894984960556\n",
      "Training NF1:  51%|████████▋        | 5080/10001 [2:27:23<2:31:26,  1.85s/batch]Batch 5100/10001 Done, mean position loss: 20.393418269157408\n",
      "Training NF1:  51%|████████▋        | 5093/10001 [2:27:25<2:19:59,  1.71s/batch]Batch 5200/10001 Done, mean position loss: 20.528943173885345\n",
      "Training NF1:  51%|████████▋        | 5086/10001 [2:27:28<2:26:52,  1.79s/batch]Batch 5100/10001 Done, mean position loss: 20.759182863235473\n",
      "Training NF1:  52%|████████▊        | 5191/10001 [2:27:37<2:21:45,  1.77s/batch]Batch 5100/10001 Done, mean position loss: 20.83216765165329\n",
      "Training NF1:  50%|████████▌        | 5015/10001 [2:27:39<2:35:44,  1.87s/batch]Batch 5100/10001 Done, mean position loss: 20.79122909784317\n",
      "Training NF1:  51%|████████▋        | 5127/10001 [2:27:41<2:34:41,  1.90s/batch]Batch 5100/10001 Done, mean position loss: 20.760161991119386\n",
      "Training NF1:  51%|████████▋        | 5114/10001 [2:27:49<2:12:34,  1.63s/batch]Batch 5100/10001 Done, mean position loss: 21.138373036384586\n",
      "Training NF1:  52%|████████▊        | 5194/10001 [2:27:51<2:00:23,  1.50s/batch]Batch 5200/10001 Done, mean position loss: 20.501935067176817\n",
      "Training NF1:  51%|████████▋        | 5110/10001 [2:27:54<1:57:51,  1.45s/batch]Batch 5200/10001 Done, mean position loss: 20.630717573165896\n",
      "Training NF1:  51%|████████▋        | 5135/10001 [2:27:55<2:57:43,  2.19s/batch]Batch 5100/10001 Done, mean position loss: 20.476114661693572\n",
      "Training NF1:  51%|████████▋        | 5120/10001 [2:27:59<2:21:53,  1.74s/batch]Batch 5100/10001 Done, mean position loss: 20.39471684217453\n",
      "Training NF1:  51%|████████▋        | 5100/10001 [2:27:59<2:12:38,  1.62s/batch]Batch 5100/10001 Done, mean position loss: 20.75438430070877\n",
      "Training NF1:  52%|████████▊        | 5174/10001 [2:28:00<2:32:14,  1.89s/batch]Batch 5100/10001 Done, mean position loss: 21.120789749622343\n",
      "Training NF1:  51%|████████▋        | 5076/10001 [2:28:01<3:02:28,  2.22s/batch]Batch 5100/10001 Done, mean position loss: 21.11004059791565\n",
      "Training NF1:  51%|████████▋        | 5110/10001 [2:28:04<2:19:55,  1.72s/batch]Batch 5200/10001 Done, mean position loss: 20.533408613204955\n",
      "Training NF1:  52%|████████▊        | 5190/10001 [2:28:07<2:06:03,  1.57s/batch]Batch 5100/10001 Done, mean position loss: 21.624931645393374\n",
      "Training NF1:  52%|████████▊        | 5220/10001 [2:28:27<2:10:58,  1.64s/batch]Batch 5200/10001 Done, mean position loss: 21.029310007095336\n",
      "Training NF1:  51%|████████▋        | 5147/10001 [2:28:32<2:14:34,  1.66s/batch]Batch 5200/10001 Done, mean position loss: 21.05504779338837\n",
      "Training NF1:  52%|████████▊        | 5218/10001 [2:28:34<2:16:10,  1.71s/batch]Batch 5100/10001 Done, mean position loss: 21.007269263267517\n",
      "Training NF1:  51%|████████▋        | 5128/10001 [2:28:41<2:03:01,  1.51s/batch]Batch 5200/10001 Done, mean position loss: 20.53834808111191\n",
      "Training NF1:  52%|████████▊        | 5154/10001 [2:28:42<2:31:01,  1.87s/batch]Batch 5200/10001 Done, mean position loss: 20.51667364358902\n",
      "Training NF1:  51%|████████▊        | 5149/10001 [2:28:47<2:20:25,  1.74s/batch]Batch 5100/10001 Done, mean position loss: 20.370668323040007\n",
      "Training NF1:  51%|████████▋        | 5143/10001 [2:28:51<2:06:31,  1.56s/batch]Batch 5200/10001 Done, mean position loss: 20.66939834356308\n",
      "Training NF1:  51%|████████▋        | 5136/10001 [2:29:01<2:22:28,  1.76s/batch]Batch 5200/10001 Done, mean position loss: 20.781788969039916\n",
      "Training NF1:  52%|████████▉        | 5227/10001 [2:29:24<1:54:34,  1.44s/batch]Batch 5200/10001 Done, mean position loss: 20.905631120204923\n",
      "Training NF1:  52%|████████▉        | 5226/10001 [2:29:26<2:27:19,  1.85s/batch]Batch 5200/10001 Done, mean position loss: 20.924807641506195\n",
      "Training NF1:  52%|████████▊        | 5198/10001 [2:29:27<1:56:53,  1.46s/batch]Batch 5200/10001 Done, mean position loss: 20.889031932353973\n",
      "Training NF1:  52%|████████▊        | 5154/10001 [2:29:28<2:36:54,  1.94s/batch]Batch 5200/10001 Done, mean position loss: 20.684919798374175\n",
      "Training NF1:  52%|████████▊        | 5183/10001 [2:29:32<2:19:01,  1.73s/batch]Batch 5200/10001 Done, mean position loss: 20.66900512456894\n",
      "Training NF1:  51%|████████▋        | 5079/10001 [2:29:33<2:41:17,  1.97s/batch]Batch 5200/10001 Done, mean position loss: 20.60280025243759\n",
      "Training NF1:  52%|████████▊        | 5177/10001 [2:29:39<2:13:34,  1.66s/batch]Batch 5200/10001 Done, mean position loss: 20.80165219545364\n",
      "Training NF1:  52%|████████▊        | 5192/10001 [2:29:45<2:08:36,  1.60s/batch]Batch 5100/10001 Done, mean position loss: 20.65391751050949\n",
      "Training NF1:  52%|████████▊        | 5162/10001 [2:29:48<2:22:22,  1.77s/batch]Batch 5200/10001 Done, mean position loss: 20.588813619613646\n",
      "Training NF1:  52%|████████▊        | 5193/10001 [2:29:49<2:40:14,  2.00s/batch]Batch 5200/10001 Done, mean position loss: 20.668499507904052\n",
      "Training NF1:  52%|████████▊        | 5187/10001 [2:29:52<2:16:11,  1.70s/batch]Batch 5200/10001 Done, mean position loss: 20.444971499443053\n",
      "Training NF1:  52%|████████▊        | 5199/10001 [2:30:00<2:17:50,  1.72s/batch]Batch 5200/10001 Done, mean position loss: 20.57118628740311\n",
      "Training NF1:  52%|████████▊        | 5208/10001 [2:30:01<2:22:55,  1.79s/batch]Batch 5200/10001 Done, mean position loss: 20.76385807991028\n",
      "Training NF1:  52%|████████▊        | 5165/10001 [2:30:03<2:14:58,  1.67s/batch]Batch 5200/10001 Done, mean position loss: 20.701904282569885\n",
      "Training NF1:  52%|████████▊        | 5152/10001 [2:30:13<1:54:50,  1.42s/batch]Batch 5100/10001 Done, mean position loss: 20.725318903923032\n",
      "Training NF1:  51%|████████▋        | 5103/10001 [2:30:17<2:30:20,  1.84s/batch]Batch 5200/10001 Done, mean position loss: 21.015069680213927\n",
      "Training NF1:  52%|████████▉        | 5229/10001 [2:30:16<2:14:38,  1.69s/batch]Batch 5200/10001 Done, mean position loss: 20.394961473941805\n",
      "Training NF1:  52%|████████▊        | 5218/10001 [2:30:18<1:58:04,  1.48s/batch]Batch 5200/10001 Done, mean position loss: 20.754591197967528\n",
      "Training NF1:  52%|████████▊        | 5187/10001 [2:30:21<2:22:12,  1.77s/batch]Batch 5300/10001 Done, mean position loss: 20.509657824039458\n",
      "Training NF1:  52%|████████▊        | 5179/10001 [2:30:27<2:13:00,  1.65s/batch]Batch 5200/10001 Done, mean position loss: 20.794254834651944\n",
      "Training NF1:  52%|████████▉        | 5233/10001 [2:30:33<2:24:14,  1.82s/batch]Batch 5200/10001 Done, mean position loss: 20.825651576519014\n",
      "Training NF1:  53%|████████▉        | 5275/10001 [2:30:38<2:15:08,  1.72s/batch]Batch 5200/10001 Done, mean position loss: 21.144576189517974\n",
      "Training NF1:  51%|████████▋        | 5117/10001 [2:30:40<2:11:29,  1.62s/batch]Batch 5200/10001 Done, mean position loss: 20.753157410621643\n",
      "Training NF1:  52%|████████▊        | 5200/10001 [2:30:40<1:56:22,  1.45s/batch]Batch 5300/10001 Done, mean position loss: 20.514899833202364\n",
      "Training NF1:  53%|█████████        | 5302/10001 [2:30:41<2:11:50,  1.68s/batch]Batch 5200/10001 Done, mean position loss: 20.488081040382383\n",
      "Training NF1:  52%|████████▉        | 5239/10001 [2:30:44<2:30:14,  1.89s/batch]Batch 5200/10001 Done, mean position loss: 20.766886110305787\n",
      "Training NF1:  52%|████████▊        | 5204/10001 [2:30:45<2:28:04,  1.85s/batch]Batch 5300/10001 Done, mean position loss: 20.629725930690768\n",
      "Training NF1:  52%|████████▊        | 5210/10001 [2:30:55<2:09:20,  1.62s/batch]Batch 5200/10001 Done, mean position loss: 20.39143138408661\n",
      "Training NF1:  53%|████████▉        | 5285/10001 [2:30:56<1:59:18,  1.52s/batch]Batch 5200/10001 Done, mean position loss: 21.15101146697998\n",
      "Training NF1:  53%|████████▉        | 5287/10001 [2:31:00<2:21:48,  1.80s/batch]Batch 5300/10001 Done, mean position loss: 20.548677706718443\n",
      "Training NF1:  53%|████████▉        | 5252/10001 [2:31:00<2:17:09,  1.73s/batch]Batch 5200/10001 Done, mean position loss: 21.07325886487961\n",
      "Training NF1:  52%|████████▉        | 5241/10001 [2:31:07<2:03:22,  1.56s/batch]Batch 5200/10001 Done, mean position loss: 21.5695609331131\n",
      "Training NF1:  53%|█████████        | 5325/10001 [2:31:24<2:39:23,  2.05s/batch]Batch 5300/10001 Done, mean position loss: 21.05791347503662\n",
      "Batch 5300/10001 Done, mean position loss: 21.000210995674134\n",
      "Training NF1:  52%|████████▊        | 5219/10001 [2:31:32<2:14:49,  1.69s/batch]Batch 5300/10001 Done, mean position loss: 20.553352386951445\n",
      "Training NF1:  53%|████████▉        | 5273/10001 [2:31:35<2:14:55,  1.71s/batch]Batch 5200/10001 Done, mean position loss: 21.01190787315369\n",
      "Training NF1:  52%|████████▉        | 5241/10001 [2:31:37<2:15:43,  1.71s/batch]Batch 5300/10001 Done, mean position loss: 20.511130378246307\n",
      "Training NF1:  52%|████████▉        | 5222/10001 [2:31:38<2:33:10,  1.92s/batch]Batch 5200/10001 Done, mean position loss: 20.385680742263794\n",
      "Training NF1:  53%|████████▉        | 5279/10001 [2:31:44<2:03:06,  1.56s/batch]Batch 5300/10001 Done, mean position loss: 20.672771162986756\n",
      "Training NF1:  52%|████████▊        | 5212/10001 [2:31:56<2:11:03,  1.64s/batch]Batch 5300/10001 Done, mean position loss: 20.77175502061844\n",
      "Training NF1:  53%|████████▉        | 5277/10001 [2:32:20<2:33:14,  1.95s/batch]Batch 5300/10001 Done, mean position loss: 20.887104337215423\n",
      "Training NF1:  53%|████████▉        | 5273/10001 [2:32:20<2:23:42,  1.82s/batch]Batch 5300/10001 Done, mean position loss: 20.923381745815277\n",
      "Training NF1:  53%|█████████        | 5335/10001 [2:32:21<2:23:26,  1.84s/batch]Batch 5300/10001 Done, mean position loss: 20.598751423358916\n",
      "Training NF1:  53%|████████▉        | 5265/10001 [2:32:21<2:11:11,  1.66s/batch]Batch 5300/10001 Done, mean position loss: 20.65960760593414\n",
      "Training NF1:  52%|████████▉        | 5246/10001 [2:32:23<2:08:42,  1.62s/batch]Batch 5300/10001 Done, mean position loss: 20.89368489265442\n",
      "Training NF1:  52%|████████▉        | 5248/10001 [2:32:27<2:26:38,  1.85s/batch]Batch 5300/10001 Done, mean position loss: 20.67279070138931\n",
      "Training NF1:  53%|████████▉        | 5252/10001 [2:32:33<2:15:58,  1.72s/batch]Batch 5300/10001 Done, mean position loss: 20.814554257392885\n",
      "Training NF1:  53%|████████▉        | 5280/10001 [2:32:36<2:36:08,  1.98s/batch]Batch 5200/10001 Done, mean position loss: 20.64678143501282\n",
      "Training NF1:  53%|████████▉        | 5284/10001 [2:32:39<2:01:18,  1.54s/batch]Batch 5300/10001 Done, mean position loss: 20.580732851028444\n",
      "Training NF1:  53%|█████████        | 5311/10001 [2:32:41<2:22:14,  1.82s/batch]Batch 5300/10001 Done, mean position loss: 20.440604596137998\n",
      "Training NF1:  53%|█████████        | 5313/10001 [2:32:44<2:16:10,  1.74s/batch]Batch 5300/10001 Done, mean position loss: 20.68728949069977\n",
      "Training NF1:  54%|█████████        | 5366/10001 [2:32:50<2:29:13,  1.93s/batch]Batch 5300/10001 Done, mean position loss: 20.740163424015044\n",
      "Training NF1:  53%|████████▉        | 5292/10001 [2:32:59<2:28:40,  1.89s/batch]Batch 5300/10001 Done, mean position loss: 20.575674407482147\n",
      "Training NF1:  53%|█████████        | 5320/10001 [2:33:00<2:22:32,  1.83s/batch]Batch 5300/10001 Done, mean position loss: 20.705417621135716\n",
      "Training NF1:  52%|████████▊        | 5196/10001 [2:33:09<2:38:24,  1.98s/batch]Batch 5300/10001 Done, mean position loss: 20.740291006565094\n",
      "Training NF1:  54%|█████████        | 5363/10001 [2:33:09<2:05:45,  1.63s/batch]Batch 5300/10001 Done, mean position loss: 20.390717463493345\n",
      "Training NF1:  52%|████████▊        | 5200/10001 [2:33:15<2:12:45,  1.66s/batch]Batch 5300/10001 Done, mean position loss: 20.99670393228531\n",
      "Training NF1:  53%|████████▉        | 5259/10001 [2:33:17<2:10:38,  1.65s/batch]Batch 5200/10001 Done, mean position loss: 20.69529594659805\n",
      "Training NF1:  53%|█████████        | 5303/10001 [2:33:18<1:59:10,  1.52s/batch]Batch 5300/10001 Done, mean position loss: 20.772202467918397\n",
      "Training NF1:  54%|█████████        | 5363/10001 [2:33:18<2:44:28,  2.13s/batch]Batch 5400/10001 Done, mean position loss: 20.517419047355652\n",
      "Training NF1:  53%|█████████        | 5299/10001 [2:33:24<2:08:55,  1.65s/batch]Batch 5300/10001 Done, mean position loss: 20.755810687541963\n",
      "Training NF1:  52%|████████▉        | 5229/10001 [2:33:25<2:19:53,  1.76s/batch]Batch 5300/10001 Done, mean position loss: 20.844241807460783\n",
      "Training NF1:  53%|████████▉        | 5284/10001 [2:33:27<2:12:54,  1.69s/batch]Batch 5300/10001 Done, mean position loss: 21.145701076984405\n",
      "Training NF1:  54%|█████████▏       | 5376/10001 [2:33:29<2:27:49,  1.92s/batch]Batch 5300/10001 Done, mean position loss: 20.485241386890415\n",
      "Training NF1:  53%|█████████        | 5315/10001 [2:33:32<2:10:30,  1.67s/batch]Batch 5400/10001 Done, mean position loss: 20.62434351205826\n",
      "Training NF1:  53%|█████████        | 5313/10001 [2:33:37<2:37:50,  2.02s/batch]Batch 5400/10001 Done, mean position loss: 20.497951152324674\n",
      "Training NF1:  53%|█████████        | 5345/10001 [2:33:40<2:13:08,  1.72s/batch]Batch 5300/10001 Done, mean position loss: 20.758012013435362\n",
      "Training NF1:  53%|█████████        | 5318/10001 [2:33:46<2:30:40,  1.93s/batch]Batch 5300/10001 Done, mean position loss: 20.39591155767441\n",
      "Training NF1:  54%|█████████▏       | 5382/10001 [2:33:50<2:30:15,  1.95s/batch]Batch 5400/10001 Done, mean position loss: 20.54319282054901\n",
      "Training NF1:  54%|█████████        | 5353/10001 [2:33:56<2:16:07,  1.76s/batch]Batch 5300/10001 Done, mean position loss: 21.118796906471253\n",
      "Training NF1:  54%|█████████▏       | 5375/10001 [2:33:57<2:37:36,  2.04s/batch]Batch 5300/10001 Done, mean position loss: 21.56115392446518\n",
      "Training NF1:  53%|█████████        | 5318/10001 [2:33:57<2:02:51,  1.57s/batch]Batch 5300/10001 Done, mean position loss: 21.144073705673215\n",
      "Training NF1:  53%|█████████        | 5318/10001 [2:34:13<2:18:45,  1.78s/batch]Batch 5400/10001 Done, mean position loss: 21.024280984401702\n",
      "Training NF1:  54%|█████████▏       | 5403/10001 [2:34:16<2:02:26,  1.60s/batch]Batch 5400/10001 Done, mean position loss: 21.00842176437378\n",
      "Training NF1:  53%|█████████        | 5332/10001 [2:34:21<2:15:01,  1.74s/batch]Batch 5300/10001 Done, mean position loss: 20.98574748516083\n",
      "Training NF1:  54%|█████████        | 5351/10001 [2:34:22<2:25:43,  1.88s/batch]Batch 5400/10001 Done, mean position loss: 20.543176279067993\n",
      "Training NF1:  53%|█████████        | 5328/10001 [2:34:30<2:15:17,  1.74s/batch]Batch 5400/10001 Done, mean position loss: 20.493675916194917\n",
      "Training NF1:  54%|█████████        | 5359/10001 [2:34:34<2:16:37,  1.77s/batch]Batch 5300/10001 Done, mean position loss: 20.376612241268155\n",
      "Training NF1:  54%|█████████▏       | 5384/10001 [2:34:40<2:16:31,  1.77s/batch]Batch 5400/10001 Done, mean position loss: 20.670360238552092\n",
      "Training NF1:  54%|█████████▏       | 5414/10001 [2:34:50<1:48:01,  1.41s/batch]Batch 5400/10001 Done, mean position loss: 20.760602843761447\n",
      "Training NF1:  54%|█████████▏       | 5394/10001 [2:35:04<1:54:32,  1.49s/batch]Batch 5400/10001 Done, mean position loss: 20.919885938167575\n",
      "Training NF1:  54%|█████████▏       | 5369/10001 [2:35:04<2:15:36,  1.76s/batch]Batch 5400/10001 Done, mean position loss: 20.89678730726242\n",
      "Training NF1:  54%|█████████        | 5351/10001 [2:35:12<2:25:31,  1.88s/batch]Batch 5400/10001 Done, mean position loss: 20.605135347843166\n",
      "Training NF1:  54%|█████████▏       | 5369/10001 [2:35:16<1:52:50,  1.46s/batch]Batch 5400/10001 Done, mean position loss: 20.86790757894516\n",
      "Training NF1:  55%|█████████▎       | 5457/10001 [2:35:18<2:24:16,  1.90s/batch]Batch 5400/10001 Done, mean position loss: 20.64208542585373\n",
      "Training NF1:  53%|█████████        | 5328/10001 [2:35:20<2:26:25,  1.88s/batch]Batch 5400/10001 Done, mean position loss: 20.682864515781404\n",
      "Training NF1:  55%|█████████▎       | 5459/10001 [2:35:22<2:25:09,  1.92s/batch]Batch 5400/10001 Done, mean position loss: 20.81324813127518\n",
      "Training NF1:  54%|█████████▏       | 5398/10001 [2:35:32<2:10:31,  1.70s/batch]Batch 5300/10001 Done, mean position loss: 20.648678781986238\n",
      "Training NF1:  55%|█████████▎       | 5479/10001 [2:35:34<2:13:14,  1.77s/batch]Batch 5400/10001 Done, mean position loss: 20.673653905391696\n",
      "Training NF1:  54%|█████████▏       | 5397/10001 [2:35:39<2:18:07,  1.80s/batch]Batch 5400/10001 Done, mean position loss: 20.43610699415207\n",
      "Training NF1:  54%|█████████▏       | 5376/10001 [2:35:39<2:40:57,  2.09s/batch]Batch 5400/10001 Done, mean position loss: 20.593798992633822\n",
      "Training NF1:  54%|█████████        | 5368/10001 [2:35:46<1:58:23,  1.53s/batch]Batch 5400/10001 Done, mean position loss: 20.749558062553405\n",
      "Training NF1:  54%|█████████▏       | 5412/10001 [2:35:59<2:41:40,  2.11s/batch]Batch 5400/10001 Done, mean position loss: 20.559878640174865\n",
      "Training NF1:  54%|█████████▏       | 5428/10001 [2:36:00<2:03:53,  1.63s/batch]Batch 5400/10001 Done, mean position loss: 20.74891788482666\n",
      "Training NF1:  54%|█████████▏       | 5400/10001 [2:36:01<2:13:17,  1.74s/batch]Batch 5400/10001 Done, mean position loss: 20.691672139167785\n",
      "Training NF1:  54%|█████████▏       | 5403/10001 [2:36:02<1:40:42,  1.31s/batch]Batch 5400/10001 Done, mean position loss: 20.40325685739517\n",
      "Training NF1:  54%|█████████▏       | 5399/10001 [2:36:10<2:06:05,  1.64s/batch]Batch 5500/10001 Done, mean position loss: 20.510493798255922\n",
      "Training NF1:  54%|█████████▏       | 5387/10001 [2:36:12<1:45:11,  1.37s/batch]Batch 5300/10001 Done, mean position loss: 20.701671030521393\n",
      "Training NF1:  53%|█████████        | 5301/10001 [2:36:13<2:30:58,  1.93s/batch]Batch 5400/10001 Done, mean position loss: 20.75344178199768\n",
      "Training NF1:  55%|█████████▎       | 5460/10001 [2:36:13<2:06:25,  1.67s/batch]Batch 5400/10001 Done, mean position loss: 20.98714498281479\n",
      "Training NF1:  54%|█████████▏       | 5437/10001 [2:36:17<2:28:22,  1.95s/batch]Batch 5400/10001 Done, mean position loss: 20.837268803119656\n",
      "Training NF1:  55%|█████████▎       | 5488/10001 [2:36:17<2:05:31,  1.67s/batch]Batch 5400/10001 Done, mean position loss: 20.75188331604004\n",
      "Training NF1:  54%|█████████▏       | 5439/10001 [2:36:20<2:18:42,  1.82s/batch]Batch 5400/10001 Done, mean position loss: 20.47380347251892\n",
      "Training NF1:  54%|█████████▏       | 5413/10001 [2:36:21<2:14:21,  1.76s/batch]Batch 5500/10001 Done, mean position loss: 20.61296512365341\n",
      "Training NF1:  54%|█████████▏       | 5392/10001 [2:36:22<2:22:16,  1.85s/batch]Batch 5400/10001 Done, mean position loss: 21.0877640080452\n",
      "Training NF1:  54%|█████████▎       | 5447/10001 [2:36:37<2:11:39,  1.73s/batch]Batch 5500/10001 Done, mean position loss: 20.488432998657228\n",
      "Training NF1:  53%|█████████        | 5338/10001 [2:36:40<2:14:13,  1.73s/batch]Batch 5400/10001 Done, mean position loss: 20.399770443439486\n",
      "Training NF1:  55%|█████████▎       | 5512/10001 [2:36:40<2:09:46,  1.73s/batch]Batch 5500/10001 Done, mean position loss: 20.53238707780838\n",
      "Training NF1:  55%|█████████▎       | 5477/10001 [2:36:41<2:11:26,  1.74s/batch]Batch 5400/10001 Done, mean position loss: 20.75668159723282\n",
      "Batch 5400/10001 Done, mean position loss: 21.069795236587524\n",
      "Training NF1:  54%|█████████▏       | 5431/10001 [2:36:54<2:10:38,  1.72s/batch]Batch 5400/10001 Done, mean position loss: 21.564489617347718\n",
      "Training NF1:  55%|█████████▎       | 5486/10001 [2:37:00<2:13:36,  1.78s/batch]Batch 5400/10001 Done, mean position loss: 21.15924908876419\n",
      "Training NF1:  55%|█████████▎       | 5467/10001 [2:37:11<2:09:26,  1.71s/batch]Batch 5500/10001 Done, mean position loss: 20.978654284477233\n",
      "Training NF1:  55%|█████████▎       | 5468/10001 [2:37:13<2:07:18,  1.69s/batch]Batch 5500/10001 Done, mean position loss: 20.985699715614317\n",
      "Training NF1:  55%|█████████▎       | 5474/10001 [2:37:21<1:51:40,  1.48s/batch]Batch 5400/10001 Done, mean position loss: 20.982278058528898\n",
      "Batch 5500/10001 Done, mean position loss: 20.532621114254\n",
      "Training NF1:  55%|█████████▎       | 5459/10001 [2:37:25<2:22:21,  1.88s/batch]Batch 5400/10001 Done, mean position loss: 20.375518872737885\n",
      "Training NF1:  54%|█████████▏       | 5403/10001 [2:37:24<2:01:09,  1.58s/batch]Batch 5500/10001 Done, mean position loss: 20.493216915130617\n",
      "Training NF1:  54%|█████████▏       | 5434/10001 [2:37:36<1:56:24,  1.53s/batch]Batch 5500/10001 Done, mean position loss: 20.66719518184662\n",
      "Training NF1:  55%|█████████▎       | 5459/10001 [2:37:51<2:07:03,  1.68s/batch]Batch 5500/10001 Done, mean position loss: 20.780366313457492\n",
      "Training NF1:  55%|█████████▍       | 5528/10001 [2:37:58<1:54:36,  1.54s/batch]Batch 5500/10001 Done, mean position loss: 20.904545452594757\n",
      "Training NF1:  54%|█████████▏       | 5438/10001 [2:38:00<2:12:29,  1.74s/batch]Batch 5500/10001 Done, mean position loss: 20.926866347789762\n",
      "Training NF1:  55%|█████████▎       | 5499/10001 [2:38:07<2:02:22,  1.63s/batch]Batch 5500/10001 Done, mean position loss: 20.66990852355957\n",
      "Training NF1:  55%|█████████▎       | 5490/10001 [2:38:08<2:11:35,  1.75s/batch]Batch 5500/10001 Done, mean position loss: 20.596390771865842\n",
      "Training NF1:  55%|█████████▎       | 5506/10001 [2:38:08<2:00:25,  1.61s/batch]Batch 5500/10001 Done, mean position loss: 20.88784630537033\n",
      "Training NF1:  56%|█████████▍       | 5572/10001 [2:38:11<2:06:55,  1.72s/batch]Batch 5500/10001 Done, mean position loss: 20.65008590936661\n",
      "Training NF1:  54%|█████████▏       | 5429/10001 [2:38:13<2:07:06,  1.67s/batch]Batch 5500/10001 Done, mean position loss: 20.796382162570954\n",
      "Training NF1:  55%|█████████▎       | 5513/10001 [2:38:27<2:23:23,  1.92s/batch]Batch 5500/10001 Done, mean position loss: 20.6831357049942\n",
      "Training NF1:  55%|█████████▍       | 5534/10001 [2:38:30<2:09:35,  1.74s/batch]Batch 5400/10001 Done, mean position loss: 20.645320765972137\n",
      "Training NF1:  54%|█████████▏       | 5379/10001 [2:38:34<2:15:54,  1.76s/batch]Batch 5500/10001 Done, mean position loss: 20.577057149410248\n",
      "Training NF1:  55%|█████████▎       | 5459/10001 [2:38:34<2:03:20,  1.63s/batch]Batch 5500/10001 Done, mean position loss: 20.431022663116455\n",
      "Training NF1:  56%|█████████▍       | 5572/10001 [2:38:34<2:07:31,  1.73s/batch]Batch 5500/10001 Done, mean position loss: 20.71676814556122\n",
      "Training NF1:  55%|█████████▍       | 5526/10001 [2:38:49<2:04:51,  1.67s/batch]Batch 5500/10001 Done, mean position loss: 20.755241103172303\n",
      "Training NF1:  55%|█████████▎       | 5495/10001 [2:38:53<1:59:40,  1.59s/batch]Batch 5500/10001 Done, mean position loss: 20.704502403736115\n",
      "Training NF1:  55%|█████████▎       | 5487/10001 [2:38:54<1:57:48,  1.57s/batch]Batch 5500/10001 Done, mean position loss: 20.384853200912474\n",
      "Training NF1:  55%|█████████▍       | 5528/10001 [2:38:58<2:10:28,  1.75s/batch]Batch 5500/10001 Done, mean position loss: 20.55478257894516\n",
      "Training NF1:  55%|█████████▎       | 5504/10001 [2:38:58<2:04:51,  1.67s/batch]Batch 5600/10001 Done, mean position loss: 20.513148200511935\n",
      "Training NF1:  55%|█████████▎       | 5506/10001 [2:39:01<1:58:12,  1.58s/batch]Batch 5500/10001 Done, mean position loss: 20.75230038881302\n",
      "Training NF1:  55%|█████████▎       | 5477/10001 [2:39:03<2:09:22,  1.72s/batch]Batch 5500/10001 Done, mean position loss: 20.824011838436128\n",
      "Training NF1:  55%|█████████▍       | 5540/10001 [2:39:05<2:15:20,  1.82s/batch]Batch 5500/10001 Done, mean position loss: 20.991196150779725\n",
      "Training NF1:  55%|█████████▍       | 5524/10001 [2:39:10<2:10:38,  1.75s/batch]Batch 5500/10001 Done, mean position loss: 20.749905743598937\n",
      "Training NF1:  55%|█████████▎       | 5498/10001 [2:39:13<2:12:23,  1.76s/batch]Batch 5400/10001 Done, mean position loss: 20.70740560531616\n",
      "Training NF1:  55%|█████████▎       | 5504/10001 [2:39:14<2:05:51,  1.68s/batch]Batch 5600/10001 Done, mean position loss: 20.629542036056517\n",
      "Training NF1:  56%|█████████▍       | 5561/10001 [2:39:16<2:02:23,  1.65s/batch]Batch 5500/10001 Done, mean position loss: 20.481834907531738\n",
      "Training NF1:  55%|█████████▍       | 5516/10001 [2:39:18<2:03:18,  1.65s/batch]Batch 5500/10001 Done, mean position loss: 21.084335784912106\n",
      "Training NF1:  55%|█████████▎       | 5513/10001 [2:39:23<2:03:38,  1.65s/batch]Batch 5600/10001 Done, mean position loss: 20.477752978801725\n",
      "Training NF1:  55%|█████████▍       | 5535/10001 [2:39:32<2:05:34,  1.69s/batch]Batch 5500/10001 Done, mean position loss: 21.091711459159853\n",
      "Training NF1:  55%|█████████▎       | 5511/10001 [2:39:33<2:16:17,  1.82s/batch]Batch 5500/10001 Done, mean position loss: 20.394158186912534\n",
      "Training NF1:  55%|█████████▎       | 5476/10001 [2:39:36<2:05:54,  1.67s/batch]Batch 5600/10001 Done, mean position loss: 20.530664370059966\n",
      "Training NF1:  56%|█████████▍       | 5573/10001 [2:39:37<2:09:21,  1.75s/batch]Batch 5500/10001 Done, mean position loss: 20.764435808658597\n",
      "Training NF1:  56%|█████████▍       | 5569/10001 [2:39:44<1:53:02,  1.53s/batch]Batch 5500/10001 Done, mean position loss: 21.126580202579497\n",
      "Training NF1:  55%|█████████▎       | 5507/10001 [2:39:47<2:03:17,  1.65s/batch]Batch 5500/10001 Done, mean position loss: 21.56747975587845\n",
      "Training NF1:  55%|█████████▎       | 5513/10001 [2:39:58<2:08:47,  1.72s/batch]Batch 5600/10001 Done, mean position loss: 20.984898750782012\n",
      "Training NF1:  55%|█████████▎       | 5489/10001 [2:40:01<2:33:47,  2.05s/batch]Batch 5600/10001 Done, mean position loss: 21.003726329803467\n",
      "Training NF1:  56%|█████████▍       | 5560/10001 [2:40:14<1:55:08,  1.56s/batch]Batch 5600/10001 Done, mean position loss: 20.53991135597229\n",
      "Training NF1:  55%|█████████▍       | 5546/10001 [2:40:17<2:01:12,  1.63s/batch]Batch 5500/10001 Done, mean position loss: 20.991873166561128\n",
      "Batch 5600/10001 Done, mean position loss: 20.49469002485275\n",
      "Training NF1:  56%|█████████▌       | 5635/10001 [2:40:23<2:03:15,  1.69s/batch]Batch 5500/10001 Done, mean position loss: 20.36576499223709\n",
      "Training NF1:  56%|█████████▍       | 5552/10001 [2:40:28<2:21:49,  1.91s/batch]Batch 5600/10001 Done, mean position loss: 20.66913476705551\n",
      "Training NF1:  55%|█████████▎       | 5475/10001 [2:40:44<2:13:35,  1.77s/batch]Batch 5600/10001 Done, mean position loss: 20.76667839050293\n",
      "Training NF1:  56%|█████████▌       | 5599/10001 [2:40:51<2:14:43,  1.84s/batch]Batch 5600/10001 Done, mean position loss: 20.891414017677306\n",
      "Training NF1:  56%|█████████▍       | 5573/10001 [2:40:54<1:55:00,  1.56s/batch]Batch 5600/10001 Done, mean position loss: 20.914648129940034\n",
      "Training NF1:  56%|█████████▍       | 5562/10001 [2:40:59<2:06:47,  1.71s/batch]Batch 5600/10001 Done, mean position loss: 20.878851170539857\n",
      "Training NF1:  56%|█████████▍       | 5577/10001 [2:41:00<2:30:13,  2.04s/batch]Batch 5600/10001 Done, mean position loss: 20.59639463663101\n",
      "Training NF1:  56%|█████████▍       | 5567/10001 [2:41:02<2:24:39,  1.96s/batch]Batch 5600/10001 Done, mean position loss: 20.785882523059847\n",
      "Training NF1:  56%|█████████▌       | 5639/10001 [2:41:01<2:09:23,  1.78s/batch]Batch 5600/10001 Done, mean position loss: 20.66600578784943\n",
      "Training NF1:  56%|█████████▍       | 5588/10001 [2:41:03<2:13:54,  1.82s/batch]Batch 5600/10001 Done, mean position loss: 20.641629927158355\n",
      "Training NF1:  56%|█████████▌       | 5613/10001 [2:41:20<2:06:45,  1.73s/batch]Batch 5600/10001 Done, mean position loss: 20.67088472366333\n",
      "Training NF1:  56%|█████████▌       | 5620/10001 [2:41:23<1:57:08,  1.60s/batch]Batch 5600/10001 Done, mean position loss: 20.723776724338535\n",
      "Training NF1:  56%|█████████▍       | 5587/10001 [2:41:25<2:01:23,  1.65s/batch]Batch 5600/10001 Done, mean position loss: 20.583189821243288\n",
      "Training NF1:  55%|█████████▎       | 5497/10001 [2:41:25<2:39:56,  2.13s/batch]Batch 5600/10001 Done, mean position loss: 20.437884089946746\n",
      "Training NF1:  56%|█████████▍       | 5571/10001 [2:41:31<1:58:30,  1.61s/batch]Batch 5500/10001 Done, mean position loss: 20.652560551166534\n",
      "Training NF1:  55%|█████████▍       | 5546/10001 [2:41:43<2:16:36,  1.84s/batch]Batch 5600/10001 Done, mean position loss: 20.38566929101944\n",
      "Training NF1:  56%|█████████▍       | 5552/10001 [2:41:44<2:17:28,  1.85s/batch]Batch 5600/10001 Done, mean position loss: 20.751266055107116\n",
      "Training NF1:  56%|█████████▌       | 5633/10001 [2:41:47<1:46:18,  1.46s/batch]Batch 5600/10001 Done, mean position loss: 20.69020013332367\n",
      "Training NF1:  56%|█████████▌       | 5650/10001 [2:41:48<2:06:03,  1.74s/batch]Batch 5600/10001 Done, mean position loss: 20.540764203071596\n",
      "Training NF1:  56%|█████████▌       | 5602/10001 [2:41:51<2:06:23,  1.72s/batch]Batch 5700/10001 Done, mean position loss: 20.51697142124176\n",
      "Training NF1:  56%|█████████▌       | 5622/10001 [2:41:55<1:52:37,  1.54s/batch]Batch 5600/10001 Done, mean position loss: 20.750557510852815\n",
      "Training NF1:  56%|█████████▌       | 5595/10001 [2:41:54<2:04:21,  1.69s/batch]Batch 5600/10001 Done, mean position loss: 20.81491274356842\n",
      "Training NF1:  57%|█████████▋       | 5666/10001 [2:42:05<2:02:01,  1.69s/batch]Batch 5600/10001 Done, mean position loss: 20.742381618022918\n",
      "Training NF1:  56%|█████████▌       | 5629/10001 [2:42:06<2:03:00,  1.69s/batch]Batch 5600/10001 Done, mean position loss: 21.003206341266633\n",
      "Training NF1:  56%|█████████▌       | 5639/10001 [2:42:09<1:58:38,  1.63s/batch]Batch 5700/10001 Done, mean position loss: 20.620260095596315\n",
      "Training NF1:  57%|█████████▋       | 5702/10001 [2:42:11<1:46:32,  1.49s/batch]Batch 5600/10001 Done, mean position loss: 21.081804308891297\n",
      "Training NF1:  56%|█████████▌       | 5603/10001 [2:42:13<1:59:49,  1.63s/batch]Batch 5500/10001 Done, mean position loss: 20.68903037548065\n",
      "Training NF1:  57%|█████████▋       | 5695/10001 [2:42:17<2:22:29,  1.99s/batch]Batch 5600/10001 Done, mean position loss: 20.459595975875853\n",
      "Training NF1:  56%|█████████▌       | 5619/10001 [2:42:20<2:04:41,  1.71s/batch]Batch 5700/10001 Done, mean position loss: 20.461581709384916\n",
      "Training NF1:  56%|█████████▌       | 5648/10001 [2:42:23<2:19:41,  1.93s/batch]Batch 5600/10001 Done, mean position loss: 21.03560225248337\n",
      "Training NF1:  56%|█████████▌       | 5637/10001 [2:42:26<2:09:15,  1.78s/batch]Batch 5600/10001 Done, mean position loss: 20.385482501983642\n",
      "Training NF1:  57%|█████████▋       | 5680/10001 [2:42:28<1:41:24,  1.41s/batch]Batch 5700/10001 Done, mean position loss: 20.5316410279274\n",
      "Training NF1:  57%|█████████▌       | 5655/10001 [2:42:34<1:57:55,  1.63s/batch]Batch 5600/10001 Done, mean position loss: 20.765626225471497\n",
      "Training NF1:  56%|█████████▌       | 5599/10001 [2:42:40<2:34:13,  2.10s/batch]Batch 5600/10001 Done, mean position loss: 21.582996590137483\n",
      "Training NF1:  56%|█████████▌       | 5621/10001 [2:42:44<2:12:38,  1.82s/batch]Batch 5600/10001 Done, mean position loss: 21.121020343303684\n",
      "Training NF1:  57%|█████████▌       | 5659/10001 [2:42:46<2:09:53,  1.79s/batch]Batch 5700/10001 Done, mean position loss: 20.990289168357847\n",
      "Training NF1:  57%|█████████▋       | 5665/10001 [2:42:57<2:34:59,  2.14s/batch]Batch 5700/10001 Done, mean position loss: 21.02674142599106\n",
      "Training NF1:  56%|█████████▌       | 5636/10001 [2:43:08<2:20:29,  1.93s/batch]Batch 5700/10001 Done, mean position loss: 20.5356800866127\n",
      "Training NF1:  56%|█████████▌       | 5628/10001 [2:43:12<1:56:31,  1.60s/batch]Batch 5600/10001 Done, mean position loss: 20.96699506282806\n",
      "Training NF1:  56%|█████████▌       | 5599/10001 [2:43:17<2:12:57,  1.81s/batch]Batch 5700/10001 Done, mean position loss: 20.487503066062928\n",
      "Training NF1:  56%|█████████▌       | 5633/10001 [2:43:20<1:59:36,  1.64s/batch]Batch 5600/10001 Done, mean position loss: 20.35616148471832\n",
      "Training NF1:  57%|█████████▋       | 5731/10001 [2:43:21<1:40:09,  1.41s/batch]Batch 5700/10001 Done, mean position loss: 20.66006044149399\n",
      "Training NF1:  55%|█████████▍       | 5548/10001 [2:43:40<1:46:22,  1.43s/batch]Batch 5700/10001 Done, mean position loss: 20.759943463802337\n",
      "Training NF1:  57%|█████████▋       | 5733/10001 [2:43:41<2:01:03,  1.70s/batch]Batch 5700/10001 Done, mean position loss: 20.895347979068756\n",
      "Training NF1:  56%|█████████▌       | 5639/10001 [2:43:48<2:07:53,  1.76s/batch]Batch 5700/10001 Done, mean position loss: 20.856052157878874\n",
      "Training NF1:  57%|█████████▌       | 5660/10001 [2:43:55<2:30:04,  2.07s/batch]Batch 5700/10001 Done, mean position loss: 20.655015778541568\n",
      "Training NF1:  56%|█████████▌       | 5643/10001 [2:43:58<1:56:08,  1.60s/batch]Batch 5700/10001 Done, mean position loss: 20.791837749481203\n",
      "Training NF1:  57%|█████████▋       | 5701/10001 [2:43:59<1:58:02,  1.65s/batch]Batch 5700/10001 Done, mean position loss: 20.635500605106355\n",
      "Training NF1:  57%|█████████▊       | 5744/10001 [2:43:59<1:52:07,  1.58s/batch]Batch 5700/10001 Done, mean position loss: 20.597453789710997\n",
      "Training NF1:  57%|█████████▋       | 5703/10001 [2:44:01<1:40:10,  1.40s/batch]Batch 5700/10001 Done, mean position loss: 20.885823903083804\n",
      "Training NF1:  57%|█████████▋       | 5664/10001 [2:44:17<2:14:08,  1.86s/batch]Batch 5700/10001 Done, mean position loss: 20.6656370639801\n",
      "Training NF1:  57%|█████████▋       | 5725/10001 [2:44:20<2:14:11,  1.88s/batch]Batch 5700/10001 Done, mean position loss: 20.570454549789428\n",
      "Training NF1:  57%|█████████▋       | 5720/10001 [2:44:21<1:49:13,  1.53s/batch]Batch 5700/10001 Done, mean position loss: 20.43614000082016\n",
      "Training NF1:  57%|█████████▋       | 5665/10001 [2:44:23<2:01:52,  1.69s/batch]Batch 5700/10001 Done, mean position loss: 20.698510053157804\n",
      "Training NF1:  57%|█████████▋       | 5720/10001 [2:44:29<1:55:53,  1.62s/batch]Batch 5600/10001 Done, mean position loss: 20.655222752094268\n",
      "Training NF1:  56%|█████████▌       | 5605/10001 [2:44:37<2:21:15,  1.93s/batch]Batch 5700/10001 Done, mean position loss: 20.74491258621216\n",
      "Training NF1:  58%|█████████▊       | 5799/10001 [2:44:37<1:51:23,  1.59s/batch]Batch 5700/10001 Done, mean position loss: 20.38654299736023\n",
      "Training NF1:  57%|█████████▋       | 5666/10001 [2:44:39<2:23:03,  1.98s/batch]Batch 5700/10001 Done, mean position loss: 20.762383067607878\n",
      "Training NF1:  57%|█████████▋       | 5667/10001 [2:44:41<2:13:30,  1.85s/batch]Batch 5800/10001 Done, mean position loss: 20.506411061286926\n",
      "Training NF1:  57%|█████████▋       | 5680/10001 [2:44:41<2:05:46,  1.75s/batch]Batch 5700/10001 Done, mean position loss: 20.677884199619292\n",
      "Training NF1:  57%|█████████▋       | 5669/10001 [2:44:47<2:12:37,  1.84s/batch]Batch 5700/10001 Done, mean position loss: 20.539822187423706\n",
      "Training NF1:  57%|█████████▋       | 5711/10001 [2:44:54<2:01:21,  1.70s/batch]Batch 5700/10001 Done, mean position loss: 20.807699229717254\n",
      "Training NF1:  57%|█████████▋       | 5716/10001 [2:45:02<1:52:51,  1.58s/batch]Batch 5700/10001 Done, mean position loss: 21.00815222978592\n",
      "Training NF1:  57%|█████████▋       | 5680/10001 [2:45:02<2:03:50,  1.72s/batch]Batch 5700/10001 Done, mean position loss: 21.09121826171875\n",
      "Training NF1:  57%|█████████▊       | 5743/10001 [2:45:05<2:12:51,  1.87s/batch]Batch 5800/10001 Done, mean position loss: 20.624732797145846\n",
      "Training NF1:  57%|█████████▋       | 5692/10001 [2:45:06<2:02:53,  1.71s/batch]Batch 5700/10001 Done, mean position loss: 20.747992818355563\n",
      "Training NF1:  58%|█████████▊       | 5777/10001 [2:45:10<2:03:14,  1.75s/batch]Batch 5600/10001 Done, mean position loss: 20.695857038497927\n",
      "Training NF1:  57%|█████████▊       | 5742/10001 [2:45:12<1:57:51,  1.66s/batch]Batch 5800/10001 Done, mean position loss: 20.477123148441315\n",
      "Training NF1:  57%|█████████▋       | 5667/10001 [2:45:14<2:12:39,  1.84s/batch]Batch 5700/10001 Done, mean position loss: 21.053599343299865\n",
      "Training NF1:  57%|█████████▋       | 5732/10001 [2:45:17<2:12:13,  1.86s/batch]Batch 5700/10001 Done, mean position loss: 20.46262731552124\n",
      "Training NF1:  58%|█████████▊       | 5797/10001 [2:45:21<1:58:37,  1.69s/batch]Batch 5700/10001 Done, mean position loss: 20.380270247459414\n",
      "Training NF1:  58%|█████████▊       | 5771/10001 [2:45:25<2:20:03,  1.99s/batch]Batch 5700/10001 Done, mean position loss: 20.756237087249755\n",
      "Training NF1:  57%|█████████▋       | 5694/10001 [2:45:28<2:13:53,  1.87s/batch]Batch 5800/10001 Done, mean position loss: 20.521827418804165\n",
      "Training NF1:  57%|█████████▋       | 5719/10001 [2:45:39<2:06:03,  1.77s/batch]Batch 5700/10001 Done, mean position loss: 21.574789345264435\n",
      "Training NF1:  58%|█████████▊       | 5759/10001 [2:45:40<1:49:04,  1.54s/batch]Batch 5800/10001 Done, mean position loss: 21.00301221847534\n",
      "Training NF1:  57%|█████████▋       | 5732/10001 [2:45:43<1:54:08,  1.60s/batch]Batch 5700/10001 Done, mean position loss: 21.122236580848693\n",
      "Training NF1:  58%|█████████▊       | 5753/10001 [2:45:54<1:46:13,  1.50s/batch]Batch 5800/10001 Done, mean position loss: 21.044733927249908\n",
      "Training NF1:  58%|█████████▊       | 5798/10001 [2:46:07<2:00:35,  1.72s/batch]Batch 5800/10001 Done, mean position loss: 20.52497866153717\n",
      "Training NF1:  58%|█████████▊       | 5764/10001 [2:46:14<2:21:20,  2.00s/batch]Batch 5800/10001 Done, mean position loss: 20.482721531391142\n",
      "Training NF1:  57%|█████████▊       | 5739/10001 [2:46:15<2:07:54,  1.80s/batch]Batch 5700/10001 Done, mean position loss: 20.95714411497116\n",
      "Training NF1:  57%|█████████▊       | 5737/10001 [2:46:16<2:09:21,  1.82s/batch]Batch 5700/10001 Done, mean position loss: 20.357411470413204\n",
      "Training NF1:  58%|█████████▊       | 5779/10001 [2:46:18<2:01:08,  1.72s/batch]Batch 5800/10001 Done, mean position loss: 20.641150333881377\n",
      "Training NF1:  58%|█████████▊       | 5768/10001 [2:46:32<1:56:20,  1.65s/batch]Batch 5800/10001 Done, mean position loss: 20.757210159301756\n",
      "Training NF1:  57%|█████████▊       | 5742/10001 [2:46:37<2:00:51,  1.70s/batch]Batch 5800/10001 Done, mean position loss: 20.906319494247434\n",
      "Training NF1:  57%|█████████▋       | 5716/10001 [2:46:42<2:02:48,  1.72s/batch]Batch 5800/10001 Done, mean position loss: 20.89432223558426\n",
      "Training NF1:  58%|█████████▊       | 5756/10001 [2:46:46<1:53:49,  1.61s/batch]Batch 5800/10001 Done, mean position loss: 20.63060076713562\n",
      "Training NF1:  58%|█████████▊       | 5796/10001 [2:46:48<1:55:06,  1.64s/batch]Batch 5800/10001 Done, mean position loss: 20.66731009721756\n",
      "Training NF1:  58%|█████████▊       | 5782/10001 [2:46:57<2:07:58,  1.82s/batch]Batch 5800/10001 Done, mean position loss: 20.604555547237396\n",
      "Training NF1:  58%|█████████▊       | 5790/10001 [2:46:58<2:02:47,  1.75s/batch]Batch 5800/10001 Done, mean position loss: 20.80814919233322\n",
      "Training NF1:  59%|█████████▉       | 5852/10001 [2:47:00<2:30:36,  2.18s/batch]Batch 5800/10001 Done, mean position loss: 20.89569594144821\n",
      "Training NF1:  58%|█████████▊       | 5808/10001 [2:47:08<1:52:38,  1.61s/batch]Batch 5800/10001 Done, mean position loss: 20.657272422313692\n",
      "Training NF1:  58%|█████████▉       | 5842/10001 [2:47:15<2:23:53,  2.08s/batch]Batch 5800/10001 Done, mean position loss: 20.45041040658951\n",
      "Training NF1:  58%|█████████▊       | 5780/10001 [2:47:17<2:06:30,  1.80s/batch]Batch 5800/10001 Done, mean position loss: 20.712032251358032\n",
      "Training NF1:  58%|█████████▉       | 5834/10001 [2:47:17<2:14:46,  1.94s/batch]Batch 5800/10001 Done, mean position loss: 20.574556393623354\n",
      "Training NF1:  58%|█████████▊       | 5809/10001 [2:47:28<1:54:16,  1.64s/batch]Batch 5700/10001 Done, mean position loss: 20.646384179592133\n",
      "Training NF1:  58%|█████████▊       | 5774/10001 [2:47:30<1:47:13,  1.52s/batch]Batch 5800/10001 Done, mean position loss: 20.382503118515015\n",
      "Training NF1:  58%|█████████▊       | 5775/10001 [2:47:31<1:39:58,  1.42s/batch]Batch 5800/10001 Done, mean position loss: 20.75409187793732\n",
      "Training NF1:  59%|█████████▉       | 5865/10001 [2:47:34<1:56:25,  1.69s/batch]Batch 5900/10001 Done, mean position loss: 20.512046353816984\n",
      "Training NF1:  57%|█████████▋       | 5682/10001 [2:47:36<2:04:42,  1.73s/batch]Batch 5800/10001 Done, mean position loss: 20.758180546760556\n",
      "Training NF1:  58%|█████████▉       | 5849/10001 [2:47:37<1:43:13,  1.49s/batch]Batch 5800/10001 Done, mean position loss: 20.684526855945588\n",
      "Training NF1:  59%|██████████       | 5889/10001 [2:47:42<1:57:34,  1.72s/batch]Batch 5800/10001 Done, mean position loss: 20.5502387881279\n",
      "Training NF1:  58%|█████████▊       | 5808/10001 [2:47:48<1:47:53,  1.54s/batch]Batch 5800/10001 Done, mean position loss: 20.825291221141818\n",
      "Training NF1:  58%|█████████▊       | 5799/10001 [2:47:52<1:58:31,  1.69s/batch]Batch 5800/10001 Done, mean position loss: 21.077926416397094\n",
      "Training NF1:  58%|█████████▉       | 5840/10001 [2:47:55<1:51:08,  1.60s/batch]Batch 5800/10001 Done, mean position loss: 20.97828617095947\n",
      "Training NF1:  57%|█████████▋       | 5697/10001 [2:48:01<2:16:27,  1.90s/batch]Batch 5900/10001 Done, mean position loss: 20.61324080467224\n",
      "Training NF1:  59%|█████████▉       | 5870/10001 [2:48:03<1:55:34,  1.68s/batch]Batch 5900/10001 Done, mean position loss: 20.474228096008304\n",
      "Training NF1:  58%|█████████▉       | 5834/10001 [2:48:06<1:44:25,  1.50s/batch]Batch 5800/10001 Done, mean position loss: 21.073071365356444\n",
      "Training NF1:  58%|█████████▊       | 5763/10001 [2:48:07<2:06:41,  1.79s/batch]Batch 5800/10001 Done, mean position loss: 20.7635622882843\n",
      "Training NF1:  58%|█████████▉       | 5847/10001 [2:48:07<2:09:26,  1.87s/batch]Batch 5700/10001 Done, mean position loss: 20.681182458400727\n",
      "Training NF1:  59%|█████████▉       | 5861/10001 [2:48:14<2:04:36,  1.81s/batch]Batch 5800/10001 Done, mean position loss: 20.7433665561676\n",
      "Training NF1:  58%|█████████▉       | 5814/10001 [2:48:15<2:05:22,  1.80s/batch]Batch 5800/10001 Done, mean position loss: 20.443813173770906\n",
      "Training NF1:  58%|█████████▉       | 5849/10001 [2:48:18<1:45:09,  1.52s/batch]Batch 5800/10001 Done, mean position loss: 20.379375047683716\n",
      "Training NF1:  58%|█████████▊       | 5791/10001 [2:48:25<2:10:12,  1.86s/batch]Batch 5900/10001 Done, mean position loss: 20.528945155143738\n",
      "Training NF1:  59%|██████████       | 5922/10001 [2:48:38<1:57:34,  1.73s/batch]Batch 5900/10001 Done, mean position loss: 20.992518289089205\n",
      "Training NF1:  59%|█████████▉       | 5863/10001 [2:48:42<2:06:05,  1.83s/batch]Batch 5800/10001 Done, mean position loss: 21.09829010486603\n",
      "Training NF1:  59%|█████████▉       | 5860/10001 [2:48:44<1:44:51,  1.52s/batch]Batch 5900/10001 Done, mean position loss: 21.012720761299132\n",
      "Training NF1:  59%|██████████       | 5942/10001 [2:48:45<1:47:33,  1.59s/batch]Batch 5800/10001 Done, mean position loss: 21.597524065971378\n",
      "Training NF1:  58%|█████████▉       | 5849/10001 [2:49:00<2:00:00,  1.73s/batch]Batch 5900/10001 Done, mean position loss: 20.53175536870956\n",
      "Training NF1:  60%|██████████       | 5954/10001 [2:49:07<1:58:48,  1.76s/batch]Batch 5900/10001 Done, mean position loss: 20.481133966445924\n",
      "Training NF1:  59%|█████████▉       | 5880/10001 [2:49:12<2:13:42,  1.95s/batch]Batch 5800/10001 Done, mean position loss: 20.358479635715483\n",
      "Training NF1:  59%|██████████       | 5943/10001 [2:49:13<1:54:32,  1.69s/batch]Batch 5800/10001 Done, mean position loss: 20.961045248508455\n",
      "Training NF1:  59%|█████████▉       | 5858/10001 [2:49:16<2:19:41,  2.02s/batch]Batch 5900/10001 Done, mean position loss: 20.654647274017336\n",
      "Training NF1:  58%|█████████▉       | 5844/10001 [2:49:25<1:58:23,  1.71s/batch]Batch 5900/10001 Done, mean position loss: 20.75456005334854\n",
      "Training NF1:  60%|██████████       | 5954/10001 [2:49:32<2:01:45,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 20.894919643402098\n",
      "Training NF1:  59%|█████████▉       | 5868/10001 [2:49:34<1:59:23,  1.73s/batch]Batch 5900/10001 Done, mean position loss: 20.896537327766417\n",
      "Training NF1:  59%|██████████       | 5908/10001 [2:49:44<2:05:53,  1.85s/batch]Batch 5900/10001 Done, mean position loss: 20.647394986152648\n",
      "Training NF1:  58%|█████████▉       | 5838/10001 [2:49:48<2:02:56,  1.77s/batch]Batch 5900/10001 Done, mean position loss: 20.790100617408754\n",
      "Training NF1:  59%|█████████▉       | 5865/10001 [2:49:49<1:56:01,  1.68s/batch]Batch 5900/10001 Done, mean position loss: 20.66876819372177\n",
      "Training NF1:  59%|██████████       | 5890/10001 [2:49:51<2:11:09,  1.91s/batch]Batch 5900/10001 Done, mean position loss: 20.605836477279663\n",
      "Training NF1:  59%|█████████▉       | 5878/10001 [2:50:00<2:17:17,  2.00s/batch]Batch 5900/10001 Done, mean position loss: 20.896034846305845\n",
      "Training NF1:  59%|█████████▉       | 5860/10001 [2:50:03<2:02:55,  1.78s/batch]Batch 5900/10001 Done, mean position loss: 20.65216889858246\n",
      "Training NF1:  59%|█████████▉       | 5871/10001 [2:50:07<1:44:49,  1.52s/batch]Batch 5900/10001 Done, mean position loss: 20.705351457595825\n",
      "Training NF1:  59%|█████████▉       | 5865/10001 [2:50:13<2:05:20,  1.82s/batch]Batch 5900/10001 Done, mean position loss: 20.460088908672333\n",
      "Training NF1:  59%|██████████       | 5887/10001 [2:50:16<1:57:34,  1.71s/batch]Batch 5900/10001 Done, mean position loss: 20.557645189762113\n",
      "Training NF1:  59%|██████████       | 5908/10001 [2:50:24<2:00:56,  1.77s/batch]Batch 5800/10001 Done, mean position loss: 20.63043029785156\n",
      "Training NF1:  59%|██████████       | 5922/10001 [2:50:24<2:06:03,  1.85s/batch]Batch 5900/10001 Done, mean position loss: 20.745674760341643\n",
      "Training NF1:  59%|██████████       | 5917/10001 [2:50:28<2:05:57,  1.85s/batch]Batch 6000/10001 Done, mean position loss: 20.49560467481613\n",
      "Training NF1:  59%|██████████       | 5944/10001 [2:50:28<2:02:51,  1.82s/batch]Batch 5900/10001 Done, mean position loss: 20.402953915596008\n",
      "Training NF1:  59%|██████████       | 5946/10001 [2:50:31<1:50:04,  1.63s/batch]Batch 5900/10001 Done, mean position loss: 20.698179295063017\n",
      "Training NF1:  59%|██████████       | 5898/10001 [2:50:35<1:52:43,  1.65s/batch]Batch 5900/10001 Done, mean position loss: 20.736047151088716\n",
      "Training NF1:  59%|█████████▉       | 5880/10001 [2:50:39<1:58:42,  1.73s/batch]Batch 5900/10001 Done, mean position loss: 20.813075361251833\n",
      "Training NF1:  59%|██████████       | 5912/10001 [2:50:39<1:21:18,  1.19s/batch]Batch 5900/10001 Done, mean position loss: 20.53375272512436\n",
      "Training NF1:  59%|██████████       | 5920/10001 [2:50:50<1:44:42,  1.54s/batch]Batch 5900/10001 Done, mean position loss: 20.998623194694517\n",
      "Batch 5900/10001 Done, mean position loss: 21.082078719139098\n",
      "Training NF1:  59%|██████████       | 5895/10001 [2:50:50<1:45:54,  1.55s/batch]Batch 6000/10001 Done, mean position loss: 20.613905289173125\n",
      "Training NF1:  59%|██████████       | 5903/10001 [2:50:53<1:42:21,  1.50s/batch]Batch 6000/10001 Done, mean position loss: 20.474503786563872\n",
      "Training NF1:  59%|██████████       | 5934/10001 [2:51:00<1:54:17,  1.69s/batch]Batch 5900/10001 Done, mean position loss: 21.079931004047396\n",
      "Training NF1:  59%|██████████       | 5917/10001 [2:51:00<2:04:01,  1.82s/batch]Batch 5800/10001 Done, mean position loss: 20.68775387763977\n",
      "Training NF1:  59%|██████████       | 5929/10001 [2:51:02<2:20:02,  2.06s/batch]Batch 5900/10001 Done, mean position loss: 20.721073279380796\n",
      "Training NF1:  60%|██████████▏      | 6008/10001 [2:51:03<1:58:39,  1.78s/batch]Batch 5900/10001 Done, mean position loss: 20.785128040313722\n",
      "Training NF1:  60%|██████████▏      | 5988/10001 [2:51:10<2:14:12,  2.01s/batch]Batch 5900/10001 Done, mean position loss: 20.45564812660217\n",
      "Training NF1:  59%|██████████       | 5890/10001 [2:51:16<1:55:55,  1.69s/batch]Batch 5900/10001 Done, mean position loss: 20.388823742866517\n",
      "Training NF1:  60%|██████████▏      | 6016/10001 [2:51:22<2:01:56,  1.84s/batch]Batch 6000/10001 Done, mean position loss: 20.508545763492585\n",
      "Training NF1:  59%|██████████       | 5932/10001 [2:51:36<2:01:33,  1.79s/batch]Batch 5900/10001 Done, mean position loss: 21.10532305955887\n",
      "Training NF1:  60%|██████████▏      | 5978/10001 [2:51:36<2:23:18,  2.14s/batch]Batch 6000/10001 Done, mean position loss: 21.00636399745941\n",
      "Training NF1:  60%|██████████▏      | 6003/10001 [2:51:39<1:56:46,  1.75s/batch]Batch 6000/10001 Done, mean position loss: 21.047263464927674\n",
      "Training NF1:  60%|██████████       | 5954/10001 [2:51:42<2:09:52,  1.93s/batch]Batch 5900/10001 Done, mean position loss: 21.59075349807739\n",
      "Training NF1:  60%|██████████▏      | 5997/10001 [2:51:52<1:50:31,  1.66s/batch]Batch 6000/10001 Done, mean position loss: 20.530863823890684\n",
      "Training NF1:  60%|██████████▏      | 5965/10001 [2:52:00<1:44:17,  1.55s/batch]Batch 6000/10001 Done, mean position loss: 20.487192668914794\n",
      "Training NF1:  60%|██████████▏      | 5988/10001 [2:52:03<2:03:00,  1.84s/batch]Batch 6000/10001 Done, mean position loss: 20.641977689266206\n",
      "Training NF1:  60%|██████████▎      | 6031/10001 [2:52:14<1:59:51,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 20.343805389404295\n",
      "Training NF1:  59%|█████████▉       | 5862/10001 [2:52:15<2:26:51,  2.13s/batch]Batch 5900/10001 Done, mean position loss: 20.950040397644045\n",
      "Training NF1:  59%|██████████       | 5921/10001 [2:52:18<2:20:13,  2.06s/batch]Batch 6000/10001 Done, mean position loss: 20.768361508846283\n",
      "Training NF1:  59%|██████████       | 5943/10001 [2:52:25<1:58:47,  1.76s/batch]Batch 6000/10001 Done, mean position loss: 20.870901896953583\n",
      "Training NF1:  60%|██████████▏      | 5992/10001 [2:52:25<2:11:34,  1.97s/batch]Batch 6000/10001 Done, mean position loss: 20.89355170726776\n",
      "Training NF1:  60%|██████████▏      | 6016/10001 [2:52:43<1:43:03,  1.55s/batch]Batch 6000/10001 Done, mean position loss: 20.62135454416275\n",
      "Training NF1:  60%|██████████▏      | 6012/10001 [2:52:45<2:01:26,  1.83s/batch]Batch 6000/10001 Done, mean position loss: 20.659173159599305\n",
      "Training NF1:  60%|██████████       | 5952/10001 [2:52:46<2:05:24,  1.86s/batch]Batch 6000/10001 Done, mean position loss: 20.788373391628266\n",
      "Training NF1:  60%|██████████▎      | 6033/10001 [2:52:54<1:46:48,  1.62s/batch]Batch 6000/10001 Done, mean position loss: 20.88541956424713\n",
      "Training NF1:  61%|██████████▎      | 6069/10001 [2:52:58<1:58:11,  1.80s/batch]Batch 6000/10001 Done, mean position loss: 20.6539359998703\n",
      "Training NF1:  60%|██████████▎      | 6037/10001 [2:52:59<1:59:33,  1.81s/batch]Batch 6000/10001 Done, mean position loss: 20.6029076385498\n",
      "Training NF1:  60%|██████████▏      | 6010/10001 [2:53:02<2:04:09,  1.87s/batch]Batch 6000/10001 Done, mean position loss: 20.710666797161103\n",
      "Training NF1:  60%|██████████▏      | 5996/10001 [2:53:13<1:59:58,  1.80s/batch]Batch 6000/10001 Done, mean position loss: 20.4496627163887\n",
      "Training NF1:  60%|██████████▏      | 5997/10001 [2:53:15<1:53:15,  1.70s/batch]Batch 6000/10001 Done, mean position loss: 20.76300623893738\n",
      "Training NF1:  60%|██████████▏      | 5959/10001 [2:53:17<2:14:50,  2.00s/batch]Batch 6000/10001 Done, mean position loss: 20.563561012744902\n",
      "Training NF1:  60%|██████████▏      | 6011/10001 [2:53:19<1:51:46,  1.68s/batch]Batch 6100/10001 Done, mean position loss: 20.485367090702056\n",
      "Training NF1:  60%|██████████▎      | 6050/10001 [2:53:22<1:39:14,  1.51s/batch]Batch 6000/10001 Done, mean position loss: 20.73328027009964\n",
      "Training NF1:  59%|█████████▉       | 5882/10001 [2:53:26<2:11:19,  1.91s/batch]Batch 6000/10001 Done, mean position loss: 20.692671854496\n",
      "Training NF1:  60%|██████████▏      | 6025/10001 [2:53:25<1:40:08,  1.51s/batch]Batch 6000/10001 Done, mean position loss: 20.38209366798401\n",
      "Training NF1:  61%|██████████▎      | 6073/10001 [2:53:27<1:52:25,  1.72s/batch]Batch 5900/10001 Done, mean position loss: 20.608652815818786\n",
      "Training NF1:  61%|██████████▎      | 6061/10001 [2:53:35<1:49:34,  1.67s/batch]Batch 6000/10001 Done, mean position loss: 20.54648715019226\n",
      "Training NF1:  60%|██████████▏      | 5990/10001 [2:53:37<1:48:56,  1.63s/batch]Batch 6000/10001 Done, mean position loss: 20.80588748931885\n",
      "Training NF1:  60%|██████████       | 5953/10001 [2:53:43<1:45:01,  1.56s/batch]Batch 6000/10001 Done, mean position loss: 21.08080366611481\n",
      "Training NF1:  59%|██████████       | 5911/10001 [2:53:44<2:03:57,  1.82s/batch]Batch 6000/10001 Done, mean position loss: 20.989477217197418\n",
      "Training NF1:  60%|██████████       | 5954/10001 [2:53:46<2:23:41,  2.13s/batch]Batch 6100/10001 Done, mean position loss: 20.600547325611114\n",
      "Training NF1:  59%|██████████       | 5915/10001 [2:53:51<1:59:03,  1.75s/batch]Batch 6100/10001 Done, mean position loss: 20.480362734794618\n",
      "Training NF1:  60%|██████████▏      | 5996/10001 [2:53:57<2:01:28,  1.82s/batch]Batch 6000/10001 Done, mean position loss: 20.73497120141983\n",
      "Training NF1:  60%|██████████▏      | 6027/10001 [2:53:58<1:52:19,  1.70s/batch]Batch 5900/10001 Done, mean position loss: 20.66945646286011\n",
      "Training NF1:  61%|██████████▍      | 6105/10001 [2:53:58<1:46:15,  1.64s/batch]Batch 6000/10001 Done, mean position loss: 21.091933012008667\n",
      "Training NF1:  61%|██████████▎      | 6095/10001 [2:54:06<1:54:43,  1.76s/batch]Batch 6000/10001 Done, mean position loss: 20.766977741718293\n",
      "Training NF1:  61%|██████████▎      | 6051/10001 [2:54:11<1:47:17,  1.63s/batch]Batch 6000/10001 Done, mean position loss: 20.455182049274445\n",
      "Training NF1:  60%|██████████▎      | 6031/10001 [2:54:15<1:49:34,  1.66s/batch]Batch 6000/10001 Done, mean position loss: 20.378946232795716\n",
      "Training NF1:  60%|██████████▎      | 6032/10001 [2:54:17<1:50:59,  1.68s/batch]Batch 6100/10001 Done, mean position loss: 20.513127834796904\n",
      "Training NF1:  61%|██████████▎      | 6099/10001 [2:54:29<1:58:35,  1.82s/batch]Batch 6100/10001 Done, mean position loss: 21.003265120983123\n",
      "Training NF1:  59%|██████████       | 5937/10001 [2:54:32<2:04:24,  1.84s/batch]Batch 6100/10001 Done, mean position loss: 21.030615653991703\n",
      "Training NF1:  61%|██████████▎      | 6077/10001 [2:54:33<1:51:59,  1.71s/batch]Batch 6000/10001 Done, mean position loss: 21.109057970046997\n",
      "Training NF1:  61%|██████████▎      | 6099/10001 [2:54:38<1:59:30,  1.84s/batch]Batch 6000/10001 Done, mean position loss: 21.583834347724917\n",
      "Training NF1:  61%|██████████▎      | 6062/10001 [2:54:43<1:57:47,  1.79s/batch]Batch 6100/10001 Done, mean position loss: 20.52693156480789\n",
      "Training NF1:  60%|██████████▎      | 6039/10001 [2:54:48<1:50:04,  1.67s/batch]Batch 6100/10001 Done, mean position loss: 20.496058027744294\n",
      "Training NF1:  61%|██████████▍      | 6112/10001 [2:54:49<1:38:43,  1.52s/batch]Batch 6100/10001 Done, mean position loss: 20.64789695262909\n",
      "Training NF1:  61%|██████████▎      | 6075/10001 [2:55:10<1:59:34,  1.83s/batch]Batch 6100/10001 Done, mean position loss: 20.76587825059891\n",
      "Training NF1:  61%|██████████▍      | 6124/10001 [2:55:09<1:48:36,  1.68s/batch]Batch 6000/10001 Done, mean position loss: 20.332068581581115\n",
      "Training NF1:  60%|██████████▏      | 6001/10001 [2:55:09<1:52:27,  1.69s/batch]Batch 6000/10001 Done, mean position loss: 20.94499259710312\n",
      "Training NF1:  60%|██████████▏      | 6022/10001 [2:55:14<1:43:16,  1.56s/batch]Batch 6100/10001 Done, mean position loss: 20.873072607517244\n",
      "Training NF1:  61%|██████████▍      | 6131/10001 [2:55:20<1:47:52,  1.67s/batch]Batch 6100/10001 Done, mean position loss: 20.87353529214859\n",
      "Training NF1:  62%|██████████▍      | 6161/10001 [2:55:31<1:56:00,  1.81s/batch]Batch 6100/10001 Done, mean position loss: 20.625247271060942\n",
      "Training NF1:  61%|██████████▎      | 6077/10001 [2:55:38<2:06:18,  1.93s/batch]Batch 6100/10001 Done, mean position loss: 20.79676022768021\n",
      "Training NF1:  61%|██████████▍      | 6130/10001 [2:55:37<1:47:34,  1.67s/batch]Batch 6100/10001 Done, mean position loss: 20.657427027225495\n",
      "Training NF1:  61%|██████████▍      | 6114/10001 [2:55:43<1:49:46,  1.69s/batch]Batch 6100/10001 Done, mean position loss: 20.89148639202118\n",
      "Training NF1:  61%|██████████▎      | 6090/10001 [2:55:49<2:00:05,  1.84s/batch]Batch 6100/10001 Done, mean position loss: 20.605305364131926\n",
      "Training NF1:  61%|██████████▍      | 6119/10001 [2:55:51<1:51:14,  1.72s/batch]Batch 6100/10001 Done, mean position loss: 20.69747071027756\n",
      "Training NF1:  61%|██████████▎      | 6060/10001 [2:55:56<2:08:44,  1.96s/batch]Batch 6100/10001 Done, mean position loss: 20.651047277450562\n",
      "Training NF1:  61%|██████████▍      | 6119/10001 [2:56:07<1:42:37,  1.59s/batch]Batch 6100/10001 Done, mean position loss: 20.451662395000458\n",
      "Training NF1:  60%|██████████▎      | 6036/10001 [2:56:10<1:49:24,  1.66s/batch]Batch 6100/10001 Done, mean position loss: 20.55466319799423\n",
      "Training NF1:  61%|██████████▎      | 6069/10001 [2:56:10<1:38:47,  1.51s/batch]Batch 6100/10001 Done, mean position loss: 20.74547362804413\n",
      "Training NF1:  60%|██████████▏      | 5977/10001 [2:56:14<2:03:27,  1.84s/batch]Batch 6100/10001 Done, mean position loss: 20.739018824100494\n",
      "Training NF1:  60%|██████████▏      | 5999/10001 [2:56:15<2:01:45,  1.83s/batch]Batch 6100/10001 Done, mean position loss: 20.685756492614743\n",
      "Training NF1:  61%|██████████▎      | 6070/10001 [2:56:15<1:56:01,  1.77s/batch]Batch 6200/10001 Done, mean position loss: 20.493549885749815\n",
      "Training NF1:  60%|██████████▏      | 5980/10001 [2:56:19<2:01:28,  1.81s/batch]Batch 6000/10001 Done, mean position loss: 20.6133761382103\n",
      "Training NF1:  61%|██████████▎      | 6096/10001 [2:56:21<1:43:24,  1.59s/batch]Batch 6100/10001 Done, mean position loss: 20.37954314947128\n",
      "Training NF1:  61%|██████████▍      | 6105/10001 [2:56:28<1:52:28,  1.73s/batch]Batch 6100/10001 Done, mean position loss: 20.82986891269684\n",
      "Training NF1:  61%|██████████▎      | 6079/10001 [2:56:29<1:38:42,  1.51s/batch]Batch 6100/10001 Done, mean position loss: 20.54428212404251\n",
      "Training NF1:  61%|██████████▎      | 6083/10001 [2:56:36<2:19:02,  2.13s/batch]Batch 6100/10001 Done, mean position loss: 21.07941596031189\n",
      "Training NF1:  62%|██████████▌      | 6182/10001 [2:56:38<1:42:36,  1.61s/batch]Batch 6200/10001 Done, mean position loss: 20.614698605537413\n",
      "Training NF1:  61%|██████████▎      | 6090/10001 [2:56:38<1:45:41,  1.62s/batch]Batch 6100/10001 Done, mean position loss: 20.967019553184507\n",
      "Training NF1:  62%|██████████▌      | 6186/10001 [2:56:44<1:39:10,  1.56s/batch]Batch 6200/10001 Done, mean position loss: 20.486187508106234\n",
      "Training NF1:  61%|██████████▎      | 6059/10001 [2:56:54<1:57:31,  1.79s/batch]Batch 6100/10001 Done, mean position loss: 20.74230524539948\n",
      "Training NF1:  62%|██████████▍      | 6177/10001 [2:56:53<1:50:32,  1.73s/batch]Batch 6100/10001 Done, mean position loss: 21.042878558635714\n",
      "Training NF1:  62%|██████████▍      | 6158/10001 [2:57:00<1:42:50,  1.61s/batch]Batch 6100/10001 Done, mean position loss: 20.749889805316926\n",
      "Training NF1:  62%|██████████▌      | 6211/10001 [2:57:00<1:47:20,  1.70s/batch]Batch 6000/10001 Done, mean position loss: 20.677017288208006\n",
      "Training NF1:  61%|██████████▍      | 6119/10001 [2:57:09<2:00:37,  1.86s/batch]Batch 6100/10001 Done, mean position loss: 20.386351935863495\n",
      "Training NF1:  60%|██████████▏      | 6006/10001 [2:57:10<2:05:26,  1.88s/batch]Batch 6100/10001 Done, mean position loss: 20.454864728450772\n",
      "Training NF1:  61%|██████████▎      | 6070/10001 [2:57:12<2:21:47,  2.16s/batch]Batch 6200/10001 Done, mean position loss: 20.509571793079374\n",
      "Training NF1:  61%|██████████▍      | 6123/10001 [2:57:21<2:15:05,  2.09s/batch]Batch 6200/10001 Done, mean position loss: 21.00689731359482\n",
      "Training NF1:  61%|██████████▎      | 6100/10001 [2:57:21<1:55:05,  1.77s/batch]Batch 6200/10001 Done, mean position loss: 21.031957156658173\n",
      "Training NF1:  61%|██████████▍      | 6109/10001 [2:57:22<1:44:39,  1.61s/batch]Batch 6100/10001 Done, mean position loss: 21.100942649841308\n",
      "Training NF1:  62%|██████████▍      | 6161/10001 [2:57:33<1:46:31,  1.66s/batch]Batch 6100/10001 Done, mean position loss: 21.619850072860714\n",
      "Training NF1:  61%|██████████▍      | 6138/10001 [2:57:35<1:59:20,  1.85s/batch]Batch 6200/10001 Done, mean position loss: 20.521387186050415\n",
      "Training NF1:  62%|██████████▍      | 6169/10001 [2:57:42<2:17:55,  2.16s/batch]Batch 6200/10001 Done, mean position loss: 20.635606684684753\n",
      "Training NF1:  62%|██████████▌      | 6222/10001 [2:57:50<1:48:57,  1.73s/batch]Batch 6200/10001 Done, mean position loss: 20.490374698638917\n",
      "Training NF1:  61%|██████████▍      | 6132/10001 [2:57:55<1:55:10,  1.79s/batch]Batch 6200/10001 Done, mean position loss: 20.739005980491637\n",
      "Training NF1:  62%|██████████▍      | 6164/10001 [2:58:08<1:57:39,  1.84s/batch]Batch 6100/10001 Done, mean position loss: 20.339621081352234\n",
      "Training NF1:  62%|██████████▍      | 6177/10001 [2:58:10<1:58:49,  1.86s/batch]Batch 6100/10001 Done, mean position loss: 20.96119671344757\n",
      "Training NF1:  62%|██████████▌      | 6214/10001 [2:58:13<1:51:38,  1.77s/batch]Batch 6200/10001 Done, mean position loss: 20.870051221847532\n",
      "Training NF1:  61%|██████████▍      | 6107/10001 [2:58:17<1:47:34,  1.66s/batch]Batch 6200/10001 Done, mean position loss: 20.885406069755554\n",
      "Training NF1:  62%|██████████▌      | 6218/10001 [2:58:21<2:09:45,  2.06s/batch]Batch 6200/10001 Done, mean position loss: 20.631648223400113\n",
      "Training NF1:  61%|██████████▍      | 6114/10001 [2:58:30<1:55:07,  1.78s/batch]Batch 6200/10001 Done, mean position loss: 20.77645126819611\n",
      "Training NF1:  62%|██████████▌      | 6235/10001 [2:58:37<2:02:52,  1.96s/batch]Batch 6200/10001 Done, mean position loss: 20.665423221588135\n",
      "Training NF1:  62%|██████████▌      | 6229/10001 [2:58:42<2:03:00,  1.96s/batch]Batch 6200/10001 Done, mean position loss: 20.872477025985717\n",
      "Training NF1:  63%|██████████▋      | 6251/10001 [2:58:45<1:46:55,  1.71s/batch]Batch 6200/10001 Done, mean position loss: 20.600395851135254\n",
      "Training NF1:  62%|██████████▍      | 6165/10001 [2:58:46<1:46:38,  1.67s/batch]Batch 6200/10001 Done, mean position loss: 20.694106616973876\n",
      "Training NF1:  63%|██████████▋      | 6252/10001 [2:58:51<1:54:22,  1.83s/batch]Batch 6200/10001 Done, mean position loss: 20.657051525115968\n",
      "Training NF1:  62%|██████████▌      | 6210/10001 [2:59:02<1:57:22,  1.86s/batch]Batch 6200/10001 Done, mean position loss: 20.55320009231567\n",
      "Training NF1:  63%|██████████▋      | 6251/10001 [2:59:04<1:44:11,  1.67s/batch]Batch 6200/10001 Done, mean position loss: 20.74937767744064\n",
      "Training NF1:  62%|██████████▌      | 6215/10001 [2:59:07<1:33:30,  1.48s/batch]Batch 6200/10001 Done, mean position loss: 20.740288228988646\n",
      "Training NF1:  62%|██████████▌      | 6243/10001 [2:59:08<1:48:45,  1.74s/batch]Batch 6200/10001 Done, mean position loss: 20.435842690467833\n",
      "Training NF1:  62%|██████████▌      | 6230/10001 [2:59:10<1:52:52,  1.80s/batch]Batch 6300/10001 Done, mean position loss: 20.49801844596863\n",
      "Training NF1:  61%|██████████▎      | 6099/10001 [2:59:13<1:48:54,  1.67s/batch]Batch 6200/10001 Done, mean position loss: 20.683585839271547\n",
      "Training NF1:  62%|██████████▌      | 6206/10001 [2:59:16<1:57:34,  1.86s/batch]Batch 6100/10001 Done, mean position loss: 20.614089438915254\n",
      "Training NF1:  63%|██████████▋      | 6267/10001 [2:59:17<1:56:22,  1.87s/batch]Batch 6200/10001 Done, mean position loss: 20.37568758010864\n",
      "Training NF1:  62%|██████████▌      | 6182/10001 [2:59:24<1:55:56,  1.82s/batch]Batch 6200/10001 Done, mean position loss: 20.544981784820557\n",
      "Training NF1:  62%|██████████▍      | 6168/10001 [2:59:26<1:42:20,  1.60s/batch]Batch 6200/10001 Done, mean position loss: 20.818606588840485\n",
      "Training NF1:  62%|██████████▍      | 6165/10001 [2:59:32<1:50:21,  1.73s/batch]Batch 6200/10001 Done, mean position loss: 21.07594487667084\n",
      "Training NF1:  62%|██████████▌      | 6216/10001 [2:59:34<1:46:26,  1.69s/batch]Batch 6300/10001 Done, mean position loss: 20.587457780838015\n",
      "Training NF1:  62%|██████████▌      | 6217/10001 [2:59:35<1:51:03,  1.76s/batch]Batch 6300/10001 Done, mean position loss: 20.483351287841796\n",
      "Training NF1:  63%|██████████▋      | 6253/10001 [2:59:41<1:38:36,  1.58s/batch]Batch 6200/10001 Done, mean position loss: 20.984042336940764\n",
      "Training NF1:  63%|██████████▋      | 6284/10001 [2:59:46<1:42:57,  1.66s/batch]Batch 6200/10001 Done, mean position loss: 21.06427163362503\n",
      "Training NF1:  62%|██████████▌      | 6227/10001 [2:59:48<2:01:12,  1.93s/batch]Batch 6200/10001 Done, mean position loss: 20.739280867576596\n",
      "Training NF1:  62%|██████████▌      | 6207/10001 [2:59:58<2:05:36,  1.99s/batch]Batch 6200/10001 Done, mean position loss: 20.717264358997344\n",
      "Training NF1:  63%|██████████▋      | 6316/10001 [3:00:00<1:50:29,  1.80s/batch]Batch 6100/10001 Done, mean position loss: 20.676364803314208\n",
      "Training NF1:  63%|██████████▋      | 6255/10001 [3:00:02<1:56:26,  1.86s/batch]Batch 6200/10001 Done, mean position loss: 20.37704699277878\n",
      "Training NF1:  63%|██████████▋      | 6276/10001 [3:00:07<1:50:02,  1.77s/batch]Batch 6200/10001 Done, mean position loss: 20.466988415718077\n",
      "Training NF1:  63%|██████████▋      | 6265/10001 [3:00:16<2:14:29,  2.16s/batch]Batch 6300/10001 Done, mean position loss: 20.504359018802642\n",
      "Training NF1:  62%|██████████▌      | 6211/10001 [3:00:16<1:51:47,  1.77s/batch]Batch 6300/10001 Done, mean position loss: 21.01666221141815\n",
      "Training NF1:  62%|██████████▌      | 6179/10001 [3:00:20<1:49:35,  1.72s/batch]Batch 6200/10001 Done, mean position loss: 21.131176702976227\n",
      "Batch 6300/10001 Done, mean position loss: 20.990432450771333\n",
      "Training NF1:  62%|██████████▌      | 6238/10001 [3:00:31<1:45:12,  1.68s/batch]Batch 6300/10001 Done, mean position loss: 20.511353178024294\n",
      "Training NF1:  62%|██████████▌      | 6209/10001 [3:00:33<1:40:13,  1.59s/batch]Batch 6200/10001 Done, mean position loss: 21.593794937133787\n",
      "Training NF1:  63%|██████████▋      | 6292/10001 [3:00:37<1:50:07,  1.78s/batch]Batch 6300/10001 Done, mean position loss: 20.637422184944153\n",
      "Training NF1:  62%|██████████▌      | 6249/10001 [3:00:51<1:45:48,  1.69s/batch]Batch 6300/10001 Done, mean position loss: 20.727666251659393\n",
      "Training NF1:  62%|██████████▌      | 6191/10001 [3:00:52<1:56:50,  1.84s/batch]Batch 6300/10001 Done, mean position loss: 20.474383623600005\n",
      "Training NF1:  63%|██████████▋      | 6252/10001 [3:00:56<1:58:47,  1.90s/batch]Batch 6200/10001 Done, mean position loss: 20.331733434200288\n",
      "Training NF1:  63%|██████████▊      | 6329/10001 [3:01:06<1:48:20,  1.77s/batch]Batch 6300/10001 Done, mean position loss: 20.85391715288162\n",
      "Training NF1:  62%|██████████▌      | 6248/10001 [3:01:10<1:51:20,  1.78s/batch]Batch 6200/10001 Done, mean position loss: 20.937052192687986\n",
      "Training NF1:  63%|██████████▋      | 6275/10001 [3:01:14<1:50:03,  1.77s/batch]Batch 6300/10001 Done, mean position loss: 20.630687370300294\n",
      "Training NF1:  63%|██████████▋      | 6272/10001 [3:01:20<1:52:18,  1.81s/batch]Batch 6300/10001 Done, mean position loss: 20.769370601177215\n",
      "Training NF1:  63%|██████████▋      | 6258/10001 [3:01:22<1:37:38,  1.57s/batch]Batch 6300/10001 Done, mean position loss: 20.87704439163208\n",
      "Training NF1:  61%|██████████▍      | 6150/10001 [3:01:35<2:11:48,  2.05s/batch]Batch 6300/10001 Done, mean position loss: 20.635177714824675\n",
      "Training NF1:  63%|██████████▋      | 6295/10001 [3:01:42<1:48:56,  1.76s/batch]Batch 6300/10001 Done, mean position loss: 20.700461888313292\n",
      "Training NF1:  63%|██████████▋      | 6314/10001 [3:01:43<1:47:08,  1.74s/batch]Batch 6300/10001 Done, mean position loss: 20.585044219493867\n",
      "Training NF1:  63%|██████████▋      | 6284/10001 [3:01:45<1:56:20,  1.88s/batch]Batch 6300/10001 Done, mean position loss: 20.88403844356537\n",
      "Training NF1:  63%|██████████▋      | 6298/10001 [3:01:51<1:44:30,  1.69s/batch]Batch 6300/10001 Done, mean position loss: 20.662373297214508\n",
      "Training NF1:  64%|██████████▊      | 6395/10001 [3:01:56<2:09:04,  2.15s/batch]Batch 6300/10001 Done, mean position loss: 20.75165020942688\n",
      "Training NF1:  63%|██████████▋      | 6297/10001 [3:02:03<1:53:39,  1.84s/batch]Batch 6300/10001 Done, mean position loss: 20.55727462530136\n",
      "Training NF1:  62%|██████████▌      | 6196/10001 [3:02:05<2:01:44,  1.92s/batch]Batch 6300/10001 Done, mean position loss: 20.429374599456786\n",
      "Training NF1:  63%|██████████▋      | 6313/10001 [3:02:07<1:48:36,  1.77s/batch]Batch 6400/10001 Done, mean position loss: 20.474879324436188\n",
      "Training NF1:  63%|██████████▊      | 6330/10001 [3:02:11<1:30:58,  1.49s/batch]Batch 6300/10001 Done, mean position loss: 20.692536306381225\n",
      "Training NF1:  63%|██████████▋      | 6284/10001 [3:02:13<1:55:28,  1.86s/batch]Batch 6200/10001 Done, mean position loss: 20.61567728281021\n",
      "Training NF1:  63%|██████████▋      | 6314/10001 [3:02:15<1:56:22,  1.89s/batch]Batch 6300/10001 Done, mean position loss: 20.758682527542113\n",
      "Training NF1:  63%|██████████▋      | 6278/10001 [3:02:17<1:47:37,  1.73s/batch]Batch 6300/10001 Done, mean position loss: 20.546038131713864\n",
      "Training NF1:  63%|██████████▋      | 6320/10001 [3:02:18<2:00:47,  1.97s/batch]Batch 6300/10001 Done, mean position loss: 20.37675756931305\n",
      "Training NF1:  63%|██████████▋      | 6322/10001 [3:02:21<1:37:39,  1.59s/batch]Batch 6400/10001 Done, mean position loss: 20.59288400411606\n",
      "Training NF1:  63%|██████████▋      | 6308/10001 [3:02:25<1:39:02,  1.61s/batch]Batch 6300/10001 Done, mean position loss: 20.816917495727537\n",
      "Training NF1:  63%|██████████▋      | 6281/10001 [3:02:29<1:52:28,  1.81s/batch]Batch 6300/10001 Done, mean position loss: 21.07827399969101\n",
      "Training NF1:  63%|██████████▋      | 6317/10001 [3:02:31<1:52:19,  1.83s/batch]Batch 6400/10001 Done, mean position loss: 20.480086901187896\n",
      "Training NF1:  64%|██████████▊      | 6381/10001 [3:02:39<1:47:53,  1.79s/batch]Batch 6300/10001 Done, mean position loss: 20.985142815113065\n",
      "Training NF1:  64%|██████████▊      | 6383/10001 [3:02:42<1:44:01,  1.73s/batch]Batch 6300/10001 Done, mean position loss: 21.060752761363986\n",
      "Training NF1:  64%|██████████▊      | 6380/10001 [3:02:44<1:36:34,  1.60s/batch]Batch 6300/10001 Done, mean position loss: 20.749538233280184\n",
      "Training NF1:  63%|██████████▋      | 6269/10001 [3:02:56<1:50:25,  1.78s/batch]Batch 6300/10001 Done, mean position loss: 20.385776586532593\n",
      "Training NF1:  63%|██████████▊      | 6345/10001 [3:02:59<1:39:23,  1.63s/batch]Batch 6300/10001 Done, mean position loss: 20.711528849601745\n",
      "Training NF1:  64%|██████████▊      | 6392/10001 [3:03:03<1:39:37,  1.66s/batch]Batch 6300/10001 Done, mean position loss: 20.4650198173523\n",
      "Training NF1:  63%|██████████▊      | 6350/10001 [3:03:09<1:56:47,  1.92s/batch]Batch 6200/10001 Done, mean position loss: 20.6997789645195\n",
      "Training NF1:  64%|██████████▊      | 6370/10001 [3:03:14<1:41:31,  1.68s/batch]Batch 6400/10001 Done, mean position loss: 20.50806744813919\n",
      "Training NF1:  63%|██████████▋      | 6311/10001 [3:03:14<1:48:35,  1.77s/batch]Batch 6400/10001 Done, mean position loss: 20.984087424278258\n",
      "Training NF1:  64%|██████████▊      | 6351/10001 [3:03:15<2:00:57,  1.99s/batch]Batch 6300/10001 Done, mean position loss: 21.126028192043304\n",
      "Training NF1:  63%|██████████▊      | 6341/10001 [3:03:19<1:41:55,  1.67s/batch]Batch 6400/10001 Done, mean position loss: 20.516178278923036\n",
      "Training NF1:  63%|██████████▋      | 6313/10001 [3:03:19<1:45:49,  1.72s/batch]Batch 6400/10001 Done, mean position loss: 21.03773549079895\n",
      "Training NF1:  63%|██████████▊      | 6346/10001 [3:03:28<1:50:44,  1.82s/batch]Batch 6300/10001 Done, mean position loss: 21.553668460845948\n",
      "Training NF1:  63%|██████████▋      | 6324/10001 [3:03:36<1:48:26,  1.77s/batch]Batch 6400/10001 Done, mean position loss: 20.633807914257048\n",
      "Training NF1:  64%|██████████▉      | 6418/10001 [3:03:52<2:53:02,  2.90s/batch]Batch 6400/10001 Done, mean position loss: 20.470597114562988\n",
      "Training NF1:  63%|██████████▊      | 6348/10001 [3:03:55<3:17:20,  3.24s/batch]Batch 6400/10001 Done, mean position loss: 20.760449399948122\n",
      "Training NF1:  64%|██████████▊      | 6391/10001 [3:04:08<3:16:36,  3.27s/batch]Batch 6300/10001 Done, mean position loss: 20.34085896253586\n",
      "Training NF1:  64%|██████████▊      | 6393/10001 [3:04:14<3:05:33,  3.09s/batch]Batch 6400/10001 Done, mean position loss: 20.856750569343568\n",
      "Training NF1:  64%|██████████▊      | 6371/10001 [3:04:41<3:40:58,  3.65s/batch]Batch 6400/10001 Done, mean position loss: 20.775328056812285\n",
      "Training NF1:  64%|██████████▉      | 6435/10001 [3:04:42<3:26:24,  3.47s/batch]Batch 6400/10001 Done, mean position loss: 20.62749993085861\n",
      "Training NF1:  64%|██████████▊      | 6374/10001 [3:04:46<3:55:45,  3.90s/batch]Batch 6300/10001 Done, mean position loss: 20.97554792165756\n",
      "Training NF1:  64%|██████████▊      | 6370/10001 [3:04:50<3:37:22,  3.59s/batch]Batch 6400/10001 Done, mean position loss: 20.883787510395052\n",
      "Training NF1:  64%|██████████▊      | 6352/10001 [3:05:14<4:00:28,  3.95s/batch]Batch 6400/10001 Done, mean position loss: 20.65094077825546\n",
      "Training NF1:  64%|██████████▉      | 6450/10001 [3:05:41<3:33:09,  3.60s/batch]Batch 6400/10001 Done, mean position loss: 20.58396043777466\n",
      "Training NF1:  64%|██████████▉      | 6421/10001 [3:05:49<3:49:35,  3.85s/batch]Batch 6400/10001 Done, mean position loss: 20.866741430759433\n",
      "Training NF1:  64%|██████████▊      | 6378/10001 [3:05:50<4:04:56,  4.06s/batch]Batch 6400/10001 Done, mean position loss: 20.681522874832154\n",
      "Training NF1:  64%|██████████▊      | 6396/10001 [3:06:07<3:59:10,  3.98s/batch]Batch 6400/10001 Done, mean position loss: 20.638578481674195\n",
      "Training NF1:  64%|██████████▊      | 6392/10001 [3:06:08<3:56:22,  3.93s/batch]Batch 6400/10001 Done, mean position loss: 20.562975108623505\n",
      "Training NF1:  64%|██████████▉      | 6425/10001 [3:06:13<3:32:01,  3.56s/batch]Batch 6400/10001 Done, mean position loss: 20.743001599311828\n",
      "Training NF1:  64%|██████████▊      | 6362/10001 [3:06:23<3:23:25,  3.35s/batch]Batch 6500/10001 Done, mean position loss: 20.47812644481659\n",
      "Training NF1:  63%|██████████▊      | 6328/10001 [3:06:24<4:15:11,  4.17s/batch]Batch 6400/10001 Done, mean position loss: 20.417447690963748\n",
      "Training NF1:  64%|██████████▊      | 6386/10001 [3:06:29<4:25:44,  4.41s/batch]Batch 6400/10001 Done, mean position loss: 20.689019780158993\n",
      "Training NF1:  65%|██████████▉      | 6461/10001 [3:06:38<4:05:20,  4.16s/batch]Batch 6300/10001 Done, mean position loss: 20.61453673839569\n",
      "Training NF1:  64%|██████████▊      | 6389/10001 [3:06:41<4:03:20,  4.04s/batch]Batch 6400/10001 Done, mean position loss: 20.733356370925904\n",
      "Training NF1:  64%|██████████▊      | 6391/10001 [3:06:47<3:46:02,  3.76s/batch]Batch 6400/10001 Done, mean position loss: 20.529992237091065\n",
      "Training NF1:  65%|██████████▉      | 6469/10001 [3:06:52<3:32:59,  3.62s/batch]Batch 6500/10001 Done, mean position loss: 20.578980412483215\n",
      "Training NF1:  63%|██████████▋      | 6272/10001 [3:06:57<3:41:03,  3.56s/batch]Batch 6400/10001 Done, mean position loss: 20.37182711839676\n",
      "Training NF1:  65%|███████████      | 6472/10001 [3:07:17<3:15:57,  3.33s/batch]Batch 6400/10001 Done, mean position loss: 20.799449203014373\n",
      "Training NF1:  64%|██████████▉      | 6407/10001 [3:07:19<3:24:51,  3.42s/batch]Batch 6500/10001 Done, mean position loss: 20.48290229797363\n",
      "Training NF1:  64%|██████████▊      | 6396/10001 [3:07:23<3:44:08,  3.73s/batch]Batch 6400/10001 Done, mean position loss: 21.076479806900025\n",
      "Training NF1:  65%|███████████      | 6507/10001 [3:07:43<3:44:39,  3.86s/batch]Batch 6400/10001 Done, mean position loss: 20.719575004577635\n",
      "Training NF1:  64%|██████████▉      | 6435/10001 [3:07:44<3:51:33,  3.90s/batch]Batch 6400/10001 Done, mean position loss: 20.974838402271274\n",
      "Training NF1:  64%|██████████▉      | 6428/10001 [3:07:51<3:57:25,  3.99s/batch]Batch 6400/10001 Done, mean position loss: 21.057605171203612\n",
      "Training NF1:  64%|██████████▉      | 6428/10001 [3:08:07<3:34:25,  3.60s/batch]Batch 6400/10001 Done, mean position loss: 20.38264013290405\n",
      "Training NF1:  64%|██████████▉      | 6435/10001 [3:08:17<3:47:26,  3.83s/batch]Batch 6400/10001 Done, mean position loss: 20.72815415620804\n",
      "Training NF1:  64%|██████████▊      | 6371/10001 [3:08:29<4:39:13,  4.62s/batch]Batch 6400/10001 Done, mean position loss: 20.45131615877151\n",
      "Training NF1:  64%|██████████▉      | 6434/10001 [3:08:54<3:55:05,  3.95s/batch]Batch 6500/10001 Done, mean position loss: 20.5247070479393\n",
      "Training NF1:  64%|██████████▊      | 6377/10001 [3:08:55<4:15:02,  4.22s/batch]Batch 6300/10001 Done, mean position loss: 20.687484405040742\n",
      "Training NF1:  64%|██████████▉      | 6425/10001 [3:08:54<3:36:33,  3.63s/batch]Batch 6400/10001 Done, mean position loss: 21.06627308368683\n",
      "Training NF1:  64%|██████████▉      | 6435/10001 [3:08:58<4:03:44,  4.10s/batch]Batch 6500/10001 Done, mean position loss: 20.999775156974792\n",
      "Training NF1:  65%|███████████      | 6480/10001 [3:09:02<3:40:44,  3.76s/batch]Batch 6500/10001 Done, mean position loss: 20.51011156797409\n",
      "Training NF1:  64%|██████████▊      | 6381/10001 [3:09:10<3:53:43,  3.87s/batch]Batch 6500/10001 Done, mean position loss: 21.014855837821962\n",
      "Training NF1:  64%|██████████▉      | 6418/10001 [3:09:30<3:28:52,  3.50s/batch]Batch 6400/10001 Done, mean position loss: 21.581418075561523\n",
      "Training NF1:  65%|███████████      | 6494/10001 [3:09:44<3:39:36,  3.76s/batch]Batch 6500/10001 Done, mean position loss: 20.63305869102478\n",
      "Training NF1:  63%|██████████▋      | 6322/10001 [3:10:09<3:38:41,  3.57s/batch]Batch 6500/10001 Done, mean position loss: 20.466859266757965\n",
      "Training NF1:  64%|██████████▊      | 6387/10001 [3:10:16<3:53:20,  3.87s/batch]Batch 6500/10001 Done, mean position loss: 20.758843078613282\n",
      "Training NF1:  66%|███████████▏     | 6557/10001 [3:10:18<3:40:15,  3.84s/batch]Batch 6500/10001 Done, mean position loss: 20.858517038822175\n",
      "Training NF1:  64%|██████████▊      | 6389/10001 [3:10:22<3:20:30,  3.33s/batch]Batch 6400/10001 Done, mean position loss: 20.34564584493637\n",
      "Training NF1:  65%|███████████      | 6518/10001 [3:10:42<3:20:23,  3.45s/batch]Batch 6500/10001 Done, mean position loss: 20.779545879364015\n",
      "Training NF1:  65%|███████████      | 6502/10001 [3:10:45<3:07:35,  3.22s/batch]Batch 6500/10001 Done, mean position loss: 20.901471004486083\n",
      "Training NF1:  65%|██████████▉      | 6471/10001 [3:10:50<3:29:00,  3.55s/batch]Batch 6500/10001 Done, mean position loss: 20.615301699638366\n",
      "Training NF1:  65%|███████████      | 6477/10001 [3:11:06<3:45:29,  3.84s/batch]Batch 6400/10001 Done, mean position loss: 20.973759672641755\n",
      "Training NF1:  64%|██████████▊      | 6380/10001 [3:11:29<3:24:37,  3.39s/batch]Batch 6500/10001 Done, mean position loss: 20.63802583932877\n",
      "Training NF1:  65%|██████████▉      | 6470/10001 [3:11:56<3:16:40,  3.34s/batch]Batch 6500/10001 Done, mean position loss: 20.60352241754532\n",
      "Training NF1:  65%|███████████      | 6496/10001 [3:12:01<4:00:11,  4.11s/batch]Batch 6500/10001 Done, mean position loss: 20.671266927719117\n",
      "Training NF1:  65%|███████████      | 6485/10001 [3:12:02<3:44:19,  3.83s/batch]Batch 6500/10001 Done, mean position loss: 20.863842771053314\n",
      "Training NF1:  65%|███████████      | 6491/10001 [3:12:19<2:49:04,  2.89s/batch]Batch 6500/10001 Done, mean position loss: 20.560570471286773\n",
      "Training NF1:  66%|███████████▏     | 6584/10001 [3:12:23<3:38:49,  3.84s/batch]Batch 6500/10001 Done, mean position loss: 20.65025711774826\n",
      "Training NF1:  65%|███████████      | 6539/10001 [3:12:29<3:18:06,  3.43s/batch]Batch 6500/10001 Done, mean position loss: 20.755979299545288\n",
      "Training NF1:  65%|███████████      | 6503/10001 [3:12:32<3:34:01,  3.67s/batch]Batch 6600/10001 Done, mean position loss: 20.492641649246217\n",
      "Training NF1:  65%|███████████      | 6532/10001 [3:12:33<3:36:15,  3.74s/batch]Batch 6500/10001 Done, mean position loss: 20.401071279048917\n",
      "Training NF1:  66%|███████████▏     | 6604/10001 [3:12:42<3:22:31,  3.58s/batch]Batch 6500/10001 Done, mean position loss: 20.681844935417175\n",
      "Training NF1:  65%|███████████      | 6495/10001 [3:12:47<3:22:46,  3.47s/batch]Batch 6600/10001 Done, mean position loss: 20.583241498470308\n",
      "Training NF1:  65%|███████████      | 6483/10001 [3:12:49<4:08:10,  4.23s/batch]Batch 6400/10001 Done, mean position loss: 20.612121210098266\n",
      "Training NF1:  65%|███████████      | 6516/10001 [3:12:55<3:21:01,  3.46s/batch]Batch 6500/10001 Done, mean position loss: 20.73897268772125\n",
      "Training NF1:  66%|███████████▏     | 6571/10001 [3:13:06<3:42:41,  3.90s/batch]Batch 6500/10001 Done, mean position loss: 20.52562686920166\n",
      "Training NF1:  65%|███████████      | 6515/10001 [3:13:11<3:11:08,  3.29s/batch]Batch 6500/10001 Done, mean position loss: 20.364029119014738\n",
      "Training NF1:  65%|███████████▏     | 6546/10001 [3:13:20<2:58:31,  3.10s/batch]Batch 6600/10001 Done, mean position loss: 20.474410281181335\n",
      "Training NF1:  66%|███████████▏     | 6612/10001 [3:13:26<2:56:05,  3.12s/batch]Batch 6500/10001 Done, mean position loss: 20.802273638248444\n",
      "Training NF1:  65%|███████████      | 6477/10001 [3:13:29<3:16:27,  3.34s/batch]Batch 6500/10001 Done, mean position loss: 21.055215325355533\n",
      "Training NF1:  65%|███████████      | 6476/10001 [3:13:55<3:14:11,  3.31s/batch]Batch 6500/10001 Done, mean position loss: 20.992934951782225\n",
      "Training NF1:  65%|███████████      | 6496/10001 [3:13:58<3:44:43,  3.85s/batch]Batch 6500/10001 Done, mean position loss: 21.058734567165374\n",
      "Training NF1:  64%|██████████▉      | 6424/10001 [3:14:09<3:35:34,  3.62s/batch]Batch 6500/10001 Done, mean position loss: 20.71609449625015\n",
      "Training NF1:  66%|███████████▏     | 6559/10001 [3:14:17<3:26:53,  3.61s/batch]Batch 6500/10001 Done, mean position loss: 20.387452678680418\n",
      "Training NF1:  66%|███████████▏     | 6580/10001 [3:14:28<3:19:41,  3.50s/batch]Batch 6500/10001 Done, mean position loss: 20.442282671928403\n",
      "Training NF1:  65%|███████████      | 6534/10001 [3:14:33<3:14:35,  3.37s/batch]Batch 6500/10001 Done, mean position loss: 20.72456343173981\n",
      "Training NF1:  66%|███████████▏     | 6592/10001 [3:14:43<3:34:48,  3.78s/batch]Batch 6600/10001 Done, mean position loss: 20.519146463871003\n",
      "Training NF1:  66%|███████████▏     | 6568/10001 [3:14:51<3:56:56,  4.14s/batch]Batch 6600/10001 Done, mean position loss: 20.509287860393524\n",
      "Training NF1:  66%|███████████▎     | 6637/10001 [3:15:00<3:27:06,  3.69s/batch]Batch 6600/10001 Done, mean position loss: 21.003467111587526\n",
      "Training NF1:  65%|███████████      | 6538/10001 [3:15:01<3:26:21,  3.58s/batch]Batch 6500/10001 Done, mean position loss: 21.109443876743313\n",
      "Training NF1:  66%|███████████▏     | 6580/10001 [3:15:03<3:52:03,  4.07s/batch]Batch 6400/10001 Done, mean position loss: 20.686639142036437\n",
      "Training NF1:  66%|███████████▎     | 6649/10001 [3:15:15<3:40:01,  3.94s/batch]Batch 6600/10001 Done, mean position loss: 20.993531308174134\n",
      "Training NF1:  66%|███████████▏     | 6586/10001 [3:15:27<3:26:57,  3.64s/batch]Batch 6500/10001 Done, mean position loss: 21.591138229370117\n",
      "Training NF1:  66%|███████████▏     | 6565/10001 [3:15:51<3:28:01,  3.63s/batch]Batch 6600/10001 Done, mean position loss: 20.632858078479767\n",
      "Training NF1:  65%|███████████      | 6495/10001 [3:16:13<4:34:16,  4.69s/batch]Batch 6600/10001 Done, mean position loss: 20.469484355449673\n",
      "Training NF1:  66%|███████████▏     | 6600/10001 [3:16:25<3:57:18,  4.19s/batch]Batch 6600/10001 Done, mean position loss: 20.742842049598693\n",
      "Training NF1:  65%|███████████      | 6518/10001 [3:16:29<3:37:35,  3.75s/batch]Batch 6600/10001 Done, mean position loss: 20.86468464612961\n",
      "Training NF1:  66%|███████████▏     | 6583/10001 [3:16:38<3:48:22,  4.01s/batch]Batch 6500/10001 Done, mean position loss: 20.31505310535431\n",
      "Training NF1:  66%|███████████▏     | 6586/10001 [3:16:49<3:38:10,  3.83s/batch]Batch 6600/10001 Done, mean position loss: 20.889693486690526\n",
      "Training NF1:  67%|███████████▎     | 6666/10001 [3:16:52<3:11:30,  3.45s/batch]Batch 6600/10001 Done, mean position loss: 20.787955524921415\n",
      "Training NF1:  66%|███████████▏     | 6580/10001 [3:16:59<3:51:09,  4.05s/batch]Batch 6600/10001 Done, mean position loss: 20.60515873432159\n",
      "Training NF1:  65%|███████████      | 6544/10001 [3:17:16<3:50:41,  4.00s/batch]Batch 6500/10001 Done, mean position loss: 20.948299820423127\n",
      "Training NF1:  66%|███████████▎     | 6624/10001 [3:17:46<3:33:01,  3.78s/batch]Batch 6600/10001 Done, mean position loss: 20.650566675662994\n",
      "Training NF1:  66%|███████▎   | 6630/10001 [12:15:50<3111:59:55, 3323.40s/batch]10001 [12:15:34<9166:22:57, 9679.96s/batch]Batch 6600/10001 Done, mean position loss: 20.579613633155823\n",
      "Training NF1:  66%|███████▏   | 6576/10001 [12:15:51<2214:17:59, 2327.44s/batch]Batch 6600/10001 Done, mean position loss: 20.88642985343933\n",
      "Training NF1:  66%|████████▌    | 6634/10001 [12:16:06<749:01:12, 800.85s/batch]Batch 6700/10001 Done, mean position loss: 20.476969764232635\n",
      "Training NF1:  66%|████████▌    | 6605/10001 [12:16:07<529:46:37, 561.60s/batch]Batch 6600/10001 Done, mean position loss: 20.691077105998993\n",
      "Training NF1:  66%|████████▌    | 6574/10001 [12:16:10<374:51:16, 393.78s/batch]Batch 6600/10001 Done, mean position loss: 20.559150140285492\n",
      "Training NF1:  66%|████████▌    | 6616/10001 [12:16:16<183:09:06, 194.78s/batch]Batch 6600/10001 Done, mean position loss: 20.649220888614654\n",
      "Training NF1:  66%|█████████▉     | 6594/10001 [12:16:31<46:42:15, 49.35s/batch]Batch 6600/10001 Done, mean position loss: 20.72947453022003\n",
      "Training NF1:  67%|█████████▉     | 6660/10001 [12:16:31<32:53:59, 35.45s/batch]Batch 6600/10001 Done, mean position loss: 20.68824982404709\n",
      "Training NF1:  67%|██████████     | 6669/10001 [12:16:34<23:36:18, 25.50s/batch]Batch 6600/10001 Done, mean position loss: 20.723300650119782\n",
      "Training NF1:  66%|█████████▉     | 6595/10001 [12:16:35<24:16:19, 25.65s/batch]Batch 6500/10001 Done, mean position loss: 20.61524976015091\n",
      "Training NF1:  66%|█████████▊     | 6578/10001 [12:16:36<24:33:02, 25.82s/batch]Batch 6700/10001 Done, mean position loss: 20.583717813491823\n",
      "Training NF1:  66%|█████████▊     | 6582/10001 [12:16:38<24:38:08, 25.94s/batch]Batch 6600/10001 Done, mean position loss: 20.41137331008911\n",
      "Training NF1:  66%|██████████▌     | 6615/10001 [12:16:51<5:08:54,  5.47s/batch]Batch 6600/10001 Done, mean position loss: 20.368572063446045\n",
      "Training NF1:  67%|██████████▋     | 6671/10001 [12:16:52<6:36:36,  7.15s/batch]Batch 6600/10001 Done, mean position loss: 20.52797233104706\n",
      "Training NF1:  66%|██████████▋     | 6650/10001 [12:16:55<4:56:17,  5.31s/batch]Batch 6600/10001 Done, mean position loss: 20.81355194568634\n",
      "Training NF1:  66%|██████████▋     | 6650/10001 [12:16:55<3:38:34,  3.91s/batch]Batch 6700/10001 Done, mean position loss: 20.46958225250244\n",
      "Training NF1:  66%|█████████▉     | 6607/10001 [12:32:09<45:06:17, 47.84s/batch]Batch 6600/10001 Done, mean position loss: 21.05300709724426\n",
      "Training NF1:  66%|██████████▌     | 6628/10001 [12:32:30<7:56:00,  8.47s/batch]Batch 6600/10001 Done, mean position loss: 21.036094081401828\n",
      "Training NF1:  67%|██████████▋     | 6664/10001 [12:32:32<4:37:07,  4.98s/batch]Batch 6600/10001 Done, mean position loss: 20.971304392814638\n",
      "Training NF1:  67%|██████████▊     | 6721/10001 [12:32:39<4:28:08,  4.91s/batch]Batch 6600/10001 Done, mean position loss: 20.715635447502137\n",
      "Training NF1:  65%|██████████▍     | 6525/10001 [12:32:45<3:04:48,  3.19s/batch]Batch 6600/10001 Done, mean position loss: 20.382639863491057\n",
      "Training NF1:  66%|██████████▌     | 6610/10001 [12:32:58<2:21:02,  2.50s/batch]Batch 6600/10001 Done, mean position loss: 20.724264826774597\n",
      "Batch 6600/10001 Done, mean position loss: 20.44953964233398\n",
      "Training NF1:  66%|██████████▌     | 6614/10001 [12:33:07<2:38:39,  2.81s/batch]Batch 6700/10001 Done, mean position loss: 20.507440598011016\n",
      "Training NF1:  67%|██████████▊     | 6743/10001 [12:33:07<2:33:15,  2.82s/batch]Batch 6700/10001 Done, mean position loss: 20.50192105293274\n",
      "Training NF1:  67%|██████████▋     | 6678/10001 [12:33:20<3:11:05,  3.45s/batch]Batch 6700/10001 Done, mean position loss: 21.03177880525589\n",
      "Training NF1:  66%|██████████▌     | 6617/10001 [12:33:21<2:34:42,  2.74s/batch]Batch 6600/10001 Done, mean position loss: 21.095316722393036\n",
      "Training NF1:  65%|██████████▍     | 6538/10001 [12:33:25<2:48:11,  2.91s/batch]Batch 6500/10001 Done, mean position loss: 20.707354266643524\n",
      "Training NF1:  67%|██████████▊     | 6737/10001 [12:33:30<2:13:58,  2.46s/batch]Batch 6700/10001 Done, mean position loss: 20.991324489116668\n",
      "Training NF1:  67%|██████████▋     | 6680/10001 [12:33:40<2:20:24,  2.54s/batch]Batch 6600/10001 Done, mean position loss: 21.55508151769638\n",
      "Training NF1:  66%|██████████▋     | 6646/10001 [12:34:05<3:16:51,  3.52s/batch]Batch 6700/10001 Done, mean position loss: 20.64130591869354\n",
      "Training NF1:  67%|██████████▋     | 6658/10001 [12:34:18<3:00:45,  3.24s/batch]Batch 6700/10001 Done, mean position loss: 20.741014091968538\n",
      "Training NF1:  67%|██████████▊     | 6727/10001 [12:34:25<3:04:06,  3.37s/batch]Batch 6700/10001 Done, mean position loss: 20.461648359298707\n",
      "Training NF1:  67%|██████████▋     | 6681/10001 [12:34:28<2:41:46,  2.92s/batch]Batch 6700/10001 Done, mean position loss: 20.861243431568145\n",
      "Training NF1:  65%|██████████▍     | 6526/10001 [12:34:43<2:57:38,  3.07s/batch]Batch 6700/10001 Done, mean position loss: 20.77539557695389\n",
      "Training NF1:  66%|██████████▌     | 6564/10001 [12:34:43<2:52:14,  3.01s/batch]Batch 6700/10001 Done, mean position loss: 20.901101677417756\n",
      "Training NF1:  67%|██████████▋     | 6678/10001 [12:34:45<3:00:32,  3.26s/batch]Batch 6600/10001 Done, mean position loss: 20.337077429294585\n",
      "Training NF1:  67%|██████████▋     | 6717/10001 [12:34:52<3:05:40,  3.39s/batch]Batch 6700/10001 Done, mean position loss: 20.609590363502505\n",
      "Training NF1:  67%|██████████▊     | 6722/10001 [12:35:10<3:11:27,  3.50s/batch]Batch 6600/10001 Done, mean position loss: 20.93081279993057\n",
      "Training NF1:  68%|██████████▊     | 6794/10001 [12:35:38<3:03:13,  3.43s/batch]Batch 6700/10001 Done, mean position loss: 20.646097509860994\n",
      "Training NF1:  67%|██████████▊     | 6729/10001 [12:36:24<7:30:38,  8.26s/batch]Batch 6700/10001 Done, mean position loss: 20.876870021820068\n",
      "Training NF1:  68%|██████████▊     | 6787/10001 [12:36:45<8:16:11,  9.26s/batch]Batch 6800/10001 Done, mean position loss: 20.47852885723114\n",
      "Training NF1:  67%|██████████▋     | 6656/10001 [12:36:48<8:38:53,  9.31s/batch]Batch 6700/10001 Done, mean position loss: 20.58397295475006\n",
      "Training NF1:  67%|██████████     | 6687/10001 [12:41:30<29:37:42, 32.19s/batch]Batch 6700/10001 Done, mean position loss: 20.55258673429489\n",
      "Training NF1:  66%|█████████▉     | 6632/10001 [12:41:38<15:56:56, 17.04s/batch]Batch 6700/10001 Done, mean position loss: 20.683719100952146\n",
      "Training NF1:  67%|██████████     | 6699/10001 [12:41:39<11:35:10, 12.63s/batch]Batch 6700/10001 Done, mean position loss: 20.638763790130618\n",
      "Training NF1:  67%|██████████▋     | 6687/10001 [12:41:43<3:59:18,  4.33s/batch]Batch 6700/10001 Done, mean position loss: 20.72409662723541\n",
      "Training NF1:  67%|██████████▊     | 6748/10001 [12:41:46<4:10:09,  4.61s/batch]Batch 6700/10001 Done, mean position loss: 20.719519774913785\n",
      "Training NF1:  68%|██████████▉     | 6799/10001 [12:41:47<3:10:03,  3.56s/batch]Batch 6700/10001 Done, mean position loss: 20.680188965797424\n",
      "Training NF1:  67%|██████████▋     | 6682/10001 [12:41:50<2:28:48,  2.69s/batch]Batch 6800/10001 Done, mean position loss: 20.482615041732785\n",
      "Training NF1:  67%|██████████▋     | 6683/10001 [12:41:52<2:12:36,  2.40s/batch]Batch 6800/10001 Done, mean position loss: 20.589591994285584\n",
      "Training NF1:  67%|██████████▋     | 6677/10001 [12:41:53<2:23:02,  2.58s/batch]Batch 6700/10001 Done, mean position loss: 20.412883303165437\n",
      "Training NF1:  68%|██████████▉     | 6803/10001 [12:41:54<1:58:52,  2.23s/batch]Batch 6700/10001 Done, mean position loss: 20.809468789100645\n",
      "Training NF1:  68%|██████████▊     | 6770/10001 [12:41:54<2:01:30,  2.26s/batch]Batch 6600/10001 Done, mean position loss: 20.602913479804993\n",
      "Training NF1:  68%|██████████▊     | 6766/10001 [12:41:55<1:39:04,  1.84s/batch]Batch 6700/10001 Done, mean position loss: 20.376673328876496\n",
      "Training NF1:  67%|██████████▋     | 6714/10001 [12:42:03<1:44:36,  1.91s/batch]Batch 6700/10001 Done, mean position loss: 20.530891773700716\n",
      "Training NF1:  67%|██████████▋     | 6685/10001 [12:42:07<1:41:47,  1.84s/batch]Batch 6700/10001 Done, mean position loss: 21.0631153011322\n",
      "Training NF1:  68%|██████████▊     | 6751/10001 [12:42:21<1:37:26,  1.80s/batch]Batch 6700/10001 Done, mean position loss: 21.030992481708527\n",
      "Training NF1:  68%|██████████▊     | 6762/10001 [12:42:23<1:32:59,  1.72s/batch]Batch 6700/10001 Done, mean position loss: 20.71984552383423\n",
      "Training NF1:  67%|██████████▋     | 6713/10001 [12:42:24<1:33:14,  1.70s/batch]Batch 6700/10001 Done, mean position loss: 20.986117546558383\n",
      "Training NF1:  67%|██████████▋     | 6695/10001 [12:42:34<1:28:11,  1.60s/batch]Batch 6700/10001 Done, mean position loss: 20.379177877902983\n",
      "Training NF1:  67%|██████████▊     | 6743/10001 [12:42:38<1:08:34,  1.26s/batch]Batch 6700/10001 Done, mean position loss: 20.45755951166153\n",
      "Training NF1:  67%|██████████▋     | 6710/10001 [12:42:38<1:36:04,  1.75s/batch]Batch 6800/10001 Done, mean position loss: 20.504485416412354\n",
      "Training NF1:  67%|██████████▋     | 6690/10001 [12:42:44<1:53:01,  2.05s/batch]Batch 6700/10001 Done, mean position loss: 20.73108383178711\n",
      "Training NF1:  67%|██████████▊     | 6733/10001 [12:42:50<1:49:38,  2.01s/batch]Batch 6800/10001 Done, mean position loss: 20.501460075378418\n",
      "Training NF1:  68%|██████████▉     | 6850/10001 [12:42:51<1:28:56,  1.69s/batch]Batch 6800/10001 Done, mean position loss: 21.01030135393143\n",
      "Training NF1:  67%|██████████▋     | 6697/10001 [12:42:53<1:16:24,  1.39s/batch]Batch 6700/10001 Done, mean position loss: 21.114002203941347\n",
      "Training NF1:  67%|██████████▊     | 6724/10001 [12:43:00<1:31:23,  1.67s/batch]Batch 6700/10001 Done, mean position loss: 21.581946690082546\n",
      "Training NF1:  67%|██████████▋     | 6701/10001 [12:43:01<1:37:18,  1.77s/batch]Batch 6600/10001 Done, mean position loss: 20.700872039794923\n",
      "Training NF1:  66%|██████████▌     | 6640/10001 [12:43:01<1:52:50,  2.01s/batch]Batch 6800/10001 Done, mean position loss: 21.003415071964262\n",
      "Training NF1:  67%|██████████▋     | 6719/10001 [12:43:14<1:36:52,  1.77s/batch]Batch 6800/10001 Done, mean position loss: 20.61521327495575\n",
      "Training NF1:  68%|██████████▊     | 6757/10001 [12:43:23<1:26:31,  1.60s/batch]Batch 6800/10001 Done, mean position loss: 20.46101351737976\n",
      "Training NF1:  68%|██████████▉     | 6799/10001 [12:43:24<1:17:41,  1.46s/batch]Batch 6800/10001 Done, mean position loss: 20.72935683965683\n",
      "Training NF1:  67%|██████████▊     | 6727/10001 [12:43:27<1:31:59,  1.69s/batch]Batch 6800/10001 Done, mean position loss: 20.84519306898117\n",
      "Training NF1:  68%|██████████▊     | 6769/10001 [12:43:38<1:29:25,  1.66s/batch]Batch 6800/10001 Done, mean position loss: 20.768789575099944\n",
      "Training NF1:  67%|██████████▋     | 6689/10001 [12:43:41<1:50:25,  2.00s/batch]Batch 6700/10001 Done, mean position loss: 20.324609820842742\n",
      "Training NF1:  68%|██████████▉     | 6832/10001 [12:43:44<1:49:02,  2.06s/batch]Batch 6800/10001 Done, mean position loss: 20.92125105857849\n",
      "Training NF1:  68%|██████████▊     | 6791/10001 [12:43:46<1:28:54,  1.66s/batch]Batch 6800/10001 Done, mean position loss: 20.599777648448942\n",
      "Training NF1:  67%|██████████▋     | 6715/10001 [12:44:02<1:29:12,  1.63s/batch]Batch 6800/10001 Done, mean position loss: 20.66082943201065\n",
      "Training NF1:  68%|██████████▉     | 6826/10001 [12:44:06<1:29:56,  1.70s/batch]Batch 6700/10001 Done, mean position loss: 20.967127964496612\n",
      "Training NF1:  68%|██████████▊     | 6784/10001 [12:44:11<1:57:34,  2.19s/batch]Batch 6800/10001 Done, mean position loss: 20.53237972021103\n",
      "Training NF1:  68%|██████████▊     | 6789/10001 [12:44:12<1:24:33,  1.58s/batch]Batch 6800/10001 Done, mean position loss: 20.872055444717404\n",
      "Training NF1:  68%|██████████▊     | 6786/10001 [12:44:14<1:46:28,  1.99s/batch]Batch 6900/10001 Done, mean position loss: 20.482491719722745\n",
      "Training NF1:  68%|██████████▉     | 6836/10001 [12:44:21<1:19:17,  1.50s/batch]Batch 6800/10001 Done, mean position loss: 20.592828962802884\n",
      "Training NF1:  69%|███████████     | 6896/10001 [12:44:31<1:19:46,  1.54s/batch]Batch 6800/10001 Done, mean position loss: 20.684754128456113\n",
      "Training NF1:  68%|██████████▊     | 6788/10001 [12:44:33<1:27:50,  1.64s/batch]Batch 6800/10001 Done, mean position loss: 20.730279455184935\n",
      "Training NF1:  68%|██████████▊     | 6767/10001 [12:44:34<1:34:48,  1.76s/batch]Batch 6800/10001 Done, mean position loss: 20.637880926132205\n",
      "Training NF1:  68%|██████████▉     | 6817/10001 [12:44:38<1:27:29,  1.65s/batch]Batch 6800/10001 Done, mean position loss: 20.65444202184677\n",
      "Training NF1:  68%|██████████▊     | 6763/10001 [12:44:39<1:41:05,  1.87s/batch]Batch 6900/10001 Done, mean position loss: 20.478523161411285\n",
      "Training NF1:  67%|██████████▊     | 6721/10001 [12:44:41<1:47:55,  1.97s/batch]Batch 6800/10001 Done, mean position loss: 20.716995074748993\n",
      "Training NF1:  69%|███████████     | 6917/10001 [12:44:41<1:29:38,  1.74s/batch]Batch 6900/10001 Done, mean position loss: 20.58131192445755\n",
      "Training NF1:  69%|███████████     | 6904/10001 [12:44:45<1:37:19,  1.89s/batch]Batch 6800/10001 Done, mean position loss: 20.412322194576262\n",
      "Training NF1:  68%|██████████▉     | 6804/10001 [12:44:46<1:29:42,  1.68s/batch]Batch 6800/10001 Done, mean position loss: 20.792541007995602\n",
      "Training NF1:  69%|██████████▉     | 6868/10001 [12:44:48<1:54:05,  2.19s/batch]Batch 6700/10001 Done, mean position loss: 20.598916280269624\n",
      "Training NF1:  68%|██████████▊     | 6785/10001 [12:44:53<1:26:57,  1.62s/batch]Batch 6800/10001 Done, mean position loss: 20.53045485019684\n",
      "Training NF1:  69%|██████████▉     | 6875/10001 [12:44:55<1:13:05,  1.40s/batch]Batch 6800/10001 Done, mean position loss: 20.370193152427674\n",
      "Training NF1:  69%|███████████     | 6877/10001 [12:45:10<1:28:36,  1.70s/batch]Batch 6800/10001 Done, mean position loss: 21.058718483448025\n",
      "Training NF1:  67%|██████████▊     | 6738/10001 [12:45:13<1:44:30,  1.92s/batch]Batch 6800/10001 Done, mean position loss: 21.044331459999086\n",
      "Training NF1:  68%|██████████▉     | 6829/10001 [12:45:20<1:34:34,  1.79s/batch]Batch 6800/10001 Done, mean position loss: 20.712684867382052\n",
      "Training NF1:  68%|██████████▉     | 6815/10001 [12:45:21<1:42:46,  1.94s/batch]Batch 6800/10001 Done, mean position loss: 20.94906370162964\n",
      "Training NF1:  68%|██████████▊     | 6795/10001 [12:45:23<1:22:10,  1.54s/batch]Batch 6900/10001 Done, mean position loss: 20.495334465503692\n",
      "Training NF1:  68%|██████████▊     | 6764/10001 [12:45:33<1:21:21,  1.51s/batch]Batch 6800/10001 Done, mean position loss: 20.465698382854463\n",
      "Training NF1:  69%|██████████▉     | 6855/10001 [12:45:34<1:39:27,  1.90s/batch]Batch 6800/10001 Done, mean position loss: 20.375364456176758\n",
      "Training NF1:  69%|██████████▉     | 6866/10001 [12:45:38<1:15:54,  1.45s/batch]Batch 6800/10001 Done, mean position loss: 20.728567399978637\n",
      "Training NF1:  69%|███████████     | 6937/10001 [12:45:40<1:35:03,  1.86s/batch]Batch 6800/10001 Done, mean position loss: 21.596187279224395\n",
      "Training NF1:  67%|██████████▊     | 6733/10001 [12:45:42<1:33:27,  1.72s/batch]Batch 6800/10001 Done, mean position loss: 21.095231828689577\n",
      "Training NF1:  69%|██████████▉     | 6870/10001 [12:45:44<1:26:08,  1.65s/batch]Batch 6900/10001 Done, mean position loss: 20.51277459383011\n",
      "Training NF1:  69%|██████████▉     | 6871/10001 [12:45:47<1:33:19,  1.79s/batch]Batch 6900/10001 Done, mean position loss: 21.01215368747711\n",
      "Training NF1:  69%|██████████▉     | 6856/10001 [12:45:57<1:37:55,  1.87s/batch]Batch 6900/10001 Done, mean position loss: 21.02618399143219\n",
      "Training NF1:  68%|██████████▉     | 6850/10001 [12:45:58<1:39:15,  1.89s/batch]Batch 6700/10001 Done, mean position loss: 20.678288886547087\n",
      "Training NF1:  69%|███████████     | 6909/10001 [12:46:01<1:37:59,  1.90s/batch]Batch 6900/10001 Done, mean position loss: 20.613039467334744\n",
      "Training NF1:  68%|██████████▉     | 6846/10001 [12:46:16<1:24:22,  1.60s/batch]Batch 6900/10001 Done, mean position loss: 20.46012624502182\n",
      "Training NF1:  70%|███████████▏    | 6955/10001 [12:46:17<1:25:53,  1.69s/batch]Batch 6900/10001 Done, mean position loss: 20.725736176967622\n",
      "Training NF1:  68%|██████████▉     | 6827/10001 [12:46:26<1:38:45,  1.87s/batch]Batch 6900/10001 Done, mean position loss: 20.867629256248478\n",
      "Training NF1:  69%|██████████▉     | 6867/10001 [12:46:33<1:30:52,  1.74s/batch]Batch 6900/10001 Done, mean position loss: 20.900156915187836\n",
      "Training NF1:  69%|███████████     | 6939/10001 [12:46:33<1:27:31,  1.72s/batch]Batch 6900/10001 Done, mean position loss: 20.77616134643555\n",
      "Training NF1:  68%|██████████▊     | 6784/10001 [12:46:38<1:38:51,  1.84s/batch]Batch 6800/10001 Done, mean position loss: 20.337209680080413\n",
      "Training NF1:  68%|██████████▉     | 6801/10001 [12:46:38<1:27:51,  1.65s/batch]Batch 6900/10001 Done, mean position loss: 20.599872286319734\n",
      "Training NF1:  69%|███████████     | 6879/10001 [12:46:53<1:19:55,  1.54s/batch]Batch 6900/10001 Done, mean position loss: 20.626676330566404\n",
      "Training NF1:  68%|██████████▊     | 6776/10001 [12:47:00<1:24:10,  1.57s/batch]Batch 6900/10001 Done, mean position loss: 20.55883683204651\n",
      "Training NF1:  69%|██████████▉     | 6854/10001 [12:47:04<1:19:02,  1.51s/batch]Batch 6900/10001 Done, mean position loss: 20.882571113109588\n",
      "Training NF1:  69%|██████████▉     | 6856/10001 [12:47:07<1:18:10,  1.49s/batch]Batch 7000/10001 Done, mean position loss: 20.482361776828764\n",
      "Training NF1:  69%|██████████▉     | 6858/10001 [12:47:11<1:28:58,  1.70s/batch]Batch 6800/10001 Done, mean position loss: 20.97547669887543\n",
      "Training NF1:  67%|██████████▊     | 6743/10001 [12:47:13<1:36:35,  1.78s/batch]Batch 6900/10001 Done, mean position loss: 20.568038892745975\n",
      "Training NF1:  69%|██████████▉     | 6873/10001 [12:47:27<1:19:33,  1.53s/batch]Batch 6900/10001 Done, mean position loss: 20.668180401325227\n",
      "Training NF1:  69%|███████████     | 6942/10001 [12:47:28<1:39:18,  1.95s/batch]Batch 6900/10001 Done, mean position loss: 20.658667635917663\n",
      "Training NF1:  70%|███████████▏    | 6961/10001 [12:47:30<1:23:56,  1.66s/batch]Batch 7000/10001 Done, mean position loss: 20.471311876773832\n",
      "Training NF1:  69%|███████████     | 6921/10001 [12:47:33<1:16:39,  1.49s/batch]Batch 6900/10001 Done, mean position loss: 20.707563376426698\n",
      "Training NF1:  70%|███████████▏    | 7003/10001 [12:47:33<1:22:22,  1.65s/batch]Batch 7000/10001 Done, mean position loss: 20.5836217713356\n",
      "Training NF1:  69%|███████████     | 6945/10001 [12:47:33<1:31:43,  1.80s/batch]Batch 6900/10001 Done, mean position loss: 20.695827457904812\n",
      "Training NF1:  69%|███████████     | 6899/10001 [12:47:33<1:32:53,  1.80s/batch]Batch 6900/10001 Done, mean position loss: 20.72207972049713\n",
      "Training NF1:  69%|███████████     | 6881/10001 [12:47:38<1:33:20,  1.79s/batch]Batch 6900/10001 Done, mean position loss: 20.80336712360382\n",
      "Training NF1:  70%|███████████▏    | 6967/10001 [12:47:40<1:29:04,  1.76s/batch]Batch 6900/10001 Done, mean position loss: 20.406883471012115\n",
      "Training NF1:  69%|███████████     | 6876/10001 [12:47:43<1:40:30,  1.93s/batch]Batch 6800/10001 Done, mean position loss: 20.60492576122284\n",
      "Training NF1:  69%|██████████▉     | 6875/10001 [12:47:48<1:38:41,  1.89s/batch]Batch 6900/10001 Done, mean position loss: 20.533697628974913\n",
      "Training NF1:  69%|███████████     | 6928/10001 [12:47:51<1:27:45,  1.71s/batch]Batch 6900/10001 Done, mean position loss: 20.361083204746247\n",
      "Training NF1:  69%|███████████     | 6938/10001 [12:48:02<1:24:33,  1.66s/batch]Batch 6900/10001 Done, mean position loss: 21.054494616985323\n",
      "Training NF1:  70%|███████████▏    | 6962/10001 [12:48:13<1:30:55,  1.80s/batch]Batch 6900/10001 Done, mean position loss: 20.713663973808288\n",
      "Training NF1:  68%|██████████▉     | 6839/10001 [12:48:14<1:26:25,  1.64s/batch]Batch 6900/10001 Done, mean position loss: 21.04366131544113\n",
      "Training NF1:  70%|███████████▏    | 6985/10001 [12:48:15<1:20:32,  1.60s/batch]Batch 6900/10001 Done, mean position loss: 20.948531086444852\n",
      "Training NF1:  69%|███████████     | 6894/10001 [12:48:16<1:20:58,  1.56s/batch]Batch 7000/10001 Done, mean position loss: 20.49755663394928\n",
      "Training NF1:  70%|███████████▏    | 6967/10001 [12:48:28<1:35:45,  1.89s/batch]Batch 6900/10001 Done, mean position loss: 20.45744453191757\n",
      "Training NF1:  69%|███████████     | 6902/10001 [12:48:29<1:16:07,  1.47s/batch]Batch 6900/10001 Done, mean position loss: 20.38057850599289\n",
      "Training NF1:  70%|███████████▏    | 6978/10001 [12:48:32<1:31:51,  1.82s/batch]Batch 6900/10001 Done, mean position loss: 21.12154989719391\n",
      "Training NF1:  69%|███████████     | 6914/10001 [12:48:35<1:21:03,  1.58s/batch]Batch 6900/10001 Done, mean position loss: 20.709797365665437\n",
      "Training NF1:  70%|███████████▏    | 6997/10001 [12:48:38<1:32:38,  1.85s/batch]Batch 6900/10001 Done, mean position loss: 21.575703840255738\n",
      "Training NF1:  69%|███████████     | 6940/10001 [12:48:39<1:39:30,  1.95s/batch]Batch 7000/10001 Done, mean position loss: 20.999836323261263\n",
      "Training NF1:  69%|███████████     | 6937/10001 [12:48:42<1:16:28,  1.50s/batch]Batch 7000/10001 Done, mean position loss: 20.503836541175843\n",
      "Training NF1:  69%|███████████     | 6910/10001 [12:48:46<1:33:44,  1.82s/batch]Batch 7000/10001 Done, mean position loss: 21.006050226688384\n",
      "Training NF1:  70%|███████████     | 6952/10001 [12:48:53<1:42:10,  2.01s/batch]Batch 6800/10001 Done, mean position loss: 20.6863622546196\n",
      "Training NF1:  69%|███████████     | 6924/10001 [12:48:53<1:30:38,  1.77s/batch]Batch 7000/10001 Done, mean position loss: 20.621730625629425\n",
      "Training NF1:  70%|███████████▏    | 6958/10001 [12:49:05<1:23:36,  1.65s/batch]Batch 7000/10001 Done, mean position loss: 20.719418330192568\n",
      "Training NF1:  69%|███████████     | 6890/10001 [12:49:09<1:31:43,  1.77s/batch]Batch 7000/10001 Done, mean position loss: 20.456857554912567\n",
      "Training NF1:  70%|███████████▏    | 6960/10001 [12:49:20<1:33:52,  1.85s/batch]Batch 7000/10001 Done, mean position loss: 20.769888026714327\n",
      "Training NF1:  70%|███████████▏    | 6970/10001 [12:49:24<1:19:37,  1.58s/batch]Batch 7000/10001 Done, mean position loss: 20.841818890571595\n",
      "Training NF1:  69%|███████████     | 6950/10001 [12:49:26<1:17:24,  1.52s/batch]Batch 7000/10001 Done, mean position loss: 20.882585122585297\n",
      "Training NF1:  70%|███████████▏    | 6989/10001 [12:49:28<1:26:04,  1.71s/batch]Batch 6900/10001 Done, mean position loss: 20.338683891296387\n",
      "Training NF1:  69%|███████████     | 6883/10001 [12:49:32<1:40:03,  1.93s/batch]Batch 7000/10001 Done, mean position loss: 20.614535484313965\n",
      "Training NF1:  70%|███████████▏    | 7026/10001 [12:49:48<1:16:24,  1.54s/batch]Batch 7000/10001 Done, mean position loss: 20.547395517826082\n",
      "Training NF1:  70%|███████████▏    | 6980/10001 [12:49:50<1:15:47,  1.51s/batch]Batch 7000/10001 Done, mean position loss: 20.63625270366669\n",
      "Training NF1:  70%|███████████▏    | 6990/10001 [12:49:58<1:33:45,  1.87s/batch]Batch 7100/10001 Done, mean position loss: 20.4767036485672\n",
      "Training NF1:  70%|███████████▏    | 6981/10001 [12:49:58<1:22:57,  1.65s/batch]Batch 7000/10001 Done, mean position loss: 20.855805456638336\n",
      "Training NF1:  70%|███████████▏    | 7018/10001 [12:50:02<1:22:46,  1.67s/batch]Batch 7000/10001 Done, mean position loss: 20.564540705680848\n",
      "Training NF1:  70%|███████████▏    | 6985/10001 [12:50:05<1:25:56,  1.71s/batch]Batch 6900/10001 Done, mean position loss: 20.955837900638578\n",
      "Training NF1:  70%|███████████▏    | 6968/10001 [12:50:13<1:25:38,  1.69s/batch]Batch 7000/10001 Done, mean position loss: 20.683412601947783\n",
      "Training NF1:  70%|███████████▏    | 6972/10001 [12:50:17<1:24:09,  1.67s/batch]Batch 7100/10001 Done, mean position loss: 20.459899725914\n",
      "Training NF1:  69%|███████████     | 6889/10001 [12:50:17<1:21:04,  1.56s/batch]Batch 7000/10001 Done, mean position loss: 20.64991192102432\n",
      "Training NF1:  70%|███████████▏    | 6961/10001 [12:50:18<1:30:27,  1.79s/batch]Batch 7100/10001 Done, mean position loss: 20.568913223743436\n",
      "Training NF1:  70%|███████████▏    | 7007/10001 [12:50:29<1:36:53,  1.94s/batch]Batch 7000/10001 Done, mean position loss: 20.71328279495239\n",
      "Training NF1:  70%|███████████▎    | 7037/10001 [12:50:32<1:22:13,  1.66s/batch]Batch 7000/10001 Done, mean position loss: 20.679802405834195\n",
      "Training NF1:  69%|███████████     | 6918/10001 [12:50:34<1:29:39,  1.74s/batch]Batch 7000/10001 Done, mean position loss: 20.714940321445464\n",
      "Training NF1:  70%|███████████▏    | 7005/10001 [12:50:34<1:20:29,  1.61s/batch]Batch 7000/10001 Done, mean position loss: 20.416852872371674\n",
      "Training NF1:  70%|███████████▏    | 6971/10001 [12:50:37<1:23:35,  1.66s/batch]Batch 6900/10001 Done, mean position loss: 20.597740139961243\n",
      "Training NF1:  70%|███████████▏    | 7005/10001 [12:50:39<1:20:54,  1.62s/batch]Batch 7000/10001 Done, mean position loss: 20.787740151882172\n",
      "Training NF1:  71%|███████████▎    | 7087/10001 [12:50:40<1:25:12,  1.75s/batch]Batch 7000/10001 Done, mean position loss: 20.53869750022888\n",
      "Training NF1:  71%|███████████▍    | 7129/10001 [12:50:44<1:10:26,  1.47s/batch]Batch 7000/10001 Done, mean position loss: 20.372569932937623\n",
      "Training NF1:  71%|███████████▎    | 7073/10001 [12:50:58<1:15:31,  1.55s/batch]Batch 7000/10001 Done, mean position loss: 21.023792197704317\n",
      "Training NF1:  69%|███████████     | 6914/10001 [12:51:02<1:26:29,  1.68s/batch]Batch 7000/10001 Done, mean position loss: 21.043709192276\n",
      "Training NF1:  70%|███████████▏    | 6999/10001 [12:51:05<1:28:45,  1.77s/batch]Batch 7100/10001 Done, mean position loss: 20.48635375261307\n",
      "Training NF1:  70%|███████████▏    | 6986/10001 [12:51:07<1:11:30,  1.42s/batch]Batch 7000/10001 Done, mean position loss: 20.958407690525057\n",
      "Training NF1:  70%|███████████▏    | 6991/10001 [12:51:12<1:24:52,  1.69s/batch]Batch 7000/10001 Done, mean position loss: 20.707786331176756\n",
      "Training NF1:  70%|███████████▏    | 7030/10001 [12:51:26<1:13:06,  1.48s/batch]Batch 7000/10001 Done, mean position loss: 20.36330634355545\n",
      "Training NF1:  70%|███████████▎    | 7041/10001 [12:51:29<1:36:50,  1.96s/batch]Batch 7000/10001 Done, mean position loss: 20.449199633598326\n",
      "Training NF1:  71%|███████████▎    | 7068/10001 [12:51:29<1:23:51,  1.72s/batch]Batch 7000/10001 Done, mean position loss: 21.108818142414094\n",
      "Training NF1:  69%|███████████     | 6890/10001 [12:51:30<1:23:54,  1.62s/batch]Batch 7000/10001 Done, mean position loss: 21.57151406288147\n",
      "Training NF1:  70%|███████████▏    | 7020/10001 [12:51:32<1:31:12,  1.84s/batch]Batch 7100/10001 Done, mean position loss: 20.503251254558563\n",
      "Training NF1:  70%|███████████     | 6951/10001 [12:51:35<1:37:18,  1.91s/batch]Batch 7000/10001 Done, mean position loss: 20.711997261047365\n",
      "Training NF1:  71%|███████████▎    | 7103/10001 [12:51:36<1:22:47,  1.71s/batch]Batch 7100/10001 Done, mean position loss: 21.01579979658127\n",
      "Training NF1:  70%|███████████▎    | 7046/10001 [12:51:38<1:26:20,  1.75s/batch]Batch 7100/10001 Done, mean position loss: 21.009232289791107\n",
      "Training NF1:  70%|███████████▎    | 7046/10001 [12:51:48<1:15:31,  1.53s/batch]Batch 7100/10001 Done, mean position loss: 20.63567266702652\n",
      "Training NF1:  71%|███████████▎    | 7086/10001 [12:51:51<1:31:24,  1.88s/batch]Batch 6900/10001 Done, mean position loss: 20.677701063156128\n",
      "Training NF1:  71%|███████████▎    | 7076/10001 [12:51:59<1:23:11,  1.71s/batch]Batch 7100/10001 Done, mean position loss: 20.733390202522276\n",
      "Training NF1:  69%|███████████     | 6946/10001 [12:52:00<1:36:16,  1.89s/batch]Batch 7100/10001 Done, mean position loss: 20.461704699993135\n",
      "Training NF1:  71%|███████████▎    | 7098/10001 [12:52:12<1:25:40,  1.77s/batch]Batch 7100/10001 Done, mean position loss: 20.771417529582976\n",
      "Training NF1:  71%|███████████▎    | 7056/10001 [12:52:17<1:23:37,  1.70s/batch]Batch 7100/10001 Done, mean position loss: 20.840917236804962\n",
      "Training NF1:  70%|███████████▏    | 7023/10001 [12:52:17<1:26:24,  1.74s/batch]Batch 7100/10001 Done, mean position loss: 20.872692914009093\n",
      "Training NF1:  70%|███████████▏    | 6960/10001 [12:52:24<1:29:22,  1.76s/batch]Batch 7000/10001 Done, mean position loss: 20.336814482212066\n",
      "Training NF1:  70%|███████████▏    | 7028/10001 [12:52:26<1:29:29,  1.81s/batch]Batch 7100/10001 Done, mean position loss: 20.607625277042388\n",
      "Training NF1:  71%|███████████▎    | 7098/10001 [12:52:42<1:25:10,  1.76s/batch]Batch 7100/10001 Done, mean position loss: 20.548227870464324\n",
      "Training NF1:  70%|███████████▎    | 7046/10001 [12:52:46<1:25:18,  1.73s/batch]Batch 7100/10001 Done, mean position loss: 20.637067263126372\n",
      "Training NF1:  71%|███████████▎    | 7060/10001 [12:52:49<1:24:52,  1.73s/batch]Batch 7200/10001 Done, mean position loss: 20.470351762771607\n",
      "Training NF1:  71%|███████████▍    | 7143/10001 [12:52:50<1:23:07,  1.75s/batch]Batch 7100/10001 Done, mean position loss: 20.86392941713333\n",
      "Training NF1:  71%|███████████▎    | 7061/10001 [12:52:59<1:26:58,  1.77s/batch]Batch 7100/10001 Done, mean position loss: 20.566360359191897\n",
      "Training NF1:  71%|███████████▎    | 7065/10001 [12:53:06<1:28:32,  1.81s/batch]Batch 7000/10001 Done, mean position loss: 20.942703630924225\n",
      "Training NF1:  70%|███████████▏    | 7026/10001 [12:53:07<1:26:11,  1.74s/batch]Batch 7100/10001 Done, mean position loss: 20.66882422208786\n",
      "Training NF1:  71%|███████████▎    | 7097/10001 [12:53:09<1:27:53,  1.82s/batch]Batch 7200/10001 Done, mean position loss: 20.56034579753876\n",
      "Training NF1:  71%|███████████▎    | 7073/10001 [12:53:16<1:35:48,  1.96s/batch]Batch 7100/10001 Done, mean position loss: 20.654168150424958\n",
      "Training NF1:  71%|███████████▎    | 7056/10001 [12:53:17<1:34:12,  1.92s/batch]Batch 7200/10001 Done, mean position loss: 20.452128181457518\n",
      "Training NF1:  71%|███████████▍    | 7149/10001 [12:53:24<1:28:12,  1.86s/batch]Batch 7100/10001 Done, mean position loss: 20.427276840209963\n",
      "Training NF1:  71%|███████████▎    | 7066/10001 [12:53:25<1:28:28,  1.81s/batch]Batch 7100/10001 Done, mean position loss: 20.712901918888093\n",
      "Training NF1:  72%|███████████▌    | 7210/10001 [12:53:25<1:21:12,  1.75s/batch]Batch 7100/10001 Done, mean position loss: 20.689207429885865\n",
      "Training NF1:  71%|███████████▍    | 7122/10001 [12:53:35<1:27:10,  1.82s/batch]Batch 7100/10001 Done, mean position loss: 20.80104343175888\n",
      "Training NF1:  71%|███████████▎    | 7100/10001 [12:53:35<1:22:53,  1.71s/batch]Batch 7100/10001 Done, mean position loss: 20.533046174049375\n",
      "Training NF1:  72%|███████████▌    | 7216/10001 [12:53:37<1:42:59,  2.22s/batch]Batch 7100/10001 Done, mean position loss: 20.71096683740616\n",
      "Training NF1:  71%|███████████▎    | 7071/10001 [12:53:41<1:21:59,  1.68s/batch]Batch 7100/10001 Done, mean position loss: 20.372153944969178\n",
      "Training NF1:  71%|███████████▎    | 7085/10001 [12:53:41<1:24:11,  1.73s/batch]Batch 7000/10001 Done, mean position loss: 20.5959534573555\n",
      "Training NF1:  72%|███████████▌    | 7239/10001 [12:53:58<1:37:35,  2.12s/batch]Batch 7100/10001 Done, mean position loss: 21.004498119354245\n",
      "Training NF1:  71%|███████████▍    | 7121/10001 [12:54:00<1:14:15,  1.55s/batch]Batch 7100/10001 Done, mean position loss: 21.025747277736663\n",
      "Training NF1:  70%|███████████▎    | 7034/10001 [12:54:05<1:34:35,  1.91s/batch]Batch 7100/10001 Done, mean position loss: 20.979402630329133\n",
      "Training NF1:  71%|███████████▍    | 7117/10001 [12:54:08<1:10:54,  1.48s/batch]Batch 7200/10001 Done, mean position loss: 20.49797679424286\n",
      "Training NF1:  71%|███████████▍    | 7132/10001 [12:54:11<1:29:27,  1.87s/batch]Batch 7100/10001 Done, mean position loss: 20.70808130979538\n",
      "Training NF1:  70%|███████████▎    | 7045/10001 [12:54:24<1:30:43,  1.84s/batch]Batch 7100/10001 Done, mean position loss: 20.36513043165207\n",
      "Training NF1:  71%|███████████▍    | 7114/10001 [12:54:26<1:35:19,  1.98s/batch]Batch 7200/10001 Done, mean position loss: 20.989464280605315\n",
      "Training NF1:  70%|███████████▏    | 7028/10001 [12:54:28<1:19:28,  1.60s/batch]Batch 7100/10001 Done, mean position loss: 20.45896649837494\n",
      "Training NF1:  71%|███████████▍    | 7117/10001 [12:54:31<1:25:46,  1.78s/batch]Batch 7100/10001 Done, mean position loss: 21.061875414848327\n",
      "Training NF1:  71%|███████████▍    | 7133/10001 [12:54:32<1:23:10,  1.74s/batch]Batch 7200/10001 Done, mean position loss: 20.49514327049255\n",
      "Training NF1:  72%|███████████▍    | 7186/10001 [12:54:34<1:10:05,  1.49s/batch]Batch 7100/10001 Done, mean position loss: 21.565909001827237\n",
      "Training NF1:  71%|███████████▎    | 7101/10001 [12:54:34<1:24:26,  1.75s/batch]Batch 7100/10001 Done, mean position loss: 20.744849061965944\n",
      "Training NF1:  72%|███████████▍    | 7183/10001 [12:54:41<1:23:57,  1.79s/batch]Batch 7200/10001 Done, mean position loss: 21.01061490058899\n",
      "Training NF1:  72%|███████████▌    | 7197/10001 [12:54:45<1:23:42,  1.79s/batch]Batch 7200/10001 Done, mean position loss: 20.631686472892763\n",
      "Training NF1:  71%|███████████▎    | 7081/10001 [12:54:50<1:25:44,  1.76s/batch]Batch 7000/10001 Done, mean position loss: 20.6783380484581\n",
      "Training NF1:  72%|███████████▍    | 7159/10001 [12:54:52<1:22:33,  1.74s/batch]Batch 7200/10001 Done, mean position loss: 20.452472758293155\n",
      "Training NF1:  73%|███████████▌    | 7259/10001 [12:54:52<1:28:21,  1.93s/batch]Batch 7200/10001 Done, mean position loss: 20.74952327489853\n",
      "Training NF1:  71%|███████████▍    | 7122/10001 [12:55:00<1:27:20,  1.82s/batch]Batch 7200/10001 Done, mean position loss: 20.767497777938843\n",
      "Training NF1:  72%|███████████▍    | 7161/10001 [12:55:11<1:10:20,  1.49s/batch]Batch 7200/10001 Done, mean position loss: 20.842953684329984\n",
      "Training NF1:  72%|███████████▍    | 7183/10001 [12:55:15<1:21:50,  1.74s/batch]Batch 7200/10001 Done, mean position loss: 20.898914542198185\n",
      "Training NF1:  72%|███████████▍    | 7187/10001 [12:55:21<1:34:10,  2.01s/batch]Batch 7200/10001 Done, mean position loss: 20.591201481819155\n",
      "Training NF1:  72%|███████████▌    | 7205/10001 [12:55:28<1:25:51,  1.84s/batch]Batch 7100/10001 Done, mean position loss: 20.348906881809235\n",
      "Training NF1:  72%|███████████▍    | 7173/10001 [12:55:37<1:15:24,  1.60s/batch]Batch 7200/10001 Done, mean position loss: 20.5293647480011\n",
      "Training NF1:  72%|███████████▌    | 7218/10001 [12:55:45<1:29:03,  1.92s/batch]Batch 7300/10001 Done, mean position loss: 20.47555597782135\n",
      "Batch 7200/10001 Done, mean position loss: 20.639237401485445\n",
      "Training NF1:  72%|███████████▍    | 7182/10001 [12:55:48<1:12:01,  1.53s/batch]Batch 7200/10001 Done, mean position loss: 20.860928378105164\n",
      "Training NF1:  73%|███████████▋    | 7295/10001 [12:55:56<1:08:46,  1.53s/batch]Batch 7200/10001 Done, mean position loss: 20.563782689571383\n",
      "Training NF1:  72%|███████████▍    | 7187/10001 [12:56:02<1:21:25,  1.74s/batch]Batch 7100/10001 Done, mean position loss: 20.95840740680695\n",
      "Training NF1:  72%|███████████▍    | 7170/10001 [12:56:05<1:14:57,  1.59s/batch]Batch 7200/10001 Done, mean position loss: 20.664862165451048\n",
      "Training NF1:  72%|███████████▌    | 7240/10001 [12:56:08<1:16:47,  1.67s/batch]Batch 7300/10001 Done, mean position loss: 20.4540934920311\n",
      "Training NF1:  73%|███████████▋    | 7269/10001 [12:56:08<1:23:38,  1.84s/batch]Batch 7300/10001 Done, mean position loss: 20.56934513807297\n",
      "Training NF1:  72%|███████████▌    | 7249/10001 [12:56:09<1:23:45,  1.83s/batch]Batch 7200/10001 Done, mean position loss: 20.6518150639534\n",
      "Training NF1:  72%|███████████▍    | 7178/10001 [12:56:21<1:23:17,  1.77s/batch]Batch 7200/10001 Done, mean position loss: 20.41759481906891\n",
      "Training NF1:  72%|███████████▌    | 7195/10001 [12:56:24<1:14:40,  1.60s/batch]Batch 7200/10001 Done, mean position loss: 20.65655660390854\n",
      "Training NF1:  71%|███████████▍    | 7133/10001 [12:56:24<1:17:35,  1.62s/batch]Batch 7200/10001 Done, mean position loss: 20.700475175380706\n",
      "Training NF1:  72%|███████████▌    | 7231/10001 [12:56:31<1:12:02,  1.56s/batch]Batch 7200/10001 Done, mean position loss: 20.36799505472183\n",
      "Training NF1:  72%|███████████▌    | 7205/10001 [12:56:31<1:18:16,  1.68s/batch]Batch 7200/10001 Done, mean position loss: 20.544653780460358\n",
      "Training NF1:  72%|███████████▌    | 7249/10001 [12:56:34<1:28:54,  1.94s/batch]Batch 7200/10001 Done, mean position loss: 20.795339925289152\n",
      "Training NF1:  73%|███████████▋    | 7285/10001 [12:56:34<1:22:52,  1.83s/batch]Batch 7100/10001 Done, mean position loss: 20.62452852964401\n",
      "Training NF1:  72%|███████████▌    | 7245/10001 [12:56:37<1:28:39,  1.93s/batch]Batch 7200/10001 Done, mean position loss: 20.697059173583988\n",
      "Training NF1:  73%|███████████▋    | 7270/10001 [12:56:46<1:23:37,  1.84s/batch]Batch 7200/10001 Done, mean position loss: 21.040885429382325\n",
      "Training NF1:  72%|███████████▌    | 7229/10001 [12:56:57<1:21:56,  1.77s/batch]Batch 7200/10001 Done, mean position loss: 21.04548714637756\n",
      "Training NF1:  72%|███████████▌    | 7243/10001 [12:57:00<1:10:45,  1.54s/batch]Batch 7200/10001 Done, mean position loss: 20.69226140022278\n",
      "Training NF1:  72%|███████████▌    | 7211/10001 [12:57:02<1:15:13,  1.62s/batch]Batch 7200/10001 Done, mean position loss: 20.9725435256958\n",
      "Training NF1:  72%|███████████▌    | 7190/10001 [12:57:03<1:12:41,  1.55s/batch]Batch 7300/10001 Done, mean position loss: 20.486435012817385\n",
      "Training NF1:  72%|███████████▌    | 7198/10001 [12:57:18<1:24:22,  1.81s/batch]Batch 7200/10001 Done, mean position loss: 20.36876292228699\n",
      "Training NF1:  71%|███████████▍    | 7144/10001 [12:57:18<1:28:11,  1.85s/batch]Batch 7300/10001 Done, mean position loss: 20.978381662368776\n",
      "Training NF1:  73%|███████████▌    | 7256/10001 [12:57:23<1:19:15,  1.73s/batch]Batch 7200/10001 Done, mean position loss: 20.716536815166474\n",
      "Training NF1:  72%|███████████▌    | 7237/10001 [12:57:27<1:19:13,  1.72s/batch]Batch 7200/10001 Done, mean position loss: 20.474116849899296\n",
      "Training NF1:  72%|███████████▌    | 7248/10001 [12:57:29<1:19:04,  1.72s/batch]Batch 7200/10001 Done, mean position loss: 21.537049429416655\n",
      "Training NF1:  73%|███████████▌    | 7255/10001 [12:57:33<1:23:02,  1.81s/batch]Batch 7300/10001 Done, mean position loss: 20.511611783504485\n",
      "Batch 7200/10001 Done, mean position loss: 21.09057877779007\n",
      "Training NF1:  72%|███████████▌    | 7222/10001 [12:57:38<1:19:56,  1.73s/batch]Batch 7300/10001 Done, mean position loss: 20.977966742515562\n",
      "Training NF1:  73%|███████████▋    | 7306/10001 [12:57:41<1:10:58,  1.58s/batch]Batch 7300/10001 Done, mean position loss: 20.456200971603394\n",
      "Training NF1:  72%|███████████▌    | 7242/10001 [12:57:44<1:39:39,  2.17s/batch]Batch 7300/10001 Done, mean position loss: 20.63589197397232\n",
      "Training NF1:  73%|███████████▌    | 7256/10001 [12:57:47<1:25:53,  1.88s/batch]Batch 7300/10001 Done, mean position loss: 20.718682560920712\n",
      "Training NF1:  73%|███████████▋    | 7276/10001 [12:57:52<1:26:18,  1.90s/batch]Batch 7100/10001 Done, mean position loss: 20.69038078069687\n",
      "Training NF1:  72%|███████████▌    | 7248/10001 [12:57:54<1:13:52,  1.61s/batch]Batch 7300/10001 Done, mean position loss: 20.74618397474289\n",
      "Training NF1:  73%|███████████▋    | 7311/10001 [12:58:04<1:11:03,  1.58s/batch]Batch 7300/10001 Done, mean position loss: 20.844033529758455\n",
      "Training NF1:  73%|███████████▌    | 7262/10001 [12:58:17<1:14:39,  1.64s/batch]Batch 7300/10001 Done, mean position loss: 20.919725682735443\n",
      "Training NF1:  73%|███████████▌    | 7261/10001 [12:58:18<1:21:12,  1.78s/batch]Batch 7300/10001 Done, mean position loss: 20.588906149864197\n",
      "Training NF1:  73%|███████████▋    | 7298/10001 [12:58:28<1:22:24,  1.83s/batch]Batch 7200/10001 Done, mean position loss: 20.345335743427277\n",
      "Training NF1:  73%|███████████▋    | 7342/10001 [12:58:34<1:31:36,  2.07s/batch]Batch 7300/10001 Done, mean position loss: 20.54247162103653\n",
      "Training NF1:  74%|███████████▊    | 7398/10001 [12:58:43<1:23:42,  1.93s/batch]Batch 7300/10001 Done, mean position loss: 20.852994117736817\n",
      "Training NF1:  74%|███████████▊    | 7391/10001 [12:58:43<1:23:27,  1.92s/batch]Batch 7300/10001 Done, mean position loss: 20.640103046894076\n",
      "Training NF1:  74%|███████████▊    | 7394/10001 [12:58:48<1:18:22,  1.80s/batch]Batch 7400/10001 Done, mean position loss: 20.480856590270996\n",
      "Training NF1:  72%|███████████▌    | 7200/10001 [12:58:57<1:26:58,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 20.569875183105466\n",
      "Training NF1:  73%|███████████▌    | 7255/10001 [12:58:58<1:21:25,  1.78s/batch]Batch 7200/10001 Done, mean position loss: 20.963006381988524\n",
      "Training NF1:  73%|███████████▊    | 7348/10001 [12:59:01<1:28:24,  2.00s/batch]Batch 7400/10001 Done, mean position loss: 20.45605916261673\n",
      "Training NF1:  72%|███████████▌    | 7223/10001 [12:59:06<1:18:32,  1.70s/batch]Batch 7400/10001 Done, mean position loss: 20.57025122642517\n",
      "Training NF1:  74%|███████████▊    | 7368/10001 [12:59:07<1:18:16,  1.78s/batch]Batch 7300/10001 Done, mean position loss: 20.63261218070984\n",
      "Training NF1:  73%|███████████▋    | 7273/10001 [12:59:08<1:14:42,  1.64s/batch]Batch 7300/10001 Done, mean position loss: 20.680233666896818\n",
      "Training NF1:  73%|███████████▋    | 7332/10001 [12:59:16<1:37:29,  2.19s/batch]Batch 7300/10001 Done, mean position loss: 20.411346185207368\n",
      "Training NF1:  73%|███████████▋    | 7308/10001 [12:59:21<1:24:35,  1.88s/batch]Batch 7300/10001 Done, mean position loss: 20.68013310432434\n",
      "Training NF1:  74%|███████████▊    | 7371/10001 [12:59:25<1:15:14,  1.72s/batch]Batch 7300/10001 Done, mean position loss: 20.539872224330903\n",
      "Training NF1:  73%|███████████▋    | 7305/10001 [12:59:27<1:16:41,  1.71s/batch]Batch 7300/10001 Done, mean position loss: 20.364075834751127\n",
      "Training NF1:  73%|███████████▋    | 7273/10001 [12:59:28<1:13:12,  1.61s/batch]Batch 7300/10001 Done, mean position loss: 20.786651651859284\n",
      "Training NF1:  73%|███████████▋    | 7285/10001 [12:59:29<1:16:43,  1.69s/batch]Batch 7300/10001 Done, mean position loss: 20.696727163791657\n",
      "Training NF1:  74%|███████████▊    | 7375/10001 [12:59:32<1:06:21,  1.52s/batch]Batch 7200/10001 Done, mean position loss: 20.618615369796753\n",
      "Training NF1:  72%|███████████▌    | 7201/10001 [12:59:32<1:16:52,  1.65s/batch]Batch 7300/10001 Done, mean position loss: 20.7264763712883\n",
      "Training NF1:  73%|███████████▋    | 7274/10001 [12:59:44<1:38:03,  2.16s/batch]Batch 7300/10001 Done, mean position loss: 21.040838534832\n",
      "Training NF1:  73%|███████████▋    | 7326/10001 [12:59:53<1:16:33,  1.72s/batch]Batch 7300/10001 Done, mean position loss: 21.059349846839904\n",
      "Training NF1:  72%|███████████▌    | 7234/10001 [12:59:57<1:18:32,  1.70s/batch]Batch 7300/10001 Done, mean position loss: 20.709569873809812\n",
      "Training NF1:  73%|███████████▋    | 7323/10001 [12:59:58<1:12:36,  1.63s/batch]Batch 7300/10001 Done, mean position loss: 20.972809112071992\n",
      "Training NF1:  73%|███████████▋    | 7288/10001 [13:00:05<1:16:28,  1.69s/batch]Batch 7400/10001 Done, mean position loss: 20.482001698017122\n",
      "Training NF1:  73%|███████████▋    | 7290/10001 [13:00:15<1:31:59,  2.04s/batch]Batch 7300/10001 Done, mean position loss: 20.373753538131712\n",
      "Training NF1:  73%|███████████▌    | 7264/10001 [13:00:18<1:25:04,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 20.69368726968765\n",
      "Training NF1:  74%|███████████▊    | 7390/10001 [13:00:21<1:06:38,  1.53s/batch]Batch 7400/10001 Done, mean position loss: 20.97338589668274\n",
      "Training NF1:  74%|███████████▉    | 7449/10001 [13:00:28<1:19:24,  1.87s/batch]Batch 7300/10001 Done, mean position loss: 20.483663523197173\n",
      "Training NF1:  74%|███████████▊    | 7395/10001 [13:00:32<1:09:56,  1.61s/batch]Batch 7300/10001 Done, mean position loss: 21.10362877845764\n",
      "Training NF1:  74%|███████████▊    | 7363/10001 [13:00:32<1:18:29,  1.79s/batch]Batch 7400/10001 Done, mean position loss: 20.509528512954713\n",
      "Training NF1:  73%|███████████▋    | 7338/10001 [13:00:34<1:22:17,  1.85s/batch]Batch 7400/10001 Done, mean position loss: 20.96945400953293\n",
      "Training NF1:  74%|███████████▊    | 7358/10001 [13:00:36<1:20:04,  1.82s/batch]Batch 7300/10001 Done, mean position loss: 21.57821009635925\n",
      "Training NF1:  74%|███████████▊    | 7395/10001 [13:00:40<1:23:08,  1.91s/batch]Batch 7400/10001 Done, mean position loss: 20.456967375278474\n",
      "Training NF1:  74%|███████████▊    | 7403/10001 [13:00:42<1:12:41,  1.68s/batch]Batch 7400/10001 Done, mean position loss: 20.729299280643463\n",
      "Training NF1:  74%|███████████▊    | 7358/10001 [13:00:44<1:20:15,  1.82s/batch]Batch 7400/10001 Done, mean position loss: 20.628724298477174\n",
      "Training NF1:  72%|███████████▌    | 7244/10001 [13:00:50<1:16:14,  1.66s/batch]Batch 7400/10001 Done, mean position loss: 20.754733898639678\n",
      "Training NF1:  74%|███████████▊    | 7366/10001 [13:00:50<1:22:31,  1.88s/batch]Batch 7200/10001 Done, mean position loss: 20.6909134221077\n",
      "Training NF1:  73%|███████████▋    | 7341/10001 [13:00:53<1:17:58,  1.76s/batch]Batch 7400/10001 Done, mean position loss: 20.8691831278801\n",
      "Training NF1:  73%|███████████▋    | 7272/10001 [13:01:04<1:23:07,  1.83s/batch]Batch 7400/10001 Done, mean position loss: 20.613277747631074\n",
      "Training NF1:  74%|███████████▊    | 7360/10001 [13:01:21<1:23:52,  1.91s/batch]Batch 7300/10001 Done, mean position loss: 20.345925974845887\n",
      "Training NF1:  74%|███████████▉    | 7450/10001 [13:01:29<1:11:52,  1.69s/batch]Batch 7400/10001 Done, mean position loss: 20.880222873687742\n",
      "Training NF1:  73%|███████████▋    | 7312/10001 [13:01:38<1:15:10,  1.68s/batch]Batch 7400/10001 Done, mean position loss: 20.534683027267455\n",
      "Training NF1:  74%|███████████▉    | 7434/10001 [13:01:40<1:18:33,  1.84s/batch]Batch 7400/10001 Done, mean position loss: 20.85769750356674\n",
      "Training NF1:  74%|███████████▊    | 7361/10001 [13:01:40<1:18:46,  1.79s/batch]Batch 7500/10001 Done, mean position loss: 20.48720504999161\n",
      "Training NF1:  74%|███████████▊    | 7372/10001 [13:01:42<1:14:59,  1.71s/batch]Batch 7400/10001 Done, mean position loss: 20.619151854515074\n",
      "Training NF1:  74%|███████████▉    | 7435/10001 [13:01:48<1:01:39,  1.44s/batch]Batch 7400/10001 Done, mean position loss: 20.569441690444947\n",
      "Training NF1:  74%|█████████████▍    | 7439/10001 [13:01:54<59:13,  1.39s/batch]Batch 7300/10001 Done, mean position loss: 20.9362357878685\n",
      "Training NF1:  74%|███████████▉    | 7449/10001 [13:01:56<1:22:39,  1.94s/batch]Batch 7500/10001 Done, mean position loss: 20.54353133201599\n",
      "Training NF1:  75%|███████████▉    | 7499/10001 [13:01:59<1:07:07,  1.61s/batch]Batch 7400/10001 Done, mean position loss: 20.642965059280396\n",
      "Training NF1:  75%|████████████    | 7505/10001 [13:02:03<1:10:13,  1.69s/batch]Batch 7500/10001 Done, mean position loss: 20.463695380687714\n",
      "Training NF1:  75%|███████████▉    | 7451/10001 [13:02:05<1:11:27,  1.68s/batch]Batch 7400/10001 Done, mean position loss: 20.68130785226822\n",
      "Training NF1:  74%|███████████▉    | 7442/10001 [13:02:14<1:15:49,  1.78s/batch]Batch 7400/10001 Done, mean position loss: 20.410022044181822\n",
      "Training NF1:  74%|███████████▊    | 7417/10001 [13:02:16<1:16:24,  1.77s/batch]Batch 7400/10001 Done, mean position loss: 20.667767841815948\n",
      "Training NF1:  74%|███████████▊    | 7359/10001 [13:02:16<1:03:30,  1.44s/batch]Batch 7400/10001 Done, mean position loss: 20.695106778144837\n",
      "Training NF1:  74%|███████████▊    | 7397/10001 [13:02:20<1:32:34,  2.13s/batch]Batch 7400/10001 Done, mean position loss: 20.386619412899016\n",
      "Training NF1:  75%|███████████▉    | 7482/10001 [13:02:22<1:10:19,  1.68s/batch]Batch 7400/10001 Done, mean position loss: 20.526571617126464\n",
      "Training NF1:  73%|███████████▌    | 7253/10001 [13:02:27<1:28:35,  1.93s/batch]Batch 7400/10001 Done, mean position loss: 20.759168493747712\n",
      "Training NF1:  75%|███████████▉    | 7468/10001 [13:02:28<1:05:30,  1.55s/batch]Batch 7400/10001 Done, mean position loss: 21.05125299692154\n",
      "Training NF1:  74%|███████████▊    | 7418/10001 [13:02:31<1:05:45,  1.53s/batch]Batch 7400/10001 Done, mean position loss: 20.71398912191391\n",
      "Training NF1:  75%|███████████▉    | 7481/10001 [13:02:36<1:10:32,  1.68s/batch]Batch 7300/10001 Done, mean position loss: 20.62229118824005\n",
      "Training NF1:  75%|███████████▉    | 7474/10001 [13:02:39<1:12:20,  1.72s/batch]Batch 7400/10001 Done, mean position loss: 21.048872499465944\n",
      "Training NF1:  75%|███████████▉    | 7470/10001 [13:02:49<1:20:08,  1.90s/batch]Batch 7400/10001 Done, mean position loss: 20.967190034389496\n",
      "Training NF1:  74%|███████████▊    | 7420/10001 [13:02:51<1:23:54,  1.95s/batch]Batch 7400/10001 Done, mean position loss: 20.712933812141415\n",
      "Training NF1:  73%|███████████▋    | 7315/10001 [13:02:56<1:05:33,  1.46s/batch]Batch 7500/10001 Done, mean position loss: 20.48811016559601\n",
      "Training NF1:  73%|███████████▋    | 7320/10001 [13:03:03<1:00:44,  1.36s/batch]Batch 7400/10001 Done, mean position loss: 20.712245676517487\n",
      "Training NF1:  75%|███████████▉    | 7455/10001 [13:03:04<1:13:09,  1.72s/batch]Batch 7400/10001 Done, mean position loss: 20.36704379081726\n",
      "Training NF1:  75%|████████████    | 7550/10001 [13:03:08<1:18:54,  1.93s/batch]Batch 7500/10001 Done, mean position loss: 20.97234559059143\n",
      "Training NF1:  74%|███████████▉    | 7438/10001 [13:03:19<1:10:06,  1.64s/batch]Batch 7500/10001 Done, mean position loss: 20.49826425552368\n",
      "Training NF1:  74%|███████████▉    | 7425/10001 [13:03:21<1:11:06,  1.66s/batch]Batch 7400/10001 Done, mean position loss: 20.45323481321335\n",
      "Training NF1:  76%|████████████    | 7551/10001 [13:03:26<1:06:18,  1.62s/batch]Batch 7500/10001 Done, mean position loss: 20.978108277320864\n",
      "Training NF1:  75%|███████████▉    | 7457/10001 [13:03:32<1:12:09,  1.70s/batch]Batch 7500/10001 Done, mean position loss: 20.73537659168243\n",
      "Training NF1:  75%|████████████    | 7502/10001 [13:03:33<1:10:41,  1.70s/batch]Batch 7400/10001 Done, mean position loss: 21.091382811069487\n",
      "Training NF1:  74%|███████████▉    | 7441/10001 [13:03:33<1:18:22,  1.84s/batch]Batch 7500/10001 Done, mean position loss: 20.63565513134003\n",
      "Batch 7400/10001 Done, mean position loss: 21.560893099308014\n",
      "Training NF1:  74%|███████████▉    | 7444/10001 [13:03:37<1:21:43,  1.92s/batch]Batch 7500/10001 Done, mean position loss: 20.45461447715759\n",
      "Training NF1:  76%|█████████████▌    | 7561/10001 [13:03:45<57:41,  1.42s/batch]Batch 7500/10001 Done, mean position loss: 20.74290770292282\n",
      "Training NF1:  74%|█████████████▎    | 7427/10001 [13:03:45<56:43,  1.32s/batch]Batch 7500/10001 Done, mean position loss: 20.846313843727113\n",
      "Training NF1:  74%|███████████▉    | 7436/10001 [13:03:50<1:14:32,  1.74s/batch]Batch 7300/10001 Done, mean position loss: 20.682849090099335\n",
      "Training NF1:  75%|████████████    | 7524/10001 [13:03:56<1:07:35,  1.64s/batch]Batch 7500/10001 Done, mean position loss: 20.58772818565369\n",
      "Training NF1:  75%|███████████▉    | 7464/10001 [13:04:10<1:16:52,  1.82s/batch]Batch 7400/10001 Done, mean position loss: 20.328933181762693\n",
      "Training NF1:  75%|███████████▉    | 7497/10001 [13:04:24<1:08:31,  1.64s/batch]Batch 7500/10001 Done, mean position loss: 20.87161562681198\n",
      "Training NF1:  75%|███████████▉    | 7474/10001 [13:04:24<1:33:41,  2.22s/batch]Batch 7500/10001 Done, mean position loss: 20.629391875267032\n",
      "Training NF1:  75%|███████████▉    | 7464/10001 [13:04:31<1:11:10,  1.68s/batch]Batch 7500/10001 Done, mean position loss: 20.851681380271913\n",
      "Training NF1:  75%|███████████▉    | 7489/10001 [13:04:33<1:11:37,  1.71s/batch]Batch 7500/10001 Done, mean position loss: 20.53487377166748\n",
      "Training NF1:  75%|█████████████▍    | 7467/10001 [13:04:35<58:46,  1.39s/batch]Batch 7500/10001 Done, mean position loss: 20.57075333595276\n",
      "Training NF1:  75%|███████████▉    | 7466/10001 [13:04:39<1:04:46,  1.53s/batch]Batch 7600/10001 Done, mean position loss: 20.487977705001832\n",
      "Training NF1:  76%|████████████▏   | 7593/10001 [13:04:45<1:11:59,  1.79s/batch]Batch 7400/10001 Done, mean position loss: 20.928549654483795\n",
      "Training NF1:  75%|█████████████▍    | 7486/10001 [13:04:45<58:16,  1.39s/batch]Batch 7500/10001 Done, mean position loss: 20.632000460624692\n",
      "Training NF1:  74%|█████████████▍    | 7447/10001 [13:04:51<59:25,  1.40s/batch]Batch 7600/10001 Done, mean position loss: 20.460695312023162\n",
      "Training NF1:  76%|████████████    | 7575/10001 [13:04:51<1:13:16,  1.81s/batch]Batch 7500/10001 Done, mean position loss: 20.67363654613495\n",
      "Training NF1:  75%|████████████    | 7506/10001 [13:05:00<1:06:37,  1.60s/batch]Batch 7600/10001 Done, mean position loss: 20.569993176460265\n",
      "Training NF1:  75%|███████████▉    | 7483/10001 [13:05:11<1:27:51,  2.09s/batch]Batch 7500/10001 Done, mean position loss: 20.393925814628602\n",
      "Training NF1:  75%|███████████▉    | 7488/10001 [13:05:11<1:14:02,  1.77s/batch]Batch 7500/10001 Done, mean position loss: 20.69483982563019\n",
      "Training NF1:  75%|████████████    | 7550/10001 [13:05:11<1:10:06,  1.72s/batch]Batch 7500/10001 Done, mean position loss: 20.674851334095003\n",
      "Training NF1:  75%|████████████    | 7519/10001 [13:05:17<1:16:39,  1.85s/batch]Batch 7500/10001 Done, mean position loss: 20.532780940532685\n",
      "Training NF1:  75%|█████████████▌    | 7506/10001 [13:05:17<50:32,  1.22s/batch]Batch 7500/10001 Done, mean position loss: 20.788149187564848\n",
      "Training NF1:  75%|███████████▉    | 7488/10001 [13:05:19<1:15:39,  1.81s/batch]Batch 7500/10001 Done, mean position loss: 20.356034221649168\n",
      "Training NF1:  75%|████████████    | 7518/10001 [13:05:21<1:12:44,  1.76s/batch]Batch 7500/10001 Done, mean position loss: 20.71407783508301\n",
      "Training NF1:  76%|████████████▏   | 7582/10001 [13:05:20<1:04:57,  1.61s/batch]Batch 7500/10001 Done, mean position loss: 21.04547157764435\n",
      "Training NF1:  76%|████████████    | 7575/10001 [13:05:23<1:28:45,  2.20s/batch]Batch 7400/10001 Done, mean position loss: 20.61594565629959\n",
      "Training NF1:  76%|████████████▏   | 7589/10001 [13:05:32<1:02:17,  1.55s/batch]Batch 7500/10001 Done, mean position loss: 21.021961410045627\n",
      "Training NF1:  75%|████████████    | 7526/10001 [13:05:36<1:07:33,  1.64s/batch]Batch 7600/10001 Done, mean position loss: 20.495417308807372\n",
      "Training NF1:  76%|█████████████▋    | 7595/10001 [13:05:40<52:27,  1.31s/batch]Batch 7500/10001 Done, mean position loss: 20.947906506061557\n",
      "Training NF1:  75%|███████████▉    | 7456/10001 [13:05:45<1:23:32,  1.97s/batch]Batch 7500/10001 Done, mean position loss: 20.701672224998475\n",
      "Training NF1:  76%|████████████▏   | 7610/10001 [13:05:51<1:21:20,  2.04s/batch]Batch 7600/10001 Done, mean position loss: 20.97006965875626\n",
      "Training NF1:  75%|████████████    | 7520/10001 [13:05:55<1:28:52,  2.15s/batch]Batch 7500/10001 Done, mean position loss: 20.370524010658265\n",
      "Training NF1:  75%|████████████    | 7520/10001 [13:05:57<1:25:03,  2.06s/batch]Batch 7500/10001 Done, mean position loss: 20.71867417812347\n",
      "Training NF1:  75%|████████████    | 7515/10001 [13:06:09<1:17:30,  1.87s/batch]Batch 7600/10001 Done, mean position loss: 20.49590751647949\n",
      "Training NF1:  76%|████████████▏   | 7623/10001 [13:06:14<1:04:23,  1.62s/batch]Batch 7500/10001 Done, mean position loss: 20.449085426330566\n",
      "Training NF1:  76%|████████████▏   | 7649/10001 [13:06:19<1:10:18,  1.79s/batch]Batch 7600/10001 Done, mean position loss: 20.73131268978119\n",
      "Training NF1:  76%|████████████▏   | 7620/10001 [13:06:27<1:14:15,  1.87s/batch]Batch 7600/10001 Done, mean position loss: 20.99327784061432\n",
      "Training NF1:  76%|████████████    | 7569/10001 [13:06:29<1:08:25,  1.69s/batch]Batch 7600/10001 Done, mean position loss: 20.6232692527771\n",
      "Training NF1:  75%|████████████    | 7546/10001 [13:06:30<1:13:43,  1.80s/batch]Batch 7500/10001 Done, mean position loss: 21.5456444978714\n",
      "Training NF1:  75%|████████████    | 7526/10001 [13:06:32<1:06:26,  1.61s/batch]Batch 7500/10001 Done, mean position loss: 21.09433799266815\n",
      "Training NF1:  76%|████████████    | 7575/10001 [13:06:37<1:21:27,  2.01s/batch]Batch 7600/10001 Done, mean position loss: 20.73983425617218\n",
      "Training NF1:  75%|█████████████▌    | 7547/10001 [13:06:37<57:51,  1.41s/batch]Batch 7600/10001 Done, mean position loss: 20.45988158941269\n",
      "Training NF1:  76%|████████████▏   | 7619/10001 [13:06:41<1:08:57,  1.74s/batch]Batch 7600/10001 Done, mean position loss: 20.85824249982834\n",
      "Training NF1:  76%|████████████    | 7566/10001 [13:06:50<1:16:30,  1.89s/batch]Batch 7400/10001 Done, mean position loss: 20.689033205509187\n",
      "Training NF1:  76%|████████████▏   | 7607/10001 [13:06:52<1:18:36,  1.97s/batch]Batch 7600/10001 Done, mean position loss: 20.572364733219146\n",
      "Training NF1:  76%|████████████    | 7560/10001 [13:07:07<1:07:06,  1.65s/batch]Batch 7500/10001 Done, mean position loss: 20.325380172729492\n",
      "Training NF1:  76%|████████████▏   | 7594/10001 [13:07:12<1:01:28,  1.53s/batch]Batch 7600/10001 Done, mean position loss: 20.610576786994933\n",
      "Training NF1:  76%|████████████    | 7571/10001 [13:07:22<1:13:24,  1.81s/batch]Batch 7600/10001 Done, mean position loss: 20.873909509181978\n",
      "Training NF1:  76%|████████████▏   | 7625/10001 [13:07:24<1:10:56,  1.79s/batch]Batch 7600/10001 Done, mean position loss: 20.84131399154663\n",
      "Training NF1:  75%|███████████▉    | 7470/10001 [13:07:24<1:05:53,  1.56s/batch]Batch 7600/10001 Done, mean position loss: 20.535469126701358\n",
      "Training NF1:  77%|████████████▎   | 7690/10001 [13:07:31<1:07:22,  1.75s/batch]Batch 7600/10001 Done, mean position loss: 20.558825380802155\n",
      "Training NF1:  76%|████████████    | 7572/10001 [13:07:31<1:18:40,  1.94s/batch]Batch 7700/10001 Done, mean position loss: 20.490546190738677\n",
      "Training NF1:  77%|████████████▎   | 7696/10001 [13:07:41<1:05:30,  1.71s/batch]Batch 7500/10001 Done, mean position loss: 20.923093910217283\n",
      "Training NF1:  76%|████████████▏   | 7610/10001 [13:07:47<1:09:38,  1.75s/batch]Batch 7600/10001 Done, mean position loss: 20.64173115491867\n",
      "Training NF1:  75%|████████████    | 7526/10001 [13:07:49<1:10:41,  1.71s/batch]Batch 7700/10001 Done, mean position loss: 20.454049746990204\n",
      "Training NF1:  77%|████████████▎   | 7701/10001 [13:07:49<1:08:19,  1.78s/batch]Batch 7700/10001 Done, mean position loss: 20.564551672935487\n",
      "Training NF1:  77%|████████████▎   | 7678/10001 [13:07:51<1:16:15,  1.97s/batch]Batch 7600/10001 Done, mean position loss: 20.637189147472384\n",
      "Training NF1:  76%|████████████    | 7576/10001 [13:08:04<1:07:16,  1.66s/batch]Batch 7600/10001 Done, mean position loss: 20.40073526144028\n",
      "Training NF1:  76%|████████████▏   | 7579/10001 [13:08:06<1:03:36,  1.58s/batch]Batch 7600/10001 Done, mean position loss: 20.700538432598115\n",
      "Training NF1:  77%|████████████▎   | 7668/10001 [13:08:08<1:13:28,  1.89s/batch]Batch 7600/10001 Done, mean position loss: 20.69840081691742\n",
      "Training NF1:  77%|████████████▎   | 7714/10001 [13:08:10<1:04:30,  1.69s/batch]Batch 7600/10001 Done, mean position loss: 20.52367878675461\n",
      "Training NF1:  76%|████████████▏   | 7599/10001 [13:08:15<1:11:45,  1.79s/batch]Batch 7600/10001 Done, mean position loss: 20.35671990156174\n",
      "Training NF1:  77%|████████████▏   | 7656/10001 [13:08:16<1:08:52,  1.76s/batch]Batch 7500/10001 Done, mean position loss: 20.60503231048584\n",
      "Training NF1:  76%|████████████    | 7570/10001 [13:08:19<1:11:41,  1.77s/batch]Batch 7600/10001 Done, mean position loss: 20.794476437568665\n",
      "Training NF1:  76%|████████████▏   | 7630/10001 [13:08:21<1:06:31,  1.68s/batch]Batch 7600/10001 Done, mean position loss: 21.04557522535324\n",
      "Training NF1:  77%|█████████████▊    | 7695/10001 [13:08:21<58:19,  1.52s/batch]Batch 7600/10001 Done, mean position loss: 20.703492660522464\n",
      "Training NF1:  77%|████████████▎   | 7673/10001 [13:08:31<1:11:37,  1.85s/batch]Batch 7700/10001 Done, mean position loss: 20.491459572315215\n",
      "Batch 7600/10001 Done, mean position loss: 20.992732043266294\n",
      "Training NF1:  76%|████████████▏   | 7643/10001 [13:08:36<1:15:13,  1.91s/batch]Batch 7600/10001 Done, mean position loss: 20.946013698577882\n",
      "Training NF1:  77%|████████████▎   | 7681/10001 [13:08:42<1:05:46,  1.70s/batch]Batch 7600/10001 Done, mean position loss: 20.694156382083893\n",
      "Training NF1:  77%|████████████▎   | 7685/10001 [13:08:49<1:09:10,  1.79s/batch]Batch 7700/10001 Done, mean position loss: 20.973347589969634\n",
      "Training NF1:  76%|████████████▏   | 7624/10001 [13:08:52<1:18:41,  1.99s/batch]Batch 7600/10001 Done, mean position loss: 20.365188088417053\n",
      "Training NF1:  77%|████████████▎   | 7686/10001 [13:08:52<1:16:35,  1.98s/batch]Batch 7600/10001 Done, mean position loss: 20.722348487377168\n",
      "Training NF1:  77%|████████████▎   | 7684/10001 [13:09:03<1:04:08,  1.66s/batch]Batch 7700/10001 Done, mean position loss: 20.491185126304625\n",
      "Training NF1:  77%|████████████▎   | 7698/10001 [13:09:13<1:02:44,  1.63s/batch]Batch 7600/10001 Done, mean position loss: 20.471237657070162\n",
      "Training NF1:  77%|████████████▎   | 7692/10001 [13:09:17<1:06:58,  1.74s/batch]Batch 7700/10001 Done, mean position loss: 20.757760655879974\n",
      "Training NF1:  77%|████████████▎   | 7693/10001 [13:09:18<1:03:58,  1.66s/batch]Batch 7700/10001 Done, mean position loss: 20.98897588968277\n",
      "Training NF1:  78%|████████████▍   | 7755/10001 [13:09:22<1:03:15,  1.69s/batch]Batch 7600/10001 Done, mean position loss: 21.566256163120272\n",
      "Training NF1:  77%|████████████▎   | 7695/10001 [13:09:22<1:11:17,  1.85s/batch]Batch 7700/10001 Done, mean position loss: 20.443033223152163\n",
      "Training NF1:  77%|████████████▏   | 7655/10001 [13:09:24<1:10:16,  1.80s/batch]Batch 7700/10001 Done, mean position loss: 20.62827851295471\n",
      "Training NF1:  76%|████████████▏   | 7585/10001 [13:09:30<1:02:12,  1.55s/batch]Batch 7700/10001 Done, mean position loss: 20.751876637935638\n",
      "Training NF1:  77%|████████████▎   | 7692/10001 [13:09:31<1:05:20,  1.70s/batch]Batch 7600/10001 Done, mean position loss: 21.07022411584854\n",
      "Training NF1:  77%|█████████████▊    | 7666/10001 [13:09:35<59:40,  1.53s/batch]Batch 7700/10001 Done, mean position loss: 20.844734115600588\n",
      "Training NF1:  77%|████████████▏   | 7652/10001 [13:09:46<1:05:04,  1.66s/batch]Batch 7500/10001 Done, mean position loss: 20.67855635643005\n",
      "Batch 7700/10001 Done, mean position loss: 20.565050778388976\n",
      "Training NF1:  77%|████████████▎   | 7686/10001 [13:09:59<1:06:28,  1.72s/batch]Batch 7600/10001 Done, mean position loss: 20.3294122838974\n",
      "Training NF1:  77%|████████████▎   | 7735/10001 [13:10:05<1:17:09,  2.04s/batch]Batch 7700/10001 Done, mean position loss: 20.609316372871398\n",
      "Training NF1:  77%|████████████▎   | 7697/10001 [13:10:19<1:07:57,  1.77s/batch]Batch 7700/10001 Done, mean position loss: 20.887539842128753\n",
      "Training NF1:  77%|████████████▎   | 7677/10001 [13:10:19<1:09:47,  1.80s/batch]Batch 7700/10001 Done, mean position loss: 20.83103364467621\n",
      "Training NF1:  77%|████████████▎   | 7733/10001 [13:10:20<1:20:34,  2.13s/batch]Batch 7800/10001 Done, mean position loss: 20.475538318157195\n",
      "Training NF1:  77%|████████████▎   | 7694/10001 [13:10:22<1:10:52,  1.84s/batch]Batch 7700/10001 Done, mean position loss: 20.55041191101074\n",
      "Training NF1:  76%|████████████▏   | 7632/10001 [13:10:24<1:14:57,  1.90s/batch]Batch 7700/10001 Done, mean position loss: 20.52582640171051\n",
      "Training NF1:  77%|████████████▍   | 7743/10001 [13:10:34<1:13:00,  1.94s/batch]Batch 7700/10001 Done, mean position loss: 20.65784489631653\n",
      "Training NF1:  76%|████████████▏   | 7645/10001 [13:10:38<1:08:55,  1.76s/batch]Batch 7800/10001 Done, mean position loss: 20.462297191619875\n",
      "Training NF1:  78%|████████████▍   | 7755/10001 [13:10:40<1:06:22,  1.77s/batch]Batch 7800/10001 Done, mean position loss: 20.567219235897063\n",
      "Training NF1:  78%|████████████▍   | 7801/10001 [13:10:40<1:05:35,  1.79s/batch]Batch 7600/10001 Done, mean position loss: 20.907378256320953\n",
      "Training NF1:  78%|████████████▍   | 7778/10001 [13:10:47<1:01:00,  1.65s/batch]Batch 7700/10001 Done, mean position loss: 20.62375240087509\n",
      "Training NF1:  78%|████████████▌   | 7821/10001 [13:10:54<1:12:13,  1.99s/batch]Batch 7700/10001 Done, mean position loss: 20.661026163101194\n",
      "Training NF1:  76%|████████████▏   | 7614/10001 [13:11:01<1:07:22,  1.69s/batch]Batch 7700/10001 Done, mean position loss: 20.41940576791763\n",
      "Training NF1:  78%|████████████▍   | 7762/10001 [13:11:01<1:00:52,  1.63s/batch]Batch 7700/10001 Done, mean position loss: 20.69038873434067\n",
      "Training NF1:  77%|████████████▎   | 7707/10001 [13:11:04<1:02:34,  1.64s/batch]Batch 7700/10001 Done, mean position loss: 20.53239444732666\n",
      "Training NF1:  77%|████████████▎   | 7694/10001 [13:11:09<1:08:29,  1.78s/batch]Batch 7700/10001 Done, mean position loss: 20.356199328899386\n",
      "Training NF1:  77%|████████████▎   | 7683/10001 [13:11:11<1:13:05,  1.89s/batch]Batch 7700/10001 Done, mean position loss: 20.781067221164705\n",
      "Training NF1:  78%|████████████▍   | 7765/10001 [13:11:17<1:15:19,  2.02s/batch]Batch 7600/10001 Done, mean position loss: 20.596869156360626\n",
      "Training NF1:  77%|████████████▎   | 7720/10001 [13:11:18<1:08:49,  1.81s/batch]Batch 7700/10001 Done, mean position loss: 21.024037051200867\n",
      "Training NF1:  77%|████████████▎   | 7687/10001 [13:11:19<1:14:15,  1.93s/batch]Batch 7700/10001 Done, mean position loss: 20.68595010519028\n",
      "Training NF1:  77%|████████████▎   | 7734/10001 [13:11:22<1:10:10,  1.86s/batch]Batch 7700/10001 Done, mean position loss: 21.023762295246122\n",
      "Training NF1:  77%|████████████▎   | 7707/10001 [13:11:23<1:11:38,  1.87s/batch]Batch 7800/10001 Done, mean position loss: 20.487847530841826\n",
      "Training NF1:  77%|████████████▎   | 7695/10001 [13:11:34<1:15:15,  1.96s/batch]Batch 7700/10001 Done, mean position loss: 20.95918039083481\n",
      "Training NF1:  77%|████████████▎   | 7733/10001 [13:11:42<1:11:02,  1.88s/batch]Batch 7700/10001 Done, mean position loss: 20.69216203927994\n",
      "Training NF1:  77%|████████████▎   | 7677/10001 [13:11:45<1:08:14,  1.76s/batch]Batch 7700/10001 Done, mean position loss: 20.358329100608827\n",
      "Training NF1:  78%|████████████▍   | 7751/10001 [13:11:48<1:02:48,  1.68s/batch]Batch 7800/10001 Done, mean position loss: 21.00039587020874\n",
      "Training NF1:  78%|██████████████    | 7790/10001 [13:11:52<55:09,  1.50s/batch]Batch 7700/10001 Done, mean position loss: 20.7270817899704\n",
      "Training NF1:  77%|████████████▎   | 7672/10001 [13:12:02<1:17:12,  1.99s/batch]Batch 7800/10001 Done, mean position loss: 20.488468611240386\n",
      "Training NF1:  78%|████████████▍   | 7773/10001 [13:12:08<1:07:56,  1.83s/batch]Batch 7800/10001 Done, mean position loss: 20.73412362098694\n",
      "Training NF1:  78%|████████████▍   | 7793/10001 [13:12:09<1:06:00,  1.79s/batch]Batch 7700/10001 Done, mean position loss: 20.47683287143707\n",
      "Training NF1:  77%|████████████▍   | 7741/10001 [13:12:13<1:05:00,  1.73s/batch]Batch 7800/10001 Done, mean position loss: 20.98175075531006\n",
      "Training NF1:  76%|████████████▏   | 7632/10001 [13:12:16<1:11:09,  1.80s/batch]Batch 7800/10001 Done, mean position loss: 20.449808359146118\n",
      "Training NF1:  77%|████████████▎   | 7726/10001 [13:12:17<1:04:29,  1.70s/batch]Batch 7700/10001 Done, mean position loss: 21.557492470741273\n",
      "Training NF1:  77%|████████████▎   | 7685/10001 [13:12:26<1:16:48,  1.99s/batch]Batch 7800/10001 Done, mean position loss: 20.62303302049637\n",
      "Training NF1:  77%|████████████▎   | 7725/10001 [13:12:27<1:17:10,  2.03s/batch]Batch 7800/10001 Done, mean position loss: 20.74020219564438\n",
      "Training NF1:  78%|████████████▍   | 7777/10001 [13:12:31<1:01:48,  1.67s/batch]Batch 7700/10001 Done, mean position loss: 21.086271719932554\n",
      "Training NF1:  78%|████████████▍   | 7778/10001 [13:12:34<1:12:21,  1.95s/batch]Batch 7800/10001 Done, mean position loss: 20.85427899122238\n",
      "Training NF1:  78%|████████████▍   | 7760/10001 [13:12:40<1:11:46,  1.92s/batch]Batch 7800/10001 Done, mean position loss: 20.576959602832794\n",
      "Training NF1:  78%|████████████▌   | 7824/10001 [13:12:44<1:06:52,  1.84s/batch]Batch 7600/10001 Done, mean position loss: 20.665675377845766\n",
      "Training NF1:  79%|████████████▌   | 7879/10001 [13:12:53<1:07:26,  1.91s/batch]Batch 7700/10001 Done, mean position loss: 20.316240556240082\n",
      "Training NF1:  78%|█████████████▉    | 7772/10001 [13:13:00<58:16,  1.57s/batch]Batch 7800/10001 Done, mean position loss: 20.620114350318907\n",
      "Training NF1:  78%|████████████▍   | 7793/10001 [13:13:12<1:05:45,  1.79s/batch]Batch 7800/10001 Done, mean position loss: 20.88166130781174\n",
      "Training NF1:  77%|████████████▎   | 7734/10001 [13:13:16<1:10:03,  1.85s/batch]Batch 7800/10001 Done, mean position loss: 20.829151034355164\n",
      "Training NF1:  77%|████████████▎   | 7735/10001 [13:13:19<1:12:31,  1.92s/batch]Batch 7900/10001 Done, mean position loss: 20.48298439502716\n",
      "Training NF1:  78%|████████████▍   | 7776/10001 [13:13:19<1:09:56,  1.89s/batch]Batch 7800/10001 Done, mean position loss: 20.569362637996676\n",
      "Training NF1:  78%|████████████▍   | 7770/10001 [13:13:26<1:11:37,  1.93s/batch]Batch 7800/10001 Done, mean position loss: 20.52105700016022\n",
      "Training NF1:  78%|████████████▌   | 7818/10001 [13:13:30<1:04:40,  1.78s/batch]Batch 7800/10001 Done, mean position loss: 20.661672637462615\n",
      "Training NF1:  78%|████████████▍   | 7769/10001 [13:13:33<1:02:48,  1.69s/batch]Batch 7900/10001 Done, mean position loss: 20.472188606262208\n",
      "Training NF1:  78%|████████████▍   | 7757/10001 [13:13:36<1:23:21,  2.23s/batch]Batch 7900/10001 Done, mean position loss: 20.582761685848233\n",
      "Training NF1:  79%|██████████████▏   | 7863/10001 [13:13:38<58:08,  1.63s/batch]Batch 7700/10001 Done, mean position loss: 20.92589314699173\n",
      "Training NF1:  76%|█████████████▋    | 7634/10001 [13:13:43<58:56,  1.49s/batch]Batch 7800/10001 Done, mean position loss: 20.637360696792605\n",
      "Training NF1:  78%|████████████▍   | 7796/10001 [13:13:51<1:03:57,  1.74s/batch]Batch 7800/10001 Done, mean position loss: 20.6570320892334\n",
      "Training NF1:  78%|████████████▌   | 7850/10001 [13:13:55<1:02:49,  1.75s/batch]Batch 7800/10001 Done, mean position loss: 20.71848328590393\n",
      "Training NF1:  79%|████████████▌   | 7876/10001 [13:14:00<1:02:52,  1.78s/batch]Batch 7800/10001 Done, mean position loss: 20.541897728443146\n",
      "Training NF1:  78%|████████████▍   | 7774/10001 [13:14:04<1:00:15,  1.62s/batch]Batch 7800/10001 Done, mean position loss: 20.41174293756485\n",
      "Training NF1:  79%|████████████▋   | 7893/10001 [13:14:06<1:12:55,  2.08s/batch]Batch 7800/10001 Done, mean position loss: 20.347747855186462\n",
      "Training NF1:  78%|████████████▍   | 7757/10001 [13:14:08<1:08:54,  1.84s/batch]Batch 7800/10001 Done, mean position loss: 20.77111634016037\n",
      "Training NF1:  78%|████████████▌   | 7835/10001 [13:14:21<1:10:26,  1.95s/batch]Batch 7900/10001 Done, mean position loss: 20.491717894077304\n",
      "Training NF1:  77%|████████████▎   | 7700/10001 [13:14:22<1:15:01,  1.96s/batch]Batch 7800/10001 Done, mean position loss: 20.703873949050905\n",
      "Training NF1:  78%|████████████▍   | 7775/10001 [13:14:23<1:11:30,  1.93s/batch]Batch 7700/10001 Done, mean position loss: 20.599809417724607\n",
      "Training NF1:  78%|██████████████    | 7799/10001 [13:14:25<59:38,  1.63s/batch]Batch 7800/10001 Done, mean position loss: 21.055293493270874\n",
      "Training NF1:  78%|██████████████    | 7838/10001 [13:14:25<58:16,  1.62s/batch]Batch 7800/10001 Done, mean position loss: 21.031968429088593\n",
      "Training NF1:  78%|█████████████▉    | 7778/10001 [13:14:27<57:11,  1.54s/batch]Batch 7800/10001 Done, mean position loss: 20.94418766736984\n",
      "Training NF1:  79%|██████████████▏   | 7856/10001 [13:14:38<57:17,  1.60s/batch]Batch 7800/10001 Done, mean position loss: 20.365061268806457\n",
      "Training NF1:  78%|████████████▍   | 7782/10001 [13:14:43<1:13:36,  1.99s/batch]Batch 7900/10001 Done, mean position loss: 20.947539105415345\n",
      "Training NF1:  79%|██████████████▏   | 7887/10001 [13:14:49<53:22,  1.51s/batch]Batch 7800/10001 Done, mean position loss: 20.707748622894286\n",
      "Training NF1:  78%|████████████▌   | 7836/10001 [13:14:56<1:04:28,  1.79s/batch]Batch 7800/10001 Done, mean position loss: 20.726699328422544\n",
      "Training NF1:  78%|████████████▌   | 7847/10001 [13:14:58<1:25:36,  2.38s/batch]Batch 7900/10001 Done, mean position loss: 20.49476676225662\n",
      "Training NF1:  78%|████████████▌   | 7821/10001 [13:15:02<1:12:21,  1.99s/batch]Batch 7900/10001 Done, mean position loss: 20.726859917640688\n",
      "Training NF1:  79%|██████████████▏   | 7859/10001 [13:15:08<58:49,  1.65s/batch]Batch 7800/10001 Done, mean position loss: 20.465057907104494\n",
      "Training NF1:  79%|██████████████▎   | 7928/10001 [13:15:08<54:37,  1.58s/batch]Batch 7900/10001 Done, mean position loss: 20.972011156082154\n",
      "Training NF1:  79%|██████████████▏   | 7889/10001 [13:15:14<59:39,  1.70s/batch]Batch 7900/10001 Done, mean position loss: 20.44995290517807\n",
      "Training NF1:  78%|████████████▌   | 7842/10001 [13:15:18<1:13:36,  2.05s/batch]Batch 7800/10001 Done, mean position loss: 21.58143043279648\n",
      "Training NF1:  79%|██████████████▏   | 7875/10001 [13:15:22<57:39,  1.63s/batch]Batch 7900/10001 Done, mean position loss: 20.635392832756043\n",
      "Training NF1:  78%|████████████▍   | 7798/10001 [13:15:24<1:12:47,  1.98s/batch]Batch 7900/10001 Done, mean position loss: 20.760953052043917\n",
      "Training NF1:  78%|██████████████    | 7840/10001 [13:15:29<56:22,  1.57s/batch]Batch 7800/10001 Done, mean position loss: 21.074477412700652\n",
      "Training NF1:  79%|████████████▌   | 7860/10001 [13:15:33<1:04:51,  1.82s/batch]Batch 7900/10001 Done, mean position loss: 20.8472128534317\n",
      "Training NF1:  79%|██████████████▏   | 7889/10001 [13:15:35<59:14,  1.68s/batch]Batch 7900/10001 Done, mean position loss: 20.581426801681516\n",
      "Training NF1:  78%|████████████▌   | 7829/10001 [13:15:47<1:08:55,  1.90s/batch]Batch 7700/10001 Done, mean position loss: 20.669501657485963\n",
      "Training NF1:  79%|██████████████▎   | 7926/10001 [13:15:53<57:51,  1.67s/batch]Batch 7800/10001 Done, mean position loss: 20.31323665857315\n",
      "Training NF1:  79%|████████████▌   | 7879/10001 [13:15:55<1:07:38,  1.91s/batch]Batch 7900/10001 Done, mean position loss: 20.59967790842056\n",
      "Training NF1:  79%|████████████▌   | 7879/10001 [13:16:09<1:03:43,  1.80s/batch]Batch 7900/10001 Done, mean position loss: 20.882216782569884\n",
      "Training NF1:  79%|████████████▋   | 7904/10001 [13:16:14<1:02:56,  1.80s/batch]Batch 8000/10001 Done, mean position loss: 20.472575268745423\n",
      "Training NF1:  78%|████████████▌   | 7844/10001 [13:16:15<1:04:29,  1.79s/batch]Batch 7900/10001 Done, mean position loss: 20.560706510543824\n",
      "Training NF1:  80%|██████████████▎   | 7968/10001 [13:16:17<52:25,  1.55s/batch]Batch 7900/10001 Done, mean position loss: 20.834337227344513\n",
      "Training NF1:  79%|██████████████▎   | 7935/10001 [13:16:21<55:17,  1.61s/batch]Batch 7900/10001 Done, mean position loss: 20.523524899482727\n",
      "Training NF1:  78%|████████████▌   | 7842/10001 [13:16:26<1:06:17,  1.84s/batch]Batch 8000/10001 Done, mean position loss: 20.449150347709654\n",
      "Training NF1:  79%|████████████▋   | 7944/10001 [13:16:26<1:04:37,  1.88s/batch]Batch 8000/10001 Done, mean position loss: 20.56120095729828\n",
      "Training NF1:  80%|██████████████▎   | 7977/10001 [13:16:33<58:06,  1.72s/batch]Batch 7900/10001 Done, mean position loss: 20.66681703567505\n",
      "Training NF1:  79%|████████████▌   | 7878/10001 [13:16:42<1:00:40,  1.71s/batch]Batch 7800/10001 Done, mean position loss: 20.932800529003146\n",
      "Training NF1:  80%|██████████████▍   | 8020/10001 [13:16:47<57:39,  1.75s/batch]Batch 7900/10001 Done, mean position loss: 20.68173629522324\n",
      "Training NF1:  79%|████████████▋   | 7901/10001 [13:16:48<1:07:45,  1.94s/batch]Batch 7900/10001 Done, mean position loss: 20.623601303100585\n",
      "Training NF1:  79%|██████████████▏   | 7890/10001 [13:16:51<54:44,  1.56s/batch]Batch 7900/10001 Done, mean position loss: 20.699660050868985\n",
      "Training NF1:  79%|████████████▋   | 7909/10001 [13:17:02<1:04:33,  1.85s/batch]Batch 7900/10001 Done, mean position loss: 20.352771451473238\n",
      "Training NF1:  80%|██████████████▍   | 8029/10001 [13:17:02<50:31,  1.54s/batch]Batch 7900/10001 Done, mean position loss: 20.52835765838623\n",
      "Training NF1:  80%|██████████████▍   | 8024/10001 [13:17:06<56:20,  1.71s/batch]Batch 7900/10001 Done, mean position loss: 20.402618725299835\n",
      "Training NF1:  79%|████████████▌   | 7859/10001 [13:17:12<1:09:45,  1.95s/batch]Batch 7900/10001 Done, mean position loss: 20.778794839382172\n",
      "Training NF1:  79%|████████████▋   | 7907/10001 [13:17:13<1:07:40,  1.94s/batch]Batch 7900/10001 Done, mean position loss: 20.70017827987671\n",
      "Training NF1:  79%|██████████████▎   | 7933/10001 [13:17:17<59:51,  1.74s/batch]Batch 8000/10001 Done, mean position loss: 20.49159244298935\n",
      "Training NF1:  79%|██████████████▏   | 7912/10001 [13:17:22<54:34,  1.57s/batch]Batch 7900/10001 Done, mean position loss: 21.031809840202335\n",
      "Training NF1:  80%|████████████▊   | 7972/10001 [13:17:24<1:05:35,  1.94s/batch]Batch 7800/10001 Done, mean position loss: 20.600404303073883\n",
      "Training NF1:  79%|████████████▋   | 7914/10001 [13:17:26<1:03:38,  1.83s/batch]Batch 7900/10001 Done, mean position loss: 20.997479300498963\n",
      "Training NF1:  80%|██████████████▎   | 7966/10001 [13:17:29<54:42,  1.61s/batch]Batch 7900/10001 Done, mean position loss: 20.93876454591751\n",
      "Training NF1:  79%|████████████▋   | 7903/10001 [13:17:34<1:08:02,  1.95s/batch]Batch 8000/10001 Done, mean position loss: 21.000935254096984\n",
      "Training NF1:  79%|████████████▋   | 7948/10001 [13:17:39<1:05:51,  1.92s/batch]Batch 7900/10001 Done, mean position loss: 20.357446358203887\n",
      "Training NF1:  79%|████████████▌   | 7863/10001 [13:17:47<1:03:20,  1.78s/batch]Batch 7900/10001 Done, mean position loss: 20.687816002368926\n",
      "Training NF1:  79%|████████████▋   | 7915/10001 [13:17:50<1:09:28,  2.00s/batch]Batch 8000/10001 Done, mean position loss: 20.725776357650755\n",
      "Training NF1:  79%|████████████▋   | 7934/10001 [13:18:01<1:00:33,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.492128286361694\n",
      "Training NF1:  79%|████████████▋   | 7928/10001 [13:18:02<1:08:38,  1.99s/batch]Batch 7900/10001 Done, mean position loss: 20.686798799037934\n",
      "Training NF1:  79%|████████████▋   | 7905/10001 [13:18:10<1:05:33,  1.88s/batch]Batch 7900/10001 Done, mean position loss: 20.455591485500335\n",
      "Training NF1:  79%|██████████████▎   | 7933/10001 [13:18:11<56:50,  1.65s/batch]Batch 8000/10001 Done, mean position loss: 20.96288646697998\n",
      "Training NF1:  80%|██████████████▎   | 7972/10001 [13:18:15<58:17,  1.72s/batch]Batch 7900/10001 Done, mean position loss: 21.545686781406403\n",
      "Training NF1:  80%|████████████▊   | 7991/10001 [13:18:17<1:10:23,  2.10s/batch]Batch 8000/10001 Done, mean position loss: 20.45432953596115\n",
      "Training NF1:  79%|████████████▋   | 7936/10001 [13:18:18<1:09:04,  2.01s/batch]Batch 8000/10001 Done, mean position loss: 20.625233149528505\n",
      "Training NF1:  79%|██████████████▎   | 7941/10001 [13:18:26<53:40,  1.56s/batch]Batch 7900/10001 Done, mean position loss: 21.082036879062652\n",
      "Training NF1:  79%|████████████▋   | 7936/10001 [13:18:27<1:04:33,  1.88s/batch]Batch 8000/10001 Done, mean position loss: 20.83687794446945\n",
      "Training NF1:  80%|██████████████▎   | 7974/10001 [13:18:29<48:57,  1.45s/batch]Batch 8000/10001 Done, mean position loss: 20.731335725784298\n",
      "Training NF1:  80%|██████████████▍   | 7991/10001 [13:18:35<54:40,  1.63s/batch]Batch 8000/10001 Done, mean position loss: 20.581736760139467\n",
      "Training NF1:  80%|████████████▊   | 8024/10001 [13:18:51<1:10:16,  2.13s/batch]Batch 8000/10001 Done, mean position loss: 20.595456290245053\n",
      "Training NF1:  80%|████████████▊   | 8015/10001 [13:18:52<1:02:55,  1.90s/batch]Batch 7800/10001 Done, mean position loss: 20.667577559947965\n",
      "Training NF1:  80%|████████████▊   | 8017/10001 [13:18:55<1:01:11,  1.85s/batch]Batch 7900/10001 Done, mean position loss: 20.306727511882784\n",
      "Training NF1:  80%|██████████████▍   | 8022/10001 [13:19:05<58:11,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.8974640250206\n",
      "Training NF1:  81%|██████████████▍   | 8053/10001 [13:19:08<53:29,  1.65s/batch]Batch 8100/10001 Done, mean position loss: 20.475001945495606\n",
      "Training NF1:  80%|██████████████▎   | 7985/10001 [13:19:13<56:55,  1.69s/batch]Batch 8000/10001 Done, mean position loss: 20.52924391746521\n",
      "Training NF1:  80%|██████████████▎   | 7980/10001 [13:19:14<53:55,  1.60s/batch]Batch 8000/10001 Done, mean position loss: 20.56761718034744\n",
      "Training NF1:  79%|██████████████▎   | 7931/10001 [13:19:19<59:47,  1.73s/batch]Batch 8100/10001 Done, mean position loss: 20.444827940464023\n",
      "Training NF1:  80%|██████████████▍   | 8036/10001 [13:19:19<54:25,  1.66s/batch]Batch 8000/10001 Done, mean position loss: 20.845384960174563\n",
      "Training NF1:  79%|██████████████▎   | 7921/10001 [13:19:29<52:20,  1.51s/batch]Batch 8100/10001 Done, mean position loss: 20.558420104980467\n",
      "Training NF1:  80%|████████████▊   | 7978/10001 [13:19:34<1:14:24,  2.21s/batch]Batch 8000/10001 Done, mean position loss: 20.661829137802123\n",
      "Training NF1:  80%|████████████▊   | 8036/10001 [13:19:40<1:13:02,  2.23s/batch]Batch 7900/10001 Done, mean position loss: 20.95496713399887\n",
      "Training NF1:  80%|████████████▊   | 7978/10001 [13:19:42<1:01:54,  1.84s/batch]Batch 8000/10001 Done, mean position loss: 20.685835890769958\n",
      "Training NF1:  81%|██████████████▌   | 8069/10001 [13:19:49<56:38,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.634800803661346\n",
      "Training NF1:  81%|██████████████▍   | 8053/10001 [13:19:50<56:43,  1.75s/batch]Batch 8000/10001 Done, mean position loss: 20.69291667938232\n",
      "Training NF1:  79%|██████████████▎   | 7938/10001 [13:19:58<55:17,  1.61s/batch]Batch 8000/10001 Done, mean position loss: 20.354811429977417\n",
      "Training NF1:  80%|████████████▊   | 7985/10001 [13:19:58<1:02:04,  1.85s/batch]Batch 8000/10001 Done, mean position loss: 20.529619462490082\n",
      "Training NF1:  81%|██████████████▌   | 8067/10001 [13:20:08<53:48,  1.67s/batch]Batch 8000/10001 Done, mean position loss: 20.410462903976438\n",
      "Training NF1:  80%|██████████████▍   | 8040/10001 [13:20:12<58:45,  1.80s/batch]Batch 8100/10001 Done, mean position loss: 20.483239743709564\n",
      "Training NF1:  80%|████████████▊   | 8014/10001 [13:20:15<1:10:26,  2.13s/batch]Batch 8000/10001 Done, mean position loss: 20.695129141807556\n",
      "Training NF1:  80%|██████████████▍   | 7992/10001 [13:20:18<59:30,  1.78s/batch]Batch 8000/10001 Done, mean position loss: 20.771421620845796\n",
      "Training NF1:  81%|██████████████▌   | 8087/10001 [13:20:21<52:24,  1.64s/batch]Batch 8000/10001 Done, mean position loss: 21.015928587913514\n",
      "Training NF1:  81%|██████████████▌   | 8089/10001 [13:20:24<51:30,  1.62s/batch]Batch 8000/10001 Done, mean position loss: 20.986541626453402\n",
      "Training NF1:  80%|████████████▊   | 8017/10001 [13:20:25<1:01:03,  1.85s/batch]Batch 7900/10001 Done, mean position loss: 20.588332331180574\n",
      "Training NF1:  80%|██████████████▍   | 8011/10001 [13:20:32<51:07,  1.54s/batch]Batch 8000/10001 Done, mean position loss: 20.950501985549927\n",
      "Training NF1:  81%|██████████████▌   | 8096/10001 [13:20:34<48:08,  1.52s/batch]Batch 8000/10001 Done, mean position loss: 20.363769154548645\n",
      "Training NF1:  81%|██████████████▌   | 8073/10001 [13:20:35<58:01,  1.81s/batch]Batch 8100/10001 Done, mean position loss: 21.00658153295517\n",
      "Training NF1:  80%|██████████████▎   | 7964/10001 [13:20:42<55:54,  1.65s/batch]Batch 8100/10001 Done, mean position loss: 20.73138256788254\n",
      "Training NF1:  81%|██████████████▌   | 8106/10001 [13:20:44<56:26,  1.79s/batch]Batch 8000/10001 Done, mean position loss: 20.68260451555252\n",
      "Training NF1:  80%|████████████▊   | 8013/10001 [13:20:54<1:03:28,  1.92s/batch]Batch 8000/10001 Done, mean position loss: 20.729420824050905\n",
      "Training NF1:  81%|██████████████▌   | 8114/10001 [13:20:58<51:36,  1.64s/batch]Batch 8100/10001 Done, mean position loss: 20.48294516324997\n",
      "Training NF1:  80%|██████████████▍   | 8026/10001 [13:21:06<49:24,  1.50s/batch]Batch 8100/10001 Done, mean position loss: 20.96462299823761\n",
      "Training NF1:  80%|██████████████▍   | 8033/10001 [13:21:08<54:52,  1.67s/batch]Batch 8000/10001 Done, mean position loss: 20.46499440193176\n",
      "Training NF1:  82%|██████████████▋   | 8168/10001 [13:21:09<55:04,  1.80s/batch]Batch 8000/10001 Done, mean position loss: 21.56473511695862\n",
      "Training NF1:  80%|██████████████▍   | 8046/10001 [13:21:09<55:19,  1.70s/batch]Batch 8100/10001 Done, mean position loss: 20.439113152027133\n",
      "Training NF1:  80%|██████████████▍   | 8028/10001 [13:21:22<56:15,  1.71s/batch]Batch 8100/10001 Done, mean position loss: 20.629872863292697\n",
      "Training NF1:  80%|██████████████▍   | 8030/10001 [13:21:25<52:35,  1.60s/batch]Batch 8000/10001 Done, mean position loss: 21.061902794837955\n",
      "Training NF1:  81%|██████████████▍   | 8052/10001 [13:21:26<48:45,  1.50s/batch]Batch 8100/10001 Done, mean position loss: 20.805950260162355\n",
      "Training NF1:  81%|██████████████▋   | 8134/10001 [13:21:31<57:38,  1.85s/batch]Batch 8100/10001 Done, mean position loss: 20.7497016620636\n",
      "Training NF1:  81%|██████████████▌   | 8121/10001 [13:21:32<47:48,  1.53s/batch]Batch 8100/10001 Done, mean position loss: 20.57092784643173\n",
      "Training NF1:  81%|██████████████▌   | 8112/10001 [13:21:39<49:14,  1.56s/batch]Batch 8000/10001 Done, mean position loss: 20.321606729030606\n",
      "Training NF1:  80%|████████████▊   | 8012/10001 [13:21:46<1:10:28,  2.13s/batch]Batch 8100/10001 Done, mean position loss: 20.594619522094725\n",
      "Training NF1:  81%|██████████████▌   | 8090/10001 [13:21:47<54:10,  1.70s/batch]Batch 7900/10001 Done, mean position loss: 20.67772148370743\n",
      "Training NF1:  81%|██████████████▌   | 8118/10001 [13:21:55<55:31,  1.77s/batch]Batch 8100/10001 Done, mean position loss: 20.87844451904297\n",
      "Training NF1:  80%|████████████▊   | 8040/10001 [13:22:03<1:00:36,  1.85s/batch]Batch 8100/10001 Done, mean position loss: 20.523547031879424\n",
      "Training NF1:  80%|██████████████▍   | 8036/10001 [13:22:09<55:25,  1.69s/batch]Batch 8200/10001 Done, mean position loss: 20.48976000547409\n",
      "Training NF1:  80%|██████████████▍   | 7987/10001 [13:22:11<44:15,  1.32s/batch]Batch 8100/10001 Done, mean position loss: 20.553374469280243\n",
      "Training NF1:  80%|████████████▊   | 8045/10001 [13:22:14<1:08:27,  2.10s/batch]Batch 8100/10001 Done, mean position loss: 20.83914343357086\n",
      "Training NF1:  81%|██████████████▋   | 8128/10001 [13:22:17<54:50,  1.76s/batch]Batch 8200/10001 Done, mean position loss: 20.447634572982786\n",
      "Training NF1:  81%|████████████▉   | 8073/10001 [13:22:17<1:01:29,  1.91s/batch]Batch 8200/10001 Done, mean position loss: 20.563978180885314\n",
      "Training NF1:  81%|██████████████▋   | 8136/10001 [13:22:25<45:48,  1.47s/batch]Batch 8100/10001 Done, mean position loss: 20.670351719856264\n",
      "Training NF1:  81%|██████████████▌   | 8099/10001 [13:22:35<58:42,  1.85s/batch]Batch 8000/10001 Done, mean position loss: 20.928218557834626\n",
      "Training NF1:  81%|██████████████▌   | 8078/10001 [13:22:38<55:27,  1.73s/batch]Batch 8100/10001 Done, mean position loss: 20.67536030769348\n",
      "Training NF1:  81%|██████████████▌   | 8090/10001 [13:22:44<39:41,  1.25s/batch]Batch 8100/10001 Done, mean position loss: 20.712013049125673\n",
      "Training NF1:  81%|██████████████▌   | 8082/10001 [13:22:45<59:35,  1.86s/batch]Batch 8100/10001 Done, mean position loss: 20.52613577365875\n",
      "Training NF1:  81%|██████████████▋   | 8130/10001 [13:22:49<42:59,  1.38s/batch]Batch 8100/10001 Done, mean position loss: 20.633736979961395\n",
      "Training NF1:  81%|██████████████▋   | 8141/10001 [13:22:53<54:59,  1.77s/batch]Batch 8100/10001 Done, mean position loss: 20.409467151165007\n",
      "Training NF1:  80%|██████████████▍   | 8043/10001 [13:22:55<59:34,  1.83s/batch]Batch 8100/10001 Done, mean position loss: 20.35912904500961\n",
      "Training NF1:  81%|██████████████▋   | 8138/10001 [13:23:03<52:08,  1.68s/batch]Batch 8100/10001 Done, mean position loss: 20.680527651309966\n",
      "Training NF1:  82%|██████████████▋   | 8183/10001 [13:23:08<53:24,  1.76s/batch]Batch 8100/10001 Done, mean position loss: 20.7727100110054\n",
      "Training NF1:  82%|██████████████▋   | 8163/10001 [13:23:08<39:45,  1.30s/batch]Batch 8200/10001 Done, mean position loss: 20.47610123157501\n",
      "Training NF1:  80%|██████████████▍   | 7996/10001 [13:23:10<58:36,  1.75s/batch]Batch 8100/10001 Done, mean position loss: 20.99153201818466\n",
      "Training NF1:  81%|██████████████▋   | 8141/10001 [13:23:16<55:56,  1.80s/batch]Batch 8100/10001 Done, mean position loss: 21.020107929706572\n",
      "Training NF1:  81%|██████████████▋   | 8137/10001 [13:23:19<58:19,  1.88s/batch]Batch 8000/10001 Done, mean position loss: 20.5965212225914\n",
      "Training NF1:  81%|██████████████▌   | 8116/10001 [13:23:25<42:11,  1.34s/batch]Batch 8200/10001 Done, mean position loss: 20.956363098621367\n",
      "Training NF1:  82%|██████████████▋   | 8184/10001 [13:23:26<54:06,  1.79s/batch]Batch 8100/10001 Done, mean position loss: 20.941153800487516\n",
      "Training NF1:  81%|██████████████▋   | 8127/10001 [13:23:29<41:38,  1.33s/batch]Batch 8100/10001 Done, mean position loss: 20.67998396158218\n",
      "Training NF1:  82%|██████████████▋   | 8186/10001 [13:23:29<53:48,  1.78s/batch]Batch 8100/10001 Done, mean position loss: 20.348521494865416\n",
      "Training NF1:  82%|██████████████▋   | 8189/10001 [13:23:35<45:26,  1.50s/batch]Batch 8200/10001 Done, mean position loss: 20.72711733341217\n",
      "Training NF1:  80%|██████████████▍   | 8043/10001 [13:23:46<50:08,  1.54s/batch]Batch 8200/10001 Done, mean position loss: 20.47227684497833\n",
      "Training NF1:  82%|█████████████   | 8165/10001 [13:23:53<1:01:48,  2.02s/batch]Batch 8100/10001 Done, mean position loss: 20.700271985530854\n",
      "Training NF1:  81%|██████████████▌   | 8118/10001 [13:23:55<48:34,  1.55s/batch]Batch 8200/10001 Done, mean position loss: 20.97042042970657\n",
      "Training NF1:  83%|██████████████▊   | 8259/10001 [13:23:57<50:23,  1.74s/batch]Batch 8200/10001 Done, mean position loss: 20.433320994377134\n",
      "Training NF1:  81%|██████████████▌   | 8122/10001 [13:24:00<44:45,  1.43s/batch]Batch 8100/10001 Done, mean position loss: 21.557380516529083\n",
      "Training NF1:  81%|██████████████▌   | 8122/10001 [13:24:03<56:59,  1.82s/batch]Batch 8100/10001 Done, mean position loss: 20.45916922092438\n",
      "Training NF1:  81%|██████████████▌   | 8104/10001 [13:24:07<53:01,  1.68s/batch]Batch 8200/10001 Done, mean position loss: 20.814304049015043\n",
      "Training NF1:  82%|██████████████▊   | 8207/10001 [13:24:17<46:22,  1.55s/batch]Batch 8200/10001 Done, mean position loss: 20.633716850280763\n",
      "Training NF1:  82%|██████████████▋   | 8178/10001 [13:24:18<43:46,  1.44s/batch]Batch 8100/10001 Done, mean position loss: 21.11146353006363\n",
      "Training NF1:  81%|██████████████▌   | 8118/10001 [13:24:20<57:11,  1.82s/batch]Batch 8200/10001 Done, mean position loss: 20.571394073963162\n",
      "Training NF1:  81%|██████████████▋   | 8136/10001 [13:24:26<43:30,  1.40s/batch]Batch 8200/10001 Done, mean position loss: 20.75124849319458\n",
      "Training NF1:  82%|██████████████▋   | 8180/10001 [13:24:36<56:54,  1.88s/batch]Batch 8200/10001 Done, mean position loss: 20.610061361789704\n",
      "Training NF1:  82%|██████████████▋   | 8165/10001 [13:24:38<55:24,  1.81s/batch]Batch 8100/10001 Done, mean position loss: 20.326085441112518\n",
      "Training NF1:  82%|██████████████▊   | 8212/10001 [13:24:45<54:05,  1.81s/batch]Batch 8000/10001 Done, mean position loss: 20.66936404705048\n",
      "Training NF1:  82%|██████████████▊   | 8235/10001 [13:24:51<43:44,  1.49s/batch]Batch 8200/10001 Done, mean position loss: 20.503903114795683\n",
      "Training NF1:  82%|██████████████▊   | 8220/10001 [13:24:54<50:52,  1.71s/batch]Batch 8200/10001 Done, mean position loss: 20.560587899684904\n",
      "Training NF1:  82%|█████████████   | 8163/10001 [13:24:57<1:00:45,  1.98s/batch]Batch 8200/10001 Done, mean position loss: 20.878941183090213\n",
      "Training NF1:  82%|██████████████▊   | 8202/10001 [13:24:58<51:08,  1.71s/batch]Batch 8300/10001 Done, mean position loss: 20.47507069826126\n",
      "Training NF1:  82%|██████████████▋   | 8170/10001 [13:25:07<59:47,  1.96s/batch]Batch 8300/10001 Done, mean position loss: 20.56041886806488\n",
      "Training NF1:  82%|██████████████▋   | 8171/10001 [13:25:11<58:09,  1.91s/batch]Batch 8200/10001 Done, mean position loss: 20.846528260707856\n",
      "Training NF1:  82%|██████████████▊   | 8235/10001 [13:25:15<34:31,  1.17s/batch]Batch 8200/10001 Done, mean position loss: 20.665924577713014\n",
      "Training NF1:  82%|██████████████▊   | 8249/10001 [13:25:16<51:39,  1.77s/batch]Batch 8300/10001 Done, mean position loss: 20.45399255037308\n",
      "Training NF1:  82%|██████████████▋   | 8194/10001 [13:25:25<58:59,  1.96s/batch]Batch 8100/10001 Done, mean position loss: 20.926680710315704\n",
      "Training NF1:  82%|██████████████▋   | 8182/10001 [13:25:29<43:50,  1.45s/batch]Batch 8200/10001 Done, mean position loss: 20.66907124042511\n",
      "Training NF1:  83%|██████████████▊   | 8260/10001 [13:25:35<50:12,  1.73s/batch]Batch 8200/10001 Done, mean position loss: 20.51958676338196\n",
      "Training NF1:  82%|██████████████▊   | 8206/10001 [13:25:38<54:37,  1.83s/batch]Batch 8200/10001 Done, mean position loss: 20.71382424354553\n",
      "Training NF1:  82%|██████████████▊   | 8206/10001 [13:25:44<45:55,  1.54s/batch]Batch 8200/10001 Done, mean position loss: 20.407661738395692\n",
      "Training NF1:  83%|██████████████▉   | 8283/10001 [13:25:45<51:20,  1.79s/batch]Batch 8200/10001 Done, mean position loss: 20.636282598972322\n",
      "Training NF1:  83%|██████████████▉   | 8284/10001 [13:25:46<47:29,  1.66s/batch]Batch 8200/10001 Done, mean position loss: 20.67515162706375\n",
      "Training NF1:  82%|██████████████▊   | 8249/10001 [13:25:52<51:06,  1.75s/batch]Batch 8200/10001 Done, mean position loss: 20.369991512298586\n",
      "Training NF1:  83%|██████████████▉   | 8265/10001 [13:25:57<59:27,  2.06s/batch]Batch 8300/10001 Done, mean position loss: 20.476424124240875\n",
      "Training NF1:  83%|██████████████▊   | 8259/10001 [13:25:58<59:00,  2.03s/batch]Batch 8200/10001 Done, mean position loss: 21.005468020439146\n",
      "Training NF1:  82%|██████████████▊   | 8243/10001 [13:26:01<53:58,  1.84s/batch]Batch 8200/10001 Done, mean position loss: 21.012631447315215\n",
      "Training NF1:  82%|█████████████▏  | 8212/10001 [13:26:04<1:00:24,  2.03s/batch]Batch 8200/10001 Done, mean position loss: 20.769381318092346\n",
      "Training NF1:  82%|██████████████▋   | 8178/10001 [13:26:15<48:27,  1.60s/batch]Batch 8100/10001 Done, mean position loss: 20.597152903079987\n",
      "Training NF1:  82%|██████████████▊   | 8197/10001 [13:26:16<40:38,  1.35s/batch]Batch 8300/10001 Done, mean position loss: 20.937784898281095\n",
      "Training NF1:  83%|███████████████   | 8335/10001 [13:26:17<50:40,  1.82s/batch]Batch 8200/10001 Done, mean position loss: 20.708605568408966\n",
      "Training NF1:  82%|█████████████▏  | 8216/10001 [13:26:21<1:02:18,  2.09s/batch]Batch 8200/10001 Done, mean position loss: 20.359914259910582\n",
      "Training NF1:  82%|██████████████▊   | 8223/10001 [13:26:23<51:35,  1.74s/batch]Batch 8200/10001 Done, mean position loss: 20.929854044914244\n",
      "Training NF1:  82%|██████████████▊   | 8231/10001 [13:26:27<43:06,  1.46s/batch]Batch 8300/10001 Done, mean position loss: 20.738893973827363\n",
      "Training NF1:  82%|██████████████▋   | 8183/10001 [13:26:43<48:10,  1.59s/batch]Batch 8300/10001 Done, mean position loss: 20.469466841220857\n",
      "Training NF1:  83%|██████████████▉   | 8314/10001 [13:26:45<41:13,  1.47s/batch]Batch 8200/10001 Done, mean position loss: 20.733527381420135\n",
      "Batch 8300/10001 Done, mean position loss: 20.421453604698183\n",
      "Training NF1:  82%|██████████████▋   | 8173/10001 [13:26:45<49:46,  1.63s/batch]Batch 8300/10001 Done, mean position loss: 20.951325538158414\n",
      "Training NF1:  81%|██████████████▌   | 8073/10001 [13:26:54<53:22,  1.66s/batch]Batch 8200/10001 Done, mean position loss: 21.566215372085573\n",
      "Training NF1:  82%|█████████████▏  | 8208/10001 [13:26:58<1:07:31,  2.26s/batch]Batch 8200/10001 Done, mean position loss: 20.451431658267975\n",
      "Training NF1:  84%|███████████████   | 8367/10001 [13:27:04<38:43,  1.42s/batch]Batch 8300/10001 Done, mean position loss: 20.826416325569152\n",
      "Training NF1:  83%|██████████████▉   | 8305/10001 [13:27:10<50:58,  1.80s/batch]Batch 8300/10001 Done, mean position loss: 20.63296498775482\n",
      "Training NF1:  82%|██████████████▊   | 8231/10001 [13:27:15<50:52,  1.72s/batch]Batch 8300/10001 Done, mean position loss: 20.568177721500398\n",
      "Training NF1:  82%|██████████████▊   | 8233/10001 [13:27:15<56:13,  1.91s/batch]Batch 8200/10001 Done, mean position loss: 21.09479210138321\n",
      "Training NF1:  84%|███████████████   | 8383/10001 [13:27:26<38:13,  1.42s/batch]Batch 8300/10001 Done, mean position loss: 20.73676040649414\n",
      "Training NF1:  83%|██████████████▉   | 8303/10001 [13:27:29<49:43,  1.76s/batch]Batch 8300/10001 Done, mean position loss: 20.589745707511902\n",
      "Training NF1:  83%|██████████████▊   | 8254/10001 [13:27:36<50:02,  1.72s/batch]Batch 8200/10001 Done, mean position loss: 20.336251628398898\n",
      "Training NF1:  84%|███████████████   | 8352/10001 [13:27:44<43:31,  1.58s/batch]Batch 8100/10001 Done, mean position loss: 20.677265295982362\n",
      "Training NF1:  82%|██████████████▊   | 8231/10001 [13:27:46<52:19,  1.77s/batch]Batch 8300/10001 Done, mean position loss: 20.522257933616636\n",
      "Training NF1:  83%|██████████████▊   | 8264/10001 [13:27:50<44:27,  1.54s/batch]Batch 8300/10001 Done, mean position loss: 20.892800562381744\n",
      "Training NF1:  83%|██████████████▉   | 8319/10001 [13:27:55<44:13,  1.58s/batch]Batch 8400/10001 Done, mean position loss: 20.45894255399704\n",
      "Training NF1:  82%|██████████████▋   | 8187/10001 [13:27:58<48:20,  1.60s/batch]Batch 8300/10001 Done, mean position loss: 20.552236936092378\n",
      "Training NF1:  82%|██████████████▊   | 8226/10001 [13:27:59<49:18,  1.67s/batch]Batch 8300/10001 Done, mean position loss: 20.8549794793129\n",
      "Training NF1:  83%|██████████████▊   | 8256/10001 [13:28:03<45:22,  1.56s/batch]Batch 8400/10001 Done, mean position loss: 20.566183886528016\n",
      "Training NF1:  83%|██████████████▉   | 8265/10001 [13:28:16<49:57,  1.73s/batch]Batch 8300/10001 Done, mean position loss: 20.642355241775512\n",
      "Training NF1:  84%|███████████████▏  | 8414/10001 [13:28:16<44:38,  1.69s/batch]Batch 8400/10001 Done, mean position loss: 20.447769317626953\n",
      "Training NF1:  84%|███████████████   | 8367/10001 [13:28:20<45:22,  1.67s/batch]Batch 8200/10001 Done, mean position loss: 20.92173421382904\n",
      "Training NF1:  83%|██████████████▉   | 8290/10001 [13:28:26<46:39,  1.64s/batch]Batch 8300/10001 Done, mean position loss: 20.51163164615631\n",
      "Training NF1:  83%|██████████████▉   | 8281/10001 [13:28:26<39:22,  1.37s/batch]Batch 8300/10001 Done, mean position loss: 20.655730488300325\n",
      "Training NF1:  83%|██████████████▉   | 8304/10001 [13:28:31<45:08,  1.60s/batch]Batch 8300/10001 Done, mean position loss: 20.718642189502717\n",
      "Training NF1:  83%|██████████████▊   | 8257/10001 [13:28:39<54:32,  1.88s/batch]Batch 8300/10001 Done, mean position loss: 20.684752402305605\n",
      "Training NF1:  83%|██████████████▉   | 8309/10001 [13:28:41<53:13,  1.89s/batch]Batch 8300/10001 Done, mean position loss: 20.62392123222351\n",
      "Training NF1:  84%|███████████████   | 8355/10001 [13:28:46<43:39,  1.59s/batch]Batch 8300/10001 Done, mean position loss: 20.4127197265625\n",
      "Training NF1:  84%|███████████████   | 8364/10001 [13:28:53<49:33,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 21.021410238742828\n",
      "Training NF1:  83%|██████████████▉   | 8298/10001 [13:28:54<51:38,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 20.36061490535736\n",
      "Training NF1:  82%|██████████████▊   | 8246/10001 [13:28:55<51:47,  1.77s/batch]Batch 8400/10001 Done, mean position loss: 20.470920867919922\n",
      "Training NF1:  83%|██████████████▉   | 8305/10001 [13:29:00<44:10,  1.56s/batch]Batch 8300/10001 Done, mean position loss: 20.972470998764038\n",
      "Training NF1:  84%|███████████████   | 8379/10001 [13:29:02<47:34,  1.76s/batch]Batch 8300/10001 Done, mean position loss: 20.757360372543335\n",
      "Training NF1:  83%|██████████████▉   | 8319/10001 [13:29:11<53:17,  1.90s/batch]Batch 8400/10001 Done, mean position loss: 20.98108653306961\n",
      "Training NF1:  83%|██████████████▉   | 8316/10001 [13:29:12<46:40,  1.66s/batch]Batch 8200/10001 Done, mean position loss: 20.589542698860168\n",
      "Training NF1:  84%|███████████████▏  | 8411/10001 [13:29:14<57:58,  2.19s/batch]Batch 8300/10001 Done, mean position loss: 20.685307035446165\n",
      "Training NF1:  82%|██████████████▊   | 8234/10001 [13:29:17<53:07,  1.80s/batch]Batch 8400/10001 Done, mean position loss: 20.72632133245468\n",
      "Training NF1:  83%|██████████████▉   | 8321/10001 [13:29:21<49:45,  1.78s/batch]Batch 8300/10001 Done, mean position loss: 20.361455647945405\n",
      "Training NF1:  83%|██████████████▉   | 8320/10001 [13:29:27<53:28,  1.91s/batch]Batch 8300/10001 Done, mean position loss: 20.92681384086609\n",
      "Training NF1:  84%|███████████████   | 8364/10001 [13:29:39<50:17,  1.84s/batch]Batch 8400/10001 Done, mean position loss: 20.475011134147646\n",
      "Training NF1:  84%|███████████████   | 8377/10001 [13:29:40<49:13,  1.82s/batch]Batch 8400/10001 Done, mean position loss: 20.436280126571653\n",
      "Training NF1:  83%|███████████████   | 8344/10001 [13:29:42<50:39,  1.83s/batch]Batch 8300/10001 Done, mean position loss: 20.718704812526703\n",
      "Training NF1:  83%|██████████████▉   | 8286/10001 [13:29:41<44:45,  1.57s/batch]Batch 8400/10001 Done, mean position loss: 20.979262413978574\n",
      "Training NF1:  83%|██████████████▉   | 8319/10001 [13:29:47<47:31,  1.70s/batch]Batch 8300/10001 Done, mean position loss: 21.555576412677762\n",
      "Training NF1:  83%|██████████████▉   | 8295/10001 [13:29:58<50:28,  1.78s/batch]Batch 8300/10001 Done, mean position loss: 20.42239726781845\n",
      "Training NF1:  84%|███████████████   | 8376/10001 [13:29:59<51:43,  1.91s/batch]Batch 8400/10001 Done, mean position loss: 20.836142494678498\n",
      "Training NF1:  82%|█████████████   | 8177/10001 [13:30:03<1:02:07,  2.04s/batch]Batch 8400/10001 Done, mean position loss: 20.638637738227846\n",
      "Training NF1:  84%|███████████████   | 8356/10001 [13:30:04<47:07,  1.72s/batch]Batch 8400/10001 Done, mean position loss: 20.585688333511353\n",
      "Training NF1:  83%|██████████████▉   | 8330/10001 [13:30:10<46:09,  1.66s/batch]Batch 8300/10001 Done, mean position loss: 21.07057564258575\n",
      "Training NF1:  84%|███████████████   | 8379/10001 [13:30:15<45:40,  1.69s/batch]Batch 8400/10001 Done, mean position loss: 20.72144132614136\n",
      "Training NF1:  84%|███████████████   | 8373/10001 [13:30:24<45:55,  1.69s/batch]Batch 8400/10001 Done, mean position loss: 20.60556143283844\n",
      "Training NF1:  84%|███████████████   | 8360/10001 [13:30:33<59:29,  2.18s/batch]Batch 8300/10001 Done, mean position loss: 20.318006579875945\n",
      "Training NF1:  83%|██████████████▉   | 8280/10001 [13:30:40<56:13,  1.96s/batch]Batch 8400/10001 Done, mean position loss: 20.8552432513237\n",
      "Training NF1:  84%|███████████████▏  | 8436/10001 [13:30:43<46:50,  1.80s/batch]Batch 8400/10001 Done, mean position loss: 20.502906060218812\n",
      "Training NF1:  84%|███████████████   | 8384/10001 [13:30:47<39:50,  1.48s/batch]Batch 8200/10001 Done, mean position loss: 20.67773288965225\n",
      "Training NF1:  84%|███████████████   | 8370/10001 [13:30:49<44:06,  1.62s/batch]Batch 8500/10001 Done, mean position loss: 20.479549570083616\n",
      "Training NF1:  84%|███████████████▏  | 8424/10001 [13:30:53<50:31,  1.92s/batch]Batch 8400/10001 Done, mean position loss: 20.826589205265044\n",
      "Training NF1:  84%|███████████████   | 8387/10001 [13:30:56<47:20,  1.76s/batch]Batch 8400/10001 Done, mean position loss: 20.565201148986816\n",
      "Training NF1:  83%|██████████████▉   | 8327/10001 [13:30:57<46:02,  1.65s/batch]Batch 8500/10001 Done, mean position loss: 20.565400934219358\n",
      "Training NF1:  83%|██████████████▉   | 8267/10001 [13:31:11<55:31,  1.92s/batch]Batch 8500/10001 Done, mean position loss: 20.450877439975738\n",
      "Training NF1:  84%|███████████████   | 8353/10001 [13:31:14<42:41,  1.55s/batch]Batch 8400/10001 Done, mean position loss: 20.641232948303223\n",
      "Training NF1:  84%|███████████████   | 8398/10001 [13:31:16<47:37,  1.78s/batch]Batch 8400/10001 Done, mean position loss: 20.66227434158325\n",
      "Batch 8300/10001 Done, mean position loss: 20.92899564266205\n",
      "Training NF1:  83%|██████████████▉   | 8328/10001 [13:31:17<47:17,  1.70s/batch]Batch 8400/10001 Done, mean position loss: 20.514764292240145\n",
      "Training NF1:  83%|██████████████▉   | 8304/10001 [13:31:22<53:18,  1.88s/batch]Batch 8400/10001 Done, mean position loss: 20.722241311073304\n",
      "Training NF1:  82%|██████████████▊   | 8228/10001 [13:31:35<53:28,  1.81s/batch]Batch 8400/10001 Done, mean position loss: 20.68805857181549\n",
      "Training NF1:  84%|███████████████   | 8399/10001 [13:31:38<41:35,  1.56s/batch]Batch 8400/10001 Done, mean position loss: 20.635928542613982\n",
      "Training NF1:  85%|███████████████▏  | 8469/10001 [13:31:41<41:57,  1.64s/batch]Batch 8400/10001 Done, mean position loss: 21.016796908378602\n",
      "Training NF1:  84%|███████████████   | 8364/10001 [13:31:47<50:24,  1.85s/batch]Batch 8400/10001 Done, mean position loss: 20.40623398780823\n",
      "Batch 8400/10001 Done, mean position loss: 20.35493483304977\n",
      "Training NF1:  84%|███████████████   | 8403/10001 [13:31:50<44:15,  1.66s/batch]Batch 8500/10001 Done, mean position loss: 20.463065538406372\n",
      "Training NF1:  84%|███████████████   | 8385/10001 [13:31:54<45:51,  1.70s/batch]Batch 8400/10001 Done, mean position loss: 20.98394844532013\n",
      "Training NF1:  84%|███████████████▏  | 8405/10001 [13:31:54<48:03,  1.81s/batch]Batch 8400/10001 Done, mean position loss: 20.753220567703245\n",
      "Training NF1:  84%|███████████████▏  | 8417/10001 [13:32:10<54:37,  2.07s/batch]Batch 8500/10001 Done, mean position loss: 20.97462321519852\n",
      "Training NF1:  85%|███████████████▏  | 8471/10001 [13:32:10<44:10,  1.73s/batch]Batch 8500/10001 Done, mean position loss: 20.72626096010208\n",
      "Training NF1:  84%|███████████████▏  | 8419/10001 [13:32:13<48:17,  1.83s/batch]Batch 8300/10001 Done, mean position loss: 20.57408005952835\n",
      "Training NF1:  85%|███████████████▏  | 8472/10001 [13:32:16<41:13,  1.62s/batch]Batch 8400/10001 Done, mean position loss: 20.365430746078495\n",
      "Training NF1:  85%|███████████████▍  | 8547/10001 [13:32:18<37:35,  1.55s/batch]Batch 8400/10001 Done, mean position loss: 20.70013436317444\n",
      "Training NF1:  85%|███████████████▎  | 8518/10001 [13:32:21<52:23,  2.12s/batch]Batch 8400/10001 Done, mean position loss: 20.932420716285705\n",
      "Training NF1:  85%|███████████████▎  | 8483/10001 [13:32:32<53:15,  2.11s/batch]Batch 8500/10001 Done, mean position loss: 20.981804378032685\n",
      "Training NF1:  83%|███████████████   | 8347/10001 [13:32:35<50:28,  1.83s/batch]Batch 8400/10001 Done, mean position loss: 20.69107073068619\n",
      "Training NF1:  84%|███████████████▏  | 8412/10001 [13:32:35<46:38,  1.76s/batch]Batch 8500/10001 Done, mean position loss: 20.481306667327882\n",
      "Training NF1:  85%|███████████████▎  | 8491/10001 [13:32:37<49:22,  1.96s/batch]Batch 8500/10001 Done, mean position loss: 20.431639482975005\n",
      "Training NF1:  84%|███████████████▏  | 8428/10001 [13:32:39<43:29,  1.66s/batch]Batch 8400/10001 Done, mean position loss: 21.584095535278323\n",
      "Training NF1:  83%|██████████████▉   | 8323/10001 [13:32:54<53:22,  1.91s/batch]Batch 8400/10001 Done, mean position loss: 20.4457039141655\n",
      "Training NF1:  84%|███████████████▏  | 8446/10001 [13:32:55<47:15,  1.82s/batch]Batch 8500/10001 Done, mean position loss: 20.567617383003235\n",
      "Training NF1:  85%|███████████████▏  | 8455/10001 [13:32:55<48:41,  1.89s/batch]Batch 8500/10001 Done, mean position loss: 20.833104267120362\n",
      "Training NF1:  84%|███████████████   | 8397/10001 [13:33:04<52:58,  1.98s/batch]Batch 8500/10001 Done, mean position loss: 20.621299612522122\n",
      "Training NF1:  83%|██████████████▉   | 8330/10001 [13:33:08<52:42,  1.89s/batch]Batch 8500/10001 Done, mean position loss: 20.737892634868622\n",
      "Training NF1:  85%|███████████████▏  | 8466/10001 [13:33:12<41:10,  1.61s/batch]Batch 8400/10001 Done, mean position loss: 21.05611291408539\n",
      "Training NF1:  85%|███████████████▏  | 8459/10001 [13:33:20<49:30,  1.93s/batch]Batch 8500/10001 Done, mean position loss: 20.623326106071474\n",
      "Training NF1:  85%|███████████████▎  | 8489/10001 [13:33:31<41:27,  1.64s/batch]Batch 8400/10001 Done, mean position loss: 20.292976257801058\n",
      "Training NF1:  85%|███████████████▎  | 8519/10001 [13:33:37<41:35,  1.68s/batch]Batch 8500/10001 Done, mean position loss: 20.832642447948455\n",
      "Training NF1:  85%|███████████████▎  | 8534/10001 [13:33:39<42:39,  1.74s/batch]Batch 8500/10001 Done, mean position loss: 20.501351251602173\n",
      "Training NF1:  85%|███████████████▏  | 8464/10001 [13:33:42<44:39,  1.74s/batch]Batch 8500/10001 Done, mean position loss: 20.82451247930527\n",
      "Training NF1:  85%|███████████████▏  | 8464/10001 [13:33:45<50:04,  1.96s/batch]Batch 8600/10001 Done, mean position loss: 20.47390256166458\n",
      "Training NF1:  84%|███████████████▏  | 8422/10001 [13:33:48<47:03,  1.79s/batch]Batch 8300/10001 Done, mean position loss: 20.677338716983797\n",
      "Training NF1:  86%|███████████████▍  | 8557/10001 [13:33:53<49:18,  2.05s/batch]Batch 8500/10001 Done, mean position loss: 20.565416891574863\n",
      "Training NF1:  84%|███████████████   | 8358/10001 [13:33:59<49:24,  1.80s/batch]Batch 8600/10001 Done, mean position loss: 20.557215266227722\n",
      "Training NF1:  85%|███████████████▎  | 8496/10001 [13:34:08<45:04,  1.80s/batch]Batch 8600/10001 Done, mean position loss: 20.452928328514098\n",
      "Training NF1:  84%|███████████████▏  | 8436/10001 [13:34:15<50:20,  1.93s/batch]Batch 8400/10001 Done, mean position loss: 20.91066750764847\n",
      "Training NF1:  86%|███████████████▍  | 8558/10001 [13:34:16<42:58,  1.79s/batch]Batch 8500/10001 Done, mean position loss: 20.514236562252044\n",
      "Training NF1:  85%|███████████████▎  | 8490/10001 [13:34:17<44:19,  1.76s/batch]Batch 8500/10001 Done, mean position loss: 20.66783186674118\n",
      "Training NF1:  86%|███████████████▌  | 8620/10001 [13:34:19<45:00,  1.96s/batch]Batch 8500/10001 Done, mean position loss: 20.701002695560454\n",
      "Training NF1:  85%|███████████████▏  | 8457/10001 [13:34:21<46:59,  1.83s/batch]Batch 8500/10001 Done, mean position loss: 20.63578819036484\n",
      "Training NF1:  85%|███████████████▎  | 8539/10001 [13:34:30<50:29,  2.07s/batch]Batch 8500/10001 Done, mean position loss: 20.680471851825715\n",
      "Training NF1:  85%|███████████████▏  | 8471/10001 [13:34:37<45:06,  1.77s/batch]Batch 8500/10001 Done, mean position loss: 20.644384286403657\n",
      "Training NF1:  86%|███████████████▍  | 8559/10001 [13:34:37<41:51,  1.74s/batch]Batch 8500/10001 Done, mean position loss: 21.0283304810524\n",
      "Training NF1:  85%|███████████████▎  | 8515/10001 [13:34:46<39:04,  1.58s/batch]Batch 8500/10001 Done, mean position loss: 20.410590739250182\n",
      "Training NF1:  86%|███████████████▍  | 8577/10001 [13:34:50<40:40,  1.71s/batch]Batch 8500/10001 Done, mean position loss: 21.00869221687317\n",
      "Training NF1:  85%|███████████████▎  | 8521/10001 [13:34:52<51:36,  2.09s/batch]Batch 8600/10001 Done, mean position loss: 20.469181218147277\n",
      "Training NF1:  86%|███████████████▌  | 8629/10001 [13:34:54<37:47,  1.65s/batch]Batch 8500/10001 Done, mean position loss: 20.35635052204132\n",
      "Training NF1:  86%|███████████████▍  | 8571/10001 [13:34:59<45:58,  1.93s/batch]Batch 8500/10001 Done, mean position loss: 20.755156061649323\n",
      "Training NF1:  85%|███████████████▏  | 8455/10001 [13:35:09<42:48,  1.66s/batch]Batch 8600/10001 Done, mean position loss: 20.717948527336123\n",
      "Training NF1:  85%|███████████████▎  | 8494/10001 [13:35:11<49:33,  1.97s/batch]Batch 8600/10001 Done, mean position loss: 20.971927597522736\n",
      "Training NF1:  85%|███████████████▏  | 8465/10001 [13:35:14<48:42,  1.90s/batch]Batch 8500/10001 Done, mean position loss: 20.690142736434936\n",
      "Training NF1:  85%|███████████████▎  | 8535/10001 [13:35:17<44:17,  1.81s/batch]Batch 8500/10001 Done, mean position loss: 20.366908802986146\n",
      "Training NF1:  86%|███████████████▍  | 8590/10001 [13:35:20<44:02,  1.87s/batch]Batch 8400/10001 Done, mean position loss: 20.59259181499481\n",
      "Training NF1:  86%|███████████████▍  | 8569/10001 [13:35:22<43:01,  1.80s/batch]Batch 8500/10001 Done, mean position loss: 20.946367609500886\n",
      "Training NF1:  85%|███████████████▎  | 8474/10001 [13:35:30<45:33,  1.79s/batch]Batch 8500/10001 Done, mean position loss: 20.692696928977966\n",
      "Training NF1:  85%|███████████████▍  | 8544/10001 [13:35:33<50:56,  2.10s/batch]Batch 8600/10001 Done, mean position loss: 20.996998600959778\n",
      "Training NF1:  87%|███████████████▌  | 8662/10001 [13:35:38<42:38,  1.91s/batch]Batch 8500/10001 Done, mean position loss: 21.60212738752365\n",
      "Training NF1:  87%|███████████████▌  | 8654/10001 [13:35:39<36:14,  1.61s/batch]Batch 8600/10001 Done, mean position loss: 20.450512619018554\n",
      "Training NF1:  85%|███████████████▎  | 8496/10001 [13:35:41<47:31,  1.89s/batch]Batch 8600/10001 Done, mean position loss: 20.48136826515198\n",
      "Training NF1:  87%|███████████████▌  | 8660/10001 [13:35:50<41:05,  1.84s/batch]Batch 8500/10001 Done, mean position loss: 20.443709726333616\n",
      "Training NF1:  84%|███████████████   | 8371/10001 [13:35:54<43:58,  1.62s/batch]Batch 8600/10001 Done, mean position loss: 20.581099162101744\n",
      "Training NF1:  85%|███████████████▎  | 8532/10001 [13:35:55<46:47,  1.91s/batch]Batch 8600/10001 Done, mean position loss: 20.82706547498703\n",
      "Training NF1:  85%|███████████████▎  | 8526/10001 [13:36:00<40:46,  1.66s/batch]Batch 8600/10001 Done, mean position loss: 20.625139985084534\n",
      "Training NF1:  86%|███████████████▌  | 8633/10001 [13:36:08<40:08,  1.76s/batch]Batch 8600/10001 Done, mean position loss: 20.719984519481656\n",
      "Training NF1:  86%|███████████████▍  | 8557/10001 [13:36:17<43:14,  1.80s/batch]Batch 8500/10001 Done, mean position loss: 21.09298375368118\n",
      "Training NF1:  86%|███████████████▌  | 8641/10001 [13:36:21<40:54,  1.81s/batch]Batch 8600/10001 Done, mean position loss: 20.609071815013884\n",
      "Training NF1:  86%|███████████████▍  | 8594/10001 [13:36:33<42:19,  1.80s/batch]Batch 8500/10001 Done, mean position loss: 20.304120738506317\n",
      "Training NF1:  87%|███████████████▌  | 8651/10001 [13:36:38<41:56,  1.86s/batch]Batch 8600/10001 Done, mean position loss: 20.83002734184265\n",
      "Training NF1:  85%|███████████████▎  | 8507/10001 [13:36:45<56:09,  2.26s/batch]Batch 8700/10001 Done, mean position loss: 20.470599994659423\n",
      "Training NF1:  87%|███████████████▋  | 8696/10001 [13:36:46<43:49,  2.01s/batch]Batch 8600/10001 Done, mean position loss: 20.495300393104554\n",
      "Training NF1:  85%|███████████████▎  | 8532/10001 [13:36:46<41:01,  1.68s/batch]Batch 8600/10001 Done, mean position loss: 20.869964745044708\n",
      "Training NF1:  87%|███████████████▋  | 8692/10001 [13:36:46<37:06,  1.70s/batch]Batch 8400/10001 Done, mean position loss: 20.66129289150238\n",
      "Training NF1:  87%|███████████████▋  | 8693/10001 [13:36:48<35:12,  1.62s/batch]Batch 8600/10001 Done, mean position loss: 20.579304375648498\n",
      "Training NF1:  86%|███████████████▌  | 8636/10001 [13:36:56<46:54,  2.06s/batch]Batch 8700/10001 Done, mean position loss: 20.555685410499574\n",
      "Training NF1:  86%|███████████████▌  | 8640/10001 [13:37:03<38:21,  1.69s/batch]Batch 8700/10001 Done, mean position loss: 20.467658257484437\n",
      "Training NF1:  85%|███████████████▍  | 8548/10001 [13:37:14<38:43,  1.60s/batch]Batch 8600/10001 Done, mean position loss: 20.631509301662447\n",
      "Training NF1:  86%|███████████████▍  | 8588/10001 [13:37:14<42:10,  1.79s/batch]Batch 8600/10001 Done, mean position loss: 20.702498378753663\n",
      "Training NF1:  86%|███████████████▍  | 8590/10001 [13:37:16<44:24,  1.89s/batch]Batch 8600/10001 Done, mean position loss: 20.661273624897007\n",
      "Training NF1:  86%|███████████████▍  | 8601/10001 [13:37:16<41:49,  1.79s/batch]Batch 8600/10001 Done, mean position loss: 20.51932485580444\n",
      "Training NF1:  86%|███████████████▍  | 8604/10001 [13:37:22<43:14,  1.86s/batch]Batch 8500/10001 Done, mean position loss: 20.903057730197908\n",
      "Training NF1:  86%|███████████████▍  | 8598/10001 [13:37:33<42:00,  1.80s/batch]Batch 8600/10001 Done, mean position loss: 20.689116070270536\n",
      "Training NF1:  87%|███████████████▌  | 8664/10001 [13:37:36<40:35,  1.82s/batch]Batch 8600/10001 Done, mean position loss: 20.625930349826813\n",
      "Training NF1:  86%|███████████████▌  | 8614/10001 [13:37:37<39:58,  1.73s/batch]Batch 8600/10001 Done, mean position loss: 21.021540060043336\n",
      "Training NF1:  87%|███████████████▋  | 8690/10001 [13:37:37<38:25,  1.76s/batch]Batch 8600/10001 Done, mean position loss: 20.40619951725006\n",
      "Training NF1:  87%|███████████████▋  | 8740/10001 [13:37:54<45:06,  2.15s/batch]Batch 8600/10001 Done, mean position loss: 20.986219844818116\n",
      "Training NF1:  87%|███████████████▌  | 8656/10001 [13:37:59<36:46,  1.64s/batch]Batch 8700/10001 Done, mean position loss: 20.47105054616928\n",
      "Training NF1:  87%|███████████████▌  | 8667/10001 [13:38:01<37:35,  1.69s/batch]Batch 8600/10001 Done, mean position loss: 20.767530167102816\n",
      "Training NF1:  86%|███████████████▍  | 8593/10001 [13:38:02<41:45,  1.78s/batch]Batch 8600/10001 Done, mean position loss: 20.365711617469785\n",
      "Training NF1:  86%|███████████████▌  | 8630/10001 [13:38:08<39:53,  1.75s/batch]Batch 8700/10001 Done, mean position loss: 20.947025721073153\n",
      "Training NF1:  87%|███████████████▋  | 8737/10001 [13:38:08<38:03,  1.81s/batch]Batch 8700/10001 Done, mean position loss: 20.717432961463928\n",
      "Training NF1:  87%|███████████████▋  | 8749/10001 [13:38:08<35:22,  1.70s/batch]Batch 8600/10001 Done, mean position loss: 20.677205708026886\n",
      "Training NF1:  87%|███████████████▌  | 8674/10001 [13:38:16<37:42,  1.71s/batch]Batch 8600/10001 Done, mean position loss: 20.35773204803467\n",
      "Training NF1:  87%|███████████████▌  | 8668/10001 [13:38:20<39:46,  1.79s/batch]Batch 8500/10001 Done, mean position loss: 20.576476593017578\n",
      "Training NF1:  87%|███████████████▋  | 8689/10001 [13:38:21<37:13,  1.70s/batch]Batch 8600/10001 Done, mean position loss: 20.922175884246826\n",
      "Training NF1:  86%|███████████████▌  | 8629/10001 [13:38:27<38:08,  1.67s/batch]Batch 8700/10001 Done, mean position loss: 21.019439215660096\n",
      "Training NF1:  86%|███████████████▍  | 8577/10001 [13:38:33<42:42,  1.80s/batch]Batch 8700/10001 Done, mean position loss: 20.445599856376646\n",
      "Training NF1:  86%|███████████████▌  | 8612/10001 [13:38:34<36:15,  1.57s/batch]Batch 8600/10001 Done, mean position loss: 20.646574196815493\n",
      "Training NF1:  86%|███████████████▌  | 8637/10001 [13:38:41<41:45,  1.84s/batch]Batch 8600/10001 Done, mean position loss: 21.58427273750305\n",
      "Training NF1:  86%|███████████████▌  | 8624/10001 [13:38:41<36:09,  1.58s/batch]Batch 8700/10001 Done, mean position loss: 20.483692891597748\n",
      "Training NF1:  87%|███████████████▋  | 8726/10001 [13:38:42<35:10,  1.66s/batch]Batch 8600/10001 Done, mean position loss: 20.440687828063965\n",
      "Training NF1:  87%|███████████████▋  | 8726/10001 [13:38:49<37:25,  1.76s/batch]Batch 8700/10001 Done, mean position loss: 20.565065338611603\n",
      "Training NF1:  86%|███████████████▍  | 8606/10001 [13:38:52<42:38,  1.83s/batch]Batch 8700/10001 Done, mean position loss: 20.82741403579712\n",
      "Training NF1:  87%|███████████████▋  | 8700/10001 [13:39:02<33:15,  1.53s/batch]Batch 8700/10001 Done, mean position loss: 20.62015503168106\n",
      "Training NF1:  87%|███████████████▋  | 8738/10001 [13:39:03<38:33,  1.83s/batch]Batch 8700/10001 Done, mean position loss: 20.731534209251404\n",
      "Training NF1:  87%|███████████████▌  | 8657/10001 [13:39:17<36:59,  1.65s/batch]Batch 8700/10001 Done, mean position loss: 20.613388361930845\n",
      "Training NF1:  87%|███████████████▋  | 8743/10001 [13:39:18<35:11,  1.68s/batch]Batch 8600/10001 Done, mean position loss: 21.058125016689303\n",
      "Training NF1:  88%|███████████████▊  | 8751/10001 [13:39:31<32:45,  1.57s/batch]Batch 8600/10001 Done, mean position loss: 20.317947583198546\n",
      "Training NF1:  88%|███████████████▊  | 8799/10001 [13:39:34<37:15,  1.86s/batch]Batch 8700/10001 Done, mean position loss: 20.823021342754362\n",
      "Training NF1:  88%|███████████████▊  | 8792/10001 [13:39:37<36:41,  1.82s/batch]Batch 8800/10001 Done, mean position loss: 20.476907112598422\n",
      "Training NF1:  87%|███████████████▋  | 8748/10001 [13:39:37<38:10,  1.83s/batch]Batch 8700/10001 Done, mean position loss: 20.487781629562377\n",
      "Training NF1:  87%|███████████████▌  | 8665/10001 [13:39:46<38:42,  1.74s/batch]Batch 8500/10001 Done, mean position loss: 20.659781615734097\n",
      "Training NF1:  85%|███████████████▎  | 8501/10001 [13:39:47<48:27,  1.94s/batch]Batch 8700/10001 Done, mean position loss: 20.569633848667145\n",
      "Training NF1:  86%|███████████████▍  | 8585/10001 [13:39:48<40:31,  1.72s/batch]Batch 8700/10001 Done, mean position loss: 20.888154981136324\n",
      "Training NF1:  87%|███████████████▋  | 8731/10001 [13:39:54<32:38,  1.54s/batch]Batch 8800/10001 Done, mean position loss: 20.548229434490203\n",
      "Training NF1:  87%|███████████████▋  | 8748/10001 [13:40:01<38:53,  1.86s/batch]Batch 8800/10001 Done, mean position loss: 20.43989192724228\n",
      "Training NF1:  87%|███████████████▌  | 8670/10001 [13:40:05<43:31,  1.96s/batch]Batch 8700/10001 Done, mean position loss: 20.64191945552826\n",
      "Training NF1:  88%|███████████████▊  | 8805/10001 [13:40:07<31:20,  1.57s/batch]Batch 8700/10001 Done, mean position loss: 20.688473982810976\n",
      "Training NF1:  87%|███████████████▋  | 8690/10001 [13:40:11<43:52,  2.01s/batch]Batch 8700/10001 Done, mean position loss: 20.664871678352355\n",
      "Training NF1:  87%|███████████████▋  | 8689/10001 [13:40:15<42:52,  1.96s/batch]Batch 8600/10001 Done, mean position loss: 20.891187818050383\n",
      "Training NF1:  87%|███████████████▌  | 8654/10001 [13:40:16<47:18,  2.11s/batch]Batch 8700/10001 Done, mean position loss: 20.507288248538973\n",
      "Training NF1:  87%|███████████████▋  | 8700/10001 [13:40:34<38:59,  1.80s/batch]Batch 8700/10001 Done, mean position loss: 20.635330555438998\n",
      "Training NF1:  88%|███████████████▊  | 8787/10001 [13:40:34<43:05,  2.13s/batch]Batch 8700/10001 Done, mean position loss: 20.68315907716751\n",
      "Training NF1:  88%|███████████████▊  | 8771/10001 [13:40:36<36:13,  1.77s/batch]Batch 8700/10001 Done, mean position loss: 20.401698281764986\n",
      "Training NF1:  87%|███████████████▋  | 8702/10001 [13:40:37<35:54,  1.66s/batch]Batch 8700/10001 Done, mean position loss: 20.99926735639572\n",
      "Training NF1:  87%|███████████████▋  | 8685/10001 [13:40:52<40:16,  1.84s/batch]Batch 8700/10001 Done, mean position loss: 20.96298467874527\n",
      "Training NF1:  85%|███████████████▎  | 8541/10001 [13:40:56<41:17,  1.70s/batch]Batch 8700/10001 Done, mean position loss: 20.344035091400148\n",
      "Training NF1:  87%|███████████████▋  | 8698/10001 [13:40:57<35:37,  1.64s/batch]Batch 8700/10001 Done, mean position loss: 20.751394126415253\n",
      "Training NF1:  88%|███████████████▊  | 8776/10001 [13:41:00<41:35,  2.04s/batch]Batch 8800/10001 Done, mean position loss: 20.95620477676392\n",
      "Training NF1:  88%|███████████████▊  | 8769/10001 [13:41:01<39:56,  1.95s/batch]Batch 8800/10001 Done, mean position loss: 20.462897760868074\n",
      "Training NF1:  88%|███████████████▊  | 8770/10001 [13:41:03<35:36,  1.74s/batch]Batch 8700/10001 Done, mean position loss: 20.680613219738007\n",
      "Training NF1:  87%|███████████████▋  | 8684/10001 [13:41:10<39:11,  1.79s/batch]Batch 8800/10001 Done, mean position loss: 20.729732875823977\n",
      "Training NF1:  87%|███████████████▋  | 8708/10001 [13:41:15<40:32,  1.88s/batch]Batch 8700/10001 Done, mean position loss: 20.35094218969345\n",
      "Training NF1:  88%|███████████████▊  | 8806/10001 [13:41:20<42:19,  2.13s/batch]Batch 8600/10001 Done, mean position loss: 20.56843776702881\n",
      "Training NF1:  86%|███████████████▍  | 8557/10001 [13:41:23<39:56,  1.66s/batch]Batch 8700/10001 Done, mean position loss: 20.92968032836914\n",
      "Training NF1:  87%|███████████████▋  | 8750/10001 [13:41:28<32:46,  1.57s/batch]Batch 8800/10001 Done, mean position loss: 20.980322670936587\n",
      "Training NF1:  87%|███████████████▋  | 8699/10001 [13:41:29<38:40,  1.78s/batch]Batch 8800/10001 Done, mean position loss: 20.43527645587921\n",
      "Training NF1:  86%|███████████████▍  | 8563/10001 [13:41:34<43:00,  1.79s/batch]Batch 8700/10001 Done, mean position loss: 20.64983751773834\n",
      "Training NF1:  87%|███████████████▋  | 8733/10001 [13:41:36<36:41,  1.74s/batch]Batch 8800/10001 Done, mean position loss: 20.478038403987885\n",
      "Training NF1:  88%|███████████████▉  | 8823/10001 [13:41:36<28:40,  1.46s/batch]Batch 8700/10001 Done, mean position loss: 21.583993203639984\n",
      "Training NF1:  87%|███████████████▋  | 8748/10001 [13:41:41<43:04,  2.06s/batch]Batch 8700/10001 Done, mean position loss: 20.42953427553177\n",
      "Training NF1:  87%|███████████████▋  | 8739/10001 [13:41:42<34:13,  1.63s/batch]Batch 8800/10001 Done, mean position loss: 20.551769216060638\n",
      "Training NF1:  87%|███████████████▋  | 8711/10001 [13:41:43<37:05,  1.72s/batch]Batch 8800/10001 Done, mean position loss: 20.815839333534242\n",
      "Training NF1:  87%|███████████████▋  | 8745/10001 [13:41:59<39:47,  1.90s/batch]Batch 8800/10001 Done, mean position loss: 20.727482879161833\n",
      "Training NF1:  88%|███████████████▉  | 8834/10001 [13:42:00<35:25,  1.82s/batch]Batch 8800/10001 Done, mean position loss: 20.62710396051407\n",
      "Training NF1:  88%|███████████████▉  | 8824/10001 [13:42:12<34:06,  1.74s/batch]Batch 8700/10001 Done, mean position loss: 21.04728992462158\n",
      "Training NF1:  88%|███████████████▊  | 8787/10001 [13:42:19<34:30,  1.71s/batch]Batch 8800/10001 Done, mean position loss: 20.612827477455138\n",
      "Training NF1:  88%|███████████████▊  | 8753/10001 [13:42:26<28:15,  1.36s/batch]Batch 8700/10001 Done, mean position loss: 20.33240889072418\n",
      "Training NF1:  87%|███████████████▋  | 8733/10001 [13:42:29<29:15,  1.38s/batch]Batch 8800/10001 Done, mean position loss: 20.799160366058352\n",
      "Training NF1:  88%|███████████████▊  | 8769/10001 [13:42:32<33:20,  1.62s/batch]Batch 8800/10001 Done, mean position loss: 20.4853536605835\n",
      "Training NF1:  88%|███████████████▉  | 8825/10001 [13:42:39<29:48,  1.52s/batch]Batch 8900/10001 Done, mean position loss: 20.443785686492923\n",
      "Training NF1:  88%|███████████████▊  | 8756/10001 [13:42:40<33:12,  1.60s/batch]Batch 8600/10001 Done, mean position loss: 20.68465814113617\n",
      "Training NF1:  87%|███████████████▋  | 8739/10001 [13:42:44<41:54,  1.99s/batch]Batch 8800/10001 Done, mean position loss: 20.562701988220216\n",
      "Training NF1:  88%|███████████████▊  | 8787/10001 [13:42:48<40:43,  2.01s/batch]Batch 8800/10001 Done, mean position loss: 20.87190686225891\n",
      "Training NF1:  87%|███████████████▋  | 8689/10001 [13:42:51<42:14,  1.93s/batch]Batch 8900/10001 Done, mean position loss: 20.558286142349242\n",
      "Training NF1:  88%|███████████████▊  | 8816/10001 [13:42:52<28:08,  1.42s/batch]Batch 8800/10001 Done, mean position loss: 20.64953603029251\n",
      "Training NF1:  89%|███████████████▉  | 8869/10001 [13:42:57<35:35,  1.89s/batch]Batch 8900/10001 Done, mean position loss: 20.4502817440033\n",
      "Training NF1:  88%|███████████████▊  | 8777/10001 [13:43:05<30:36,  1.50s/batch]Batch 8800/10001 Done, mean position loss: 20.6995072722435\n",
      "Training NF1:  89%|███████████████▉  | 8859/10001 [13:43:11<36:16,  1.91s/batch]Batch 8800/10001 Done, mean position loss: 20.63521186828613\n",
      "Training NF1:  89%|███████████████▉  | 8870/10001 [13:43:13<38:43,  2.05s/batch]Batch 8700/10001 Done, mean position loss: 20.912343163490295\n",
      "Batch 8800/10001 Done, mean position loss: 20.502033233642578\n",
      "Training NF1:  89%|███████████████▉  | 8853/10001 [13:43:26<39:12,  2.05s/batch]Batch 8800/10001 Done, mean position loss: 20.66889862537384\n",
      "Training NF1:  88%|███████████████▊  | 8789/10001 [13:43:26<30:43,  1.52s/batch]Batch 8800/10001 Done, mean position loss: 20.386790010929108\n",
      "Training NF1:  88%|███████████████▊  | 8784/10001 [13:43:31<37:42,  1.86s/batch]Batch 8800/10001 Done, mean position loss: 21.036087169647217\n",
      "Training NF1:  89%|███████████████▉  | 8862/10001 [13:43:37<33:45,  1.78s/batch]Batch 8800/10001 Done, mean position loss: 20.61358740091324\n",
      "Training NF1:  88%|███████████████▊  | 8753/10001 [13:43:44<32:34,  1.57s/batch]Batch 8800/10001 Done, mean position loss: 20.363097088336943\n",
      "Training NF1:  88%|███████████████▊  | 8799/10001 [13:43:48<41:50,  2.09s/batch]Batch 8800/10001 Done, mean position loss: 20.9942200422287\n",
      "Training NF1:  88%|███████████████▉  | 8827/10001 [13:43:50<34:41,  1.77s/batch]Batch 8900/10001 Done, mean position loss: 20.923929216861723\n",
      "Training NF1:  89%|████████████████  | 8934/10001 [13:43:51<26:06,  1.47s/batch]Batch 8800/10001 Done, mean position loss: 20.762768657207488\n",
      "Training NF1:  89%|███████████████▉  | 8881/10001 [13:43:52<30:27,  1.63s/batch]Batch 8900/10001 Done, mean position loss: 20.44251550197601\n",
      "Training NF1:  88%|███████████████▊  | 8796/10001 [13:43:59<36:28,  1.82s/batch]Batch 8800/10001 Done, mean position loss: 20.686820995807647\n",
      "Training NF1:  89%|███████████████▉  | 8878/10001 [13:44:09<34:27,  1.84s/batch]Batch 8900/10001 Done, mean position loss: 20.70953743457794\n",
      "Training NF1:  88%|███████████████▉  | 8848/10001 [13:44:09<34:47,  1.81s/batch]Batch 8800/10001 Done, mean position loss: 20.358791787624355\n",
      "Training NF1:  88%|███████████████▊  | 8766/10001 [13:44:16<31:49,  1.55s/batch]Batch 8800/10001 Done, mean position loss: 20.92863788127899\n",
      "Training NF1:  89%|████████████████  | 8899/10001 [13:44:22<34:20,  1.87s/batch]Batch 8900/10001 Done, mean position loss: 20.428752405643465\n",
      "Training NF1:  89%|███████████████▉  | 8858/10001 [13:44:25<34:01,  1.79s/batch]Batch 8900/10001 Done, mean position loss: 20.992456901073453\n",
      "Training NF1:  89%|████████████████  | 8904/10001 [13:44:26<29:55,  1.64s/batch]Batch 8900/10001 Done, mean position loss: 20.81553888320923\n",
      "Training NF1:  88%|███████████████▉  | 8837/10001 [13:44:26<30:47,  1.59s/batch]Batch 8700/10001 Done, mean position loss: 20.56620620012283\n",
      "Training NF1:  87%|███████████████▌  | 8662/10001 [13:44:28<42:22,  1.90s/batch]Batch 8800/10001 Done, mean position loss: 21.573911850452422\n",
      "Training NF1:  88%|███████████████▉  | 8836/10001 [13:44:30<31:23,  1.62s/batch]Batch 8800/10001 Done, mean position loss: 20.431990826129912\n",
      "Training NF1:  89%|████████████████  | 8906/10001 [13:44:30<32:05,  1.76s/batch]Batch 8900/10001 Done, mean position loss: 20.4682496881485\n",
      "Training NF1:  90%|████████████████▏ | 8961/10001 [13:44:35<25:48,  1.49s/batch]Batch 8900/10001 Done, mean position loss: 20.566579036712646\n",
      "Training NF1:  89%|███████████████▉  | 8874/10001 [13:44:37<35:04,  1.87s/batch]Batch 8800/10001 Done, mean position loss: 20.64201231479645\n",
      "Training NF1:  88%|███████████████▉  | 8827/10001 [13:44:44<33:46,  1.73s/batch]Batch 8900/10001 Done, mean position loss: 20.627768700122832\n",
      "Training NF1:  89%|████████████████  | 8914/10001 [13:44:50<30:58,  1.71s/batch]Batch 8900/10001 Done, mean position loss: 20.728818118572235\n",
      "Training NF1:  89%|███████████████▉  | 8860/10001 [13:45:12<34:07,  1.79s/batch]Batch 8800/10001 Done, mean position loss: 21.054687559604645\n",
      "Training NF1:  88%|███████████████▉  | 8828/10001 [13:45:14<28:55,  1.48s/batch]Batch 8900/10001 Done, mean position loss: 20.5934089922905\n",
      "Training NF1:  89%|███████████████▉  | 8864/10001 [13:45:21<43:12,  2.28s/batch]Batch 8800/10001 Done, mean position loss: 20.32180175542831\n",
      "Training NF1:  88%|███████████████▉  | 8840/10001 [13:45:26<42:46,  2.21s/batch]Batch 8900/10001 Done, mean position loss: 20.815878329277037\n",
      "Training NF1:  90%|████████████████  | 8956/10001 [13:45:29<32:01,  1.84s/batch]Batch 9000/10001 Done, mean position loss: 20.46133142709732\n",
      "Training NF1:  89%|████████████████  | 8934/10001 [13:45:31<37:03,  2.08s/batch]Batch 8900/10001 Done, mean position loss: 20.495312368869783\n",
      "Training NF1:  88%|███████████████▊  | 8813/10001 [13:45:36<45:08,  2.28s/batch]Batch 8700/10001 Done, mean position loss: 20.6656520986557\n",
      "Training NF1:  90%|████████████████▏ | 8998/10001 [13:45:42<28:27,  1.70s/batch]Batch 8900/10001 Done, mean position loss: 20.5652685713768\n",
      "Training NF1:  89%|███████████████▉  | 8862/10001 [13:45:42<33:30,  1.76s/batch]Batch 8900/10001 Done, mean position loss: 20.64643790245056\n",
      "Training NF1:  89%|████████████████  | 8912/10001 [13:45:43<27:43,  1.53s/batch]Batch 9000/10001 Done, mean position loss: 20.558147325515748\n",
      "Training NF1:  89%|███████████████▉  | 8857/10001 [13:45:46<37:14,  1.95s/batch]Batch 8900/10001 Done, mean position loss: 20.849176321029663\n",
      "Training NF1:  89%|████████████████  | 8891/10001 [13:45:49<30:49,  1.67s/batch]Batch 9000/10001 Done, mean position loss: 20.430004403591155\n",
      "Training NF1:  88%|███████████████▊  | 8792/10001 [13:46:02<31:49,  1.58s/batch]Batch 8900/10001 Done, mean position loss: 20.702445366382598\n",
      "Training NF1:  89%|███████████████▉  | 8875/10001 [13:46:04<29:25,  1.57s/batch]Batch 8900/10001 Done, mean position loss: 20.51427239894867\n",
      "Training NF1:  87%|███████████████▋  | 8721/10001 [13:46:12<39:01,  1.83s/batch]Batch 8900/10001 Done, mean position loss: 20.647343811988833\n",
      "Training NF1:  89%|███████████████▉  | 8884/10001 [13:46:20<32:29,  1.75s/batch]Batch 8800/10001 Done, mean position loss: 20.912876868247984\n",
      "Training NF1:  89%|████████████████  | 8937/10001 [13:46:20<27:40,  1.56s/batch]Batch 8900/10001 Done, mean position loss: 20.407306554317476\n",
      "Training NF1:  90%|████████████████▏ | 8969/10001 [13:46:21<29:18,  1.70s/batch]Batch 8900/10001 Done, mean position loss: 20.692443931102755\n",
      "Training NF1:  89%|████████████████  | 8891/10001 [13:46:31<30:52,  1.67s/batch]Batch 8900/10001 Done, mean position loss: 21.01993255615234\n",
      "Training NF1:  90%|████████████████▏ | 8994/10001 [13:46:36<30:45,  1.83s/batch]Batch 8900/10001 Done, mean position loss: 20.613650946617128\n",
      "Training NF1:  90%|████████████████▏ | 8976/10001 [13:46:41<30:23,  1.78s/batch]Batch 8900/10001 Done, mean position loss: 20.35403469324112\n",
      "Training NF1:  90%|████████████████▎ | 9033/10001 [13:46:42<25:55,  1.61s/batch]Batch 8900/10001 Done, mean position loss: 20.989074811935424\n",
      "Training NF1:  90%|████████████████▏ | 8967/10001 [13:46:45<31:54,  1.85s/batch]Batch 8900/10001 Done, mean position loss: 20.774734585285188\n",
      "Training NF1:  89%|████████████████  | 8926/10001 [13:46:49<34:12,  1.91s/batch]Batch 8900/10001 Done, mean position loss: 20.674229135513308\n",
      "Training NF1:  90%|████████████████▏ | 8977/10001 [13:46:49<27:48,  1.63s/batch]Batch 9000/10001 Done, mean position loss: 20.95889033794403\n",
      "Training NF1:  89%|████████████████  | 8903/10001 [13:46:51<30:08,  1.65s/batch]Batch 9000/10001 Done, mean position loss: 20.459155526161197\n",
      "Training NF1:  90%|████████████████▏ | 9008/10001 [13:47:01<28:42,  1.73s/batch]Batch 8900/10001 Done, mean position loss: 20.35423169374466\n",
      "Training NF1:  89%|████████████████  | 8935/10001 [13:47:08<29:02,  1.63s/batch]Batch 9000/10001 Done, mean position loss: 20.712815990448\n",
      "Training NF1:  88%|███████████████▊  | 8796/10001 [13:47:12<35:34,  1.77s/batch]Batch 8900/10001 Done, mean position loss: 20.928064517974853\n",
      "Training NF1:  88%|███████████████▊  | 8798/10001 [13:47:16<37:13,  1.86s/batch]Batch 9000/10001 Done, mean position loss: 20.979271326065064\n",
      "Training NF1:  90%|████████████████▏ | 8963/10001 [13:47:19<28:35,  1.65s/batch]Batch 9000/10001 Done, mean position loss: 20.429779222011568\n",
      "Training NF1:  89%|████████████████  | 8892/10001 [13:47:20<29:28,  1.59s/batch]Batch 8800/10001 Done, mean position loss: 20.562875354290007\n",
      "Training NF1:  89%|████████████████  | 8893/10001 [13:47:22<32:08,  1.74s/batch]Batch 8900/10001 Done, mean position loss: 20.439036908149717\n",
      "Training NF1:  90%|████████████████▏ | 9011/10001 [13:47:25<30:19,  1.84s/batch]Batch 9000/10001 Done, mean position loss: 20.814300997257234\n",
      "Training NF1:  90%|████████████████▏ | 8976/10001 [13:47:26<28:34,  1.67s/batch]Batch 9000/10001 Done, mean position loss: 20.505827374458313\n",
      "Training NF1:  89%|████████████████  | 8940/10001 [13:47:30<30:49,  1.74s/batch]Batch 9000/10001 Done, mean position loss: 20.546283764839174\n",
      "Training NF1:  88%|███████████████▊  | 8808/10001 [13:47:32<32:49,  1.65s/batch]Batch 8900/10001 Done, mean position loss: 21.574567375183104\n",
      "Training NF1:  90%|████████████████▏ | 9028/10001 [13:47:36<28:46,  1.77s/batch]Batch 8900/10001 Done, mean position loss: 20.65750587940216\n",
      "Training NF1:  89%|████████████████  | 8940/10001 [13:47:45<32:49,  1.86s/batch]Batch 9000/10001 Done, mean position loss: 20.718153624534608\n",
      "Training NF1:  90%|████████████████▎ | 9034/10001 [13:47:45<27:05,  1.68s/batch]Batch 9000/10001 Done, mean position loss: 20.621385755538938\n",
      "Training NF1:  91%|████████████████▎ | 9086/10001 [13:48:07<23:33,  1.54s/batch]Batch 8900/10001 Done, mean position loss: 21.035124757289886\n",
      "Training NF1:  91%|████████████████▎ | 9094/10001 [13:48:09<24:50,  1.64s/batch]Batch 8900/10001 Done, mean position loss: 20.333603296279904\n",
      "Training NF1:  90%|████████████████▏ | 8974/10001 [13:48:11<29:10,  1.70s/batch]Batch 9000/10001 Done, mean position loss: 20.61070326089859\n",
      "Training NF1:  90%|████████████████  | 8954/10001 [13:48:15<29:25,  1.69s/batch]Batch 9000/10001 Done, mean position loss: 20.81211935997009\n",
      "Training NF1:  89%|████████████████  | 8909/10001 [13:48:22<31:33,  1.73s/batch]Batch 9100/10001 Done, mean position loss: 20.45457352399826\n",
      "Training NF1:  91%|████████████████▍ | 9103/10001 [13:48:25<25:53,  1.73s/batch]Batch 9000/10001 Done, mean position loss: 20.48586054801941\n",
      "Training NF1:  90%|████████████████  | 8954/10001 [13:48:32<29:59,  1.72s/batch]Batch 9100/10001 Done, mean position loss: 20.560822789669036\n",
      "Training NF1:  88%|███████████████▉  | 8845/10001 [13:48:39<35:36,  1.85s/batch]Batch 9000/10001 Done, mean position loss: 20.90323925495148\n",
      "Training NF1:  89%|████████████████  | 8943/10001 [13:48:38<30:43,  1.74s/batch]Batch 9000/10001 Done, mean position loss: 20.647369983196256\n",
      "Training NF1:  89%|████████████████  | 8936/10001 [13:48:39<28:51,  1.63s/batch]Batch 8800/10001 Done, mean position loss: 20.67438165664673\n",
      "Training NF1:  89%|███████████████▉  | 8881/10001 [13:48:42<32:28,  1.74s/batch]Batch 9000/10001 Done, mean position loss: 20.561802983283997\n",
      "Training NF1:  91%|████████████████▍ | 9107/10001 [13:48:44<25:40,  1.72s/batch]Batch 9100/10001 Done, mean position loss: 20.434558453559873\n",
      "Training NF1:  90%|████████████████▏ | 8991/10001 [13:48:58<26:42,  1.59s/batch]Batch 9000/10001 Done, mean position loss: 20.516763317584992\n",
      "Training NF1:  90%|████████████████▏ | 9014/10001 [13:49:00<26:44,  1.63s/batch]Batch 9000/10001 Done, mean position loss: 20.657535257339475\n",
      "Training NF1:  89%|████████████████  | 8931/10001 [13:49:02<31:41,  1.78s/batch]Batch 9000/10001 Done, mean position loss: 20.72016431808472\n",
      "Training NF1:  91%|████████████████▎ | 9086/10001 [13:49:16<26:47,  1.76s/batch]Batch 9000/10001 Done, mean position loss: 20.678984198570248\n",
      "Training NF1:  90%|████████████████▏ | 8993/10001 [13:49:17<30:30,  1.82s/batch]Batch 8900/10001 Done, mean position loss: 20.90674207210541\n",
      "Training NF1:  90%|████████████████▏ | 9011/10001 [13:49:18<33:09,  2.01s/batch]Batch 9000/10001 Done, mean position loss: 20.393645298480987\n",
      "Training NF1:  90%|████████████████▎ | 9045/10001 [13:49:26<24:09,  1.52s/batch]Batch 9000/10001 Done, mean position loss: 21.03632176876068\n",
      "Training NF1:  91%|████████████████▎ | 9077/10001 [13:49:31<25:38,  1.66s/batch]Batch 9000/10001 Done, mean position loss: 20.95511699676514\n",
      "Training NF1:  90%|████████████████▏ | 9020/10001 [13:49:32<25:13,  1.54s/batch]Batch 9000/10001 Done, mean position loss: 20.604589757919314\n",
      "Training NF1:  90%|████████████████▏ | 9022/10001 [13:49:35<30:15,  1.85s/batch]Batch 9000/10001 Done, mean position loss: 20.664305171966554\n",
      "Training NF1:  90%|████████████████▎ | 9034/10001 [13:49:35<27:36,  1.71s/batch]Batch 9000/10001 Done, mean position loss: 20.785576298236847\n",
      "Training NF1:  90%|████████████████▏ | 9024/10001 [13:49:39<28:08,  1.73s/batch]Batch 9000/10001 Done, mean position loss: 20.358081774711607\n",
      "Training NF1:  91%|████████████████▎ | 9053/10001 [13:49:42<25:52,  1.64s/batch]Batch 9100/10001 Done, mean position loss: 20.44161703824997\n",
      "Training NF1:  90%|████████████████▎ | 9038/10001 [13:49:43<33:36,  2.09s/batch]Batch 9100/10001 Done, mean position loss: 20.934285042285918\n",
      "Training NF1:  91%|████████████████▍ | 9149/10001 [13:49:55<21:42,  1.53s/batch]Batch 9000/10001 Done, mean position loss: 20.356430490016937\n",
      "Training NF1:  90%|████████████████▏ | 9022/10001 [13:50:02<29:24,  1.80s/batch]Batch 9100/10001 Done, mean position loss: 20.696308577060698\n",
      "Training NF1:  90%|████████████████▎ | 9038/10001 [13:50:05<29:13,  1.82s/batch]Batch 9100/10001 Done, mean position loss: 20.96116120815277\n",
      "Training NF1:  91%|████████████████▎ | 9056/10001 [13:50:14<25:48,  1.64s/batch]Batch 9000/10001 Done, mean position loss: 20.923121225833892\n",
      "Training NF1:  89%|████████████████  | 8899/10001 [13:50:14<31:22,  1.71s/batch]Batch 9100/10001 Done, mean position loss: 20.44567464351654\n",
      "Training NF1:  90%|████████████████▏ | 8978/10001 [13:50:18<33:15,  1.95s/batch]Batch 8900/10001 Done, mean position loss: 20.563179171085356\n",
      "Training NF1:  91%|████████████████▍ | 9111/10001 [13:50:19<25:17,  1.71s/batch]Batch 9100/10001 Done, mean position loss: 20.799543714523313\n",
      "Training NF1:  89%|████████████████  | 8904/10001 [13:50:23<29:29,  1.61s/batch]Batch 9000/10001 Done, mean position loss: 20.4326012301445\n",
      "Training NF1:  91%|████████████████▎ | 9076/10001 [13:50:23<25:21,  1.65s/batch]Batch 9100/10001 Done, mean position loss: 20.481521186828616\n",
      "Training NF1:  90%|████████████████▏ | 8983/10001 [13:50:27<29:49,  1.76s/batch]Batch 9100/10001 Done, mean position loss: 20.556321065425873\n",
      "Training NF1:  91%|████████████████▎ | 9055/10001 [13:50:31<25:24,  1.61s/batch]Batch 9000/10001 Done, mean position loss: 21.61164784669876\n",
      "Training NF1:  92%|████████████████▍ | 9162/10001 [13:50:30<25:06,  1.80s/batch]Batch 9000/10001 Done, mean position loss: 20.703622250556943\n",
      "Training NF1:  91%|████████████████▎ | 9063/10001 [13:50:33<27:33,  1.76s/batch]Batch 9100/10001 Done, mean position loss: 20.627055296897886\n",
      "Training NF1:  90%|████████████████▎ | 9043/10001 [13:50:39<26:45,  1.68s/batch]Batch 9100/10001 Done, mean position loss: 20.716585881710053\n",
      "Training NF1:  91%|████████████████▎ | 9054/10001 [13:50:58<27:50,  1.76s/batch]Batch 9000/10001 Done, mean position loss: 21.044675431251527\n",
      "Training NF1:  90%|████████████████▏ | 9004/10001 [13:51:02<24:37,  1.48s/batch]Batch 9000/10001 Done, mean position loss: 20.317330055236816\n",
      "Training NF1:  90%|████████████████▎ | 9050/10001 [13:51:03<28:45,  1.81s/batch]Batch 9100/10001 Done, mean position loss: 20.599052994251252\n",
      "Training NF1:  91%|████████████████▎ | 9058/10001 [13:51:06<29:48,  1.90s/batch]Batch 9100/10001 Done, mean position loss: 20.805814673900606\n",
      "Training NF1:  91%|████████████████▎ | 9077/10001 [13:51:10<26:29,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 20.435751960277557\n",
      "Training NF1:  91%|████████████████▎ | 9082/10001 [13:51:21<33:04,  2.16s/batch]Batch 9100/10001 Done, mean position loss: 20.4919389796257\n",
      "Training NF1:  90%|████████████████▎ | 9042/10001 [13:51:28<38:37,  2.42s/batch]Batch 9200/10001 Done, mean position loss: 20.552627861499786\n",
      "Training NF1:  91%|████████████████▎ | 9056/10001 [13:51:34<30:11,  1.92s/batch]Batch 9100/10001 Done, mean position loss: 20.89048212528229\n",
      "Training NF1:  91%|████████████████▍ | 9135/10001 [13:51:34<22:44,  1.58s/batch]Batch 9100/10001 Done, mean position loss: 20.635014097690583\n",
      "Training NF1:  90%|████████████████▎ | 9043/10001 [13:51:35<27:54,  1.75s/batch]Batch 8900/10001 Done, mean position loss: 20.673817021846773\n",
      "Training NF1:  92%|████████████████▍ | 9165/10001 [13:51:39<26:02,  1.87s/batch]Batch 9200/10001 Done, mean position loss: 20.432546129226687\n",
      "Training NF1:  90%|████████████████▎ | 9043/10001 [13:51:43<29:35,  1.85s/batch]Batch 9100/10001 Done, mean position loss: 20.548802461624145\n",
      "Training NF1:  91%|████████████████▎ | 9075/10001 [13:51:50<27:16,  1.77s/batch]Batch 9100/10001 Done, mean position loss: 20.50597799539566\n",
      "Training NF1:  91%|████████████████▍ | 9113/10001 [13:51:55<22:52,  1.55s/batch]Batch 9100/10001 Done, mean position loss: 20.708905234336854\n",
      "Training NF1:  91%|████████████████▎ | 9068/10001 [13:51:57<25:14,  1.62s/batch]Batch 9100/10001 Done, mean position loss: 20.6535697555542\n",
      "Training NF1:  90%|████████████████▏ | 8965/10001 [13:52:11<30:15,  1.75s/batch]Batch 9000/10001 Done, mean position loss: 20.913341205120087\n",
      "Batch 9100/10001 Done, mean position loss: 20.386252987384797\n",
      "Training NF1:  91%|████████████████▍ | 9116/10001 [13:52:20<22:41,  1.54s/batch]Batch 9100/10001 Done, mean position loss: 20.674464373588563\n",
      "Training NF1:  90%|████████████████▏ | 8973/10001 [13:52:26<31:05,  1.81s/batch]Batch 9100/10001 Done, mean position loss: 20.988663156032562\n",
      "Training NF1:  91%|████████████████▎ | 9067/10001 [13:52:27<30:39,  1.97s/batch]Batch 9100/10001 Done, mean position loss: 20.95927839756012\n",
      "Training NF1:  92%|████████████████▌ | 9180/10001 [13:52:29<24:56,  1.82s/batch]Batch 9100/10001 Done, mean position loss: 20.61504804134369\n",
      "Training NF1:  92%|████████████████▋ | 9248/10001 [13:52:36<25:14,  2.01s/batch]Batch 9100/10001 Done, mean position loss: 20.646347777843477\n",
      "Training NF1:  91%|████████████████▍ | 9110/10001 [13:52:37<24:11,  1.63s/batch]Batch 9100/10001 Done, mean position loss: 20.34748564004898\n",
      "Training NF1:  92%|████████████████▌ | 9180/10001 [13:52:39<24:43,  1.81s/batch]Batch 9100/10001 Done, mean position loss: 20.767032825946806\n",
      "Training NF1:  92%|████████████████▌ | 9179/10001 [13:52:41<23:37,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 20.917858459949493\n",
      "Training NF1:  91%|████████████████▍ | 9140/10001 [13:52:43<23:26,  1.63s/batch]Batch 9200/10001 Done, mean position loss: 20.458169462680814\n",
      "Training NF1:  92%|████████████████▍ | 9163/10001 [13:52:57<31:51,  2.28s/batch]Batch 9100/10001 Done, mean position loss: 20.3533562541008\n",
      "Training NF1:  92%|████████████████▌ | 9209/10001 [13:52:58<24:59,  1.89s/batch]Batch 9200/10001 Done, mean position loss: 20.71810079813004\n",
      "Training NF1:  92%|████████████████▌ | 9188/10001 [13:53:03<23:58,  1.77s/batch]Batch 9200/10001 Done, mean position loss: 20.99418056488037\n",
      "Training NF1:  93%|████████████████▋ | 9253/10001 [13:53:13<20:37,  1.65s/batch]Batch 9000/10001 Done, mean position loss: 20.560811202526093\n",
      "Training NF1:  93%|████████████████▋ | 9269/10001 [13:53:15<24:45,  2.03s/batch]Batch 9100/10001 Done, mean position loss: 20.931896419525145\n",
      "Training NF1:  92%|████████████████▌ | 9196/10001 [13:53:17<26:04,  1.94s/batch]Batch 9200/10001 Done, mean position loss: 20.43559100151062\n",
      "Training NF1:  92%|████████████████▌ | 9201/10001 [13:53:17<27:05,  2.03s/batch]Batch 9100/10001 Done, mean position loss: 20.43280426979065\n",
      "Training NF1:  90%|████████████████  | 8958/10001 [13:53:21<35:52,  2.06s/batch]Batch 9200/10001 Done, mean position loss: 20.803130154609683\n",
      "Training NF1:  92%|████████████████▍ | 9155/10001 [13:53:24<26:11,  1.86s/batch]Batch 9200/10001 Done, mean position loss: 20.48086890935898\n",
      "Training NF1:  91%|████████████████▍ | 9107/10001 [13:53:26<22:45,  1.53s/batch]Batch 9200/10001 Done, mean position loss: 20.55856378078461\n",
      "Training NF1:  91%|████████████████▎ | 9083/10001 [13:53:30<26:57,  1.76s/batch]Batch 9100/10001 Done, mean position loss: 21.568168494701386\n",
      "Training NF1:  91%|████████████████▍ | 9133/10001 [13:53:34<26:12,  1.81s/batch]Batch 9200/10001 Done, mean position loss: 20.73102724790573\n",
      "Batch 9100/10001 Done, mean position loss: 20.706913521289827\n",
      "Training NF1:  92%|████████████████▍ | 9160/10001 [13:53:37<28:31,  2.04s/batch]Batch 9200/10001 Done, mean position loss: 20.596276824474337\n",
      "Training NF1:  92%|████████████████▌ | 9189/10001 [13:54:00<24:42,  1.83s/batch]Batch 9100/10001 Done, mean position loss: 21.051975631713866\n",
      "Training NF1:  92%|████████████████▌ | 9233/10001 [13:54:02<26:51,  2.10s/batch]Batch 9100/10001 Done, mean position loss: 20.319721372127532\n",
      "Training NF1:  92%|████████████████▌ | 9200/10001 [13:54:02<21:52,  1.64s/batch]Batch 9200/10001 Done, mean position loss: 20.595770363807677\n",
      "Training NF1:  92%|████████████████▌ | 9234/10001 [13:54:04<27:28,  2.15s/batch]Batch 9200/10001 Done, mean position loss: 20.801181731224062\n",
      "Training NF1:  92%|████████████████▌ | 9222/10001 [13:54:09<18:50,  1.45s/batch]Batch 9300/10001 Done, mean position loss: 20.453801138401033\n",
      "Training NF1:  92%|████████████████▌ | 9231/10001 [13:54:20<23:29,  1.83s/batch]Batch 9200/10001 Done, mean position loss: 20.481121213436126\n",
      "Training NF1:  92%|████████████████▍ | 9162/10001 [13:54:24<27:08,  1.94s/batch]Batch 9300/10001 Done, mean position loss: 20.53753408670425\n",
      "Training NF1:  91%|████████████████▎ | 9082/10001 [13:54:30<26:00,  1.70s/batch]Batch 9200/10001 Done, mean position loss: 20.873925578594207\n",
      "Training NF1:  93%|████████████████▊ | 9317/10001 [13:54:36<16:01,  1.41s/batch]Batch 9300/10001 Done, mean position loss: 20.42835995197296\n",
      "Training NF1:  92%|████████████████▍ | 9158/10001 [13:54:36<22:32,  1.60s/batch]Batch 9200/10001 Done, mean position loss: 20.643018343448638\n",
      "Training NF1:  92%|████████████████▌ | 9171/10001 [13:54:40<23:33,  1.70s/batch]Batch 9200/10001 Done, mean position loss: 20.561303431987763\n",
      "Training NF1:  93%|████████████████▋ | 9259/10001 [13:54:43<25:36,  2.07s/batch]Batch 9000/10001 Done, mean position loss: 20.677847015857694\n",
      "Training NF1:  93%|████████████████▊ | 9314/10001 [13:54:48<21:47,  1.90s/batch]Batch 9200/10001 Done, mean position loss: 20.501426434516905\n",
      "Training NF1:  92%|████████████████▌ | 9218/10001 [13:54:51<24:47,  1.90s/batch]Batch 9200/10001 Done, mean position loss: 20.707630813121796\n",
      "Training NF1:  92%|████████████████▌ | 9233/10001 [13:54:59<23:18,  1.82s/batch]Batch 9200/10001 Done, mean position loss: 20.637544996738434\n",
      "Training NF1:  92%|████████████████▌ | 9194/10001 [13:55:04<26:01,  1.93s/batch]Batch 9100/10001 Done, mean position loss: 20.89992948293686\n",
      "Training NF1:  93%|████████████████▋ | 9272/10001 [13:55:08<20:03,  1.65s/batch]Batch 9200/10001 Done, mean position loss: 20.387677104473113\n",
      "Training NF1:  92%|████████████████▍ | 9162/10001 [13:55:15<21:51,  1.56s/batch]Batch 9200/10001 Done, mean position loss: 21.01032823085785\n",
      "Training NF1:  92%|████████████████▌ | 9228/10001 [13:55:22<21:39,  1.68s/batch]Batch 9200/10001 Done, mean position loss: 20.669328775405884\n",
      "Training NF1:  93%|████████████████▋ | 9261/10001 [13:55:25<23:21,  1.89s/batch]Batch 9200/10001 Done, mean position loss: 20.959330937862397\n",
      "Training NF1:  93%|████████████████▊ | 9336/10001 [13:55:26<18:30,  1.67s/batch]Batch 9200/10001 Done, mean position loss: 20.615841236114505\n",
      "Training NF1:  92%|████████████████▌ | 9213/10001 [13:55:31<22:04,  1.68s/batch]Batch 9200/10001 Done, mean position loss: 20.653794453144073\n",
      "Training NF1:  92%|████████████████▌ | 9230/10001 [13:55:31<20:57,  1.63s/batch]Batch 9200/10001 Done, mean position loss: 20.353317444324496\n",
      "Training NF1:  93%|████████████████▋ | 9297/10001 [13:55:33<19:42,  1.68s/batch]Batch 9200/10001 Done, mean position loss: 20.77073468208313\n",
      "Training NF1:  94%|████████████████▊ | 9351/10001 [13:55:36<17:39,  1.63s/batch]Batch 9300/10001 Done, mean position loss: 20.97574912071228\n",
      "Training NF1:  93%|████████████████▊ | 9344/10001 [13:55:40<21:31,  1.97s/batch]Batch 9300/10001 Done, mean position loss: 20.468261063098907\n",
      "Training NF1:  92%|████████████████▌ | 9218/10001 [13:55:54<20:58,  1.61s/batch]Batch 9200/10001 Done, mean position loss: 20.364844298362733\n",
      "Training NF1:  93%|████████████████▋ | 9264/10001 [13:55:56<18:49,  1.53s/batch]Batch 9300/10001 Done, mean position loss: 20.695386950969695\n",
      "Training NF1:  93%|████████████████▋ | 9265/10001 [13:55:58<19:48,  1.61s/batch]Batch 9300/10001 Done, mean position loss: 20.9486070227623\n",
      "Training NF1:  93%|████████████████▊ | 9317/10001 [13:56:10<21:28,  1.88s/batch]Batch 9100/10001 Done, mean position loss: 20.56929180383682\n",
      "Training NF1:  93%|████████████████▋ | 9293/10001 [13:56:10<20:24,  1.73s/batch]Batch 9300/10001 Done, mean position loss: 20.437910106182095\n",
      "Training NF1:  92%|████████████████▌ | 9213/10001 [13:56:14<23:33,  1.79s/batch]Batch 9300/10001 Done, mean position loss: 20.464946835041047\n",
      "Training NF1:  92%|████████████████▌ | 9230/10001 [13:56:15<21:00,  1.63s/batch]Batch 9300/10001 Done, mean position loss: 20.795582647323606\n",
      "Training NF1:  93%|████████████████▋ | 9301/10001 [13:56:15<21:09,  1.81s/batch]Batch 9200/10001 Done, mean position loss: 20.419881539344786\n",
      "Training NF1:  92%|████████████████▋ | 9241/10001 [13:56:19<29:56,  2.36s/batch]Batch 9200/10001 Done, mean position loss: 20.93474538564682\n",
      "Training NF1:  94%|████████████████▊ | 9361/10001 [13:56:24<21:48,  2.04s/batch]Batch 9300/10001 Done, mean position loss: 20.553385622501374\n",
      "Training NF1:  93%|████████████████▊ | 9309/10001 [13:56:30<21:21,  1.85s/batch]Batch 9200/10001 Done, mean position loss: 21.573224868774414\n",
      "Batch 9300/10001 Done, mean position loss: 20.600020501613617\n",
      "Training NF1:  93%|████████████████▋ | 9263/10001 [13:56:38<19:55,  1.62s/batch]Batch 9300/10001 Done, mean position loss: 20.723287858963012\n",
      "Training NF1:  92%|████████████████▌ | 9213/10001 [13:56:42<25:38,  1.95s/batch]Batch 9200/10001 Done, mean position loss: 20.664387259483338\n",
      "Training NF1:  92%|████████████████▋ | 9242/10001 [13:57:02<19:24,  1.53s/batch]Batch 9200/10001 Done, mean position loss: 21.066914136409757\n",
      "Training NF1:  93%|████████████████▊ | 9330/10001 [13:57:03<19:20,  1.73s/batch]Batch 9300/10001 Done, mean position loss: 20.812111155986784\n",
      "Training NF1:  93%|████████████████▋ | 9252/10001 [13:57:03<19:03,  1.53s/batch]Batch 9300/10001 Done, mean position loss: 20.580044808387754\n",
      "Training NF1:  92%|████████████████▌ | 9202/10001 [13:57:04<22:25,  1.68s/batch]Batch 9200/10001 Done, mean position loss: 20.319748179912565\n",
      "Training NF1:  93%|████████████████▊ | 9324/10001 [13:57:05<20:40,  1.83s/batch]Batch 9400/10001 Done, mean position loss: 20.447209916114808\n",
      "Training NF1:  93%|████████████████▊ | 9337/10001 [13:57:21<19:16,  1.74s/batch]Batch 9400/10001 Done, mean position loss: 20.556633400917054\n",
      "Training NF1:  93%|████████████████▋ | 9288/10001 [13:57:21<20:22,  1.71s/batch]Batch 9300/10001 Done, mean position loss: 20.48665455341339\n",
      "Training NF1:  91%|████████████████▍ | 9142/10001 [13:57:25<24:20,  1.70s/batch]Batch 9300/10001 Done, mean position loss: 20.857084345817565\n",
      "Training NF1:  93%|████████████████▊ | 9333/10001 [13:57:26<18:13,  1.64s/batch]Batch 9300/10001 Done, mean position loss: 20.63879311800003\n",
      "Training NF1:  94%|████████████████▊ | 9368/10001 [13:57:35<20:19,  1.93s/batch]Batch 9400/10001 Done, mean position loss: 20.437562606334687\n",
      "Training NF1:  93%|████████████████▋ | 9272/10001 [13:57:38<17:03,  1.40s/batch]Batch 9100/10001 Done, mean position loss: 20.652027442455292\n",
      "Training NF1:  93%|████████████████▊ | 9350/10001 [13:57:39<18:12,  1.68s/batch]Batch 9300/10001 Done, mean position loss: 20.540822494029996\n",
      "Training NF1:  93%|████████████████▊ | 9346/10001 [13:57:44<17:51,  1.64s/batch]Batch 9300/10001 Done, mean position loss: 20.6996545624733\n",
      "Training NF1:  92%|████████████████▋ | 9249/10001 [13:57:48<26:51,  2.14s/batch]Batch 9300/10001 Done, mean position loss: 20.503144340515135\n",
      "Training NF1:  94%|████████████████▊ | 9370/10001 [13:58:03<16:15,  1.55s/batch]Batch 9200/10001 Done, mean position loss: 20.90859309911728\n",
      "Training NF1:  92%|████████████████▌ | 9168/10001 [13:58:09<24:13,  1.74s/batch]Batch 9300/10001 Done, mean position loss: 20.635235805511478\n",
      "Training NF1:  93%|████████████████▋ | 9289/10001 [13:58:10<23:15,  1.96s/batch]Batch 9300/10001 Done, mean position loss: 20.38372274875641\n",
      "Training NF1:  93%|████████████████▋ | 9255/10001 [13:58:12<18:51,  1.52s/batch]Batch 9300/10001 Done, mean position loss: 21.01884976863861\n",
      "Training NF1:  94%|████████████████▊ | 9361/10001 [13:58:20<20:15,  1.90s/batch]Batch 9300/10001 Done, mean position loss: 20.61415629863739\n",
      "Training NF1:  94%|████████████████▉ | 9429/10001 [13:58:24<18:04,  1.90s/batch]Batch 9300/10001 Done, mean position loss: 20.356033289432528\n",
      "Training NF1:  93%|████████████████▋ | 9269/10001 [13:58:25<23:05,  1.89s/batch]Batch 9300/10001 Done, mean position loss: 20.964720046520235\n",
      "Training NF1:  91%|████████████████▍ | 9127/10001 [13:58:27<27:52,  1.91s/batch]Batch 9300/10001 Done, mean position loss: 20.75979213476181\n",
      "Training NF1:  92%|████████████████▋ | 9250/10001 [13:58:32<22:05,  1.76s/batch]Batch 9300/10001 Done, mean position loss: 20.66357621908188\n",
      "Training NF1:  93%|████████████████▋ | 9301/10001 [13:58:32<22:59,  1.97s/batch]Batch 9300/10001 Done, mean position loss: 20.66740448951721\n",
      "Training NF1:  93%|████████████████▊ | 9310/10001 [13:58:34<19:15,  1.67s/batch]Batch 9400/10001 Done, mean position loss: 20.97115911722183\n",
      "Training NF1:  93%|████████████████▊ | 9348/10001 [13:58:43<16:31,  1.52s/batch]Batch 9400/10001 Done, mean position loss: 20.453000102043152\n",
      "Training NF1:  93%|████████████████▊ | 9308/10001 [13:58:47<27:35,  2.39s/batch]Batch 9300/10001 Done, mean position loss: 20.34341349840164\n",
      "Training NF1:  93%|████████████████▊ | 9323/10001 [13:58:57<20:12,  1.79s/batch]Batch 9400/10001 Done, mean position loss: 20.92389967918396\n",
      "Training NF1:  94%|████████████████▉ | 9391/10001 [13:58:58<17:45,  1.75s/batch]Batch 9400/10001 Done, mean position loss: 20.69324132680893\n",
      "Training NF1:  91%|████████████████▍ | 9150/10001 [13:59:09<24:12,  1.71s/batch]Batch 9400/10001 Done, mean position loss: 20.48203658103943\n",
      "Training NF1:  94%|████████████████▉ | 9395/10001 [13:59:11<15:53,  1.57s/batch]Batch 9200/10001 Done, mean position loss: 20.56577674150467\n",
      "Training NF1:  94%|████████████████▉ | 9419/10001 [13:59:12<14:36,  1.51s/batch]Batch 9400/10001 Done, mean position loss: 20.43409214735031\n",
      "Training NF1:  93%|████████████████▊ | 9325/10001 [13:59:17<19:56,  1.77s/batch]Batch 9400/10001 Done, mean position loss: 20.794055836200712\n",
      "Training NF1:  95%|█████████████████ | 9463/10001 [13:59:17<17:37,  1.97s/batch]Batch 9300/10001 Done, mean position loss: 20.408577346801756\n",
      "Training NF1:  93%|████████████████▊ | 9328/10001 [13:59:22<21:00,  1.87s/batch]Batch 9400/10001 Done, mean position loss: 20.547194137573243\n",
      "Training NF1:  94%|████████████████▊ | 9361/10001 [13:59:24<17:08,  1.61s/batch]Batch 9400/10001 Done, mean position loss: 20.612270050048828\n",
      "Training NF1:  93%|████████████████▊ | 9340/10001 [13:59:26<17:54,  1.62s/batch]Batch 9300/10001 Done, mean position loss: 20.950072793960572\n",
      "Training NF1:  94%|████████████████▉ | 9428/10001 [13:59:25<18:49,  1.97s/batch]Batch 9400/10001 Done, mean position loss: 20.71318590402603\n",
      "Training NF1:  95%|█████████████████ | 9469/10001 [13:59:29<16:14,  1.83s/batch]Batch 9300/10001 Done, mean position loss: 21.58734848737717\n",
      "Training NF1:  94%|████████████████▉ | 9415/10001 [13:59:34<16:03,  1.64s/batch]Batch 9300/10001 Done, mean position loss: 20.633547151088713\n",
      "Training NF1:  94%|████████████████▉ | 9426/10001 [13:59:52<15:18,  1.60s/batch]Batch 9500/10001 Done, mean position loss: 20.462969925403595\n",
      "Training NF1:  94%|████████████████▉ | 9397/10001 [13:59:57<15:00,  1.49s/batch]Batch 9400/10001 Done, mean position loss: 20.60482742547989\n",
      "Training NF1:  94%|████████████████▊ | 9372/10001 [14:00:01<20:38,  1.97s/batch]Batch 9300/10001 Done, mean position loss: 21.072771790027616\n",
      "Training NF1:  93%|████████████████▊ | 9343/10001 [14:00:04<16:46,  1.53s/batch]Batch 9400/10001 Done, mean position loss: 20.812375962734222\n",
      "Training NF1:  94%|████████████████▊ | 9359/10001 [14:00:07<18:27,  1.72s/batch]Batch 9300/10001 Done, mean position loss: 20.314614038467404\n",
      "Training NF1:  94%|████████████████▉ | 9438/10001 [14:00:12<15:31,  1.66s/batch]Batch 9400/10001 Done, mean position loss: 20.845724318027496\n",
      "Training NF1:  94%|████████████████▊ | 9363/10001 [14:00:20<19:46,  1.86s/batch]Batch 9400/10001 Done, mean position loss: 20.48477569580078\n",
      "Training NF1:  94%|████████████████▉ | 9408/10001 [14:00:25<17:04,  1.73s/batch]Batch 9500/10001 Done, mean position loss: 20.544292809963224\n",
      "Training NF1:  95%|█████████████████ | 9501/10001 [14:00:26<15:38,  1.88s/batch]Batch 9400/10001 Done, mean position loss: 20.64872861623764\n",
      "Training NF1:  93%|████████████████▋ | 9284/10001 [14:00:30<26:04,  2.18s/batch]Batch 9400/10001 Done, mean position loss: 20.536115927696226\n",
      "Training NF1:  95%|█████████████████ | 9454/10001 [14:00:33<18:23,  2.02s/batch]Batch 9500/10001 Done, mean position loss: 20.440580790042876\n",
      "Training NF1:  95%|█████████████████ | 9455/10001 [14:00:34<15:48,  1.74s/batch]Batch 9200/10001 Done, mean position loss: 20.64331641435623\n",
      "Training NF1:  94%|████████████████▉ | 9442/10001 [14:00:34<16:04,  1.73s/batch]Batch 9400/10001 Done, mean position loss: 20.695526547431946\n",
      "Training NF1:  94%|████████████████▊ | 9372/10001 [14:00:53<16:42,  1.59s/batch]Batch 9400/10001 Done, mean position loss: 20.49433719396591\n",
      "Training NF1:  95%|█████████████████ | 9469/10001 [14:00:59<16:33,  1.87s/batch]Batch 9400/10001 Done, mean position loss: 20.634667530059815\n",
      "Training NF1:  95%|█████████████████ | 9472/10001 [14:01:03<14:00,  1.59s/batch]Batch 9300/10001 Done, mean position loss: 20.894441356658934\n",
      "Training NF1:  94%|████████████████▉ | 9408/10001 [14:01:07<19:44,  2.00s/batch]Batch 9400/10001 Done, mean position loss: 20.387495498657227\n",
      "Training NF1:  94%|████████████████▉ | 9405/10001 [14:01:06<17:37,  1.77s/batch]Batch 9400/10001 Done, mean position loss: 21.007778079509734\n",
      "Training NF1:  94%|████████████████▊ | 9359/10001 [14:01:14<17:22,  1.62s/batch]Batch 9400/10001 Done, mean position loss: 20.609100029468536\n",
      "Training NF1:  95%|█████████████████ | 9478/10001 [14:01:23<17:13,  1.98s/batch]Batch 9400/10001 Done, mean position loss: 20.95876284837723\n",
      "Training NF1:  94%|████████████████▉ | 9442/10001 [14:01:25<16:50,  1.81s/batch]Batch 9400/10001 Done, mean position loss: 20.76277171611786\n",
      "Training NF1:  94%|████████████████▉ | 9435/10001 [14:01:27<14:38,  1.55s/batch]Batch 9400/10001 Done, mean position loss: 20.349311308860777\n",
      "Training NF1:  95%|█████████████████ | 9475/10001 [14:01:27<14:26,  1.65s/batch]Batch 9500/10001 Done, mean position loss: 20.972326736450196\n",
      "Training NF1:  94%|████████████████▊ | 9374/10001 [14:01:28<20:00,  1.91s/batch]Batch 9400/10001 Done, mean position loss: 20.653122036457063\n",
      "Training NF1:  93%|████████████████▊ | 9319/10001 [14:01:35<22:40,  1.99s/batch]Batch 9400/10001 Done, mean position loss: 20.688957738876343\n",
      "Training NF1:  95%|█████████████████ | 9482/10001 [14:01:43<16:01,  1.85s/batch]Batch 9500/10001 Done, mean position loss: 20.457543494701383\n",
      "Training NF1:  95%|█████████████████▏| 9545/10001 [14:01:44<14:08,  1.86s/batch]Batch 9400/10001 Done, mean position loss: 20.35049394130707\n",
      "Training NF1:  94%|████████████████▉ | 9418/10001 [14:01:53<15:20,  1.58s/batch]Batch 9500/10001 Done, mean position loss: 20.68779878616333\n",
      "Training NF1:  95%|█████████████████ | 9453/10001 [14:01:59<17:10,  1.88s/batch]Batch 9500/10001 Done, mean position loss: 20.95884865283966\n",
      "Training NF1:  95%|█████████████████ | 9472/10001 [14:01:59<14:39,  1.66s/batch]Batch 9500/10001 Done, mean position loss: 20.472232992649076\n",
      "Training NF1:  94%|████████████████▉ | 9423/10001 [14:02:05<16:44,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.43165027618408\n",
      "Training NF1:  95%|█████████████████ | 9514/10001 [14:02:13<11:44,  1.45s/batch]Batch 9500/10001 Done, mean position loss: 20.790489108562472\n",
      "Training NF1:  93%|████████████████▋ | 9260/10001 [14:02:15<22:57,  1.86s/batch]Batch 9500/10001 Done, mean position loss: 20.54895082473755\n",
      "Training NF1:  94%|████████████████▉ | 9440/10001 [14:02:15<15:24,  1.65s/batch]Batch 9400/10001 Done, mean position loss: 20.432649841308596\n",
      "Training NF1:  95%|█████████████████ | 9503/10001 [14:02:16<14:00,  1.69s/batch]Batch 9300/10001 Done, mean position loss: 20.558715646266936\n",
      "Training NF1:  94%|████████████████▉ | 9431/10001 [14:02:20<17:22,  1.83s/batch]Batch 9400/10001 Done, mean position loss: 21.584203052520753\n",
      "Training NF1:  96%|█████████████████▏| 9582/10001 [14:02:21<11:48,  1.69s/batch]Batch 9500/10001 Done, mean position loss: 20.719642851352692\n",
      "Training NF1:  94%|████████████████▉ | 9396/10001 [14:02:24<20:38,  2.05s/batch]Batch 9500/10001 Done, mean position loss: 20.602486493587495\n",
      "Training NF1:  94%|████████████████▉ | 9407/10001 [14:02:31<17:11,  1.74s/batch]Batch 9400/10001 Done, mean position loss: 20.933291923999786\n",
      "Training NF1:  93%|████████████████▊ | 9313/10001 [14:02:39<22:17,  1.94s/batch]Batch 9400/10001 Done, mean position loss: 20.646358268260954\n",
      "Training NF1:  95%|█████████████████ | 9464/10001 [14:02:51<14:09,  1.58s/batch]Batch 9500/10001 Done, mean position loss: 20.57678720712662\n",
      "Training NF1:  95%|█████████████████ | 9454/10001 [14:02:54<16:40,  1.83s/batch]Batch 9600/10001 Done, mean position loss: 20.455567541122434\n",
      "Training NF1:  94%|████████████████▉ | 9418/10001 [14:03:03<17:48,  1.83s/batch]Batch 9400/10001 Done, mean position loss: 21.054491531848903\n",
      "Training NF1:  95%|█████████████████ | 9510/10001 [14:03:07<16:26,  2.01s/batch]Batch 9500/10001 Done, mean position loss: 20.84942783355713\n",
      "Training NF1:  95%|█████████████████ | 9478/10001 [14:03:11<17:26,  2.00s/batch]Batch 9400/10001 Done, mean position loss: 20.31792516708374\n",
      "Training NF1:  95%|█████████████████ | 9462/10001 [14:03:13<17:11,  1.91s/batch]Batch 9500/10001 Done, mean position loss: 20.800837125778198\n",
      "Training NF1:  96%|█████████████████▏| 9553/10001 [14:03:17<13:00,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.485382866859435\n",
      "Training NF1:  95%|█████████████████ | 9455/10001 [14:03:19<15:02,  1.65s/batch]Batch 9500/10001 Done, mean position loss: 20.630215239524844\n",
      "Training NF1:  95%|█████████████████ | 9466/10001 [14:03:21<13:56,  1.56s/batch]Batch 9600/10001 Done, mean position loss: 20.5547118806839\n",
      "Training NF1:  95%|█████████████████ | 9467/10001 [14:03:22<16:44,  1.88s/batch]Batch 9500/10001 Done, mean position loss: 20.546910235881803\n",
      "Training NF1:  95%|█████████████████▏| 9541/10001 [14:03:22<14:12,  1.85s/batch]Batch 9300/10001 Done, mean position loss: 20.65707416296005\n",
      "Training NF1:  95%|█████████████████ | 9483/10001 [14:03:28<14:58,  1.74s/batch]Batch 9600/10001 Done, mean position loss: 20.4424528837204\n",
      "Training NF1:  94%|████████████████▉ | 9413/10001 [14:03:31<16:46,  1.71s/batch]Batch 9500/10001 Done, mean position loss: 20.686557950973512\n",
      "Training NF1:  95%|█████████████████ | 9496/10001 [14:03:54<11:22,  1.35s/batch]Batch 9500/10001 Done, mean position loss: 20.5017892241478\n",
      "Training NF1:  95%|█████████████████ | 9503/10001 [14:03:57<12:55,  1.56s/batch]Batch 9400/10001 Done, mean position loss: 20.923757503032682\n",
      "Training NF1:  95%|█████████████████ | 9451/10001 [14:03:59<13:59,  1.53s/batch]Batch 9500/10001 Done, mean position loss: 20.636736669540404\n",
      "Training NF1:  95%|█████████████████ | 9488/10001 [14:04:00<16:53,  1.97s/batch]Batch 9500/10001 Done, mean position loss: 20.99674044847488\n",
      "Training NF1:  95%|█████████████████▏| 9533/10001 [14:04:02<11:34,  1.48s/batch]Batch 9500/10001 Done, mean position loss: 20.393687195777893\n",
      "Training NF1:  95%|█████████████████▏| 9528/10001 [14:04:12<15:15,  1.94s/batch]Batch 9500/10001 Done, mean position loss: 20.627787618637086\n",
      "Training NF1:  96%|█████████████████▎| 9629/10001 [14:04:13<11:14,  1.81s/batch]Batch 9500/10001 Done, mean position loss: 20.94855022907257\n",
      "Training NF1:  95%|█████████████████▏| 9516/10001 [14:04:20<15:15,  1.89s/batch]Batch 9600/10001 Done, mean position loss: 20.961610205173493\n",
      "Training NF1:  96%|█████████████████▎| 9634/10001 [14:04:21<10:25,  1.70s/batch]Batch 9500/10001 Done, mean position loss: 20.662710356712342\n",
      "Training NF1:  95%|█████████████████ | 9466/10001 [14:04:22<15:37,  1.75s/batch]Batch 9500/10001 Done, mean position loss: 20.369291913509368\n",
      "Training NF1:  93%|████████████████▊ | 9334/10001 [14:04:25<19:46,  1.78s/batch]Batch 9500/10001 Done, mean position loss: 20.769359164237976\n",
      "Training NF1:  95%|█████████████████ | 9509/10001 [14:04:25<13:18,  1.62s/batch]Batch 9500/10001 Done, mean position loss: 20.682059412002566\n",
      "Training NF1:  95%|█████████████████▏| 9549/10001 [14:04:33<13:34,  1.80s/batch]Batch 9600/10001 Done, mean position loss: 20.45602062702179\n",
      "Training NF1:  95%|█████████████████ | 9513/10001 [14:04:46<15:54,  1.96s/batch]Batch 9500/10001 Done, mean position loss: 20.351860065460208\n",
      "Training NF1:  95%|█████████████████ | 9475/10001 [14:04:47<14:29,  1.65s/batch]Batch 9600/10001 Done, mean position loss: 20.688152747154234\n",
      "Training NF1:  95%|█████████████████ | 9484/10001 [14:04:48<15:10,  1.76s/batch]Batch 9600/10001 Done, mean position loss: 20.476340301036835\n",
      "Training NF1:  97%|█████████████████▍| 9677/10001 [14:05:02<11:05,  2.05s/batch]Batch 9600/10001 Done, mean position loss: 20.943335642814638\n",
      "Training NF1:  95%|█████████████████ | 9514/10001 [14:05:06<12:56,  1.59s/batch]Batch 9600/10001 Done, mean position loss: 20.418694400787352\n",
      "Training NF1:  95%|█████████████████▏| 9525/10001 [14:05:07<13:49,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.415669410228727\n",
      "Training NF1:  96%|█████████████████▎| 9600/10001 [14:05:09<12:07,  1.81s/batch]Batch 9600/10001 Done, mean position loss: 20.814326355457304\n",
      "Training NF1:  94%|████████████████▉ | 9399/10001 [14:05:11<14:48,  1.48s/batch]Batch 9600/10001 Done, mean position loss: 20.563386862277984\n",
      "Training NF1:  96%|█████████████████▏| 9584/10001 [14:05:12<12:09,  1.75s/batch]Batch 9600/10001 Done, mean position loss: 20.608015069961546\n",
      "Training NF1:  96%|█████████████████▏| 9571/10001 [14:05:13<12:34,  1.75s/batch]Batch 9600/10001 Done, mean position loss: 20.71785444021225\n",
      "Training NF1:  95%|█████████████████▏| 9541/10001 [14:05:14<13:23,  1.75s/batch]Batch 9400/10001 Done, mean position loss: 20.55883033037186\n",
      "Training NF1:  95%|█████████████████ | 9496/10001 [14:05:19<16:45,  1.99s/batch]Batch 9500/10001 Done, mean position loss: 21.579600188732144\n",
      "Training NF1:  96%|█████████████████▎| 9616/10001 [14:05:27<10:34,  1.65s/batch]Batch 9500/10001 Done, mean position loss: 20.908900678157806\n",
      "Training NF1:  96%|█████████████████▏| 9555/10001 [14:05:34<12:39,  1.70s/batch]Batch 9500/10001 Done, mean position loss: 20.639435288906096\n",
      "Training NF1:  95%|█████████████████ | 9511/10001 [14:05:45<14:23,  1.76s/batch]Batch 9600/10001 Done, mean position loss: 20.59477016925812\n",
      "Training NF1:  96%|█████████████████▎| 9621/10001 [14:05:45<11:01,  1.74s/batch]Batch 9700/10001 Done, mean position loss: 20.445031776428223\n",
      "Training NF1:  96%|█████████████████▏| 9566/10001 [14:06:04<12:38,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 21.045630457401273\n",
      "Training NF1:  96%|█████████████████▎| 9633/10001 [14:06:07<10:23,  1.69s/batch]Batch 9600/10001 Done, mean position loss: 20.85889052867889\n",
      "Training NF1:  97%|█████████████████▍| 9667/10001 [14:06:09<08:59,  1.62s/batch]Batch 9600/10001 Done, mean position loss: 20.48717108726501\n",
      "Training NF1:  96%|█████████████████▎| 9635/10001 [14:06:10<10:06,  1.66s/batch]Batch 9600/10001 Done, mean position loss: 20.81240743160248\n",
      "Training NF1:  95%|█████████████████ | 9480/10001 [14:06:14<14:48,  1.71s/batch]Batch 9500/10001 Done, mean position loss: 20.322587101459504\n",
      "Training NF1:  97%|█████████████████▍| 9671/10001 [14:06:16<08:03,  1.46s/batch]Batch 9600/10001 Done, mean position loss: 20.541610648632048\n",
      "Training NF1:  96%|█████████████████▏| 9563/10001 [14:06:16<13:16,  1.82s/batch]Batch 9700/10001 Done, mean position loss: 20.550800635814667\n",
      "Training NF1:  96%|█████████████████▏| 9557/10001 [14:06:19<11:43,  1.59s/batch]Batch 9600/10001 Done, mean position loss: 20.649016828536986\n",
      "Training NF1:  96%|█████████████████▏| 9569/10001 [14:06:24<13:07,  1.82s/batch]Batch 9700/10001 Done, mean position loss: 20.450806596279143\n",
      "Training NF1:  96%|█████████████████▎| 9644/10001 [14:06:27<09:19,  1.57s/batch]Batch 9600/10001 Done, mean position loss: 20.691369795799258\n",
      "Training NF1:  96%|█████████████████▎| 9650/10001 [14:06:28<08:49,  1.51s/batch]Batch 9400/10001 Done, mean position loss: 20.675328423976897\n",
      "Training NF1:  96%|█████████████████▏| 9577/10001 [14:06:53<13:07,  1.86s/batch]Batch 9600/10001 Done, mean position loss: 20.492648968696592\n",
      "Training NF1:  96%|█████████████████▎| 9601/10001 [14:06:55<12:53,  1.93s/batch]Batch 9600/10001 Done, mean position loss: 20.646435875892635\n",
      "Training NF1:  96%|█████████████████▎| 9588/10001 [14:06:54<12:14,  1.78s/batch]Batch 9600/10001 Done, mean position loss: 21.00746339082718\n",
      "Training NF1:  96%|█████████████████▎| 9598/10001 [14:06:55<11:26,  1.70s/batch]Batch 9500/10001 Done, mean position loss: 20.890308299064635\n",
      "Training NF1:  97%|█████████████████▍| 9663/10001 [14:07:00<08:23,  1.49s/batch]Batch 9600/10001 Done, mean position loss: 20.39475413560867\n",
      "Training NF1:  96%|█████████████████▎| 9594/10001 [14:07:06<12:20,  1.82s/batch]Batch 9600/10001 Done, mean position loss: 20.933978707790374\n",
      "Training NF1:  96%|█████████████████▎| 9631/10001 [14:07:07<10:32,  1.71s/batch]Batch 9700/10001 Done, mean position loss: 20.942269167900086\n",
      "Training NF1:  96%|█████████████████▏| 9557/10001 [14:07:09<12:34,  1.70s/batch]Batch 9600/10001 Done, mean position loss: 20.611412942409515\n",
      "Training NF1:  97%|█████████████████▍| 9674/10001 [14:07:17<09:04,  1.66s/batch]Batch 9600/10001 Done, mean position loss: 20.68316967010498\n",
      "Training NF1:  97%|█████████████████▍| 9673/10001 [14:07:17<08:26,  1.54s/batch]Batch 9600/10001 Done, mean position loss: 20.769500520229336\n",
      "Training NF1:  96%|█████████████████▎| 9609/10001 [14:07:18<09:56,  1.52s/batch]Batch 9600/10001 Done, mean position loss: 20.365419342517853\n",
      "Training NF1:  96%|█████████████████▏| 9580/10001 [14:07:24<11:34,  1.65s/batch]Batch 9600/10001 Done, mean position loss: 20.659900872707368\n",
      "Training NF1:  96%|█████████████████▎| 9607/10001 [14:07:36<14:01,  2.14s/batch]Batch 9700/10001 Done, mean position loss: 20.474005663394927\n",
      "Training NF1:  95%|█████████████████ | 9480/10001 [14:07:39<14:12,  1.64s/batch]Batch 9600/10001 Done, mean position loss: 20.352328069210053\n",
      "Training NF1:  97%|█████████████████▍| 9687/10001 [14:07:42<09:14,  1.77s/batch]Batch 9700/10001 Done, mean position loss: 20.475051956176756\n",
      "Training NF1:  97%|█████████████████▍| 9687/10001 [14:07:42<09:22,  1.79s/batch]Batch 9700/10001 Done, mean position loss: 20.696420545578004\n",
      "Training NF1:  96%|█████████████████▎| 9637/10001 [14:07:58<10:37,  1.75s/batch]Batch 9700/10001 Done, mean position loss: 20.421850700378418\n",
      "Training NF1:  98%|█████████████████▌| 9780/10001 [14:08:00<08:01,  2.18s/batch]Batch 9700/10001 Done, mean position loss: 20.918274614810944\n",
      "Training NF1:  97%|█████████████████▍| 9654/10001 [14:08:00<09:47,  1.69s/batch]Batch 9600/10001 Done, mean position loss: 20.402806646823883\n",
      "Training NF1:  96%|█████████████████▎| 9626/10001 [14:08:03<11:51,  1.90s/batch]Batch 9700/10001 Done, mean position loss: 20.807647695541384\n",
      "Training NF1:  97%|█████████████████▍| 9703/10001 [14:08:06<07:41,  1.55s/batch]Batch 9700/10001 Done, mean position loss: 20.563506805896758\n",
      "Training NF1:  97%|█████████████████▍| 9719/10001 [14:08:06<08:29,  1.81s/batch]Batch 9700/10001 Done, mean position loss: 20.597928783893586\n",
      "Training NF1:  98%|█████████████████▌| 9760/10001 [14:08:08<07:23,  1.84s/batch]Batch 9700/10001 Done, mean position loss: 20.716388664245606\n",
      "Training NF1:  98%|█████████████████▌| 9763/10001 [14:08:14<06:56,  1.75s/batch]Batch 9500/10001 Done, mean position loss: 20.557773764133454\n",
      "Training NF1:  97%|█████████████████▍| 9722/10001 [14:08:15<07:53,  1.70s/batch]Batch 9600/10001 Done, mean position loss: 21.59134163618088\n",
      "Training NF1:  98%|█████████████████▌| 9767/10001 [14:08:19<05:30,  1.41s/batch]Batch 9600/10001 Done, mean position loss: 20.922017362117767\n",
      "Training NF1:  97%|█████████████████▍| 9712/10001 [14:08:27<08:36,  1.79s/batch]Batch 9600/10001 Done, mean position loss: 20.635271317958832\n",
      "Training NF1:  97%|█████████████████▌| 9728/10001 [14:08:33<07:41,  1.69s/batch]Batch 9700/10001 Done, mean position loss: 20.57079555273056\n",
      "Training NF1:  97%|█████████████████▌| 9731/10001 [14:08:37<07:14,  1.61s/batch]Batch 9800/10001 Done, mean position loss: 20.445867898464204\n",
      "Training NF1:  98%|█████████████████▌| 9766/10001 [14:08:58<06:22,  1.63s/batch]Batch 9600/10001 Done, mean position loss: 21.021824338436126\n",
      "Training NF1:  96%|█████████████████▏| 9573/10001 [14:09:02<11:05,  1.56s/batch]Batch 9700/10001 Done, mean position loss: 20.885778827667238\n",
      "Training NF1:  98%|█████████████████▋| 9799/10001 [14:09:05<05:23,  1.60s/batch]Batch 9700/10001 Done, mean position loss: 20.805260334014893\n",
      "Training NF1:  97%|█████████████████▍| 9675/10001 [14:09:08<09:00,  1.66s/batch]Batch 9600/10001 Done, mean position loss: 20.32672153234482\n",
      "Training NF1:  96%|█████████████████▎| 9639/10001 [14:09:08<10:33,  1.75s/batch]Batch 9800/10001 Done, mean position loss: 20.537125658988952\n",
      "Training NF1:  97%|█████████████████▍| 9667/10001 [14:09:09<08:43,  1.57s/batch]Batch 9700/10001 Done, mean position loss: 20.488699209690093\n",
      "Training NF1:  98%|█████████████████▌| 9751/10001 [14:09:09<06:54,  1.66s/batch]Batch 9700/10001 Done, mean position loss: 20.548618969917296\n",
      "Training NF1:  97%|█████████████████▍| 9669/10001 [14:09:13<09:08,  1.65s/batch]Batch 9700/10001 Done, mean position loss: 20.64628171443939\n",
      "Training NF1:  97%|█████████████████▍| 9710/10001 [14:09:19<08:36,  1.78s/batch]Batch 9800/10001 Done, mean position loss: 20.44970346212387\n",
      "Training NF1:  97%|█████████████████▍| 9682/10001 [14:09:23<09:46,  1.84s/batch]Batch 9700/10001 Done, mean position loss: 20.6942462682724\n",
      "Training NF1:  97%|█████████████████▍| 9708/10001 [14:09:25<08:25,  1.73s/batch]Batch 9500/10001 Done, mean position loss: 20.662118830680846\n",
      "Training NF1:  97%|█████████████████▍| 9722/10001 [14:09:48<08:50,  1.90s/batch]Batch 9700/10001 Done, mean position loss: 20.504551594257357\n",
      "Training NF1:  97%|█████████████████▍| 9700/10001 [14:09:50<08:43,  1.74s/batch]Batch 9700/10001 Done, mean position loss: 20.982751150131225\n",
      "Training NF1:  97%|█████████████████▍| 9717/10001 [14:09:51<10:15,  2.17s/batch]Batch 9700/10001 Done, mean position loss: 20.62864045381546\n",
      "Training NF1:  96%|█████████████████▏| 9558/10001 [14:09:53<11:51,  1.61s/batch]Batch 9600/10001 Done, mean position loss: 20.901093928813935\n",
      "Training NF1:  97%|█████████████████▍| 9658/10001 [14:09:54<09:28,  1.66s/batch]Batch 9700/10001 Done, mean position loss: 20.393775310516357\n",
      "Training NF1:  98%|█████████████████▌| 9778/10001 [14:09:57<06:19,  1.70s/batch]Batch 9700/10001 Done, mean position loss: 20.942521991729734\n",
      "Training NF1:  98%|█████████████████▋| 9823/10001 [14:09:57<05:25,  1.83s/batch]Batch 9800/10001 Done, mean position loss: 20.942146179676055\n",
      "Training NF1:  97%|█████████████████▍| 9708/10001 [14:10:05<08:35,  1.76s/batch]Batch 9700/10001 Done, mean position loss: 20.61080804824829\n",
      "Training NF1:  97%|█████████████████▍| 9712/10001 [14:10:12<09:39,  2.00s/batch]Batch 9700/10001 Done, mean position loss: 20.750410962104798\n",
      "Training NF1:  97%|█████████████████▍| 9714/10001 [14:10:15<08:31,  1.78s/batch]Batch 9700/10001 Done, mean position loss: 20.672874331474304\n",
      "Training NF1:  98%|█████████████████▋| 9812/10001 [14:10:18<05:58,  1.90s/batch]Batch 9700/10001 Done, mean position loss: 20.362962946891784\n",
      "Training NF1:  98%|█████████████████▌| 9790/10001 [14:10:21<07:10,  2.04s/batch]Batch 9700/10001 Done, mean position loss: 20.65901862859726\n",
      "Training NF1:  97%|█████████████████▌| 9739/10001 [14:10:31<07:51,  1.80s/batch]Batch 9800/10001 Done, mean position loss: 20.467218935489655\n",
      "Training NF1:  99%|█████████████████▊| 9865/10001 [14:10:34<04:02,  1.78s/batch]Batch 9800/10001 Done, mean position loss: 20.46987074136734\n",
      "Training NF1:  97%|█████████████████▍| 9717/10001 [14:10:35<09:09,  1.94s/batch]Batch 9700/10001 Done, mean position loss: 20.33541730642319\n",
      "Training NF1:  97%|█████████████████▍| 9685/10001 [14:10:42<11:20,  2.15s/batch]Batch 9800/10001 Done, mean position loss: 20.697568743228913\n",
      "Training NF1:  98%|█████████████████▌| 9752/10001 [14:10:55<07:34,  1.83s/batch]Batch 9800/10001 Done, mean position loss: 20.419417731761932\n",
      "Training NF1:  97%|█████████████████▍| 9689/10001 [14:10:58<08:33,  1.65s/batch]Batch 9700/10001 Done, mean position loss: 20.416066102981567\n",
      "Training NF1:  98%|█████████████████▌| 9762/10001 [14:10:59<06:47,  1.70s/batch]Batch 9800/10001 Done, mean position loss: 20.927058913707732\n",
      "Training NF1:  97%|█████████████████▌| 9743/10001 [14:11:05<07:37,  1.78s/batch]Batch 9800/10001 Done, mean position loss: 20.59626697540283\n",
      "Training NF1:  97%|█████████████████▌| 9730/10001 [14:11:05<07:56,  1.76s/batch]Batch 9800/10001 Done, mean position loss: 20.790756702423096\n",
      "Training NF1:  99%|█████████████████▊| 9868/10001 [14:11:07<03:57,  1.79s/batch]Batch 9800/10001 Done, mean position loss: 20.56350771903992\n",
      "Training NF1:  97%|█████████████████▍| 9698/10001 [14:11:11<08:42,  1.72s/batch]Batch 9800/10001 Done, mean position loss: 20.716642479896546\n",
      "Training NF1:  97%|█████████████████▌| 9746/10001 [14:11:11<07:43,  1.82s/batch]Batch 9600/10001 Done, mean position loss: 20.545251696109773\n",
      "Training NF1:  98%|█████████████████▌| 9766/10001 [14:11:12<06:54,  1.76s/batch]Batch 9700/10001 Done, mean position loss: 21.611402056217194\n",
      "Training NF1:  98%|█████████████████▋| 9827/10001 [14:11:16<04:56,  1.70s/batch]Batch 9700/10001 Done, mean position loss: 20.90236442089081\n",
      "Training NF1:  99%|█████████████████▊| 9891/10001 [14:11:20<03:04,  1.68s/batch]Batch 9700/10001 Done, mean position loss: 20.65008269071579\n",
      "Training NF1:  97%|█████████████████▍| 9709/10001 [14:11:27<10:41,  2.20s/batch]Batch 9800/10001 Done, mean position loss: 20.594941880702972\n",
      "Training NF1:  97%|█████████████████▍| 9682/10001 [14:11:36<09:43,  1.83s/batch]Batch 9900/10001 Done, mean position loss: 20.45329185009003\n",
      "Training NF1:  98%|█████████████████▌| 9770/10001 [14:11:56<06:28,  1.68s/batch]Batch 9700/10001 Done, mean position loss: 21.04130961179733\n",
      "Training NF1:  98%|█████████████████▋| 9831/10001 [14:12:00<04:50,  1.71s/batch]Batch 9800/10001 Done, mean position loss: 20.886139965057374\n",
      "Training NF1:  98%|█████████████████▋| 9845/10001 [14:12:01<04:02,  1.56s/batch]Batch 9800/10001 Done, mean position loss: 20.800159192085268\n",
      "Training NF1:  98%|█████████████████▌| 9774/10001 [14:12:02<06:06,  1.62s/batch]Batch 9900/10001 Done, mean position loss: 20.54458819627762\n",
      "Training NF1:  98%|█████████████████▋| 9806/10001 [14:12:08<04:45,  1.46s/batch]Batch 9800/10001 Done, mean position loss: 20.48987504720688\n",
      "Training NF1:  99%|█████████████████▊| 9906/10001 [14:12:11<02:32,  1.61s/batch]Batch 9800/10001 Done, mean position loss: 20.54018573999405\n",
      "Training NF1:  99%|█████████████████▊| 9922/10001 [14:12:14<02:16,  1.73s/batch]Batch 9700/10001 Done, mean position loss: 20.319036016464235\n",
      "Training NF1:  97%|█████████████████▍| 9713/10001 [14:12:17<08:20,  1.74s/batch]Batch 9800/10001 Done, mean position loss: 20.697096991539\n",
      "Training NF1:  98%|█████████████████▌| 9765/10001 [14:12:17<06:57,  1.77s/batch]Batch 9900/10001 Done, mean position loss: 20.451164040565487\n",
      "Training NF1:  98%|█████████████████▋| 9847/10001 [14:12:18<05:12,  2.03s/batch]Batch 9800/10001 Done, mean position loss: 20.636885635852813\n",
      "Training NF1:  98%|█████████████████▌| 9786/10001 [14:12:24<05:38,  1.58s/batch]Batch 9600/10001 Done, mean position loss: 20.66590458869934\n",
      "Training NF1:  99%|█████████████████▋| 9857/10001 [14:12:48<03:55,  1.63s/batch]Batch 9800/10001 Done, mean position loss: 20.397860288619995\n",
      "Training NF1:  98%|█████████████████▌| 9783/10001 [14:12:50<06:30,  1.79s/batch]Batch 9800/10001 Done, mean position loss: 20.48869521856308\n",
      "Training NF1:  98%|█████████████████▌| 9778/10001 [14:12:51<05:50,  1.57s/batch]Batch 9800/10001 Done, mean position loss: 20.632712597846986\n",
      "Training NF1:  98%|█████████████████▋| 9823/10001 [14:12:53<04:51,  1.64s/batch]Batch 9700/10001 Done, mean position loss: 20.92349745512009\n",
      "Training NF1:  98%|█████████████████▋| 9800/10001 [14:12:55<05:28,  1.63s/batch]Batch 9800/10001 Done, mean position loss: 20.957165446281433\n",
      "Training NF1:  99%|█████████████████▊| 9877/10001 [14:12:56<03:51,  1.86s/batch]Batch 9900/10001 Done, mean position loss: 20.940492496490478\n",
      "Training NF1:  97%|█████████████████▍| 9703/10001 [14:12:56<07:59,  1.61s/batch]Batch 9800/10001 Done, mean position loss: 21.004748492240907\n",
      "Training NF1:  98%|█████████████████▋| 9828/10001 [14:12:58<04:58,  1.73s/batch]Batch 9800/10001 Done, mean position loss: 20.61444350719452\n",
      "Training NF1:  98%|█████████████████▋| 9810/10001 [14:13:14<05:47,  1.82s/batch]Batch 9800/10001 Done, mean position loss: 20.67949876308441\n",
      "Training NF1:  99%|█████████████████▊| 9880/10001 [14:13:14<03:52,  1.92s/batch]Batch 9800/10001 Done, mean position loss: 20.74186945915222\n",
      "Training NF1:  99%|█████████████████▊| 9876/10001 [14:13:19<04:12,  2.02s/batch]Batch 9800/10001 Done, mean position loss: 20.674386732578277\n",
      "Training NF1:  99%|█████████████████▊| 9879/10001 [14:13:24<03:23,  1.67s/batch]Batch 9800/10001 Done, mean position loss: 20.356417474746703\n",
      "Training NF1:  98%|█████████████████▋| 9804/10001 [14:13:29<04:53,  1.49s/batch]Batch 9900/10001 Done, mean position loss: 20.461633512973783\n",
      "Training NF1:  98%|█████████████████▌| 9785/10001 [14:13:30<05:54,  1.64s/batch]Batch 9800/10001 Done, mean position loss: 20.338644185066222\n",
      "Training NF1: 100%|█████████████████▉| 9964/10001 [14:13:31<01:08,  1.86s/batch]Batch 9900/10001 Done, mean position loss: 20.463331253528594\n",
      "Training NF1:  99%|█████████████████▉| 9948/10001 [14:13:36<01:30,  1.71s/batch]Batch 9900/10001 Done, mean position loss: 20.70750960350037\n",
      "Training NF1:  98%|█████████████████▋| 9814/10001 [14:13:54<05:59,  1.92s/batch]Batch 9900/10001 Done, mean position loss: 20.95357459783554\n",
      "Training NF1:  98%|█████████████████▋| 9836/10001 [14:13:54<05:34,  2.02s/batch]Batch 9900/10001 Done, mean position loss: 20.417930862903596\n",
      "Training NF1:  99%|█████████████████▊| 9896/10001 [14:13:58<02:41,  1.53s/batch]Batch 9800/10001 Done, mean position loss: 20.39013067007065\n",
      "Training NF1:  98%|█████████████████▋| 9841/10001 [14:14:04<04:35,  1.72s/batch]Batch 9900/10001 Done, mean position loss: 20.80637851238251\n",
      "Training NF1:  99%|█████████████████▊| 9907/10001 [14:14:05<02:35,  1.66s/batch]Batch 9900/10001 Done, mean position loss: 20.541818971633912\n",
      "Batch 9900/10001 Done, mean position loss: 20.705101916790007\n",
      "Training NF1:  98%|█████████████████▌| 9762/10001 [14:14:06<06:53,  1.73s/batch]Batch 9900/10001 Done, mean position loss: 20.593009722232818\n",
      "Training NF1:  98%|█████████████████▋| 9840/10001 [14:14:10<05:32,  2.06s/batch]Batch 9800/10001 Done, mean position loss: 21.623423249721526\n",
      "Training NF1:  99%|█████████████████▊| 9907/10001 [14:14:16<02:49,  1.80s/batch]Batch 9800/10001 Done, mean position loss: 20.943328115940094\n",
      "Training NF1:  98%|█████████████████▋| 9849/10001 [14:14:17<04:03,  1.60s/batch]Batch 9800/10001 Done, mean position loss: 20.660416886806487\n",
      "Training NF1:  97%|█████████████████▍| 9665/10001 [14:14:18<10:13,  1.82s/batch]Batch 9700/10001 Done, mean position loss: 20.5381356883049\n",
      "Training NF1:  99%|█████████████████▊| 9912/10001 [14:14:26<02:50,  1.91s/batch]Batch 9900/10001 Done, mean position loss: 20.584958651065826\n",
      "Training NF1:  99%|█████████████████▉| 9937/10001 [14:14:36<01:53,  1.77s/batch]Batch 10000/10001 Done, mean position loss: 20.444803581237792\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:14:36<00:00,  5.13s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9931/10001 [14:14:58<01:51,  1.59s/batch]Batch 9900/10001 Done, mean position loss: 20.88341957092285\n",
      "Training NF1:  99%|█████████████████▉| 9946/10001 [14:14:59<01:41,  1.84s/batch]Batch 9800/10001 Done, mean position loss: 21.00979643344879\n",
      "Training NF1:  98%|█████████████████▋| 9825/10001 [14:15:05<06:27,  2.20s/batch]Batch 9900/10001 Done, mean position loss: 20.798893783092495\n",
      "Training NF1:  99%|█████████████████▊| 9903/10001 [14:15:08<02:55,  1.79s/batch]Batch 10000/10001 Done, mean position loss: 20.55875894784927\n",
      "Training NF1:  99%|█████████████████▉| 9934/10001 [14:15:08<01:49,  1.63s/batch]Batch 9900/10001 Done, mean position loss: 20.527938907146456\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:15:09<00:00,  5.13s/batch]\n",
      "Done...\n",
      "Training NF1:  98%|█████████████████▋| 9832/10001 [14:15:10<04:54,  1.74s/batch]Batch 9900/10001 Done, mean position loss: 20.505556905269625\n",
      "Training NF1:  98%|█████████████████▋| 9832/10001 [14:15:17<05:19,  1.89s/batch]Batch 9900/10001 Done, mean position loss: 20.709880027770996\n",
      "Training NF1:  99%|█████████████████▉| 9939/10001 [14:15:18<01:54,  1.85s/batch]Batch 9800/10001 Done, mean position loss: 20.299612836837767\n",
      "Training NF1:  98%|█████████████████▋| 9801/10001 [14:15:18<06:20,  1.90s/batch]Batch 9900/10001 Done, mean position loss: 20.652706768512726\n",
      "Training NF1:  99%|█████████████████▊| 9909/10001 [14:15:18<02:25,  1.58s/batch]Batch 10000/10001 Done, mean position loss: 20.44627473831177\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:15:18<00:00,  5.13s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9889/10001 [14:15:28<02:47,  1.50s/batch]Batch 9700/10001 Done, mean position loss: 20.66542708158493\n",
      "Training NF1:  98%|█████████████████▋| 9832/10001 [14:15:49<05:29,  1.95s/batch]Batch 9900/10001 Done, mean position loss: 20.391826312541962\n",
      "Training NF1:  99%|█████████████████▊| 9901/10001 [14:15:49<02:54,  1.74s/batch]Batch 9900/10001 Done, mean position loss: 20.953401758670807\n",
      "Training NF1:  99%|█████████████████▊| 9894/10001 [14:15:50<02:44,  1.54s/batch]Batch 9900/10001 Done, mean position loss: 20.63101541042328\n",
      "Training NF1:  99%|█████████████████▊| 9902/10001 [14:15:50<02:46,  1.68s/batch]Batch 9900/10001 Done, mean position loss: 20.50502555131912\n",
      "Training NF1:  99%|█████████████████▊| 9887/10001 [14:15:52<03:16,  1.72s/batch]Batch 9800/10001 Done, mean position loss: 20.89212532520294\n",
      "Training NF1:  99%|█████████████████▋| 9855/10001 [14:15:53<04:42,  1.93s/batch]Batch 10000/10001 Done, mean position loss: 20.920385410785677\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:15:53<00:00,  5.13s/batch]\n",
      "Done...\n",
      "Training NF1:  98%|█████████████████▋| 9805/10001 [14:15:58<04:37,  1.42s/batch]Batch 9900/10001 Done, mean position loss: 20.996599912643433\n",
      "Training NF1:  99%|█████████████████▋| 9859/10001 [14:16:02<03:31,  1.49s/batch]Batch 9900/10001 Done, mean position loss: 20.613344395160674\n",
      "Training NF1: 100%|█████████████████▉| 9976/10001 [14:16:09<00:36,  1.48s/batch]Batch 9900/10001 Done, mean position loss: 20.67138422727585\n",
      "Training NF1:  99%|█████████████████▊| 9901/10001 [14:16:09<02:49,  1.69s/batch]Batch 9900/10001 Done, mean position loss: 20.735925359725954\n",
      "Training NF1:  99%|█████████████████▊| 9911/10001 [14:16:14<02:06,  1.40s/batch]Batch 9900/10001 Done, mean position loss: 20.66116488456726\n",
      "Training NF1:  99%|█████████████████▊| 9910/10001 [14:16:24<02:34,  1.69s/batch]Batch 9900/10001 Done, mean position loss: 20.355923798084262\n",
      "Training NF1:  99%|█████████████████▊| 9911/10001 [14:16:25<02:26,  1.63s/batch]Batch 10000/10001 Done, mean position loss: 20.44683053970337\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:25<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9926/10001 [14:16:26<01:58,  1.59s/batch]Batch 10000/10001 Done, mean position loss: 20.475214557647703\n",
      "Training NF1:  99%|█████████████████▊| 9925/10001 [14:16:27<02:01,  1.60s/batch]Batch 9900/10001 Done, mean position loss: 20.343505425453188\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:27<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9958/10001 [14:16:40<01:04,  1.50s/batch]Batch 10000/10001 Done, mean position loss: 20.68302679538727\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:40<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  98%|█████████████████▋| 9837/10001 [14:16:50<03:57,  1.45s/batch]Batch 10000/10001 Done, mean position loss: 20.937972543239596\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:50<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▉| 9935/10001 [14:16:53<01:36,  1.46s/batch]Batch 9900/10001 Done, mean position loss: 20.412336866855618\n",
      "Training NF1:  99%|█████████████████▊| 9894/10001 [14:16:56<02:53,  1.62s/batch]Batch 10000/10001 Done, mean position loss: 20.419330677986146\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:56<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9968/10001 [14:16:57<00:58,  1.77s/batch]Batch 10000/10001 Done, mean position loss: 20.792917523384094\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:57<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9877/10001 [14:16:57<03:11,  1.54s/batch]Batch 10000/10001 Done, mean position loss: 20.61458949804306\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:16:58<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▉| 9934/10001 [14:17:00<01:42,  1.53s/batch]Batch 10000/10001 Done, mean position loss: 20.71806736469269\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:01<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9967/10001 [14:17:03<00:47,  1.40s/batch]Batch 10000/10001 Done, mean position loss: 20.53686443567276\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:03<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9995/10001 [14:17:04<00:05,  1.04batch/s]Batch 9900/10001 Done, mean position loss: 20.907019164562225\n",
      "Training NF1: 100%|█████████████████▉| 9953/10001 [14:17:06<00:55,  1.15s/batch]Batch 9900/10001 Done, mean position loss: 21.577272131443024\n",
      "Training NF1:  99%|█████████████████▊| 9903/10001 [14:17:07<01:55,  1.18s/batch]Batch 9800/10001 Done, mean position loss: 20.53810681819916\n",
      "Training NF1:  99%|█████████████████▉| 9945/10001 [14:17:07<01:17,  1.38s/batch]Batch 9900/10001 Done, mean position loss: 20.645097577571867\n",
      "Training NF1:  99%|█████████████████▊| 9930/10001 [14:17:13<01:57,  1.66s/batch]Batch 10000/10001 Done, mean position loss: 20.59990718841553\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:13<00:00,  5.14s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▉| 9941/10001 [14:17:25<01:12,  1.20s/batch]Batch 9900/10001 Done, mean position loss: 21.07575418949127\n",
      "Training NF1:  98%|█████████████████▋| 9824/10001 [14:17:38<04:05,  1.38s/batch]Batch 10000/10001 Done, mean position loss: 20.887054405212403\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:37<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9954/10001 [14:17:38<00:48,  1.03s/batch]Batch 10000/10001 Done, mean position loss: 20.797696313858033\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:38<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9976/10001 [14:17:40<00:28,  1.14s/batch]Batch 10000/10001 Done, mean position loss: 20.54498699426651\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:40<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9956/10001 [14:17:40<00:48,  1.08s/batch]Batch 10000/10001 Done, mean position loss: 20.507722635269165\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:42<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9979/10001 [14:17:42<00:23,  1.08s/batch]Batch 10000/10001 Done, mean position loss: 20.70610221862793\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:43<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9986/10001 [14:17:45<00:16,  1.11s/batch]Batch 10000/10001 Done, mean position loss: 20.658633213043213\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:17:46<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9965/10001 [14:17:47<00:41,  1.15s/batch]Batch 9900/10001 Done, mean position loss: 20.31690419435501\n",
      "Training NF1:  99%|█████████████████▉| 9940/10001 [14:17:51<00:57,  1.06batch/s]Batch 9800/10001 Done, mean position loss: 20.669379005432127\n",
      "Training NF1: 100%|█████████████████▉| 9957/10001 [14:17:59<00:46,  1.06s/batch]Batch 10000/10001 Done, mean position loss: 20.947731645107268\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:00<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  98%|█████████████████▋| 9813/10001 [14:18:02<02:29,  1.25batch/s]Batch 10000/10001 Done, mean position loss: 20.62763718366623\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:02<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9960/10001 [14:18:02<00:36,  1.13batch/s]Batch 9900/10001 Done, mean position loss: 20.893835260868073\n",
      "Training NF1: 100%|█████████████████▉| 9953/10001 [14:18:03<00:46,  1.04batch/s]Batch 10000/10001 Done, mean position loss: 21.006014268398282\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:04<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  98%|█████████████████▋| 9850/10001 [14:18:05<02:27,  1.03batch/s]Batch 10000/10001 Done, mean position loss: 20.386967229843137\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:04<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|████████████████▉| 10000/10001 [14:18:06<00:00,  1.11batch/s]Batch 10000/10001 Done, mean position loss: 20.613443925380707\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:06<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9997/10001 [14:18:05<00:03,  1.30batch/s]Batch 10000/10001 Done, mean position loss: 20.48452514410019\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:07<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9959/10001 [14:18:07<00:27,  1.55batch/s]Batch 10000/10001 Done, mean position loss: 20.682249233722686\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:07<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9972/10001 [14:18:09<00:12,  2.38batch/s]Batch 10000/10001 Done, mean position loss: 20.736325175762175\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:09<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9917/10001 [14:18:12<00:34,  2.40batch/s]Batch 10000/10001 Done, mean position loss: 20.661235275268552\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:12<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9925/10001 [14:18:16<00:26,  2.86batch/s]Batch 10000/10001 Done, mean position loss: 20.34030267715454\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:16<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9980/10001 [14:18:20<00:13,  1.58batch/s]Batch 10000/10001 Done, mean position loss: 20.353705487251283\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:19<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9965/10001 [14:18:20<00:23,  1.51batch/s]Batch 10000/10001 Done, mean position loss: 20.41298902511597\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:20<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9994/10001 [14:18:25<00:02,  3.21batch/s]Batch 9900/10001 Done, mean position loss: 20.549555425643923\n",
      "Training NF1:  99%|█████████████████▊| 9907/10001 [14:18:27<00:32,  2.90batch/s]Batch 10000/10001 Done, mean position loss: 20.954069311618802\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:27<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9889/10001 [14:18:31<00:33,  3.29batch/s]Batch 10000/10001 Done, mean position loss: 20.64800320625305\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:30<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9970/10001 [14:18:32<00:10,  3.08batch/s]Batch 10000/10001 Done, mean position loss: 21.056841542720797\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:32<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9998/10001 [14:18:33<00:00,  3.58batch/s]Batch 10000/10001 Done, mean position loss: 21.628286516666414\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:33<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1:  99%|█████████████████▊| 9931/10001 [14:18:34<00:19,  3.58batch/s]Batch 9900/10001 Done, mean position loss: 20.66665793418884\n",
      "Training NF1: 100%|█████████████████▉| 9977/10001 [14:18:34<00:06,  3.54batch/s]Batch 10000/10001 Done, mean position loss: 20.32386224269867\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:34<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9956/10001 [14:18:40<00:10,  4.40batch/s]Batch 10000/10001 Done, mean position loss: 20.897761008739472\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:40<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|█████████████████▉| 9969/10001 [14:18:50<00:07,  4.57batch/s]Batch 10000/10001 Done, mean position loss: 20.565374381542206\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:50<00:00,  5.15s/batch]\n",
      "Done...\n",
      "Training NF1: 100%|████████████████▉| 10000/10001 [14:18:56<00:00,  4.74batch/s]Batch 10000/10001 Done, mean position loss: 20.660273501873014\n",
      "Training NF1: 100%|█████████████████| 10001/10001 [14:18:56<00:00,  5.15s/batch]\n",
      "Done...\n",
      "model00...\n",
      "model01...\n",
      "model02...\n",
      "model03...\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...\n",
      "model13...\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training FF1:   3%|▋                     | 95/3201 [03:08<1:23:39,  1.62s/batch]Batch 100/3201 Done, mean position loss: 29.003079302310944\n",
      "Training FF1:   3%|▋                     | 95/3201 [03:10<1:30:50,  1.75s/batch]Batch 100/3201 Done, mean position loss: 28.72971003770828\n",
      "Training FF1:   3%|▋                     | 96/3201 [03:12<1:31:35,  1.77s/batch]Batch 100/3201 Done, mean position loss: 29.302586452960966\n",
      "Batch 100/3201 Done, mean position loss: 28.92353626489639\n",
      "Training FF1:   3%|▋                     | 98/3201 [03:13<1:31:31,  1.77s/batch]Batch 100/3201 Done, mean position loss: 28.837780385017396\n",
      "Training FF1:   3%|▋                     | 98/3201 [03:14<1:36:10,  1.86s/batch]Batch 100/3201 Done, mean position loss: 28.08867202281952\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:14<1:29:08,  1.72s/batch]Batch 100/3201 Done, mean position loss: 28.774871559143065\n",
      "Training FF1:   3%|▋                     | 99/3201 [03:15<1:32:14,  1.78s/batch]Batch 100/3201 Done, mean position loss: 29.27856061697006\n",
      "Training FF1:   3%|▋                     | 99/3201 [03:15<1:27:54,  1.70s/batch]Batch 100/3201 Done, mean position loss: 29.38145317316055\n",
      "Training FF1:   3%|▋                    | 105/3201 [03:15<1:39:22,  1.93s/batch]Batch 100/3201 Done, mean position loss: 28.835666794776916\n",
      "Training FF1:   3%|▋                     | 98/3201 [03:15<1:30:16,  1.75s/batch]Batch 100/3201 Done, mean position loss: 28.952024726867677\n",
      "Training FF1:   3%|▋                     | 96/3201 [03:16<1:38:10,  1.90s/batch]Batch 100/3201 Done, mean position loss: 29.13451163768768\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:17<1:27:41,  1.70s/batch]Batch 100/3201 Done, mean position loss: 30.084195148944854\n",
      "Training FF1:   3%|▋                    | 102/3201 [03:17<1:22:16,  1.59s/batch]Batch 100/3201 Done, mean position loss: 30.290278952121735\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:17<1:19:44,  1.54s/batch]Batch 100/3201 Done, mean position loss: 29.095264973640447\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:17<1:29:20,  1.73s/batch]Batch 100/3201 Done, mean position loss: 28.8003883433342\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:17<1:29:05,  1.72s/batch]Batch 100/3201 Done, mean position loss: 30.302811670303342\n",
      "Training FF1:   3%|▋                    | 102/3201 [03:17<1:25:42,  1.66s/batch]Batch 100/3201 Done, mean position loss: 29.16502264738083\n",
      "Training FF1:   3%|▋                     | 99/3201 [03:18<1:37:07,  1.88s/batch]Batch 100/3201 Done, mean position loss: 28.252205045223235\n",
      "Training FF1:   3%|▋                     | 98/3201 [03:18<1:25:58,  1.66s/batch]Batch 100/3201 Done, mean position loss: 28.47104560852051\n",
      "Training FF1:   3%|▋                     | 98/3201 [03:18<1:40:32,  1.94s/batch]Batch 100/3201 Done, mean position loss: 29.52625718832016\n",
      "Training FF1:   3%|▋                    | 107/3201 [03:18<1:33:04,  1.81s/batch]Batch 100/3201 Done, mean position loss: 29.031156022548675\n",
      "Training FF1:   3%|▋                    | 102/3201 [03:19<1:18:38,  1.52s/batch]Batch 100/3201 Done, mean position loss: 28.003644361495972\n",
      "Training FF1:   3%|▋                    | 104/3201 [03:19<1:25:10,  1.65s/batch]Batch 100/3201 Done, mean position loss: 28.97322524547577\n",
      "Training FF1:   3%|▋                    | 105/3201 [03:19<1:22:01,  1.59s/batch]Batch 100/3201 Done, mean position loss: 30.070447607040407\n",
      "Training FF1:   3%|▋                     | 99/3201 [03:19<1:40:57,  1.95s/batch]Batch 100/3201 Done, mean position loss: 28.943164036273956\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:19<1:37:46,  1.89s/batch]Batch 100/3201 Done, mean position loss: 28.511041696071622\n",
      "Training FF1:   3%|▋                    | 100/3201 [03:20<1:39:46,  1.93s/batch]Batch 100/3201 Done, mean position loss: 28.23076534032822\n",
      "Training FF1:   3%|▋                     | 99/3201 [03:21<1:45:40,  2.04s/batch]Batch 100/3201 Done, mean position loss: 28.91110435247421\n",
      "Batch 100/3201 Done, mean position loss: 29.160388195514678\n",
      "Training FF1:   3%|▋                    | 106/3201 [03:21<1:29:12,  1.73s/batch]Batch 100/3201 Done, mean position loss: 28.416874153614046\n",
      "Training FF1:   3%|▋                    | 103/3201 [03:22<1:24:23,  1.63s/batch]Batch 100/3201 Done, mean position loss: 28.84792230129242\n",
      "Batch 100/3201 Done, mean position loss: 28.606648907661437\n",
      "Training FF1:   3%|▋                    | 104/3201 [03:22<1:31:03,  1.76s/batch]Batch 100/3201 Done, mean position loss: 28.745261781215667\n",
      "Training FF1:   3%|▋                    | 102/3201 [03:22<1:26:53,  1.68s/batch]Batch 100/3201 Done, mean position loss: 28.73040736913681\n",
      "Training FF1:   3%|▋                    | 107/3201 [03:23<1:17:04,  1.49s/batch]Batch 100/3201 Done, mean position loss: 29.154787254333495\n",
      "Training FF1:   3%|▋                    | 103/3201 [03:24<1:22:24,  1.60s/batch]Batch 100/3201 Done, mean position loss: 29.42460695505142\n",
      "Training FF1:   3%|▋                    | 105/3201 [03:24<1:20:21,  1.56s/batch]Batch 100/3201 Done, mean position loss: 29.697156045436856\n",
      "Training FF1:   3%|▋                    | 109/3201 [03:26<1:22:46,  1.61s/batch]Batch 100/3201 Done, mean position loss: 28.65475856304169\n",
      "Training FF1:   3%|▋                    | 106/3201 [03:27<1:30:17,  1.75s/batch]Batch 100/3201 Done, mean position loss: 29.887067432403565\n",
      "Training FF1:   6%|█▎                   | 199/3201 [06:03<1:30:40,  1.81s/batch]Batch 200/3201 Done, mean position loss: 26.502163033485413\n",
      "Training FF1:   6%|█▏                   | 186/3201 [06:03<1:35:32,  1.90s/batch]Batch 200/3201 Done, mean position loss: 25.506526458263394\n",
      "Training FF1:   6%|█▎                   | 200/3201 [06:05<1:39:22,  1.99s/batch]Batch 200/3201 Done, mean position loss: 25.237302870750426\n",
      "Training FF1:   6%|█▎                   | 201/3201 [06:05<1:42:43,  2.05s/batch]Batch 200/3201 Done, mean position loss: 24.592448120117187\n",
      "Training FF1:   6%|█▎                   | 194/3201 [06:07<1:28:54,  1.77s/batch]Batch 200/3201 Done, mean position loss: 25.371050448417666\n",
      "Training FF1:   6%|█▎                   | 191/3201 [06:08<1:22:38,  1.65s/batch]Batch 200/3201 Done, mean position loss: 25.716727311611173\n",
      "Training FF1:   6%|█▎                   | 204/3201 [06:09<1:55:52,  2.32s/batch]Batch 200/3201 Done, mean position loss: 25.900954017639158\n",
      "Training FF1:   6%|█▎                   | 199/3201 [06:11<1:19:44,  1.59s/batch]Batch 200/3201 Done, mean position loss: 25.15772353887558\n",
      "Training FF1:   6%|█▎                   | 197/3201 [06:14<1:32:33,  1.85s/batch]Batch 200/3201 Done, mean position loss: 26.37231024980545\n",
      "Training FF1:   6%|█▎                   | 200/3201 [06:13<1:29:35,  1.79s/batch]Batch 200/3201 Done, mean position loss: 24.720808811187744\n",
      "Training FF1:   6%|█▎                   | 204/3201 [06:14<1:25:39,  1.72s/batch]Batch 200/3201 Done, mean position loss: 25.252089192867277\n",
      "Training FF1:   6%|█▎                   | 197/3201 [06:13<1:34:59,  1.90s/batch]Batch 200/3201 Done, mean position loss: 25.575556910037996\n",
      "Training FF1:   6%|█▎                   | 203/3201 [06:14<1:19:36,  1.59s/batch]Batch 200/3201 Done, mean position loss: 26.991888768672943\n",
      "Training FF1:   6%|█▎                   | 203/3201 [06:14<1:41:56,  2.04s/batch]Batch 200/3201 Done, mean position loss: 26.31371386051178\n",
      "Training FF1:   6%|█▎                   | 198/3201 [06:15<1:15:33,  1.51s/batch]Batch 200/3201 Done, mean position loss: 26.501759724617003\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:15<1:38:19,  1.97s/batch]Batch 200/3201 Done, mean position loss: 25.849386928081515\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:16<1:26:51,  1.74s/batch]Batch 200/3201 Done, mean position loss: 27.05292086839676\n",
      "Training FF1:   6%|█▎                   | 205/3201 [06:16<1:29:50,  1.80s/batch]Batch 200/3201 Done, mean position loss: 26.169762561321257\n",
      "Training FF1:   6%|█▎                   | 197/3201 [06:16<1:50:24,  2.21s/batch]Batch 200/3201 Done, mean position loss: 25.91477344512939\n",
      "Training FF1:   6%|█▎                   | 200/3201 [06:16<1:15:28,  1.51s/batch]Batch 200/3201 Done, mean position loss: 25.013899743556976\n",
      "Training FF1:   6%|█▎                   | 204/3201 [06:18<1:24:03,  1.68s/batch]Batch 200/3201 Done, mean position loss: 25.43126928091049\n",
      "Training FF1:   6%|█▎                   | 200/3201 [06:18<1:16:57,  1.54s/batch]Batch 200/3201 Done, mean position loss: 25.7067356801033\n",
      "Training FF1:   6%|█▎                   | 201/3201 [06:18<1:30:04,  1.80s/batch]Batch 200/3201 Done, mean position loss: 25.5306774187088\n",
      "Training FF1:   6%|█▎                   | 199/3201 [06:18<1:43:18,  2.06s/batch]Batch 200/3201 Done, mean position loss: 25.361026129722596\n",
      "Training FF1:   6%|█▎                   | 201/3201 [06:18<1:44:12,  2.08s/batch]Batch 200/3201 Done, mean position loss: 26.045351243019105\n",
      "Training FF1:   6%|█▎                   | 202/3201 [06:20<1:30:19,  1.81s/batch]Batch 200/3201 Done, mean position loss: 26.236703584194185\n",
      "Training FF1:   7%|█▎                   | 209/3201 [06:20<1:43:31,  2.08s/batch]Batch 200/3201 Done, mean position loss: 26.242727038860323\n",
      "Training FF1:   7%|█▎                   | 209/3201 [06:20<1:58:30,  2.38s/batch]Batch 200/3201 Done, mean position loss: 25.496484138965606\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:21<1:51:20,  2.23s/batch]Batch 200/3201 Done, mean position loss: 25.93996808052063\n",
      "Training FF1:   6%|█▎                   | 199/3201 [06:21<1:41:55,  2.04s/batch]Batch 200/3201 Done, mean position loss: 25.817219853401184\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:22<1:37:57,  1.96s/batch]Batch 200/3201 Done, mean position loss: 25.216241490840915\n",
      "Batch 200/3201 Done, mean position loss: 25.147160277366638\n",
      "Training FF1:   6%|█▎                   | 204/3201 [06:22<1:33:52,  1.88s/batch]Batch 200/3201 Done, mean position loss: 25.504317932128906\n",
      "Training FF1:   6%|█▎                   | 202/3201 [06:23<1:30:39,  1.81s/batch]Batch 200/3201 Done, mean position loss: 25.585563683509825\n",
      "Training FF1:   7%|█▎                   | 209/3201 [06:24<1:28:50,  1.78s/batch]Batch 200/3201 Done, mean position loss: 25.678434755802154\n",
      "Training FF1:   6%|█▎                   | 201/3201 [06:24<1:33:10,  1.86s/batch]Batch 200/3201 Done, mean position loss: 25.48509952545166\n",
      "Training FF1:   6%|█▎                   | 204/3201 [06:24<1:25:31,  1.71s/batch]Batch 200/3201 Done, mean position loss: 26.181300194263457\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:29<1:44:40,  2.10s/batch]Batch 200/3201 Done, mean position loss: 25.42324278354645\n",
      "Training FF1:   6%|█▎                   | 205/3201 [06:29<1:35:08,  1.91s/batch]Batch 200/3201 Done, mean position loss: 27.13062587738037\n",
      "Training FF1:   6%|█▎                   | 207/3201 [06:31<1:31:01,  1.82s/batch]Batch 200/3201 Done, mean position loss: 26.28105356693268\n",
      "Training FF1:   9%|█▉                   | 297/3201 [09:03<1:33:42,  1.94s/batch]Batch 300/3201 Done, mean position loss: 23.65385174036026\n",
      "Training FF1:   9%|█▉                   | 292/3201 [09:07<1:15:59,  1.57s/batch]Batch 300/3201 Done, mean position loss: 24.123940312862395\n",
      "Training FF1:   9%|█▉                   | 298/3201 [09:09<1:34:30,  1.95s/batch]Batch 300/3201 Done, mean position loss: 23.82999594926834\n",
      "Training FF1:   9%|█▉                   | 293/3201 [09:09<1:27:40,  1.81s/batch]Batch 300/3201 Done, mean position loss: 24.3819761300087\n",
      "Training FF1:   9%|█▉                   | 298/3201 [09:09<1:28:40,  1.83s/batch]Batch 300/3201 Done, mean position loss: 23.152421169281006\n",
      "Training FF1:   9%|█▉                   | 298/3201 [09:10<2:06:53,  2.62s/batch]Batch 300/3201 Done, mean position loss: 25.256035776138305\n",
      "Training FF1:   9%|█▉                   | 295/3201 [09:11<1:30:38,  1.87s/batch]Batch 300/3201 Done, mean position loss: 25.11652344703674\n",
      "Training FF1:   9%|█▉                   | 303/3201 [09:12<1:33:00,  1.93s/batch]Batch 300/3201 Done, mean position loss: 24.814092576503754\n",
      "Training FF1:   9%|█▉                   | 293/3201 [09:13<1:32:17,  1.90s/batch]Batch 300/3201 Done, mean position loss: 24.33711775302887\n",
      "Training FF1:  10%|██                   | 307/3201 [09:13<1:22:38,  1.71s/batch]Batch 300/3201 Done, mean position loss: 23.43556472301483\n",
      "Training FF1:   9%|█▉                   | 302/3201 [09:14<1:40:44,  2.08s/batch]Batch 300/3201 Done, mean position loss: 24.343902821540834\n",
      "Training FF1:   9%|█▉                   | 298/3201 [09:15<1:37:32,  2.02s/batch]Batch 300/3201 Done, mean position loss: 25.033188104629517\n",
      "Training FF1:   9%|█▉                   | 302/3201 [09:15<1:34:38,  1.96s/batch]Batch 300/3201 Done, mean position loss: 23.809310452938078\n",
      "Training FF1:   9%|█▉                   | 297/3201 [09:15<1:24:59,  1.76s/batch]Batch 300/3201 Done, mean position loss: 24.608366825580596\n",
      "Training FF1:  10%|██                   | 305/3201 [09:17<1:29:02,  1.84s/batch]Batch 300/3201 Done, mean position loss: 24.019962861537934\n",
      "Training FF1:   9%|█▉                   | 294/3201 [09:17<1:27:34,  1.81s/batch]Batch 300/3201 Done, mean position loss: 24.435533208847048\n",
      "Training FF1:   9%|█▉                   | 295/3201 [09:17<1:25:05,  1.76s/batch]Batch 300/3201 Done, mean position loss: 23.768839566707612\n",
      "Training FF1:   9%|█▉                   | 304/3201 [09:17<1:18:32,  1.63s/batch]Batch 300/3201 Done, mean position loss: 24.422227194309237\n",
      "Training FF1:  10%|██                   | 306/3201 [09:19<1:39:19,  2.06s/batch]Batch 300/3201 Done, mean position loss: 24.07492509841919\n",
      "Training FF1:   9%|█▉                   | 300/3201 [09:19<1:23:12,  1.72s/batch]Batch 300/3201 Done, mean position loss: 23.777288072109222\n",
      "Training FF1:   9%|█▉                   | 303/3201 [09:20<1:22:55,  1.72s/batch]Batch 300/3201 Done, mean position loss: 24.240546174049378\n",
      "Training FF1:   9%|█▉                   | 303/3201 [09:21<1:29:20,  1.85s/batch]Batch 300/3201 Done, mean position loss: 24.707048523426053\n",
      "Training FF1:  10%|██                   | 307/3201 [09:21<1:38:31,  2.04s/batch]Batch 300/3201 Done, mean position loss: 23.92479908466339\n",
      "Training FF1:   9%|█▉                   | 299/3201 [09:22<1:46:05,  2.19s/batch]Batch 300/3201 Done, mean position loss: 24.150003614425657\n",
      "Training FF1:  10%|██                   | 312/3201 [09:22<1:29:28,  1.86s/batch]Batch 300/3201 Done, mean position loss: 24.993921971321107\n",
      "Training FF1:   9%|█▉                   | 300/3201 [09:23<1:29:33,  1.85s/batch]Batch 300/3201 Done, mean position loss: 24.155960171222688\n",
      "Training FF1:  10%|██                   | 305/3201 [09:23<1:33:27,  1.94s/batch]Batch 300/3201 Done, mean position loss: 25.045662524700163\n",
      "Training FF1:   9%|█▉                   | 302/3201 [09:24<1:23:29,  1.73s/batch]Batch 300/3201 Done, mean position loss: 24.800628552436827\n",
      "Training FF1:  10%|██                   | 307/3201 [09:25<1:27:57,  1.82s/batch]Batch 300/3201 Done, mean position loss: 24.90178376913071\n",
      "Training FF1:  10%|██                   | 311/3201 [09:26<1:30:22,  1.88s/batch]Batch 300/3201 Done, mean position loss: 24.898958690166474\n",
      "Training FF1:  10%|██                   | 310/3201 [09:26<1:25:36,  1.78s/batch]Batch 300/3201 Done, mean position loss: 23.548148941993716\n",
      "Training FF1:  10%|██                   | 307/3201 [09:26<1:29:17,  1.85s/batch]Batch 300/3201 Done, mean position loss: 23.943173837661742\n",
      "Training FF1:  10%|██                   | 308/3201 [09:26<1:23:18,  1.73s/batch]Batch 300/3201 Done, mean position loss: 24.4884770488739\n",
      "Training FF1:  10%|██                   | 310/3201 [09:28<1:26:20,  1.79s/batch]Batch 300/3201 Done, mean position loss: 23.790355696678162\n",
      "Training FF1:  10%|██                   | 309/3201 [09:29<1:17:02,  1.60s/batch]Batch 300/3201 Done, mean position loss: 23.622515442371366\n",
      "Training FF1:   9%|█▉                   | 303/3201 [09:30<1:26:06,  1.78s/batch]Batch 300/3201 Done, mean position loss: 24.027102358341217\n",
      "Training FF1:  10%|██                   | 307/3201 [09:29<1:37:00,  2.01s/batch]Batch 300/3201 Done, mean position loss: 25.271321206092836\n",
      "Training FF1:  10%|██                   | 313/3201 [09:30<1:29:00,  1.85s/batch]Batch 300/3201 Done, mean position loss: 24.536564009189604\n",
      "Training FF1:  10%|██                   | 306/3201 [09:31<1:32:54,  1.93s/batch]Batch 300/3201 Done, mean position loss: 23.853387589454652\n",
      "Training FF1:   9%|█▉                   | 303/3201 [09:31<1:13:27,  1.52s/batch]Batch 300/3201 Done, mean position loss: 24.193979551792143\n",
      "Training FF1:  12%|██▌                  | 393/3201 [12:03<1:43:28,  2.21s/batch]Batch 400/3201 Done, mean position loss: 23.03266884326935\n",
      "Training FF1:  12%|██▌                  | 400/3201 [12:12<1:22:52,  1.78s/batch]Batch 400/3201 Done, mean position loss: 23.622844507694246\n",
      "Training FF1:  12%|██▌                  | 392/3201 [12:13<1:14:42,  1.60s/batch]Batch 400/3201 Done, mean position loss: 24.306534144878384\n",
      "Training FF1:  12%|██▌                  | 390/3201 [12:13<1:16:14,  1.63s/batch]Batch 400/3201 Done, mean position loss: 23.089120085239408\n",
      "Training FF1:  12%|██▌                  | 400/3201 [12:16<1:16:39,  1.64s/batch]Batch 400/3201 Done, mean position loss: 24.216523218154904\n",
      "Training FF1:  12%|██▌                  | 392/3201 [12:16<1:36:43,  2.07s/batch]Batch 400/3201 Done, mean position loss: 24.18756632566452\n",
      "Training FF1:  13%|██▋                  | 401/3201 [12:17<1:34:55,  2.03s/batch]Batch 400/3201 Done, mean position loss: 22.772559390068054\n",
      "Training FF1:  13%|██▋                  | 401/3201 [12:16<1:38:28,  2.11s/batch]Batch 400/3201 Done, mean position loss: 23.648632175922394\n",
      "Training FF1:  12%|██▌                  | 398/3201 [12:17<1:40:34,  2.15s/batch]Batch 400/3201 Done, mean position loss: 23.885147955417633\n",
      "Training FF1:  12%|██▌                  | 400/3201 [12:17<1:18:52,  1.69s/batch]Batch 400/3201 Done, mean position loss: 24.0023135471344\n",
      "Training FF1:  12%|██▌                  | 399/3201 [12:19<1:19:13,  1.70s/batch]Batch 400/3201 Done, mean position loss: 23.788517227172854\n",
      "Training FF1:  13%|██▋                  | 403/3201 [12:19<1:19:51,  1.71s/batch]Batch 400/3201 Done, mean position loss: 22.51728955745697\n",
      "Training FF1:  12%|██▌                  | 399/3201 [12:20<1:19:11,  1.70s/batch]Batch 400/3201 Done, mean position loss: 23.066529331207274\n",
      "Training FF1:  13%|██▋                  | 403/3201 [12:20<1:16:00,  1.63s/batch]Batch 400/3201 Done, mean position loss: 24.103997526168826\n",
      "Training FF1:  12%|██▌                  | 398/3201 [12:21<1:13:15,  1.57s/batch]Batch 400/3201 Done, mean position loss: 23.576832389831544\n",
      "Training FF1:  13%|██▋                  | 406/3201 [12:22<1:21:38,  1.75s/batch]Batch 400/3201 Done, mean position loss: 22.855661749839783\n",
      "Training FF1:  13%|██▋                  | 404/3201 [12:22<1:11:09,  1.53s/batch]Batch 400/3201 Done, mean position loss: 23.321510219573973\n",
      "Training FF1:  13%|██▋                  | 405/3201 [12:22<1:12:29,  1.56s/batch]Batch 400/3201 Done, mean position loss: 23.423916687965395\n",
      "Training FF1:  12%|██▌                  | 395/3201 [12:22<1:12:51,  1.56s/batch]Batch 400/3201 Done, mean position loss: 23.03876269340515\n",
      "Training FF1:  12%|██▌                  | 392/3201 [12:23<1:34:08,  2.01s/batch]Batch 400/3201 Done, mean position loss: 24.050688462257384\n",
      "Training FF1:  12%|██▌                  | 396/3201 [12:23<1:17:49,  1.66s/batch]Batch 400/3201 Done, mean position loss: 24.242383241653442\n",
      "Training FF1:  13%|██▋                  | 406/3201 [12:23<1:11:48,  1.54s/batch]Batch 400/3201 Done, mean position loss: 23.38319524049759\n",
      "Training FF1:  12%|██▌                  | 398/3201 [12:25<1:25:08,  1.82s/batch]Batch 400/3201 Done, mean position loss: 23.615727369785308\n",
      "Training FF1:  12%|██▌                  | 397/3201 [12:26<1:20:42,  1.73s/batch]Batch 400/3201 Done, mean position loss: 23.821496450901034\n",
      "Training FF1:  13%|██▋                  | 403/3201 [12:27<1:10:48,  1.52s/batch]Batch 400/3201 Done, mean position loss: 23.758813633918763\n",
      "Training FF1:  13%|██▋                  | 405/3201 [12:27<1:27:42,  1.88s/batch]Batch 400/3201 Done, mean position loss: 23.33541563987732\n",
      "Training FF1:  13%|██▋                  | 406/3201 [12:31<1:21:01,  1.74s/batch]Batch 400/3201 Done, mean position loss: 23.17428597688675\n",
      "Training FF1:  13%|██▋                  | 412/3201 [12:31<1:18:04,  1.68s/batch]Batch 400/3201 Done, mean position loss: 24.0220377779007\n",
      "Training FF1:  13%|██▋                  | 410/3201 [12:30<1:32:05,  1.98s/batch]Batch 400/3201 Done, mean position loss: 23.180769381523135\n",
      "Training FF1:  13%|██▋                  | 402/3201 [12:32<1:23:01,  1.78s/batch]Batch 400/3201 Done, mean position loss: 24.00185744524002\n",
      "Training FF1:  12%|██▌                  | 400/3201 [12:32<1:22:23,  1.76s/batch]Batch 400/3201 Done, mean position loss: 23.2915052318573\n",
      "Training FF1:  13%|██▋                  | 405/3201 [12:33<1:22:20,  1.77s/batch]Batch 400/3201 Done, mean position loss: 23.06679605960846\n",
      "Training FF1:  12%|██▌                  | 398/3201 [12:33<1:30:57,  1.95s/batch]Batch 400/3201 Done, mean position loss: 23.674149532318115\n",
      "Training FF1:  13%|██▋                  | 402/3201 [12:33<1:10:31,  1.51s/batch]Batch 400/3201 Done, mean position loss: 23.050202090740203\n",
      "Training FF1:  13%|██▋                  | 410/3201 [12:35<1:36:33,  2.08s/batch]Batch 400/3201 Done, mean position loss: 23.095729572772978\n",
      "Training FF1:  13%|██▋                  | 412/3201 [12:34<1:22:35,  1.78s/batch]Batch 400/3201 Done, mean position loss: 23.508109941482545\n",
      "Training FF1:  13%|██▋                  | 407/3201 [12:37<1:29:15,  1.92s/batch]Batch 400/3201 Done, mean position loss: 23.221944727897643\n",
      "Training FF1:  13%|██▋                  | 414/3201 [12:39<1:37:01,  2.09s/batch]Batch 400/3201 Done, mean position loss: 24.233838889598847\n",
      "Training FF1:  13%|██▋                  | 411/3201 [12:39<1:36:03,  2.07s/batch]Batch 400/3201 Done, mean position loss: 23.73779929161072\n",
      "Training FF1:  13%|██▋                  | 413/3201 [12:40<1:27:36,  1.89s/batch]Batch 400/3201 Done, mean position loss: 22.91205638170242\n",
      "Training FF1:  15%|███▏                 | 480/3201 [15:10<1:24:56,  1.87s/batch]Batch 500/3201 Done, mean position loss: 22.70328717947006\n",
      "Training FF1:  15%|███▏                 | 491/3201 [15:19<1:20:53,  1.79s/batch]Batch 500/3201 Done, mean position loss: 23.676449320316316\n",
      "Training FF1:  15%|███▏                 | 490/3201 [15:20<1:25:20,  1.89s/batch]Batch 500/3201 Done, mean position loss: 22.548271467685698\n",
      "Training FF1:  16%|███▎                 | 499/3201 [15:21<1:16:06,  1.69s/batch]Batch 500/3201 Done, mean position loss: 23.773998453617097\n",
      "Training FF1:  16%|███▎                 | 497/3201 [15:22<1:24:12,  1.87s/batch]Batch 500/3201 Done, mean position loss: 23.545505766868594\n",
      "Training FF1:  16%|███▎                 | 500/3201 [15:23<1:26:22,  1.92s/batch]Batch 500/3201 Done, mean position loss: 23.58053071975708\n",
      "Training FF1:  15%|███▏                 | 494/3201 [15:24<1:50:36,  2.45s/batch]Batch 500/3201 Done, mean position loss: 22.422936675548552\n",
      "Training FF1:  16%|███▎                 | 498/3201 [15:25<1:29:27,  1.99s/batch]Batch 500/3201 Done, mean position loss: 22.067934341430664\n",
      "Training FF1:  15%|███▏                 | 489/3201 [15:26<1:43:47,  2.30s/batch]Batch 500/3201 Done, mean position loss: 23.161731581687928\n",
      "Training FF1:  15%|███▏                 | 495/3201 [15:26<1:30:10,  2.00s/batch]Batch 500/3201 Done, mean position loss: 23.019685201644897\n",
      "Training FF1:  16%|███▎                 | 503/3201 [15:27<1:31:32,  2.04s/batch]Batch 500/3201 Done, mean position loss: 23.519312798976898\n",
      "Training FF1:  15%|███▎                 | 496/3201 [15:29<1:20:56,  1.80s/batch]Batch 500/3201 Done, mean position loss: 23.2929031419754\n",
      "Training FF1:  15%|███▏                 | 495/3201 [15:29<1:12:04,  1.60s/batch]Batch 500/3201 Done, mean position loss: 22.825386307239533\n",
      "Training FF1:  15%|███▏                 | 495/3201 [15:30<1:17:10,  1.71s/batch]Batch 500/3201 Done, mean position loss: 23.046281416416164\n",
      "Training FF1:  15%|███▏                 | 493/3201 [15:30<1:24:03,  1.86s/batch]Batch 500/3201 Done, mean position loss: 22.416089012622834\n",
      "Training FF1:  16%|███▎                 | 505/3201 [15:31<1:15:47,  1.69s/batch]Batch 500/3201 Done, mean position loss: 22.768392012119293\n",
      "Training FF1:  15%|███▏                 | 490/3201 [15:32<1:20:01,  1.77s/batch]Batch 500/3201 Done, mean position loss: 23.577216601371767\n",
      "Training FF1:  16%|███▎                 | 499/3201 [15:33<1:16:18,  1.69s/batch]Batch 500/3201 Done, mean position loss: 23.028547708988192\n",
      "Training FF1:  16%|███▎                 | 508/3201 [15:33<1:06:24,  1.48s/batch]Batch 500/3201 Done, mean position loss: 23.605146992206574\n",
      "Training FF1:  16%|███▎                 | 507/3201 [15:34<1:12:21,  1.61s/batch]Batch 500/3201 Done, mean position loss: 22.646052098274232\n",
      "Training FF1:  16%|███▎                 | 500/3201 [15:34<1:31:02,  2.02s/batch]Batch 500/3201 Done, mean position loss: 23.03896303892136\n",
      "Training FF1:  16%|███▎                 | 505/3201 [15:34<1:31:53,  2.04s/batch]Batch 500/3201 Done, mean position loss: 22.731614339351655\n",
      "Training FF1:  16%|███▎                 | 503/3201 [15:35<1:16:46,  1.71s/batch]Batch 500/3201 Done, mean position loss: 23.24657152175903\n",
      "Training FF1:  15%|███▎                 | 496/3201 [15:35<1:17:24,  1.72s/batch]Batch 500/3201 Done, mean position loss: 23.003030309677122\n",
      "Training FF1:  16%|███▎                 | 499/3201 [15:35<1:09:45,  1.55s/batch]Batch 500/3201 Done, mean position loss: 22.955417397022245\n",
      "Training FF1:  16%|███▎                 | 506/3201 [15:37<1:09:47,  1.55s/batch]Batch 500/3201 Done, mean position loss: 23.392023985385894\n",
      "Training FF1:  16%|███▎                 | 497/3201 [15:37<1:22:30,  1.83s/batch]Batch 500/3201 Done, mean position loss: 23.11482028722763\n",
      "Training FF1:  16%|███▎                 | 499/3201 [15:38<1:34:57,  2.11s/batch]Batch 500/3201 Done, mean position loss: 22.624496827125547\n",
      "Training FF1:  16%|███▎                 | 498/3201 [15:39<1:32:42,  2.06s/batch]Batch 500/3201 Done, mean position loss: 22.68377242088318\n",
      "Training FF1:  16%|███▎                 | 506/3201 [15:39<1:27:29,  1.95s/batch]Batch 500/3201 Done, mean position loss: 23.429220497608185\n",
      "Training FF1:  16%|███▎                 | 502/3201 [15:39<1:20:33,  1.79s/batch]Batch 500/3201 Done, mean position loss: 22.6546711063385\n",
      "Training FF1:  16%|███▎                 | 502/3201 [15:40<1:18:01,  1.73s/batch]Batch 500/3201 Done, mean position loss: 22.52775372982025\n",
      "Training FF1:  16%|███▎                 | 504/3201 [15:44<1:09:08,  1.54s/batch]Batch 500/3201 Done, mean position loss: 22.887582666873932\n",
      "Training FF1:  16%|███▎                 | 509/3201 [15:44<1:16:41,  1.71s/batch]Batch 500/3201 Done, mean position loss: 22.66363741636276\n",
      "Training FF1:  16%|███▍                 | 516/3201 [15:44<1:14:49,  1.67s/batch]Batch 500/3201 Done, mean position loss: 23.28883679628372\n",
      "Training FF1:  16%|███▎                 | 514/3201 [15:45<1:23:41,  1.87s/batch]Batch 500/3201 Done, mean position loss: 23.70306430578232\n",
      "Training FF1:  16%|███▎                 | 499/3201 [15:45<1:23:56,  1.86s/batch]Batch 500/3201 Done, mean position loss: 22.76216123342514\n",
      "Training FF1:  16%|███▎                 | 508/3201 [15:47<1:31:39,  2.04s/batch]Batch 500/3201 Done, mean position loss: 22.608135685920715\n",
      "Training FF1:  16%|███▎                 | 510/3201 [15:49<1:22:11,  1.83s/batch]Batch 500/3201 Done, mean position loss: 23.44507223844528\n",
      "Training FF1:  16%|███▎                 | 505/3201 [15:51<1:16:30,  1.70s/batch]Batch 500/3201 Done, mean position loss: 22.439382393360138\n",
      "Training FF1:  19%|███▉                 | 596/3201 [18:18<1:05:19,  1.50s/batch]Batch 600/3201 Done, mean position loss: 22.12987895488739\n",
      "Training FF1:  19%|███▉                 | 601/3201 [18:19<1:15:04,  1.73s/batch]Batch 600/3201 Done, mean position loss: 22.92421868801117\n",
      "Training FF1:  19%|███▉                 | 596/3201 [18:20<1:07:36,  1.56s/batch]Batch 600/3201 Done, mean position loss: 23.470857212543486\n",
      "Training FF1:  19%|███▉                 | 602/3201 [18:20<1:09:46,  1.61s/batch]Batch 600/3201 Done, mean position loss: 23.197343351840974\n",
      "Training FF1:  19%|███▉                 | 596/3201 [18:20<1:21:35,  1.88s/batch]Batch 600/3201 Done, mean position loss: 23.20221119403839\n",
      "Training FF1:  19%|███▉                 | 597/3201 [18:21<1:12:33,  1.67s/batch]Batch 600/3201 Done, mean position loss: 22.45076483011246\n",
      "Training FF1:  18%|███▉                 | 592/3201 [18:24<1:37:07,  2.23s/batch]Batch 600/3201 Done, mean position loss: 22.14487682580948\n",
      "Training FF1:  18%|███▉                 | 591/3201 [18:25<1:19:26,  1.83s/batch]Batch 600/3201 Done, mean position loss: 21.844870069026946\n",
      "Training FF1:  19%|████▎                  | 598/3201 [18:26<57:48,  1.33s/batch]Batch 600/3201 Done, mean position loss: 22.44399003505707\n",
      "Training FF1:  19%|███▉                 | 604/3201 [18:26<1:17:19,  1.79s/batch]Batch 600/3201 Done, mean position loss: 23.07150138378143\n",
      "Training FF1:  19%|███▉                 | 601/3201 [18:25<1:06:32,  1.54s/batch]Batch 600/3201 Done, mean position loss: 23.033722043037415\n",
      "Training FF1:  18%|███▉                 | 591/3201 [18:26<1:08:29,  1.57s/batch]Batch 600/3201 Done, mean position loss: 22.09089004278183\n",
      "Training FF1:  18%|███▊                 | 589/3201 [18:28<1:29:18,  2.05s/batch]Batch 600/3201 Done, mean position loss: 22.37349024772644\n",
      "Training FF1:  19%|███▉                 | 606/3201 [18:28<1:22:00,  1.90s/batch]Batch 600/3201 Done, mean position loss: 23.209289700984954\n",
      "Training FF1:  19%|███▉                 | 607/3201 [18:30<1:14:09,  1.72s/batch]Batch 600/3201 Done, mean position loss: 23.17478950500488\n",
      "Training FF1:  19%|███▉                 | 594/3201 [18:31<1:33:59,  2.16s/batch]Batch 600/3201 Done, mean position loss: 22.82488270044327\n",
      "Training FF1:  18%|███▉                 | 591/3201 [18:31<1:25:48,  1.97s/batch]Batch 600/3201 Done, mean position loss: 22.32444537162781\n",
      "Training FF1:  19%|███▉                 | 608/3201 [18:31<1:09:40,  1.61s/batch]Batch 600/3201 Done, mean position loss: 22.490248320102694\n",
      "Training FF1:  19%|███▉                 | 608/3201 [18:32<1:17:13,  1.79s/batch]Batch 600/3201 Done, mean position loss: 22.526123995780942\n",
      "Training FF1:  19%|███▉                 | 605/3201 [18:32<1:18:07,  1.81s/batch]Batch 600/3201 Done, mean position loss: 22.812315318584442\n",
      "Training FF1:  18%|███▊                 | 590/3201 [18:34<1:24:26,  1.94s/batch]Batch 600/3201 Done, mean position loss: 23.056815819740294\n",
      "Training FF1:  19%|███▉                 | 600/3201 [18:36<1:18:44,  1.82s/batch]Batch 600/3201 Done, mean position loss: 22.87979607820511\n",
      "Training FF1:  19%|███▉                 | 593/3201 [18:38<1:18:49,  1.81s/batch]Batch 600/3201 Done, mean position loss: 23.24444810152054\n",
      "Training FF1:  19%|███▉                 | 595/3201 [18:40<1:28:53,  2.05s/batch]Batch 600/3201 Done, mean position loss: 22.47768859863281\n",
      "Training FF1:  19%|███▉                 | 594/3201 [18:40<1:29:59,  2.07s/batch]Batch 600/3201 Done, mean position loss: 22.445512001514437\n",
      "Training FF1:  19%|███▉                 | 606/3201 [18:40<1:16:21,  1.77s/batch]Batch 600/3201 Done, mean position loss: 23.056621358394622\n",
      "Training FF1:  19%|███▉                 | 602/3201 [18:42<1:22:37,  1.91s/batch]Batch 600/3201 Done, mean position loss: 22.211095707416533\n",
      "Training FF1:  19%|███▉                 | 605/3201 [18:42<1:17:05,  1.78s/batch]Batch 600/3201 Done, mean position loss: 22.30970398664474\n",
      "Batch 600/3201 Done, mean position loss: 22.853718993663787\n",
      "Training FF1:  19%|███▉                 | 601/3201 [18:43<1:15:31,  1.74s/batch]Batch 600/3201 Done, mean position loss: 22.38781990289688\n",
      "Training FF1:  19%|███▉                 | 603/3201 [18:43<1:13:09,  1.69s/batch]Batch 600/3201 Done, mean position loss: 22.598965458869934\n",
      "Training FF1:  19%|████                 | 613/3201 [18:46<1:15:58,  1.76s/batch]Batch 600/3201 Done, mean position loss: 22.402020027637484\n",
      "Training FF1:  19%|████▍                  | 617/3201 [18:46<56:23,  1.31s/batch]Batch 600/3201 Done, mean position loss: 22.853778398036958\n",
      "Training FF1:  19%|███▉                 | 609/3201 [18:47<1:02:15,  1.44s/batch]Batch 600/3201 Done, mean position loss: 22.240295636653897\n",
      "Training FF1:  19%|████                 | 615/3201 [18:51<1:31:04,  2.11s/batch]Batch 600/3201 Done, mean position loss: 22.294719524383545\n",
      "Training FF1:  19%|████                 | 613/3201 [18:51<1:10:34,  1.64s/batch]Batch 600/3201 Done, mean position loss: 22.906039121150968\n",
      "Training FF1:  19%|███▉                 | 602/3201 [18:53<1:17:17,  1.78s/batch]Batch 600/3201 Done, mean position loss: 23.469874925613404\n",
      "Training FF1:  19%|████                 | 614/3201 [18:53<1:12:36,  1.68s/batch]Batch 600/3201 Done, mean position loss: 22.474289450645443\n",
      "Training FF1:  19%|████                 | 613/3201 [18:53<1:18:23,  1.82s/batch]Batch 600/3201 Done, mean position loss: 22.034880394935605\n",
      "Training FF1:  19%|████                 | 610/3201 [18:55<1:11:35,  1.66s/batch]Batch 600/3201 Done, mean position loss: 22.28522031068802\n",
      "Training FF1:  22%|████▌                | 689/3201 [21:15<1:23:05,  1.98s/batch]Batch 700/3201 Done, mean position loss: 23.20441974401474\n",
      "Training FF1:  21%|████▌                | 686/3201 [21:18<1:17:06,  1.84s/batch]Batch 700/3201 Done, mean position loss: 22.98514143228531\n",
      "Training FF1:  21%|████▌                | 688/3201 [21:18<1:14:24,  1.78s/batch]Batch 700/3201 Done, mean position loss: 22.858548550605775\n",
      "Training FF1:  21%|████▍                | 680/3201 [21:20<1:04:19,  1.53s/batch]Batch 700/3201 Done, mean position loss: 21.976942970752717\n",
      "Training FF1:  22%|████▌                | 692/3201 [21:21<1:11:43,  1.72s/batch]Batch 700/3201 Done, mean position loss: 21.882366960048678\n",
      "Training FF1:  22%|████▌                | 700/3201 [21:22<1:19:12,  1.90s/batch]Batch 700/3201 Done, mean position loss: 22.242425544261934\n",
      "Training FF1:  22%|████▌                | 691/3201 [21:22<1:02:55,  1.50s/batch]Batch 700/3201 Done, mean position loss: 22.83722481250763\n",
      "Training FF1:  22%|████▌                | 700/3201 [21:23<1:04:35,  1.55s/batch]Batch 700/3201 Done, mean position loss: 22.151294610500333\n",
      "Training FF1:  22%|████▌                | 703/3201 [21:24<1:06:19,  1.59s/batch]Batch 700/3201 Done, mean position loss: 22.631394894123076\n",
      "Training FF1:  22%|████▋                | 708/3201 [21:26<1:05:51,  1.58s/batch]Batch 700/3201 Done, mean position loss: 22.8748183298111\n",
      "Training FF1:  22%|████▌                | 690/3201 [21:26<1:14:12,  1.77s/batch]Batch 700/3201 Done, mean position loss: 21.938882811069487\n",
      "Training FF1:  22%|████▌                | 698/3201 [21:26<1:08:06,  1.63s/batch]Batch 700/3201 Done, mean position loss: 22.89278392791748\n",
      "Training FF1:  21%|████▍                | 684/3201 [21:26<1:08:37,  1.64s/batch]Batch 700/3201 Done, mean position loss: 22.158575670719145\n",
      "Training FF1:  22%|████▌                | 697/3201 [21:27<1:22:18,  1.97s/batch]Batch 700/3201 Done, mean position loss: 22.20892351388931\n",
      "Training FF1:  22%|████▌                | 692/3201 [21:28<1:22:41,  1.98s/batch]Batch 700/3201 Done, mean position loss: 21.644041442871092\n",
      "Training FF1:  22%|████▌                | 694/3201 [21:31<1:17:23,  1.85s/batch]Batch 700/3201 Done, mean position loss: 22.71174103498459\n",
      "Training FF1:  22%|████▋                | 705/3201 [21:32<1:10:44,  1.70s/batch]Batch 700/3201 Done, mean position loss: 22.82107595920563\n",
      "Training FF1:  22%|████▋                | 705/3201 [21:32<1:10:52,  1.70s/batch]Batch 700/3201 Done, mean position loss: 22.486089332103727\n",
      "Training FF1:  22%|████▋                | 708/3201 [21:34<1:17:50,  1.87s/batch]Batch 700/3201 Done, mean position loss: 22.955730438232422\n",
      "Training FF1:  22%|████▋                | 706/3201 [21:34<1:14:46,  1.80s/batch]Batch 700/3201 Done, mean position loss: 22.61362298488617\n",
      "Training FF1:  22%|████▋                | 706/3201 [21:34<1:20:13,  1.93s/batch]Batch 700/3201 Done, mean position loss: 22.790543954372403\n",
      "Training FF1:  22%|████▌                | 700/3201 [21:37<1:10:21,  1.69s/batch]Batch 700/3201 Done, mean position loss: 22.40024415254593\n",
      "Training FF1:  22%|████▋                | 710/3201 [21:37<1:12:52,  1.76s/batch]Batch 700/3201 Done, mean position loss: 22.231251273155213\n",
      "Training FF1:  22%|████▋                | 710/3201 [21:39<1:10:05,  1.69s/batch]Batch 700/3201 Done, mean position loss: 22.037259917259213\n",
      "Training FF1:  22%|████▌                | 704/3201 [21:38<1:01:56,  1.49s/batch]Batch 700/3201 Done, mean position loss: 22.73082500457764\n",
      "Training FF1:  22%|████▌                | 698/3201 [21:40<1:14:17,  1.78s/batch]Batch 700/3201 Done, mean position loss: 22.04182681322098\n",
      "Training FF1:  22%|████▋                | 705/3201 [21:41<1:17:02,  1.85s/batch]Batch 700/3201 Done, mean position loss: 22.555985155105592\n",
      "Training FF1:  22%|████▋                | 710/3201 [21:42<1:04:02,  1.54s/batch]Batch 700/3201 Done, mean position loss: 22.66288043498993\n",
      "Training FF1:  22%|████▌                | 696/3201 [21:43<1:20:00,  1.92s/batch]Batch 700/3201 Done, mean position loss: 21.999069020748138\n",
      "Training FF1:  22%|████▌                | 700/3201 [21:44<1:21:48,  1.96s/batch]Batch 700/3201 Done, mean position loss: 22.305462493896485\n",
      "Training FF1:  22%|████▋                | 716/3201 [21:45<1:09:41,  1.68s/batch]Batch 700/3201 Done, mean position loss: 22.20772111415863\n",
      "Training FF1:  22%|████▋                | 708/3201 [21:46<1:14:48,  1.80s/batch]Batch 700/3201 Done, mean position loss: 22.120792670249937\n",
      "Training FF1:  22%|████▋                | 717/3201 [21:49<1:08:01,  1.64s/batch]Batch 700/3201 Done, mean position loss: 22.12827613353729\n",
      "Training FF1:  22%|████▌                | 696/3201 [21:49<1:07:26,  1.62s/batch]Batch 700/3201 Done, mean position loss: 22.023000984191896\n",
      "Training FF1:  22%|████▋                | 714/3201 [21:54<1:14:54,  1.81s/batch]Batch 700/3201 Done, mean position loss: 22.59155011177063\n",
      "Batch 700/3201 Done, mean position loss: 21.661770555973053\n",
      "Training FF1:  23%|████▋                | 721/3201 [21:57<1:03:21,  1.53s/batch]Batch 700/3201 Done, mean position loss: 22.062580006122587\n",
      "Training FF1:  22%|████▋                | 710/3201 [21:57<1:04:37,  1.56s/batch]Batch 700/3201 Done, mean position loss: 23.16731544494629\n",
      "Training FF1:  23%|████▋                | 722/3201 [21:58<1:15:24,  1.83s/batch]Batch 700/3201 Done, mean position loss: 22.360339226722715\n",
      "Training FF1:  22%|████▋                | 713/3201 [21:58<1:08:57,  1.66s/batch]Batch 700/3201 Done, mean position loss: 22.068273158073424\n",
      "Training FF1:  25%|█████▏               | 796/3201 [24:13<1:10:24,  1.76s/batch]Batch 800/3201 Done, mean position loss: 22.705004265308382\n",
      "Training FF1:  25%|█████▏               | 799/3201 [24:18<1:04:40,  1.62s/batch]Batch 800/3201 Done, mean position loss: 22.6360499215126\n",
      "Training FF1:  25%|█████▋                 | 797/3201 [24:17<59:31,  1.49s/batch]Batch 800/3201 Done, mean position loss: 22.790004661083223\n",
      "Training FF1:  25%|█████▏               | 798/3201 [24:18<1:10:25,  1.76s/batch]Batch 800/3201 Done, mean position loss: 23.0051132273674\n",
      "Training FF1:  25%|█████▏               | 800/3201 [24:19<1:04:45,  1.62s/batch]Batch 800/3201 Done, mean position loss: 21.74896760225296\n",
      "Training FF1:  25%|█████▏               | 795/3201 [24:19<1:21:31,  2.03s/batch]Batch 800/3201 Done, mean position loss: 22.56836836576462\n",
      "Training FF1:  25%|█████▎               | 802/3201 [24:20<1:14:43,  1.87s/batch]Batch 800/3201 Done, mean position loss: 21.843056600093842\n",
      "Training FF1:  25%|█████▎               | 802/3201 [24:20<1:06:29,  1.66s/batch]Batch 800/3201 Done, mean position loss: 22.05025218963623\n",
      "Training FF1:  25%|█████▏               | 785/3201 [24:22<1:23:20,  2.07s/batch]Batch 800/3201 Done, mean position loss: 21.892462084293363\n",
      "Training FF1:  25%|█████▏               | 799/3201 [24:23<1:22:41,  2.07s/batch]Batch 800/3201 Done, mean position loss: 22.591675388813016\n",
      "Training FF1:  25%|█████▏               | 792/3201 [24:24<1:08:45,  1.71s/batch]Batch 800/3201 Done, mean position loss: 22.274005963802338\n",
      "Training FF1:  25%|█████▏               | 791/3201 [24:24<1:09:16,  1.72s/batch]Batch 800/3201 Done, mean position loss: 21.99112429857254\n",
      "Batch 800/3201 Done, mean position loss: 22.413523871898654\n",
      "Training FF1:  25%|█████▎               | 804/3201 [24:25<1:16:53,  1.92s/batch]Batch 800/3201 Done, mean position loss: 22.07559380531311\n",
      "Training FF1:  25%|█████▏               | 792/3201 [24:27<1:22:15,  2.05s/batch]Batch 800/3201 Done, mean position loss: 22.040291883945464\n",
      "Training FF1:  25%|█████▏               | 799/3201 [24:27<1:07:40,  1.69s/batch]Batch 800/3201 Done, mean position loss: 22.72461998462677\n",
      "Training FF1:  25%|█████▎               | 806/3201 [24:29<1:00:26,  1.51s/batch]Batch 800/3201 Done, mean position loss: 22.569774882793425\n",
      "Training FF1:  25%|█████▎               | 804/3201 [24:30<1:07:56,  1.70s/batch]Batch 800/3201 Done, mean position loss: 22.56540758609772\n",
      "Training FF1:  24%|█████▏               | 782/3201 [24:30<1:13:55,  1.83s/batch]Batch 800/3201 Done, mean position loss: 22.622023561000823\n",
      "Training FF1:  24%|█████▏               | 782/3201 [24:31<1:17:12,  1.92s/batch]Batch 800/3201 Done, mean position loss: 21.940680849552155\n",
      "Training FF1:  25%|█████▎               | 806/3201 [24:33<1:11:38,  1.79s/batch]Batch 800/3201 Done, mean position loss: 22.155757403373716\n",
      "Training FF1:  24%|█████▏               | 784/3201 [24:33<1:13:40,  1.83s/batch]Batch 800/3201 Done, mean position loss: 21.501567375659945\n",
      "Training FF1:  25%|█████▏               | 796/3201 [24:33<1:08:15,  1.70s/batch]Batch 800/3201 Done, mean position loss: 22.11885481834412\n",
      "Training FF1:  25%|█████▎               | 809/3201 [24:35<1:21:52,  2.05s/batch]Batch 800/3201 Done, mean position loss: 21.88030131816864\n",
      "Training FF1:  25%|█████▎               | 805/3201 [24:37<1:19:04,  1.98s/batch]Batch 800/3201 Done, mean position loss: 22.474776721000673\n",
      "Training FF1:  25%|█████▎               | 811/3201 [24:39<1:31:51,  2.31s/batch]Batch 800/3201 Done, mean position loss: 21.997217645645144\n",
      "Training FF1:  25%|█████▎               | 810/3201 [24:39<1:09:55,  1.75s/batch]Batch 800/3201 Done, mean position loss: 21.921514418125152\n",
      "Training FF1:  25%|█████▎               | 807/3201 [24:40<1:04:45,  1.62s/batch]Batch 800/3201 Done, mean position loss: 22.357087688446043\n",
      "Training FF1:  25%|█████▎               | 808/3201 [24:42<1:09:24,  1.74s/batch]Batch 800/3201 Done, mean position loss: 22.506445672512054\n",
      "Training FF1:  25%|█████▎               | 815/3201 [24:43<1:16:27,  1.92s/batch]Batch 800/3201 Done, mean position loss: 22.33981708288193\n",
      "Training FF1:  25%|█████▎               | 807/3201 [24:44<1:15:07,  1.88s/batch]Batch 800/3201 Done, mean position loss: 22.15684951543808\n",
      "Training FF1:  25%|█████▏               | 791/3201 [24:45<1:07:45,  1.69s/batch]Batch 800/3201 Done, mean position loss: 21.805296344757082\n",
      "Training FF1:  25%|█████▎               | 809/3201 [24:48<1:13:32,  1.84s/batch]Batch 800/3201 Done, mean position loss: 21.862346990108488\n",
      "Training FF1:  25%|█████▎               | 813/3201 [24:50<1:12:51,  1.83s/batch]Batch 800/3201 Done, mean position loss: 21.91910453081131\n",
      "Training FF1:  25%|█████▎               | 812/3201 [24:55<1:33:15,  2.34s/batch]Batch 800/3201 Done, mean position loss: 21.458691761493682\n",
      "Batch 800/3201 Done, mean position loss: 22.341345872879028\n",
      "Training FF1:  25%|█████▎               | 813/3201 [25:02<1:27:44,  2.20s/batch]Batch 800/3201 Done, mean position loss: 22.197828648090365\n",
      "Training FF1:  25%|█████▎               | 814/3201 [25:03<1:03:33,  1.60s/batch]Batch 800/3201 Done, mean position loss: 21.89754074335098\n",
      "Training FF1:  25%|█████▎               | 803/3201 [25:06<1:12:38,  1.82s/batch]Batch 800/3201 Done, mean position loss: 21.86986018419266\n",
      "Training FF1:  25%|█████▎               | 803/3201 [25:07<1:18:30,  1.96s/batch]Batch 800/3201 Done, mean position loss: 23.042556006908416\n",
      "Training FF1:  28%|█████▊               | 891/3201 [27:19<1:07:52,  1.76s/batch]Batch 900/3201 Done, mean position loss: 22.602517011165617\n",
      "Training FF1:  28%|█████▊               | 890/3201 [27:20<1:09:48,  1.81s/batch]Batch 900/3201 Done, mean position loss: 21.93474037885666\n",
      "Training FF1:  27%|█████▊               | 877/3201 [27:20<1:07:55,  1.75s/batch]Batch 900/3201 Done, mean position loss: 22.718107914924623\n",
      "Training FF1:  28%|█████▊               | 883/3201 [27:20<1:06:13,  1.71s/batch]Batch 900/3201 Done, mean position loss: 21.831513214111325\n",
      "Training FF1:  27%|█████▋               | 876/3201 [27:22<1:09:00,  1.78s/batch]Batch 900/3201 Done, mean position loss: 22.30838752746582\n",
      "Training FF1:  28%|█████▊               | 892/3201 [27:24<1:05:21,  1.70s/batch]Batch 900/3201 Done, mean position loss: 22.868486230373385\n",
      "Training FF1:  28%|█████▊               | 885/3201 [27:25<1:14:53,  1.94s/batch]Batch 900/3201 Done, mean position loss: 21.92865152359009\n",
      "Training FF1:  28%|█████▊               | 887/3201 [27:25<1:00:37,  1.57s/batch]Batch 900/3201 Done, mean position loss: 21.94324224948883\n",
      "Training FF1:  28%|█████▊               | 885/3201 [27:27<1:14:11,  1.92s/batch]Batch 900/3201 Done, mean position loss: 22.49762755155563\n",
      "Training FF1:  28%|█████▊               | 894/3201 [27:27<1:07:27,  1.75s/batch]Batch 900/3201 Done, mean position loss: 22.104223153591157\n",
      "Batch 900/3201 Done, mean position loss: 21.60072834253311\n",
      "Training FF1:  28%|██████▎                | 881/3201 [27:27<59:09,  1.53s/batch]Batch 900/3201 Done, mean position loss: 22.4432984995842\n",
      "Training FF1:  28%|█████▊               | 893/3201 [27:29<1:04:45,  1.68s/batch]Batch 900/3201 Done, mean position loss: 22.281135573387147\n",
      "Training FF1:  28%|█████▊               | 882/3201 [27:29<1:05:08,  1.69s/batch]Batch 900/3201 Done, mean position loss: 21.796279137134555\n",
      "Training FF1:  28%|██████▌                | 907/3201 [27:29<59:08,  1.55s/batch]Batch 900/3201 Done, mean position loss: 22.220529208183287\n",
      "Training FF1:  27%|█████▊               | 879/3201 [27:32<1:08:05,  1.76s/batch]Batch 900/3201 Done, mean position loss: 21.823664231300352\n",
      "Training FF1:  28%|█████▊               | 882/3201 [27:34<1:12:09,  1.87s/batch]Batch 900/3201 Done, mean position loss: 22.450321712493896\n",
      "Training FF1:  28%|█████▉               | 904/3201 [27:34<1:05:01,  1.70s/batch]Batch 900/3201 Done, mean position loss: 22.02619071722031\n",
      "Training FF1:  28%|█████▉               | 906/3201 [27:38<1:06:35,  1.74s/batch]Batch 900/3201 Done, mean position loss: 21.418178822994236\n",
      "Training FF1:  28%|█████▉               | 899/3201 [27:38<1:12:52,  1.90s/batch]Batch 900/3201 Done, mean position loss: 21.961322779655454\n",
      "Training FF1:  28%|█████▊               | 891/3201 [27:38<1:12:57,  1.89s/batch]Batch 900/3201 Done, mean position loss: 21.935577087402343\n",
      "Training FF1:  28%|█████▉               | 904/3201 [27:40<1:16:55,  2.01s/batch]Batch 900/3201 Done, mean position loss: 21.71884463071823\n",
      "Training FF1:  28%|█████▉               | 900/3201 [27:40<1:15:13,  1.96s/batch]Batch 900/3201 Done, mean position loss: 22.566061053276062\n",
      "Training FF1:  28%|█████▊               | 893/3201 [27:40<1:23:32,  2.17s/batch]Batch 900/3201 Done, mean position loss: 22.253978743553162\n",
      "Training FF1:  28%|█████▉               | 910/3201 [27:41<1:03:51,  1.67s/batch]Batch 900/3201 Done, mean position loss: 22.474655508995056\n",
      "Training FF1:  28%|█████▊               | 889/3201 [27:42<1:12:06,  1.87s/batch]Batch 900/3201 Done, mean position loss: 22.334436323642734\n",
      "Training FF1:  28%|█████▉               | 904/3201 [27:43<1:04:59,  1.70s/batch]Batch 900/3201 Done, mean position loss: 22.062223391532896\n",
      "Training FF1:  28%|█████▉               | 904/3201 [27:43<1:02:45,  1.64s/batch]Batch 900/3201 Done, mean position loss: 22.262642574310306\n",
      "Training FF1:  29%|██████               | 917/3201 [27:46<1:01:46,  1.62s/batch]Batch 900/3201 Done, mean position loss: 22.19205873250961\n",
      "Training FF1:  28%|█████▉               | 904/3201 [27:47<1:16:46,  2.01s/batch]Batch 900/3201 Done, mean position loss: 21.833720099925994\n",
      "Training FF1:  29%|██████               | 915/3201 [27:50<1:07:11,  1.76s/batch]Batch 900/3201 Done, mean position loss: 21.687256360054015\n",
      "Training FF1:  29%|██████               | 918/3201 [27:51<1:15:07,  1.97s/batch]Batch 900/3201 Done, mean position loss: 22.039463412761688\n",
      "Training FF1:  28%|█████▉               | 905/3201 [27:53<1:05:49,  1.72s/batch]Batch 900/3201 Done, mean position loss: 21.668804030418393\n",
      "Training FF1:  28%|█████▉               | 896/3201 [27:56<1:13:16,  1.91s/batch]Batch 900/3201 Done, mean position loss: 21.378260085582735\n",
      "Training FF1:  29%|██████               | 921/3201 [27:56<1:01:40,  1.62s/batch]Batch 900/3201 Done, mean position loss: 21.80816413879395\n",
      "Training FF1:  29%|██████               | 921/3201 [27:58<1:20:14,  2.11s/batch]Batch 900/3201 Done, mean position loss: 22.099930613040925\n",
      "Training FF1:  29%|██████               | 923/3201 [28:04<1:10:38,  1.86s/batch]Batch 900/3201 Done, mean position loss: 22.040517032146454\n",
      "Training FF1:  28%|█████▉               | 901/3201 [28:05<1:14:17,  1.94s/batch]Batch 900/3201 Done, mean position loss: 21.814689624309537\n",
      "Training FF1:  29%|██████               | 927/3201 [28:08<1:23:19,  2.20s/batch]Batch 900/3201 Done, mean position loss: 22.826374847888943\n",
      "Training FF1:  29%|██████               | 923/3201 [28:16<1:16:00,  2.00s/batch]Batch 900/3201 Done, mean position loss: 21.79707291126251\n",
      "Training FF1:  31%|███████                | 990/3201 [30:09<58:54,  1.60s/batch]Batch 1000/3201 Done, mean position loss: 22.570484697818756\n",
      "Training FF1:  31%|██████▍              | 987/3201 [30:13<1:09:55,  1.89s/batch]Batch 1000/3201 Done, mean position loss: 22.543949279785153\n",
      "Training FF1:  31%|██████▍              | 977/3201 [30:14<1:00:49,  1.64s/batch]Batch 1000/3201 Done, mean position loss: 21.84279060602188\n",
      "Training FF1:  31%|██████▉               | 1002/3201 [30:15<52:43,  1.44s/batch]Batch 1000/3201 Done, mean position loss: 21.83089589357376\n",
      "Training FF1:  31%|██████▌              | 998/3201 [30:18<1:04:15,  1.75s/batch]Batch 1000/3201 Done, mean position loss: 22.762924916744232\n",
      "Training FF1:  31%|██████▍              | 978/3201 [30:19<1:06:31,  1.80s/batch]Batch 1000/3201 Done, mean position loss: 22.391550652980804\n",
      "Training FF1:  31%|██████▍              | 979/3201 [30:20<1:03:21,  1.71s/batch]Batch 1000/3201 Done, mean position loss: 21.860128989219668\n",
      "Training FF1:  31%|██████▊               | 1000/3201 [30:21<53:58,  1.47s/batch]Batch 1000/3201 Done, mean position loss: 22.082444450855256\n",
      "Training FF1:  31%|███████                | 977/3201 [30:23<55:53,  1.51s/batch]Batch 1000/3201 Done, mean position loss: 22.151687729358674\n",
      "Training FF1:  31%|███████                | 990/3201 [30:23<57:22,  1.56s/batch]Batch 1000/3201 Done, mean position loss: 22.097534234523774\n",
      "Training FF1:  31%|██████▌              | 993/3201 [30:25<1:10:55,  1.93s/batch]Batch 1000/3201 Done, mean position loss: 21.472468822002412\n",
      "Training FF1:  31%|██████▌              | 999/3201 [30:25<1:06:37,  1.82s/batch]Batch 1000/3201 Done, mean position loss: 21.726600408554077\n",
      "Training FF1:  31%|███████                | 981/3201 [30:25<56:10,  1.52s/batch]Batch 1000/3201 Done, mean position loss: 21.739428915977477\n",
      "Training FF1:  32%|██████▎             | 1010/3201 [30:29<1:01:57,  1.70s/batch]Batch 1000/3201 Done, mean position loss: 21.93456369638443\n",
      "Training FF1:  31%|██████▌              | 999/3201 [30:30<1:03:11,  1.72s/batch]Batch 1000/3201 Done, mean position loss: 22.018036940097808\n",
      "Training FF1:  31%|██████▌              | 996/3201 [30:32<1:05:24,  1.78s/batch]Batch 1000/3201 Done, mean position loss: 22.209052217006686\n",
      "Training FF1:  31%|██████▏             | 1000/3201 [30:34<1:09:37,  1.90s/batch]Batch 1000/3201 Done, mean position loss: 22.061072244644166\n",
      "Training FF1:  31%|██████▏             | 1000/3201 [30:36<1:03:44,  1.74s/batch]Batch 1000/3201 Done, mean position loss: 21.367483487129213\n",
      "Training FF1:  31%|██████▌              | 997/3201 [30:37<1:01:06,  1.66s/batch]Batch 1000/3201 Done, mean position loss: 21.974396674633027\n",
      "Training FF1:  31%|██████▉               | 1004/3201 [30:37<57:15,  1.56s/batch]Batch 1000/3201 Done, mean position loss: 21.845453288555145\n",
      "Training FF1:  31%|██████▉               | 1003/3201 [30:38<57:46,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 22.297140924930574\n",
      "Training FF1:  31%|██████▍              | 985/3201 [30:39<1:12:18,  1.96s/batch]Batch 1000/3201 Done, mean position loss: 21.642858462333677\n",
      "Training FF1:  31%|███████                | 982/3201 [30:39<58:49,  1.59s/batch]Batch 1000/3201 Done, mean position loss: 21.87196209192276\n",
      "Training FF1:  31%|██████▏             | 1000/3201 [30:40<1:02:37,  1.71s/batch]Batch 1000/3201 Done, mean position loss: 22.416719160079957\n",
      "Training FF1:  31%|██████▌              | 996/3201 [30:41<1:15:34,  2.06s/batch]Batch 1000/3201 Done, mean position loss: 22.088021264076232\n",
      "Training FF1:  32%|██████▉               | 1017/3201 [30:41<50:33,  1.39s/batch]Batch 1000/3201 Done, mean position loss: 21.92982348442078\n",
      "Training FF1:  32%|██████▎             | 1011/3201 [30:42<1:00:13,  1.65s/batch]Batch 1000/3201 Done, mean position loss: 22.31719045639038\n",
      "Batch 1000/3201 Done, mean position loss: 21.993029260635375\n",
      "Training FF1:  32%|██████▎             | 1012/3201 [30:42<1:02:59,  1.73s/batch]Batch 1000/3201 Done, mean position loss: 22.2193030333519\n",
      "Training FF1:  32%|██████▎             | 1015/3201 [30:44<1:05:26,  1.80s/batch]Batch 1000/3201 Done, mean position loss: 22.23945055246353\n",
      "Training FF1:  32%|██████▎             | 1015/3201 [30:45<1:04:25,  1.77s/batch]Batch 1000/3201 Done, mean position loss: 21.735412521362306\n",
      "Training FF1:  32%|██████▉               | 1017/3201 [30:47<55:33,  1.53s/batch]Batch 1000/3201 Done, mean position loss: 21.36369369983673\n",
      "Training FF1:  32%|██████▎             | 1014/3201 [30:47<1:01:05,  1.68s/batch]Batch 1000/3201 Done, mean position loss: 21.574913327693938\n",
      "Training FF1:  31%|██████▎             | 1004/3201 [30:50<1:06:27,  1.81s/batch]Batch 1000/3201 Done, mean position loss: 21.517898788452147\n",
      "Training FF1:  32%|███████               | 1025/3201 [30:53<57:28,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 21.97037692308426\n",
      "Training FF1:  32%|███████               | 1024/3201 [30:55<56:56,  1.57s/batch]Batch 1000/3201 Done, mean position loss: 21.700294890403747\n",
      "Training FF1:  31%|██████▉               | 1006/3201 [30:59<54:32,  1.49s/batch]Batch 1000/3201 Done, mean position loss: 21.91452209472656\n",
      "Batch 1000/3201 Done, mean position loss: 21.75637844324112\n",
      "Training FF1:  32%|██████▉               | 1018/3201 [31:09<57:29,  1.58s/batch]Batch 1000/3201 Done, mean position loss: 22.81005492925644\n",
      "Training FF1:  32%|██████▍             | 1039/3201 [31:15<1:03:19,  1.76s/batch]Batch 1000/3201 Done, mean position loss: 21.7338068151474\n",
      "Training FF1:  34%|██████▊             | 1088/3201 [33:11<1:07:36,  1.92s/batch]Batch 1100/3201 Done, mean position loss: 22.55094741344452\n",
      "Training FF1:  34%|███████▍              | 1076/3201 [33:16<57:32,  1.62s/batch]Batch 1100/3201 Done, mean position loss: 22.21295761346817\n",
      "Training FF1:  34%|███████▌              | 1094/3201 [33:17<58:53,  1.68s/batch]Batch 1100/3201 Done, mean position loss: 22.035896797180175\n",
      "Training FF1:  34%|██████▊             | 1099/3201 [33:17<1:00:54,  1.74s/batch]Batch 1100/3201 Done, mean position loss: 22.45840460062027\n",
      "Training FF1:  34%|██████▊             | 1082/3201 [33:20<1:12:01,  2.04s/batch]Batch 1100/3201 Done, mean position loss: 21.802736668586732\n",
      "Training FF1:  34%|██████▊             | 1100/3201 [33:20<1:12:05,  2.06s/batch]Batch 1100/3201 Done, mean position loss: 22.701054291725157\n",
      "Training FF1:  34%|██████▉             | 1102/3201 [33:22<1:03:43,  1.82s/batch]Batch 1100/3201 Done, mean position loss: 21.960451714992523\n",
      "Training FF1:  34%|██████▊             | 1094/3201 [33:23<1:04:55,  1.85s/batch]Batch 1100/3201 Done, mean position loss: 21.780155074596408\n",
      "Training FF1:  34%|██████▊             | 1096/3201 [33:23<1:06:12,  1.89s/batch]Batch 1100/3201 Done, mean position loss: 21.813943850994107\n",
      "Training FF1:  34%|███████▌              | 1098/3201 [33:24<57:35,  1.64s/batch]Batch 1100/3201 Done, mean position loss: 22.02602218389511\n",
      "Training FF1:  34%|██████▊             | 1097/3201 [33:28<1:06:00,  1.88s/batch]Batch 1100/3201 Done, mean position loss: 21.88017391204834\n",
      "Training FF1:  34%|██████▊             | 1093/3201 [33:29<1:10:10,  2.00s/batch]Batch 1100/3201 Done, mean position loss: 21.414619443416594\n",
      "Training FF1:  34%|██████▊             | 1094/3201 [33:31<1:13:22,  2.09s/batch]Batch 1100/3201 Done, mean position loss: 21.695748724937438\n",
      "Training FF1:  34%|██████▊             | 1098/3201 [33:32<1:06:27,  1.90s/batch]Batch 1100/3201 Done, mean position loss: 21.701192240715027\n",
      "Training FF1:  34%|██████▊             | 1098/3201 [33:34<1:04:05,  1.83s/batch]Batch 1100/3201 Done, mean position loss: 21.953212318420412\n",
      "Training FF1:  35%|██████▉             | 1108/3201 [33:36<1:11:30,  2.05s/batch]Batch 1100/3201 Done, mean position loss: 21.93885736465454\n",
      "Training FF1:  34%|███████▌              | 1099/3201 [33:37<50:58,  1.46s/batch]Batch 1100/3201 Done, mean position loss: 22.02721570730209\n",
      "Training FF1:  35%|███████▌              | 1107/3201 [33:40<51:20,  1.47s/batch]Batch 1100/3201 Done, mean position loss: 21.978874485492703\n",
      "Training FF1:  34%|██████▊             | 1099/3201 [33:40<1:06:17,  1.89s/batch]Batch 1100/3201 Done, mean position loss: 21.893120598793033\n",
      "Training FF1:  35%|███████▋              | 1119/3201 [33:41<54:57,  1.58s/batch]Batch 1100/3201 Done, mean position loss: 21.82484698057175\n",
      "Training FF1:  35%|███████▋              | 1111/3201 [33:42<59:56,  1.72s/batch]Batch 1100/3201 Done, mean position loss: 21.806557059288025\n",
      "Training FF1:  35%|██████▉             | 1106/3201 [33:42<1:03:23,  1.82s/batch]Batch 1100/3201 Done, mean position loss: 21.607503650188445\n",
      "Training FF1:  35%|███████▋              | 1118/3201 [33:44<48:43,  1.40s/batch]Batch 1100/3201 Done, mean position loss: 21.340802233219147\n",
      "Training FF1:  34%|██████▊             | 1088/3201 [33:44<1:11:55,  2.04s/batch]Batch 1100/3201 Done, mean position loss: 21.970766806602477\n",
      "Training FF1:  35%|███████▌              | 1107/3201 [33:44<57:48,  1.66s/batch]Batch 1100/3201 Done, mean position loss: 21.84829513311386\n",
      "Training FF1:  34%|███████▌              | 1104/3201 [33:44<53:01,  1.52s/batch]Batch 1100/3201 Done, mean position loss: 22.3302078127861\n",
      "Training FF1:  34%|███████▌              | 1104/3201 [33:46<58:21,  1.67s/batch]Batch 1100/3201 Done, mean position loss: 22.170095221996306\n",
      "Training FF1:  35%|███████▌              | 1108/3201 [33:45<59:50,  1.72s/batch]Batch 1100/3201 Done, mean position loss: 21.492355687618257\n",
      "Training FF1:  35%|██████▉             | 1117/3201 [33:50<1:01:29,  1.77s/batch]Batch 1100/3201 Done, mean position loss: 22.160569224357605\n",
      "Training FF1:  34%|███████▍              | 1085/3201 [33:50<48:02,  1.36s/batch]Batch 1100/3201 Done, mean position loss: 22.227274742126465\n",
      "Training FF1:  35%|███████▋              | 1113/3201 [33:51<58:22,  1.68s/batch]Batch 1100/3201 Done, mean position loss: 21.661589672565462\n",
      "Training FF1:  35%|███████▋              | 1119/3201 [33:53<58:41,  1.69s/batch]Batch 1100/3201 Done, mean position loss: 21.82976676464081\n",
      "Training FF1:  35%|██████▉             | 1118/3201 [33:54<1:12:01,  2.07s/batch]Batch 1100/3201 Done, mean position loss: 21.46806950330734\n",
      "Training FF1:  35%|██████▉             | 1115/3201 [33:54<1:01:16,  1.76s/batch]Batch 1100/3201 Done, mean position loss: 22.162490541934964\n",
      "Training FF1:  34%|██████▉             | 1104/3201 [33:56<1:05:01,  1.86s/batch]Batch 1100/3201 Done, mean position loss: 21.32334946393967\n",
      "Training FF1:  35%|██████▉             | 1110/3201 [34:03<1:12:56,  2.09s/batch]Batch 1100/3201 Done, mean position loss: 21.649698112010956\n",
      "Training FF1:  34%|██████▉             | 1103/3201 [34:07<1:01:57,  1.77s/batch]Batch 1100/3201 Done, mean position loss: 21.840300467014313\n",
      "Training FF1:  35%|███████▋              | 1118/3201 [34:10<58:53,  1.70s/batch]Batch 1100/3201 Done, mean position loss: 21.733848252296447\n",
      "Training FF1:  35%|███████▋              | 1116/3201 [34:18<59:02,  1.70s/batch]Batch 1100/3201 Done, mean position loss: 22.727964332103728\n",
      "Training FF1:  35%|██████▉             | 1109/3201 [34:24<1:06:28,  1.91s/batch]Batch 1100/3201 Done, mean position loss: 21.672612891197204\n",
      "Training FF1:  36%|███████▉              | 1159/3201 [36:05<52:57,  1.56s/batch]Batch 1200/3201 Done, mean position loss: 22.076020846366884\n",
      "Training FF1:  37%|████████              | 1179/3201 [36:07<55:21,  1.64s/batch]Batch 1200/3201 Done, mean position loss: 22.51171058893204\n",
      "Training FF1:  37%|████████▏             | 1184/3201 [36:15<51:32,  1.53s/batch]Batch 1200/3201 Done, mean position loss: 21.8950044965744\n",
      "Training FF1:  37%|███████▎            | 1179/3201 [36:18<1:04:03,  1.90s/batch]Batch 1200/3201 Done, mean position loss: 22.619172153472903\n",
      "Training FF1:  36%|███████▎            | 1166/3201 [36:19<1:00:55,  1.80s/batch]Batch 1200/3201 Done, mean position loss: 21.793791038990022\n",
      "Training FF1:  37%|███████▎            | 1171/3201 [36:20<1:03:37,  1.88s/batch]Batch 1200/3201 Done, mean position loss: 21.706909081935883\n",
      "Training FF1:  38%|████████▎             | 1202/3201 [36:21<45:38,  1.37s/batch]Batch 1200/3201 Done, mean position loss: 22.250096657276153\n",
      "Training FF1:  37%|███████▍            | 1195/3201 [36:22<1:06:30,  1.99s/batch]Batch 1200/3201 Done, mean position loss: 21.810451526641845\n",
      "Training FF1:  37%|████████▏             | 1184/3201 [36:22<51:36,  1.54s/batch]Batch 1200/3201 Done, mean position loss: 21.646470766067505\n",
      "Training FF1:  37%|███████▍            | 1199/3201 [36:26<1:02:11,  1.86s/batch]Batch 1200/3201 Done, mean position loss: 21.597651338577272\n",
      "Training FF1:  37%|███████▎            | 1180/3201 [36:29<1:06:44,  1.98s/batch]Batch 1200/3201 Done, mean position loss: 21.36973788976669\n",
      "Training FF1:  37%|████████▏             | 1191/3201 [36:31<50:39,  1.51s/batch]Batch 1200/3201 Done, mean position loss: 21.847802274227142\n",
      "Training FF1:  37%|███████▍            | 1194/3201 [36:31<1:04:14,  1.92s/batch]Batch 1200/3201 Done, mean position loss: 21.94553000688553\n",
      "Training FF1:  38%|████████▎             | 1202/3201 [36:32<49:16,  1.48s/batch]Batch 1200/3201 Done, mean position loss: 21.85963772058487\n",
      "Training FF1:  37%|███████▍            | 1192/3201 [36:33<1:01:58,  1.85s/batch]Batch 1200/3201 Done, mean position loss: 21.926845667362215\n",
      "Training FF1:  37%|███████▎            | 1177/3201 [36:37<1:09:37,  2.06s/batch]Batch 1200/3201 Done, mean position loss: 21.90947509050369\n",
      "Training FF1:  38%|████████▎             | 1213/3201 [36:40<58:31,  1.77s/batch]Batch 1200/3201 Done, mean position loss: 21.774548871517183\n",
      "Training FF1:  38%|███████▌            | 1205/3201 [36:39<1:04:07,  1.93s/batch]Batch 1200/3201 Done, mean position loss: 22.04471870660782\n",
      "Training FF1:  38%|████████▎             | 1214/3201 [36:42<55:02,  1.66s/batch]Batch 1200/3201 Done, mean position loss: 21.91768264055252\n",
      "Training FF1:  37%|███████▍            | 1184/3201 [36:42<1:11:09,  2.12s/batch]Batch 1200/3201 Done, mean position loss: 21.871384265422822\n",
      "Training FF1:  37%|███████▍            | 1197/3201 [36:41<1:08:36,  2.05s/batch]Batch 1200/3201 Done, mean position loss: 21.41136382102966\n",
      "Training FF1:  38%|███████▋            | 1221/3201 [36:43<1:01:25,  1.86s/batch]Batch 1200/3201 Done, mean position loss: 21.91165323495865\n",
      "Training FF1:  38%|███████▌            | 1214/3201 [36:45<1:02:11,  1.88s/batch]Batch 1200/3201 Done, mean position loss: 21.74081772327423\n",
      "Training FF1:  38%|████████▎             | 1205/3201 [36:46<53:00,  1.59s/batch]Batch 1200/3201 Done, mean position loss: 21.921839821338654\n",
      "Training FF1:  38%|███████▌            | 1211/3201 [36:46<1:10:06,  2.11s/batch]Batch 1200/3201 Done, mean position loss: 21.611138365268708\n",
      "Training FF1:  38%|████████▎             | 1202/3201 [36:46<50:51,  1.53s/batch]Batch 1200/3201 Done, mean position loss: 21.304473726749418\n",
      "Training FF1:  38%|███████▌            | 1202/3201 [36:47<1:04:22,  1.93s/batch]Batch 1200/3201 Done, mean position loss: 21.54726435422897\n",
      "Training FF1:  37%|████████▏             | 1190/3201 [36:48<58:39,  1.75s/batch]Batch 1200/3201 Done, mean position loss: 21.805565621852875\n",
      "Training FF1:  37%|███████▍            | 1198/3201 [36:50<1:08:16,  2.05s/batch]Batch 1200/3201 Done, mean position loss: 22.159893424510955\n",
      "Training FF1:  38%|████████▍             | 1219/3201 [36:50<59:59,  1.82s/batch]Batch 1200/3201 Done, mean position loss: 21.41040329694748\n",
      "Training FF1:  38%|███████▌            | 1209/3201 [36:53<1:02:11,  1.87s/batch]Batch 1200/3201 Done, mean position loss: 22.241206729412077\n",
      "Training FF1:  38%|███████▌            | 1201/3201 [36:53<1:08:19,  2.05s/batch]Batch 1200/3201 Done, mean position loss: 21.30957547426224\n",
      "Training FF1:  38%|███████▋            | 1221/3201 [36:54<1:01:55,  1.88s/batch]Batch 1200/3201 Done, mean position loss: 22.06998627901077\n",
      "Training FF1:  38%|███████▌            | 1206/3201 [36:59<1:10:38,  2.12s/batch]Batch 1200/3201 Done, mean position loss: 22.037410352230072\n",
      "Training FF1:  38%|████████▎             | 1207/3201 [36:59<54:17,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.755389535427092\n",
      "Training FF1:  38%|████████▍             | 1224/3201 [37:07<45:57,  1.39s/batch]Batch 1200/3201 Done, mean position loss: 21.606836583614353\n",
      "Training FF1:  38%|███████▌            | 1217/3201 [37:11<1:11:53,  2.17s/batch]Batch 1200/3201 Done, mean position loss: 21.75292409658432\n",
      "Training FF1:  37%|████████▏             | 1197/3201 [37:11<50:10,  1.50s/batch]Batch 1200/3201 Done, mean position loss: 21.657972950935363\n",
      "Training FF1:  38%|███████▋            | 1226/3201 [37:18<1:06:18,  2.01s/batch]Batch 1200/3201 Done, mean position loss: 21.634422616958616\n",
      "Training FF1:  39%|███████▋            | 1234/3201 [37:23<1:01:15,  1.87s/batch]Batch 1200/3201 Done, mean position loss: 22.658027968406678\n",
      "Training FF1:  40%|████████▊             | 1285/3201 [39:01<52:39,  1.65s/batch]Batch 1300/3201 Done, mean position loss: 21.99856571674347\n",
      "Training FF1:  40%|████████▊             | 1284/3201 [39:08<55:13,  1.73s/batch]Batch 1300/3201 Done, mean position loss: 22.509539880752563\n",
      "Training FF1:  40%|████████▊             | 1290/3201 [39:12<58:40,  1.84s/batch]Batch 1300/3201 Done, mean position loss: 21.778955366611477\n",
      "Training FF1:  41%|████████▉             | 1301/3201 [39:12<47:30,  1.50s/batch]Batch 1300/3201 Done, mean position loss: 22.233306629657747\n",
      "Training FF1:  41%|████████▏           | 1307/3201 [39:13<1:06:35,  2.11s/batch]Batch 1300/3201 Done, mean position loss: 21.847987785339356\n",
      "Training FF1:  40%|████████▋             | 1271/3201 [39:14<47:05,  1.46s/batch]Batch 1300/3201 Done, mean position loss: 21.778914525508878\n",
      "Training FF1:  40%|████████▉             | 1295/3201 [39:14<53:01,  1.67s/batch]Batch 1300/3201 Done, mean position loss: 22.608016734123233\n",
      "Training FF1:  40%|████████▉             | 1292/3201 [39:15<55:19,  1.74s/batch]Batch 1300/3201 Done, mean position loss: 21.61209642410278\n",
      "Training FF1:  41%|████████            | 1297/3201 [39:22<1:06:11,  2.09s/batch]Batch 1300/3201 Done, mean position loss: 21.361167221069337\n",
      "Training FF1:  40%|███████▉            | 1270/3201 [39:23<1:13:02,  2.27s/batch]Batch 1300/3201 Done, mean position loss: 21.59123753786087\n",
      "Training FF1:  41%|████████▏           | 1302/3201 [39:24<1:00:24,  1.91s/batch]Batch 1300/3201 Done, mean position loss: 21.52219009399414\n",
      "Training FF1:  41%|████████▉             | 1300/3201 [39:27<46:09,  1.46s/batch]Batch 1300/3201 Done, mean position loss: 22.01933422088623\n",
      "Training FF1:  41%|████████▉             | 1304/3201 [39:28<44:24,  1.40s/batch]Batch 1300/3201 Done, mean position loss: 21.767248723506928\n",
      "Training FF1:  41%|█████████             | 1311/3201 [39:30<48:01,  1.52s/batch]Batch 1300/3201 Done, mean position loss: 21.904029562473298\n",
      "Training FF1:  40%|████████▊             | 1288/3201 [39:31<55:19,  1.74s/batch]Batch 1300/3201 Done, mean position loss: 21.86668989419937\n",
      "Training FF1:  41%|████████            | 1298/3201 [39:32<1:04:49,  2.04s/batch]Batch 1300/3201 Done, mean position loss: 21.848621230125428\n",
      "Training FF1:  40%|████████▉             | 1295/3201 [39:33<51:35,  1.62s/batch]Batch 1300/3201 Done, mean position loss: 21.749004077911376\n",
      "Training FF1:  41%|█████████             | 1314/3201 [39:34<51:18,  1.63s/batch]Batch 1300/3201 Done, mean position loss: 21.91015400886536\n",
      "Training FF1:  41%|████████▉             | 1300/3201 [39:34<58:23,  1.84s/batch]Batch 1300/3201 Done, mean position loss: 21.786904168128967\n",
      "Training FF1:  40%|████████▊             | 1283/3201 [39:36<56:47,  1.78s/batch]Batch 1300/3201 Done, mean position loss: 21.38841128110886\n",
      "Training FF1:  41%|████████▏           | 1307/3201 [39:39<1:07:52,  2.15s/batch]Batch 1300/3201 Done, mean position loss: 21.86157391309738\n",
      "Training FF1:  41%|████████▉             | 1305/3201 [39:39<47:31,  1.50s/batch]Batch 1300/3201 Done, mean position loss: 21.380417895317077\n",
      "Training FF1:  41%|████████▉             | 1297/3201 [39:42<57:55,  1.83s/batch]Batch 1300/3201 Done, mean position loss: 21.831840879917145\n",
      "Training FF1:  41%|█████████             | 1318/3201 [39:43<59:34,  1.90s/batch]Batch 1300/3201 Done, mean position loss: 21.508767781257628\n",
      "Training FF1:  41%|████████▉             | 1308/3201 [39:44<52:15,  1.66s/batch]Batch 1300/3201 Done, mean position loss: 21.280869634151458\n",
      "Training FF1:  41%|████████▏           | 1317/3201 [39:45<1:03:19,  2.02s/batch]Batch 1300/3201 Done, mean position loss: 21.586938745975495\n",
      "Training FF1:  41%|█████████             | 1320/3201 [39:47<59:15,  1.89s/batch]Batch 1300/3201 Done, mean position loss: 22.101965708732607\n",
      "Training FF1:  41%|█████████             | 1321/3201 [39:48<57:08,  1.82s/batch]Batch 1300/3201 Done, mean position loss: 21.903575537204745\n",
      "Training FF1:  41%|█████████             | 1312/3201 [39:50<53:00,  1.68s/batch]Batch 1300/3201 Done, mean position loss: 21.697081487178803\n",
      "Training FF1:  41%|████████▉             | 1303/3201 [39:50<46:34,  1.47s/batch]Batch 1300/3201 Done, mean position loss: 21.76197016000748\n",
      "Training FF1:  41%|████████▉             | 1301/3201 [39:50<58:56,  1.86s/batch]Batch 1300/3201 Done, mean position loss: 21.89506371974945\n",
      "Training FF1:  41%|█████████             | 1319/3201 [39:53<50:13,  1.60s/batch]Batch 1300/3201 Done, mean position loss: 22.193562142848968\n",
      "Training FF1:  41%|████████▏           | 1306/3201 [39:56<1:03:15,  2.00s/batch]Batch 1300/3201 Done, mean position loss: 21.981109046936034\n",
      "Training FF1:  41%|█████████             | 1315/3201 [39:57<58:39,  1.87s/batch]Batch 1300/3201 Done, mean position loss: 21.736343972682953\n",
      "Training FF1:  41%|████████▏           | 1306/3201 [39:59<1:00:10,  1.91s/batch]Batch 1300/3201 Done, mean position loss: 21.30265629529953\n",
      "Training FF1:  41%|████████▉             | 1303/3201 [40:02<47:43,  1.51s/batch]Batch 1300/3201 Done, mean position loss: 21.560445327758792\n",
      "Training FF1:  41%|████████▏           | 1315/3201 [40:07<1:00:54,  1.94s/batch]Batch 1300/3201 Done, mean position loss: 21.592400543689727\n",
      "Training FF1:  41%|█████████             | 1316/3201 [40:09<59:31,  1.89s/batch]Batch 1300/3201 Done, mean position loss: 21.659763038158417\n",
      "Training FF1:  41%|████████▎           | 1322/3201 [40:22<1:02:01,  1.98s/batch]Batch 1300/3201 Done, mean position loss: 22.617530672550203\n",
      "Training FF1:  41%|█████████             | 1311/3201 [40:23<48:07,  1.53s/batch]Batch 1300/3201 Done, mean position loss: 21.598211052417753\n",
      "Training FF1:  43%|█████████▌            | 1386/3201 [41:56<45:42,  1.51s/batch]Batch 1400/3201 Done, mean position loss: 21.991372623443603\n",
      "Training FF1:  43%|█████████▍            | 1378/3201 [41:59<57:30,  1.89s/batch]Batch 1400/3201 Done, mean position loss: 22.568739483356474\n",
      "Training FF1:  43%|█████████▍            | 1372/3201 [42:01<59:54,  1.97s/batch]Batch 1400/3201 Done, mean position loss: 22.463392424583432\n",
      "Training FF1:  43%|█████████▍            | 1376/3201 [42:02<51:00,  1.68s/batch]Batch 1400/3201 Done, mean position loss: 21.554154932498932\n",
      "Training FF1:  43%|████████▌           | 1380/3201 [42:06<1:04:04,  2.11s/batch]Batch 1400/3201 Done, mean position loss: 22.171077802181244\n",
      "Training FF1:  43%|█████████▍            | 1380/3201 [42:06<57:12,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 21.75443358182907\n",
      "Training FF1:  43%|█████████▍            | 1379/3201 [42:06<52:10,  1.72s/batch]Batch 1400/3201 Done, mean position loss: 21.762564415931703\n",
      "Training FF1:  43%|█████████▍            | 1374/3201 [42:07<54:44,  1.80s/batch]Batch 1400/3201 Done, mean position loss: 21.791456394195556\n",
      "Training FF1:  44%|█████████▌            | 1398/3201 [42:11<50:23,  1.68s/batch]Batch 1400/3201 Done, mean position loss: 21.487316837310793\n",
      "Training FF1:  44%|█████████▌            | 1396/3201 [42:16<45:08,  1.50s/batch]Batch 1400/3201 Done, mean position loss: 21.330182371139525\n",
      "Training FF1:  44%|█████████▌            | 1398/3201 [42:15<45:41,  1.52s/batch]Batch 1400/3201 Done, mean position loss: 21.815003879070282\n",
      "Training FF1:  43%|█████████▍            | 1381/3201 [42:17<45:12,  1.49s/batch]Batch 1400/3201 Done, mean position loss: 21.505819427967072\n",
      "Training FF1:  43%|█████████▌            | 1389/3201 [42:18<45:07,  1.49s/batch]Batch 1400/3201 Done, mean position loss: 21.717525465488436\n",
      "Training FF1:  44%|████████▋           | 1395/3201 [42:20<1:01:33,  2.05s/batch]Batch 1400/3201 Done, mean position loss: 21.742767808437346\n",
      "Training FF1:  44%|█████████▋            | 1411/3201 [42:21<48:48,  1.64s/batch]Batch 1400/3201 Done, mean position loss: 21.847661623954775\n",
      "Training FF1:  44%|█████████▋            | 1403/3201 [42:22<44:38,  1.49s/batch]Batch 1400/3201 Done, mean position loss: 21.82845001935959\n",
      "Training FF1:  43%|█████████▍            | 1372/3201 [42:24<54:36,  1.79s/batch]Batch 1400/3201 Done, mean position loss: 21.681254010200497\n",
      "Training FF1:  44%|█████████▋            | 1417/3201 [42:24<55:48,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 21.807887835502626\n",
      "Training FF1:  44%|█████████▌            | 1395/3201 [42:30<55:23,  1.84s/batch]Batch 1400/3201 Done, mean position loss: 21.33793735742569\n",
      "Training FF1:  44%|█████████▌            | 1394/3201 [42:30<56:40,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 22.03808106660843\n",
      "Training FF1:  44%|█████████▋            | 1417/3201 [42:33<52:16,  1.76s/batch]Batch 1400/3201 Done, mean position loss: 21.333273415565493\n",
      "Training FF1:  45%|█████████▊            | 1425/3201 [42:38<49:32,  1.67s/batch]Batch 1400/3201 Done, mean position loss: 21.826668150424958\n",
      "Training FF1:  43%|████████▌           | 1378/3201 [42:40<1:05:51,  2.17s/batch]Batch 1400/3201 Done, mean position loss: 21.707797045707704\n",
      "Training FF1:  44%|█████████▋            | 1401/3201 [42:40<50:27,  1.68s/batch]Batch 1400/3201 Done, mean position loss: 21.45640385627747\n",
      "Training FF1:  44%|█████████▋            | 1416/3201 [42:40<46:02,  1.55s/batch]Batch 1400/3201 Done, mean position loss: 21.839077842235564\n",
      "Training FF1:  44%|█████████▋            | 1414/3201 [42:42<47:06,  1.58s/batch]Batch 1400/3201 Done, mean position loss: 21.651679039001465\n",
      "Training FF1:  44%|█████████▋            | 1417/3201 [42:43<51:05,  1.72s/batch]Batch 1400/3201 Done, mean position loss: 21.23115438938141\n",
      "Training FF1:  44%|█████████▋            | 1415/3201 [42:44<49:39,  1.67s/batch]Batch 1400/3201 Done, mean position loss: 22.165729107856748\n",
      "Batch 1400/3201 Done, mean position loss: 22.046072850227354\n",
      "Training FF1:  44%|█████████▋            | 1415/3201 [42:44<49:00,  1.65s/batch]Batch 1400/3201 Done, mean position loss: 21.54360184907913\n",
      "Training FF1:  44%|█████████▋            | 1404/3201 [42:44<43:03,  1.44s/batch]Batch 1400/3201 Done, mean position loss: 21.721512665748598\n",
      "Training FF1:  44%|█████████▋            | 1417/3201 [42:48<55:16,  1.86s/batch]Batch 1400/3201 Done, mean position loss: 21.835145702362063\n",
      "Training FF1:  45%|█████████▊            | 1427/3201 [42:49<54:33,  1.85s/batch]Batch 1400/3201 Done, mean position loss: 21.872153604030608\n",
      "Training FF1:  45%|█████████▊            | 1427/3201 [42:50<55:30,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 21.688135852813723\n",
      "Training FF1:  44%|█████████▋            | 1413/3201 [42:55<53:30,  1.80s/batch]Batch 1400/3201 Done, mean position loss: 21.52680254936218\n",
      "Training FF1:  44%|█████████▊            | 1421/3201 [42:55<47:56,  1.62s/batch]Batch 1400/3201 Done, mean position loss: 21.273475189208984\n",
      "Training FF1:  44%|█████████▌            | 1400/3201 [42:59<54:46,  1.82s/batch]Batch 1400/3201 Done, mean position loss: 21.579699630737302\n",
      "Training FF1:  45%|█████████▊            | 1428/3201 [43:00<53:36,  1.81s/batch]Batch 1400/3201 Done, mean position loss: 21.632805457115175\n",
      "Training FF1:  45%|█████████▊            | 1425/3201 [43:15<50:52,  1.72s/batch]Batch 1400/3201 Done, mean position loss: 22.590086176395417\n",
      "Training FF1:  44%|█████████▋            | 1405/3201 [43:22<55:59,  1.87s/batch]Batch 1400/3201 Done, mean position loss: 21.585016832351684\n",
      "Training FF1:  47%|██████████▎           | 1492/3201 [44:43<50:28,  1.77s/batch]Batch 1500/3201 Done, mean position loss: 21.908492102622986\n",
      "Training FF1:  46%|██████████▏           | 1478/3201 [44:52<53:39,  1.87s/batch]Batch 1500/3201 Done, mean position loss: 22.51258271932602\n",
      "Training FF1:  47%|██████████▎           | 1492/3201 [44:56<49:32,  1.74s/batch]Batch 1500/3201 Done, mean position loss: 22.483912675380708\n",
      "Training FF1:  47%|██████████▎           | 1493/3201 [44:58<47:59,  1.69s/batch]Batch 1500/3201 Done, mean position loss: 21.42974867105484\n",
      "Training FF1:  46%|█████████▏          | 1476/3201 [44:58<1:00:40,  2.11s/batch]Batch 1500/3201 Done, mean position loss: 21.686464340686797\n",
      "Training FF1:  46%|██████████            | 1472/3201 [45:00<50:19,  1.75s/batch]Batch 1500/3201 Done, mean position loss: 21.724355096817014\n",
      "Training FF1:  46%|██████████▏           | 1475/3201 [45:00<56:14,  1.96s/batch]Batch 1500/3201 Done, mean position loss: 21.71514479398727\n",
      "Training FF1:  46%|█████████▏          | 1479/3201 [45:01<1:01:56,  2.16s/batch]Batch 1500/3201 Done, mean position loss: 22.13843400478363\n",
      "Training FF1:  47%|██████████▎           | 1495/3201 [45:02<50:39,  1.78s/batch]Batch 1500/3201 Done, mean position loss: 21.458135888576507\n",
      "Training FF1:  46%|██████████            | 1459/3201 [45:06<50:02,  1.72s/batch]Batch 1500/3201 Done, mean position loss: 21.333078258037567\n",
      "Training FF1:  46%|██████████▏           | 1485/3201 [45:10<47:56,  1.68s/batch]Batch 1500/3201 Done, mean position loss: 21.77841176748276\n",
      "Training FF1:  46%|██████████▏           | 1486/3201 [45:11<51:49,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 21.4847447681427\n",
      "Training FF1:  47%|██████████▎           | 1508/3201 [45:13<54:24,  1.93s/batch]Batch 1500/3201 Done, mean position loss: 21.8077774310112\n",
      "Training FF1:  46%|██████████▏           | 1484/3201 [45:13<50:16,  1.76s/batch]Batch 1500/3201 Done, mean position loss: 21.69411853790283\n",
      "Training FF1:  46%|██████████▏           | 1487/3201 [45:14<52:45,  1.85s/batch]Batch 1500/3201 Done, mean position loss: 21.63546050310135\n",
      "Training FF1:  47%|█████████▎          | 1491/3201 [45:17<1:00:59,  2.14s/batch]Batch 1500/3201 Done, mean position loss: 21.782316553592683\n",
      "Training FF1:  47%|██████████▎           | 1493/3201 [45:20<53:38,  1.88s/batch]Batch 1500/3201 Done, mean position loss: 21.97394927740097\n",
      "Training FF1:  47%|██████████▎           | 1506/3201 [45:21<50:06,  1.77s/batch]Batch 1500/3201 Done, mean position loss: 21.664984545707703\n",
      "Training FF1:  47%|██████████▏           | 1490/3201 [45:26<55:01,  1.93s/batch]Batch 1500/3201 Done, mean position loss: 21.763731462955477\n",
      "Training FF1:  47%|██████████▎           | 1507/3201 [45:27<45:38,  1.62s/batch]Batch 1500/3201 Done, mean position loss: 21.33520521402359\n",
      "Training FF1:  47%|█████████▎          | 1497/3201 [45:30<1:02:28,  2.20s/batch]Batch 1500/3201 Done, mean position loss: 21.61209362268448\n",
      "Training FF1:  47%|██████████▍           | 1510/3201 [45:32<47:16,  1.68s/batch]Batch 1500/3201 Done, mean position loss: 21.322013189792635\n",
      "Training FF1:  47%|██████████▎           | 1500/3201 [45:33<50:48,  1.79s/batch]Batch 1500/3201 Done, mean position loss: 21.86009332418442\n",
      "Training FF1:  48%|██████████▍           | 1522/3201 [45:34<44:38,  1.60s/batch]Batch 1500/3201 Done, mean position loss: 21.652256972789765\n",
      "Training FF1:  47%|██████████▎           | 1499/3201 [45:35<43:52,  1.55s/batch]Batch 1500/3201 Done, mean position loss: 22.085513563156127\n",
      "Training FF1:  47%|█████████▎          | 1496/3201 [45:37<1:04:22,  2.27s/batch]Batch 1500/3201 Done, mean position loss: 21.809471533298492\n",
      "Training FF1:  47%|██████████▎           | 1508/3201 [45:39<51:54,  1.84s/batch]Batch 1500/3201 Done, mean position loss: 21.395307965278626\n",
      "Training FF1:  47%|██████████▎           | 1504/3201 [45:39<45:15,  1.60s/batch]Batch 1500/3201 Done, mean position loss: 21.24770243167877\n",
      "Training FF1:  47%|██████████▎           | 1498/3201 [45:41<55:28,  1.95s/batch]Batch 1500/3201 Done, mean position loss: 22.014310240745544\n",
      "Training FF1:  47%|██████████▎           | 1499/3201 [45:41<43:42,  1.54s/batch]Batch 1500/3201 Done, mean position loss: 21.85169068098068\n",
      "Training FF1:  47%|██████████▍           | 1515/3201 [45:44<46:01,  1.64s/batch]Batch 1500/3201 Done, mean position loss: 21.697417557239532\n",
      "Training FF1:  47%|██████████▍           | 1518/3201 [45:44<54:35,  1.95s/batch]Batch 1500/3201 Done, mean position loss: 21.65655940771103\n",
      "Training FF1:  48%|██████████▌           | 1533/3201 [45:45<56:32,  2.03s/batch]Batch 1500/3201 Done, mean position loss: 21.53717430591583\n",
      "Training FF1:  47%|██████████▎           | 1508/3201 [45:47<52:43,  1.87s/batch]Batch 1500/3201 Done, mean position loss: 21.504866149425506\n",
      "Training FF1:  48%|██████████▍           | 1526/3201 [45:51<56:33,  2.03s/batch]Batch 1500/3201 Done, mean position loss: 21.796413371562956\n",
      "Training FF1:  46%|█████████▎          | 1483/3201 [45:53<1:01:23,  2.14s/batch]Batch 1500/3201 Done, mean position loss: 21.459231936931612\n",
      "Training FF1:  47%|██████████▍           | 1510/3201 [45:58<55:16,  1.96s/batch]Batch 1500/3201 Done, mean position loss: 21.62878669977188\n",
      "Training FF1:  47%|██████████▎           | 1509/3201 [45:59<48:15,  1.71s/batch]Batch 1500/3201 Done, mean position loss: 21.27578646659851\n",
      "Training FF1:  47%|██████████▍           | 1518/3201 [46:13<44:47,  1.60s/batch]Batch 1500/3201 Done, mean position loss: 22.513620483875272\n",
      "Training FF1:  48%|██████████▍           | 1524/3201 [46:25<59:43,  2.14s/batch]Batch 1500/3201 Done, mean position loss: 21.56493414402008\n",
      "Training FF1:  50%|██████████▉           | 1588/3201 [47:46<49:27,  1.84s/batch]Batch 1600/3201 Done, mean position loss: 22.460228264331818\n",
      "Training FF1:  50%|██████████▉           | 1586/3201 [47:50<51:15,  1.90s/batch]Batch 1600/3201 Done, mean position loss: 21.880702850818636\n",
      "Training FF1:  50%|██████████▉           | 1600/3201 [47:51<47:42,  1.79s/batch]Batch 1600/3201 Done, mean position loss: 22.452219927310942\n",
      "Training FF1:  50%|███████████           | 1602/3201 [47:53<40:39,  1.53s/batch]Batch 1600/3201 Done, mean position loss: 21.382945554256438\n",
      "Training FF1:  50%|██████████▉           | 1590/3201 [47:57<44:54,  1.67s/batch]Batch 1600/3201 Done, mean position loss: 21.5946471619606\n",
      "Training FF1:  50%|██████████▉           | 1595/3201 [47:59<48:19,  1.81s/batch]Batch 1600/3201 Done, mean position loss: 21.39956558942795\n",
      "Training FF1:  50%|██████████▉           | 1590/3201 [48:00<52:46,  1.97s/batch]Batch 1600/3201 Done, mean position loss: 21.667598948478698\n",
      "Training FF1:  50%|██████████▉           | 1599/3201 [48:00<43:28,  1.63s/batch]Batch 1600/3201 Done, mean position loss: 21.691327021121978\n",
      "Training FF1:  49%|██████████▊           | 1582/3201 [48:04<46:45,  1.73s/batch]Batch 1600/3201 Done, mean position loss: 22.0847278881073\n",
      "Training FF1:  50%|██████████▉           | 1585/3201 [48:08<55:03,  2.04s/batch]Batch 1600/3201 Done, mean position loss: 21.85616656780243\n",
      "Training FF1:  50%|██████████▉           | 1585/3201 [48:09<47:57,  1.78s/batch]Batch 1600/3201 Done, mean position loss: 21.315275568962097\n",
      "Training FF1:  50%|██████████▉           | 1596/3201 [48:10<48:13,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.73781606912613\n",
      "Training FF1:  50%|███████████           | 1605/3201 [48:10<43:54,  1.65s/batch]Batch 1600/3201 Done, mean position loss: 21.460962533950806\n",
      "Training FF1:  50%|███████████           | 1609/3201 [48:13<47:48,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.614124081134797\n",
      "Training FF1:  50%|███████████           | 1609/3201 [48:17<58:28,  2.20s/batch]Batch 1600/3201 Done, mean position loss: 21.924813992977143\n",
      "Training FF1:  50%|███████████           | 1609/3201 [48:18<49:56,  1.88s/batch]Batch 1600/3201 Done, mean position loss: 21.673337931632997\n",
      "Training FF1:  51%|███████████▏          | 1619/3201 [48:18<43:10,  1.64s/batch]Batch 1600/3201 Done, mean position loss: 21.73509375810623\n",
      "Training FF1:  51%|███████████           | 1617/3201 [48:19<49:23,  1.87s/batch]Batch 1600/3201 Done, mean position loss: 21.612735140323636\n",
      "Training FF1:  51%|███████████           | 1618/3201 [48:22<55:02,  2.09s/batch]Batch 1600/3201 Done, mean position loss: 21.821530590057375\n",
      "Training FF1:  50%|███████████           | 1609/3201 [48:23<47:33,  1.79s/batch]Batch 1600/3201 Done, mean position loss: 22.060218822956084\n",
      "Training FF1:  49%|██████████▊           | 1582/3201 [48:26<48:26,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.715317127704623\n",
      "Training FF1:  50%|██████████▉           | 1591/3201 [48:27<48:14,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.558046123981477\n",
      "Training FF1:  50%|███████████           | 1607/3201 [48:27<42:48,  1.61s/batch]Batch 1600/3201 Done, mean position loss: 21.300852825641634\n",
      "Training FF1:  50%|██████████▉           | 1585/3201 [48:32<53:18,  1.98s/batch]Batch 1600/3201 Done, mean position loss: 21.302186813354492\n",
      "Training FF1:  50%|██████████▉           | 1590/3201 [48:36<46:36,  1.74s/batch]Batch 1600/3201 Done, mean position loss: 21.235645225048067\n",
      "Training FF1:  50%|██████████▉           | 1600/3201 [48:36<45:04,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.968470091819764\n",
      "Training FF1:  50%|██████████▉           | 1591/3201 [48:37<43:00,  1.60s/batch]Batch 1600/3201 Done, mean position loss: 21.74812167406082\n",
      "Training FF1:  50%|███████████           | 1609/3201 [48:38<39:36,  1.49s/batch]Batch 1600/3201 Done, mean position loss: 21.359994621276854\n",
      "Training FF1:  50%|██████████▉           | 1600/3201 [48:40<48:54,  1.83s/batch]Batch 1600/3201 Done, mean position loss: 21.815361902713775\n",
      "Training FF1:  50%|██████████▉           | 1600/3201 [48:43<49:56,  1.87s/batch]Batch 1600/3201 Done, mean position loss: 21.620235979557037\n",
      "Training FF1:  50%|███████████           | 1615/3201 [48:43<52:37,  1.99s/batch]Batch 1600/3201 Done, mean position loss: 21.49334979057312\n",
      "Training FF1:  50%|██████████▉           | 1597/3201 [48:44<46:46,  1.75s/batch]Batch 1600/3201 Done, mean position loss: 21.62386972665787\n",
      "Training FF1:  51%|███████████▏          | 1626/3201 [48:44<50:06,  1.91s/batch]Batch 1600/3201 Done, mean position loss: 21.683652300834655\n",
      "Training FF1:  50%|███████████           | 1604/3201 [48:49<48:35,  1.83s/batch]Batch 1600/3201 Done, mean position loss: 21.471431555747987\n",
      "Training FF1:  50%|███████████           | 1602/3201 [48:51<52:32,  1.97s/batch]Batch 1600/3201 Done, mean position loss: 21.595795788764953\n",
      "Training FF1:  50%|██████████▉           | 1598/3201 [48:55<52:25,  1.96s/batch]Batch 1600/3201 Done, mean position loss: 21.7304873752594\n",
      "Training FF1:  50%|███████████           | 1616/3201 [48:56<47:53,  1.81s/batch]Batch 1600/3201 Done, mean position loss: 21.447989597320557\n",
      "Training FF1:  51%|███████████▏          | 1629/3201 [49:01<44:19,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.264805176258086\n",
      "Training FF1:  52%|███████████▎          | 1649/3201 [49:18<46:50,  1.81s/batch]Batch 1600/3201 Done, mean position loss: 22.468256981372832\n",
      "Training FF1:  51%|███████████▏          | 1626/3201 [49:23<49:34,  1.89s/batch]Batch 1600/3201 Done, mean position loss: 21.506223840713503\n",
      "Training FF1:  52%|███████████▍          | 1660/3201 [50:46<45:21,  1.77s/batch]Batch 1700/3201 Done, mean position loss: 21.843095490932463\n",
      "Training FF1:  52%|███████████▍          | 1664/3201 [50:48<44:20,  1.73s/batch]Batch 1700/3201 Done, mean position loss: 22.45895470380783\n",
      "Training FF1:  52%|███████████▍          | 1673/3201 [50:51<40:19,  1.58s/batch]Batch 1700/3201 Done, mean position loss: 21.361361038684844\n",
      "Training FF1:  53%|███████████▋          | 1704/3201 [50:51<32:11,  1.29s/batch]Batch 1700/3201 Done, mean position loss: 22.389412043094637\n",
      "Training FF1:  52%|███████████▌          | 1679/3201 [50:59<36:58,  1.46s/batch]Batch 1700/3201 Done, mean position loss: 22.05917994260788\n",
      "Training FF1:  53%|███████████▋          | 1697/3201 [51:01<47:36,  1.90s/batch]Batch 1700/3201 Done, mean position loss: 21.64520541667938\n",
      "Training FF1:  52%|███████████▌          | 1676/3201 [51:03<43:26,  1.71s/batch]Batch 1700/3201 Done, mean position loss: 21.831877896785734\n",
      "Training FF1:  53%|███████████▌          | 1687/3201 [51:03<47:33,  1.88s/batch]Batch 1700/3201 Done, mean position loss: 21.625901968479155\n",
      "Training FF1:  52%|███████████▌          | 1677/3201 [51:04<43:26,  1.71s/batch]Batch 1700/3201 Done, mean position loss: 21.564420788288118\n",
      "Training FF1:  52%|███████████▌          | 1678/3201 [51:06<43:45,  1.72s/batch]Batch 1700/3201 Done, mean position loss: 21.408505215644837\n",
      "Training FF1:  53%|███████████▋          | 1700/3201 [51:07<48:18,  1.93s/batch]Batch 1700/3201 Done, mean position loss: 21.29645667552948\n",
      "Training FF1:  53%|███████████▋          | 1697/3201 [51:08<38:43,  1.54s/batch]Batch 1700/3201 Done, mean position loss: 21.5453208565712\n",
      "Training FF1:  53%|███████████▋          | 1702/3201 [51:10<43:05,  1.72s/batch]Batch 1700/3201 Done, mean position loss: 21.722722008228303\n",
      "Training FF1:  53%|███████████▌          | 1691/3201 [51:10<48:08,  1.91s/batch]Batch 1700/3201 Done, mean position loss: 21.315776779651642\n",
      "Training FF1:  53%|███████████▋          | 1692/3201 [51:12<44:43,  1.78s/batch]Batch 1700/3201 Done, mean position loss: 21.83167240142822\n",
      "Training FF1:  52%|███████████▌          | 1677/3201 [51:12<48:59,  1.93s/batch]Batch 1700/3201 Done, mean position loss: 21.645054156780244\n",
      "Training FF1:  53%|███████████▌          | 1684/3201 [51:14<51:01,  2.02s/batch]Batch 1700/3201 Done, mean position loss: 21.68557968854904\n",
      "Training FF1:  53%|███████████▋          | 1705/3201 [51:16<46:30,  1.87s/batch]Batch 1700/3201 Done, mean position loss: 21.690551753044126\n",
      "Training FF1:  53%|███████████▌          | 1684/3201 [51:17<45:59,  1.82s/batch]Batch 1700/3201 Done, mean position loss: 21.86773202419281\n",
      "Training FF1:  53%|███████████▋          | 1693/3201 [51:17<52:53,  2.10s/batch]Batch 1700/3201 Done, mean position loss: 21.591175761222836\n",
      "Training FF1:  54%|███████████▊          | 1713/3201 [51:22<42:11,  1.70s/batch]Batch 1700/3201 Done, mean position loss: 22.06929999113083\n",
      "Training FF1:  53%|███████████▋          | 1707/3201 [51:26<43:20,  1.74s/batch]Batch 1700/3201 Done, mean position loss: 21.29821065187454\n",
      "Training FF1:  53%|███████████▋          | 1705/3201 [51:28<40:23,  1.62s/batch]Batch 1700/3201 Done, mean position loss: 21.261461546421053\n",
      "Training FF1:  54%|███████████▊          | 1727/3201 [51:30<39:30,  1.61s/batch]Batch 1700/3201 Done, mean position loss: 21.532659423351287\n",
      "Training FF1:  52%|███████████▍          | 1673/3201 [51:33<46:14,  1.82s/batch]Batch 1700/3201 Done, mean position loss: 21.21942401885986\n",
      "Training FF1:  53%|███████████▋          | 1703/3201 [51:36<39:33,  1.58s/batch]Batch 1700/3201 Done, mean position loss: 21.715488352775573\n",
      "Batch 1700/3201 Done, mean position loss: 21.31639513731003\n",
      "Training FF1:  54%|███████████▉          | 1730/3201 [51:36<43:36,  1.78s/batch]Batch 1700/3201 Done, mean position loss: 21.73892767190933\n",
      "Training FF1:  53%|███████████▋          | 1697/3201 [51:39<42:44,  1.70s/batch]Batch 1700/3201 Done, mean position loss: 21.945692803859707\n",
      "Training FF1:  53%|███████████▋          | 1699/3201 [51:46<55:22,  2.21s/batch]Batch 1700/3201 Done, mean position loss: 21.584890322685244\n",
      "Training FF1:  53%|███████████▌          | 1682/3201 [51:47<41:27,  1.64s/batch]Batch 1700/3201 Done, mean position loss: 21.442836191654205\n",
      "Training FF1:  54%|███████████▊          | 1726/3201 [51:47<50:38,  2.06s/batch]Batch 1700/3201 Done, mean position loss: 21.56713167667389\n",
      "Training FF1:  53%|███████████▋          | 1700/3201 [51:48<51:37,  2.06s/batch]Batch 1700/3201 Done, mean position loss: 21.460143465995788\n",
      "Training FF1:  53%|███████████▋          | 1699/3201 [51:49<40:27,  1.62s/batch]Batch 1700/3201 Done, mean position loss: 21.578584790229797\n",
      "Training FF1:  54%|███████████▊          | 1718/3201 [51:50<49:46,  2.01s/batch]Batch 1700/3201 Done, mean position loss: 21.653057405948637\n",
      "Training FF1:  53%|███████████▊          | 1711/3201 [51:53<39:26,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 21.431268939971922\n",
      "Training FF1:  54%|███████████▉          | 1733/3201 [51:59<40:19,  1.65s/batch]Batch 1700/3201 Done, mean position loss: 21.241904773712157\n",
      "Training FF1:  54%|███████████▉          | 1734/3201 [51:59<41:37,  1.70s/batch]Batch 1700/3201 Done, mean position loss: 21.69174695491791\n",
      "Training FF1:  54%|███████████▉          | 1739/3201 [52:19<41:19,  1.70s/batch]Batch 1700/3201 Done, mean position loss: 22.444006571769716\n",
      "Training FF1:  54%|███████████▊          | 1727/3201 [52:25<40:42,  1.66s/batch]Batch 1700/3201 Done, mean position loss: 21.48313273668289\n",
      "Training FF1:  55%|████████████          | 1757/3201 [53:41<38:39,  1.61s/batch]Batch 1800/3201 Done, mean position loss: 22.40002093553543\n",
      "Training FF1:  56%|████████████▏         | 1778/3201 [53:46<38:48,  1.64s/batch]Batch 1800/3201 Done, mean position loss: 22.3682275724411\n",
      "Training FF1:  55%|████████████          | 1749/3201 [53:47<42:47,  1.77s/batch]Batch 1800/3201 Done, mean position loss: 21.840538947582246\n",
      "Training FF1:  55%|████████████▏         | 1774/3201 [53:48<36:20,  1.53s/batch]Batch 1800/3201 Done, mean position loss: 21.29903033733368\n",
      "Training FF1:  55%|████████████▏         | 1765/3201 [53:55<41:28,  1.73s/batch]Batch 1800/3201 Done, mean position loss: 21.617590441703797\n",
      "Training FF1:  56%|████████████▎         | 1799/3201 [53:58<44:25,  1.90s/batch]Batch 1800/3201 Done, mean position loss: 21.78653806209564\n",
      "Training FF1:  56%|████████████▎         | 1792/3201 [54:00<42:07,  1.79s/batch]Batch 1800/3201 Done, mean position loss: 21.64236410140991\n",
      "Training FF1:  56%|████████████▎         | 1784/3201 [54:02<37:07,  1.57s/batch]Batch 1800/3201 Done, mean position loss: 22.02343580007553\n",
      "Training FF1:  57%|████████████▍         | 1811/3201 [54:03<37:55,  1.64s/batch]Batch 1800/3201 Done, mean position loss: 21.25839942932129\n",
      "Training FF1:  55%|████████████▏         | 1769/3201 [54:04<43:09,  1.81s/batch]Batch 1800/3201 Done, mean position loss: 21.79238245010376\n",
      "Training FF1:  57%|████████████▍         | 1812/3201 [54:06<42:52,  1.85s/batch]Batch 1800/3201 Done, mean position loss: 21.49986527442932\n",
      "Training FF1:  56%|████████████▎         | 1795/3201 [54:08<39:18,  1.68s/batch]Batch 1800/3201 Done, mean position loss: 21.519897589683534\n",
      "Training FF1:  56%|████████████▎         | 1798/3201 [54:08<40:32,  1.73s/batch]Batch 1800/3201 Done, mean position loss: 21.71156277656555\n",
      "Batch 1800/3201 Done, mean position loss: 22.02719158887863\n",
      "Batch 1800/3201 Done, mean position loss: 21.87084499120712\n",
      "Training FF1:  56%|████████████▎         | 1787/3201 [54:12<41:40,  1.77s/batch]Batch 1800/3201 Done, mean position loss: 21.621357402801515\n",
      "Training FF1:  56%|████████████▏         | 1777/3201 [54:12<47:07,  1.99s/batch]Batch 1800/3201 Done, mean position loss: 21.383099122047426\n",
      "Training FF1:  56%|████████████▎         | 1794/3201 [54:14<42:08,  1.80s/batch]Batch 1800/3201 Done, mean position loss: 21.56522564649582\n",
      "Training FF1:  56%|████████████▍         | 1808/3201 [54:15<43:01,  1.85s/batch]Batch 1800/3201 Done, mean position loss: 21.65441609144211\n",
      "Training FF1:  56%|████████████▍         | 1803/3201 [54:15<41:28,  1.78s/batch]Batch 1800/3201 Done, mean position loss: 21.653420324325563\n",
      "Training FF1:  57%|████████████▍         | 1813/3201 [54:17<46:02,  1.99s/batch]Batch 1800/3201 Done, mean position loss: 21.240939092636108\n",
      "Training FF1:  57%|████████████▌         | 1820/3201 [54:18<34:58,  1.52s/batch]Batch 1800/3201 Done, mean position loss: 21.495785071849824\n",
      "Training FF1:  57%|████████████▍         | 1811/3201 [54:24<32:10,  1.39s/batch]Batch 1800/3201 Done, mean position loss: 21.25232345819473\n",
      "Training FF1:  57%|████████████▌         | 1823/3201 [54:24<35:25,  1.54s/batch]Batch 1800/3201 Done, mean position loss: 21.70084834575653\n",
      "Training FF1:  56%|████████████▍         | 1807/3201 [54:24<38:08,  1.64s/batch]Batch 1800/3201 Done, mean position loss: 21.225069890022276\n",
      "Training FF1:  56%|████████████▍         | 1802/3201 [54:26<39:32,  1.70s/batch]Batch 1800/3201 Done, mean position loss: 21.18464464187622\n",
      "Training FF1:  57%|████████████▍         | 1809/3201 [54:32<45:23,  1.96s/batch]Batch 1800/3201 Done, mean position loss: 21.2742106628418\n",
      "Training FF1:  57%|████████████▌         | 1820/3201 [54:37<40:14,  1.75s/batch]Batch 1800/3201 Done, mean position loss: 21.90355421066284\n",
      "Training FF1:  56%|████████████▍         | 1808/3201 [54:37<41:38,  1.79s/batch]Batch 1800/3201 Done, mean position loss: 21.562035310268403\n",
      "Training FF1:  57%|████████████▌         | 1819/3201 [54:43<39:29,  1.71s/batch]Batch 1800/3201 Done, mean position loss: 21.41817104101181\n",
      "Training FF1:  56%|████████████▍         | 1801/3201 [54:43<43:39,  1.87s/batch]Batch 1800/3201 Done, mean position loss: 21.41553807258606\n",
      "Training FF1:  57%|████████████▍         | 1809/3201 [54:45<35:44,  1.54s/batch]Batch 1800/3201 Done, mean position loss: 21.698642163276674\n",
      "Training FF1:  57%|████████████▋         | 1837/3201 [54:49<37:56,  1.67s/batch]Batch 1800/3201 Done, mean position loss: 21.5730343747139\n",
      "Training FF1:  56%|████████████▏         | 1781/3201 [54:51<38:57,  1.65s/batch]Batch 1800/3201 Done, mean position loss: 21.64117247581482\n",
      "Training FF1:  56%|████████████▍         | 1807/3201 [54:53<35:29,  1.53s/batch]Batch 1800/3201 Done, mean position loss: 21.525146265029907\n",
      "Training FF1:  57%|████████████▌         | 1831/3201 [54:58<37:53,  1.66s/batch]Batch 1800/3201 Done, mean position loss: 21.407867090702055\n",
      "Training FF1:  57%|████████████▌         | 1831/3201 [55:03<47:57,  2.10s/batch]Batch 1800/3201 Done, mean position loss: 21.228264191150664\n",
      "Training FF1:  57%|████████████▍         | 1813/3201 [55:06<49:17,  2.13s/batch]Batch 1800/3201 Done, mean position loss: 21.65840133190155\n",
      "Training FF1:  57%|████████████▌         | 1819/3201 [55:19<29:53,  1.30s/batch]Batch 1800/3201 Done, mean position loss: 22.418472042083742\n",
      "Training FF1:  58%|████████████▋         | 1848/3201 [55:26<44:26,  1.97s/batch]Batch 1800/3201 Done, mean position loss: 21.460104744434357\n",
      "Training FF1:  59%|████████████▉         | 1885/3201 [56:37<40:07,  1.83s/batch]Batch 1900/3201 Done, mean position loss: 22.39445547580719\n",
      "Training FF1:  59%|████████████▉         | 1874/3201 [56:40<37:23,  1.69s/batch]Batch 1900/3201 Done, mean position loss: 21.29180849313736\n",
      "Training FF1:  59%|█████████████         | 1897/3201 [56:47<36:27,  1.68s/batch]Batch 1900/3201 Done, mean position loss: 21.76439398288727\n",
      "Training FF1:  59%|████████████▉         | 1887/3201 [56:50<42:31,  1.94s/batch]Batch 1900/3201 Done, mean position loss: 22.34803028345108\n",
      "Training FF1:  59%|█████████████         | 1900/3201 [56:55<42:38,  1.97s/batch]Batch 1900/3201 Done, mean position loss: 21.702620594501496\n",
      "Training FF1:  59%|████████████▉         | 1889/3201 [56:55<48:00,  2.20s/batch]Batch 1900/3201 Done, mean position loss: 21.804438490867614\n",
      "Training FF1:  59%|████████████▉         | 1882/3201 [56:57<44:49,  2.04s/batch]Batch 1900/3201 Done, mean position loss: 21.77354168176651\n",
      "Training FF1:  59%|█████████████         | 1903/3201 [56:56<31:01,  1.43s/batch]Batch 1900/3201 Done, mean position loss: 21.987948508262633\n",
      "Training FF1:  59%|█████████████         | 1893/3201 [56:57<34:43,  1.59s/batch]Batch 1900/3201 Done, mean position loss: 21.57830692768097\n",
      "Training FF1:  60%|█████████████         | 1908/3201 [57:02<35:58,  1.67s/batch]Batch 1900/3201 Done, mean position loss: 21.97570855140686\n",
      "Training FF1:  58%|████████████▊         | 1860/3201 [57:04<52:07,  2.33s/batch]Batch 1900/3201 Done, mean position loss: 21.221832053661345\n",
      "Training FF1:  59%|████████████▉         | 1884/3201 [57:04<39:12,  1.79s/batch]Batch 1900/3201 Done, mean position loss: 21.621356630325316\n",
      "Training FF1:  59%|█████████████         | 1899/3201 [57:05<37:12,  1.71s/batch]Batch 1900/3201 Done, mean position loss: 21.48959060192108\n",
      "Training FF1:  59%|█████████████         | 1904/3201 [57:06<33:43,  1.56s/batch]Batch 1900/3201 Done, mean position loss: 21.32475204706192\n",
      "Training FF1:  60%|█████████████         | 1907/3201 [57:07<38:43,  1.80s/batch]Batch 1900/3201 Done, mean position loss: 21.842258524894714\n",
      "Training FF1:  60%|█████████████▏        | 1913/3201 [57:08<37:21,  1.74s/batch]Batch 1900/3201 Done, mean position loss: 21.608121786117554\n",
      "Training FF1:  59%|████████████▉         | 1882/3201 [57:10<43:47,  1.99s/batch]Batch 1900/3201 Done, mean position loss: 21.184919934272763\n",
      "Training FF1:  59%|█████████████         | 1901/3201 [57:10<35:10,  1.62s/batch]Batch 1900/3201 Done, mean position loss: 21.55306594133377\n",
      "Training FF1:  59%|█████████████         | 1902/3201 [57:10<38:35,  1.78s/batch]Batch 1900/3201 Done, mean position loss: 21.45055243730545\n",
      "Training FF1:  59%|████████████▉         | 1881/3201 [57:16<35:38,  1.62s/batch]Batch 1900/3201 Done, mean position loss: 21.58734245300293\n",
      "Training FF1:  59%|████████████▉         | 1890/3201 [57:19<38:18,  1.75s/batch]Batch 1900/3201 Done, mean position loss: 21.645824563503265\n",
      "Training FF1:  59%|████████████▉         | 1886/3201 [57:21<38:45,  1.77s/batch]Batch 1900/3201 Done, mean position loss: 21.66271922826767\n",
      "Training FF1:  59%|█████████████         | 1900/3201 [57:21<32:27,  1.50s/batch]Batch 1900/3201 Done, mean position loss: 21.22902237415314\n",
      "Training FF1:  59%|████████████▉         | 1890/3201 [57:23<36:49,  1.69s/batch]Batch 1900/3201 Done, mean position loss: 21.16968844652176\n",
      "Training FF1:  60%|█████████████▏        | 1914/3201 [57:25<41:09,  1.92s/batch]Batch 1900/3201 Done, mean position loss: 21.471012227535248\n",
      "Training FF1:  60%|█████████████▏        | 1915/3201 [57:30<34:35,  1.61s/batch]Batch 1900/3201 Done, mean position loss: 21.20293911933899\n",
      "Training FF1:  59%|████████████▉         | 1884/3201 [57:32<44:03,  2.01s/batch]Batch 1900/3201 Done, mean position loss: 21.251983602046966\n",
      "Training FF1:  60%|█████████████▏        | 1917/3201 [57:39<51:56,  2.43s/batch]Batch 1900/3201 Done, mean position loss: 21.88783653497696\n",
      "Training FF1:  60%|█████████████▏        | 1910/3201 [57:41<37:20,  1.74s/batch]Batch 1900/3201 Done, mean position loss: 21.39601223230362\n",
      "Training FF1:  60%|█████████████▏        | 1927/3201 [57:43<39:51,  1.88s/batch]Batch 1900/3201 Done, mean position loss: 21.52859032392502\n",
      "Training FF1:  60%|█████████████▏        | 1919/3201 [57:50<43:14,  2.02s/batch]Batch 1900/3201 Done, mean position loss: 21.674206664562227\n",
      "Training FF1:  60%|█████████████▎        | 1929/3201 [57:49<42:48,  2.02s/batch]Batch 1900/3201 Done, mean position loss: 21.374966583251954\n",
      "Training FF1:  60%|█████████████▏        | 1926/3201 [57:50<41:33,  1.96s/batch]Batch 1900/3201 Done, mean position loss: 21.539702689647676\n",
      "Training FF1:  60%|█████████████▏        | 1926/3201 [57:58<39:59,  1.88s/batch]Batch 1900/3201 Done, mean position loss: 21.616981928348544\n",
      "Training FF1:  60%|█████████████▎        | 1928/3201 [57:58<37:36,  1.77s/batch]Batch 1900/3201 Done, mean position loss: 21.498085741996768\n",
      "Training FF1:  60%|█████████████▎        | 1931/3201 [58:02<43:44,  2.07s/batch]Batch 1900/3201 Done, mean position loss: 21.204823837280273\n",
      "Training FF1:  60%|█████████████▎        | 1933/3201 [58:02<37:52,  1.79s/batch]Batch 1900/3201 Done, mean position loss: 21.384726910591127\n",
      "Training FF1:  61%|█████████████▎        | 1946/3201 [58:18<35:48,  1.71s/batch]Batch 1900/3201 Done, mean position loss: 22.358930168151858\n",
      "Training FF1:  61%|█████████████▎        | 1942/3201 [58:21<43:19,  2.06s/batch]Batch 1900/3201 Done, mean position loss: 21.630145645141603\n",
      "Training FF1:  61%|█████████████▎        | 1938/3201 [58:27<35:47,  1.70s/batch]Batch 1900/3201 Done, mean position loss: 21.40338252544403\n",
      "Training FF1:  62%|█████████████▋        | 1992/3201 [59:41<33:15,  1.65s/batch]Batch 2000/3201 Done, mean position loss: 21.28597666025162\n",
      "Training FF1:  61%|█████████████▎        | 1945/3201 [59:42<36:19,  1.74s/batch]Batch 2000/3201 Done, mean position loss: 22.317563009262088\n",
      "Training FF1:  62%|█████████████▋        | 1993/3201 [59:45<35:41,  1.77s/batch]Batch 2000/3201 Done, mean position loss: 21.724158363342283\n",
      "Training FF1:  61%|█████████████▌        | 1966/3201 [59:49<37:20,  1.81s/batch]Batch 2000/3201 Done, mean position loss: 22.31384148836136\n",
      "Training FF1:  62%|█████████████▌        | 1970/3201 [59:53<28:42,  1.40s/batch]Batch 2000/3201 Done, mean position loss: 21.555199160575867\n",
      "Training FF1:  62%|█████████████▌        | 1982/3201 [59:56<35:33,  1.75s/batch]Batch 2000/3201 Done, mean position loss: 21.75035333633423\n",
      "Training FF1:  61%|█████████████▍        | 1964/3201 [59:56<41:36,  2.02s/batch]Batch 2000/3201 Done, mean position loss: 21.966544778347014\n",
      "Training FF1:  62%|█████████████▌        | 1973/3201 [59:57<33:57,  1.66s/batch]Batch 2000/3201 Done, mean position loss: 21.748693451881408\n",
      "Training FF1:  62%|█████████████▌        | 1970/3201 [59:57<38:55,  1.90s/batch]Batch 2000/3201 Done, mean position loss: 21.66786823987961\n",
      "Training FF1:  62%|█████████████▋        | 1989/3201 [59:57<34:09,  1.69s/batch]Batch 2000/3201 Done, mean position loss: 21.95133719444275\n",
      "Training FF1:  62%|█████████████▌        | 1980/3201 [59:59<37:03,  1.82s/batch]Batch 2000/3201 Done, mean position loss: 21.445233941078186\n",
      "Training FF1:  62%|████████████▍       | 1997/3201 [1:00:06<38:21,  1.91s/batch]Batch 2000/3201 Done, mean position loss: 21.484115555286408\n",
      "Batch 2000/3201 Done, mean position loss: 21.790748631954195\n",
      "Training FF1:  62%|████████████▎       | 1978/3201 [1:00:07<35:07,  1.72s/batch]Batch 2000/3201 Done, mean position loss: 21.198572478294373\n",
      "Training FF1:  61%|████████████▎       | 1967/3201 [1:00:09<36:41,  1.78s/batch]Batch 2000/3201 Done, mean position loss: 21.31376507282257\n",
      "Training FF1:  62%|████████████▍       | 1991/3201 [1:00:12<32:26,  1.61s/batch]Batch 2000/3201 Done, mean position loss: 21.537811672687532\n",
      "Training FF1:  62%|████████████▎       | 1979/3201 [1:00:13<42:41,  2.10s/batch]Batch 2000/3201 Done, mean position loss: 21.58477439880371\n",
      "Batch 2000/3201 Done, mean position loss: 21.588833487033845\n",
      "Training FF1:  63%|████████████▌       | 2018/3201 [1:00:12<38:11,  1.94s/batch]Batch 2000/3201 Done, mean position loss: 21.145484657287597\n",
      "Training FF1:  63%|████████████▌       | 2013/3201 [1:00:13<35:01,  1.77s/batch]Batch 2000/3201 Done, mean position loss: 21.21785190105438\n",
      "Training FF1:  63%|████████████▌       | 2005/3201 [1:00:17<42:27,  2.13s/batch]Batch 2000/3201 Done, mean position loss: 21.62480835199356\n",
      "Training FF1:  62%|████████████▍       | 1996/3201 [1:00:19<31:26,  1.57s/batch]Batch 2000/3201 Done, mean position loss: 21.41554706811905\n",
      "Training FF1:  62%|████████████▍       | 1985/3201 [1:00:20<47:16,  2.33s/batch]Batch 2000/3201 Done, mean position loss: 21.602695136070253\n",
      "Training FF1:  63%|████████████▌       | 2011/3201 [1:00:23<36:20,  1.83s/batch]Batch 2000/3201 Done, mean position loss: 21.577373139858246\n",
      "Training FF1:  63%|████████████▌       | 2020/3201 [1:00:27<32:59,  1.68s/batch]Batch 2000/3201 Done, mean position loss: 21.204575703144073\n",
      "Training FF1:  62%|████████████▍       | 1989/3201 [1:00:29<35:54,  1.78s/batch]Batch 2000/3201 Done, mean position loss: 21.16190901041031\n",
      "Training FF1:  62%|████████████▍       | 1986/3201 [1:00:33<36:13,  1.79s/batch]Batch 2000/3201 Done, mean position loss: 21.232786927223206\n",
      "Training FF1:  63%|████████████▌       | 2015/3201 [1:00:35<35:17,  1.79s/batch]Batch 2000/3201 Done, mean position loss: 21.35467159509659\n",
      "Training FF1:  63%|████████████▌       | 2002/3201 [1:00:36<31:55,  1.60s/batch]Batch 2000/3201 Done, mean position loss: 21.526784417629244\n",
      "Training FF1:  64%|████████████▋       | 2035/3201 [1:00:48<35:40,  1.84s/batch]Batch 2000/3201 Done, mean position loss: 21.546913156509397\n",
      "Training FF1:  62%|████████████▍       | 1991/3201 [1:00:48<35:01,  1.74s/batch]Batch 2000/3201 Done, mean position loss: 21.570833146572113\n",
      "Training FF1:  64%|████████████▊       | 2041/3201 [1:00:50<33:54,  1.75s/batch]Batch 2000/3201 Done, mean position loss: 21.865858809947966\n",
      "Training FF1:  62%|████████████▍       | 1997/3201 [1:00:51<31:05,  1.55s/batch]Batch 2000/3201 Done, mean position loss: 21.669436783790587\n",
      "Training FF1:  64%|████████████▋       | 2033/3201 [1:00:51<33:49,  1.74s/batch]Batch 2000/3201 Done, mean position loss: 21.371579959392548\n",
      "Training FF1:  64%|████████████▋       | 2034/3201 [1:00:59<39:35,  2.04s/batch]Batch 2000/3201 Done, mean position loss: 21.478704307079315\n",
      "Training FF1:  63%|████████████▋       | 2030/3201 [1:01:00<31:19,  1.60s/batch]Batch 2000/3201 Done, mean position loss: 21.359931111335754\n",
      "Training FF1:  64%|████████████▊       | 2042/3201 [1:01:05<31:39,  1.64s/batch]Batch 2000/3201 Done, mean position loss: 21.197240073680877\n",
      "Training FF1:  63%|████████████▋       | 2031/3201 [1:01:06<38:09,  1.96s/batch]Batch 2000/3201 Done, mean position loss: 22.29633401155472\n",
      "Training FF1:  64%|████████████▋       | 2037/3201 [1:01:24<29:01,  1.50s/batch]Batch 2000/3201 Done, mean position loss: 21.599949724674225\n",
      "Training FF1:  64%|████████████▋       | 2034/3201 [1:01:29<36:08,  1.86s/batch]Batch 2000/3201 Done, mean position loss: 21.394330811500552\n",
      "Training FF1:  65%|████████████▉       | 2071/3201 [1:02:31<29:31,  1.57s/batch]Batch 2100/3201 Done, mean position loss: 22.304815125465392\n",
      "Training FF1:  65%|█████████████       | 2083/3201 [1:02:32<28:46,  1.54s/batch]Batch 2100/3201 Done, mean position loss: 21.716441776752472\n",
      "Training FF1:  64%|████████████▉       | 2061/3201 [1:02:32<31:59,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.273039021492004\n",
      "Training FF1:  64%|████████████▊       | 2060/3201 [1:02:38<33:06,  1.74s/batch]Batch 2100/3201 Done, mean position loss: 22.30857235431671\n",
      "Training FF1:  65%|█████████████       | 2087/3201 [1:02:41<29:19,  1.58s/batch]Batch 2100/3201 Done, mean position loss: 21.65100528717041\n",
      "Training FF1:  66%|█████████████▏      | 2108/3201 [1:02:41<28:36,  1.57s/batch]Batch 2100/3201 Done, mean position loss: 21.55022559404373\n",
      "Training FF1:  65%|█████████████       | 2088/3201 [1:02:43<30:31,  1.65s/batch]Batch 2100/3201 Done, mean position loss: 21.776401829719543\n",
      "Training FF1:  65%|████████████▉       | 2071/3201 [1:02:45<35:27,  1.88s/batch]Batch 2100/3201 Done, mean position loss: 21.857782154083253\n",
      "Training FF1:  64%|████████████▉       | 2064/3201 [1:02:46<33:26,  1.76s/batch]Batch 2100/3201 Done, mean position loss: 21.8810449385643\n",
      "Training FF1:  65%|█████████████       | 2092/3201 [1:02:49<29:18,  1.59s/batch]Batch 2100/3201 Done, mean position loss: 21.73669344186783\n",
      "Training FF1:  65%|█████████████       | 2091/3201 [1:02:49<34:52,  1.89s/batch]Batch 2100/3201 Done, mean position loss: 21.40823251485825\n",
      "Training FF1:  65%|█████████████       | 2086/3201 [1:02:55<31:11,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.451796810626984\n",
      "Training FF1:  66%|█████████████       | 2100/3201 [1:03:00<33:07,  1.81s/batch]Batch 2100/3201 Done, mean position loss: 21.783711314201355\n",
      "Training FF1:  65%|████████████▉       | 2066/3201 [1:03:00<28:43,  1.52s/batch]Batch 2100/3201 Done, mean position loss: 21.18166558742523\n",
      "Training FF1:  66%|█████████████▏      | 2102/3201 [1:03:02<27:35,  1.51s/batch]Batch 2100/3201 Done, mean position loss: 21.063479471206662\n",
      "Training FF1:  66%|█████████████       | 2099/3201 [1:03:02<32:56,  1.79s/batch]Batch 2100/3201 Done, mean position loss: 21.54646049976349\n",
      "Training FF1:  65%|█████████████       | 2090/3201 [1:03:03<39:01,  2.11s/batch]Batch 2100/3201 Done, mean position loss: 21.519759559631346\n",
      "Training FF1:  66%|█████████████▏      | 2104/3201 [1:03:05<29:27,  1.61s/batch]Batch 2100/3201 Done, mean position loss: 21.60574682474136\n",
      "Training FF1:  65%|█████████████       | 2091/3201 [1:03:05<27:50,  1.51s/batch]Batch 2100/3201 Done, mean position loss: 21.552893974781036\n",
      "Training FF1:  64%|████████████▊       | 2054/3201 [1:03:06<42:01,  2.20s/batch]Batch 2100/3201 Done, mean position loss: 21.547839937210085\n",
      "Training FF1:  66%|█████████████▏      | 2102/3201 [1:03:07<29:48,  1.63s/batch]Batch 2100/3201 Done, mean position loss: 21.293669936656954\n",
      "Training FF1:  66%|█████████████       | 2100/3201 [1:03:09<29:44,  1.62s/batch]Batch 2100/3201 Done, mean position loss: 21.1816072678566\n",
      "Training FF1:  66%|█████████████▏      | 2104/3201 [1:03:10<29:41,  1.62s/batch]Batch 2100/3201 Done, mean position loss: 21.542253913879392\n",
      "Training FF1:  66%|█████████████▎      | 2123/3201 [1:03:11<30:48,  1.71s/batch]Batch 2100/3201 Done, mean position loss: 21.395920605659484\n",
      "Training FF1:  65%|████████████▉       | 2065/3201 [1:03:23<31:36,  1.67s/batch]Batch 2100/3201 Done, mean position loss: 21.344664988517764\n",
      "Training FF1:  66%|█████████████▏      | 2115/3201 [1:03:24<29:34,  1.63s/batch]Batch 2100/3201 Done, mean position loss: 21.183669595718385\n",
      "Training FF1:  66%|█████████████▏      | 2112/3201 [1:03:25<40:13,  2.22s/batch]Batch 2100/3201 Done, mean position loss: 21.18011859416962\n",
      "Training FF1:  67%|█████████████▎      | 2132/3201 [1:03:25<32:00,  1.80s/batch]Batch 2100/3201 Done, mean position loss: 21.142558794021607\n",
      "Training FF1:  65%|█████████████       | 2094/3201 [1:03:32<33:53,  1.84s/batch]Batch 2100/3201 Done, mean position loss: 21.490553081035614\n",
      "Training FF1:  66%|█████████████▎      | 2125/3201 [1:03:37<30:28,  1.70s/batch]Batch 2100/3201 Done, mean position loss: 21.343270363807676\n",
      "Training FF1:  66%|█████████████▎      | 2122/3201 [1:03:38<29:58,  1.67s/batch]Batch 2100/3201 Done, mean position loss: 21.518339574337006\n",
      "Training FF1:  66%|█████████████▎      | 2121/3201 [1:03:43<26:00,  1.44s/batch]Batch 2100/3201 Done, mean position loss: 21.54328192472458\n",
      "Training FF1:  66%|█████████████▏      | 2112/3201 [1:03:45<40:55,  2.26s/batch]Batch 2100/3201 Done, mean position loss: 21.66362351179123\n",
      "Training FF1:  66%|█████████████▎      | 2127/3201 [1:03:47<32:51,  1.84s/batch]Batch 2100/3201 Done, mean position loss: 21.80403451681137\n",
      "Training FF1:  67%|█████████████▍      | 2148/3201 [1:03:52<35:21,  2.02s/batch]Batch 2100/3201 Done, mean position loss: 21.349139425754547\n",
      "Training FF1:  66%|█████████████▎      | 2127/3201 [1:03:52<31:37,  1.77s/batch]Batch 2100/3201 Done, mean position loss: 21.443768854141233\n",
      "Training FF1:  67%|█████████████▎      | 2133/3201 [1:03:58<27:11,  1.53s/batch]Batch 2100/3201 Done, mean position loss: 21.163068614006043\n",
      "Training FF1:  66%|█████████████▎      | 2123/3201 [1:04:04<31:33,  1.76s/batch]Batch 2100/3201 Done, mean position loss: 22.28963471889496\n",
      "Training FF1:  67%|█████████████▍      | 2148/3201 [1:04:20<38:17,  2.18s/batch]Batch 2100/3201 Done, mean position loss: 21.572734653949738\n",
      "Training FF1:  67%|█████████████▍      | 2147/3201 [1:04:29<40:53,  2.33s/batch]Batch 2100/3201 Done, mean position loss: 21.336208145618443\n",
      "Training FF1:  67%|█████████████▍      | 2152/3201 [1:05:19<31:46,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 22.23405243873596\n",
      "Training FF1:  68%|█████████████▌      | 2168/3201 [1:05:25<25:47,  1.50s/batch]Batch 2200/3201 Done, mean position loss: 21.67711593389511\n",
      "Training FF1:  68%|█████████████▌      | 2171/3201 [1:05:27<28:58,  1.69s/batch]Batch 2200/3201 Done, mean position loss: 22.249002034664155\n",
      "Training FF1:  68%|█████████████▋      | 2187/3201 [1:05:31<28:10,  1.67s/batch]Batch 2200/3201 Done, mean position loss: 21.25839710712433\n",
      "Training FF1:  67%|█████████████▍      | 2154/3201 [1:05:36<26:40,  1.53s/batch]Batch 2200/3201 Done, mean position loss: 21.73087643146515\n",
      "Training FF1:  67%|█████████████▍      | 2160/3201 [1:05:37<36:30,  2.10s/batch]Batch 2200/3201 Done, mean position loss: 21.62853987455368\n",
      "Training FF1:  69%|█████████████▋      | 2199/3201 [1:05:37<26:22,  1.58s/batch]Batch 2200/3201 Done, mean position loss: 21.525559928417206\n",
      "Training FF1:  68%|█████████████▋      | 2189/3201 [1:05:38<29:17,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.857816417217254\n",
      "Training FF1:  68%|█████████████▋      | 2187/3201 [1:05:41<29:02,  1.72s/batch]Batch 2200/3201 Done, mean position loss: 21.742234721183777\n",
      "Training FF1:  68%|█████████████▋      | 2191/3201 [1:05:42<30:00,  1.78s/batch]Batch 2200/3201 Done, mean position loss: 21.803275449275972\n",
      "Training FF1:  68%|█████████████▋      | 2183/3201 [1:05:53<38:02,  2.24s/batch]Batch 2200/3201 Done, mean position loss: 21.505061588287354\n",
      "Training FF1:  69%|█████████████▊      | 2212/3201 [1:05:56<29:59,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 21.171293852329253\n",
      "Training FF1:  68%|█████████████▌      | 2166/3201 [1:05:57<34:27,  2.00s/batch]Batch 2200/3201 Done, mean position loss: 21.510683624744416\n",
      "Training FF1:  69%|█████████████▋      | 2195/3201 [1:05:57<30:52,  1.84s/batch]Batch 2200/3201 Done, mean position loss: 21.39512209415436\n",
      "Training FF1:  68%|█████████████▌      | 2173/3201 [1:05:59<33:38,  1.96s/batch]Batch 2200/3201 Done, mean position loss: 21.498044698238374\n",
      "Training FF1:  68%|█████████████▌      | 2172/3201 [1:05:59<38:08,  2.22s/batch]Batch 2200/3201 Done, mean position loss: 21.77806713104248\n",
      "Training FF1:  68%|█████████████▌      | 2175/3201 [1:06:00<40:02,  2.34s/batch]Batch 2200/3201 Done, mean position loss: 21.568590202331542\n",
      "Training FF1:  68%|█████████████▌      | 2179/3201 [1:06:01<33:02,  1.94s/batch]Batch 2200/3201 Done, mean position loss: 21.40864737033844\n",
      "Training FF1:  68%|█████████████▌      | 2174/3201 [1:06:03<29:37,  1.73s/batch]Batch 2200/3201 Done, mean position loss: 21.03448113679886\n",
      "Training FF1:  68%|█████████████▌      | 2175/3201 [1:06:05<34:12,  2.00s/batch]Batch 2200/3201 Done, mean position loss: 21.24393925428391\n",
      "Training FF1:  68%|█████████████▋      | 2187/3201 [1:06:06<30:46,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 21.150894124507904\n",
      "Training FF1:  68%|█████████████▋      | 2191/3201 [1:06:07<29:21,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.524285645484923\n",
      "Training FF1:  69%|█████████████▊      | 2218/3201 [1:06:08<31:42,  1.94s/batch]Batch 2200/3201 Done, mean position loss: 21.358040943145753\n",
      "Training FF1:  69%|█████████████▊      | 2204/3201 [1:06:09<33:45,  2.03s/batch]Batch 2200/3201 Done, mean position loss: 21.51066071510315\n",
      "Training FF1:  69%|█████████████▊      | 2213/3201 [1:06:20<30:07,  1.83s/batch]Batch 2200/3201 Done, mean position loss: 21.33475136756897\n",
      "Training FF1:  69%|█████████████▋      | 2193/3201 [1:06:24<36:20,  2.16s/batch]Batch 2200/3201 Done, mean position loss: 21.150321922302247\n",
      "Training FF1:  69%|█████████████▊      | 2215/3201 [1:06:26<35:30,  2.16s/batch]Batch 2200/3201 Done, mean position loss: 21.13246080160141\n",
      "Training FF1:  69%|█████████████▋      | 2195/3201 [1:06:30<32:05,  1.91s/batch]Batch 2200/3201 Done, mean position loss: 21.310590777397156\n",
      "Training FF1:  70%|█████████████▉      | 2228/3201 [1:06:31<28:42,  1.77s/batch]Batch 2200/3201 Done, mean position loss: 21.174224126338956\n",
      "Training FF1:  70%|██████████████      | 2243/3201 [1:06:39<29:07,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 21.65767504453659\n",
      "Training FF1:  69%|█████████████▉      | 2222/3201 [1:06:40<30:38,  1.88s/batch]Batch 2200/3201 Done, mean position loss: 21.441832954883576\n",
      "Training FF1:  69%|█████████████▊      | 2208/3201 [1:06:43<28:51,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.49070011138916\n",
      "Training FF1:  68%|█████████████▋      | 2183/3201 [1:06:53<35:12,  2.07s/batch]Batch 2200/3201 Done, mean position loss: 21.320183923244475\n",
      "Training FF1:  70%|██████████████      | 2244/3201 [1:06:53<26:13,  1.64s/batch]Batch 2200/3201 Done, mean position loss: 21.78849838733673\n",
      "Training FF1:  69%|█████████████▊      | 2210/3201 [1:06:56<29:37,  1.79s/batch]Batch 2200/3201 Done, mean position loss: 21.532674238681793\n",
      "Training FF1:  69%|█████████████▋      | 2196/3201 [1:06:57<32:24,  1.93s/batch]Batch 2200/3201 Done, mean position loss: 21.38839792013168\n",
      "Training FF1:  70%|█████████████▉      | 2229/3201 [1:07:02<30:23,  1.88s/batch]Batch 2200/3201 Done, mean position loss: 21.140686898231507\n",
      "Training FF1:  70%|██████████████      | 2250/3201 [1:07:04<29:09,  1.84s/batch]Batch 2200/3201 Done, mean position loss: 22.271727249622344\n",
      "Training FF1:  70%|██████████████      | 2250/3201 [1:07:28<28:11,  1.78s/batch]Batch 2200/3201 Done, mean position loss: 21.53126657247543\n",
      "Training FF1:  70%|█████████████▉      | 2231/3201 [1:07:36<29:58,  1.85s/batch]Batch 2200/3201 Done, mean position loss: 21.320519793033597\n",
      "Training FF1:  71%|██████████████▏     | 2276/3201 [1:08:21<25:45,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 22.183477566242217\n",
      "Training FF1:  70%|██████████████      | 2249/3201 [1:08:22<29:28,  1.86s/batch]Batch 2300/3201 Done, mean position loss: 22.218191604614255\n",
      "Training FF1:  71%|██████████████▏     | 2264/3201 [1:08:24<24:08,  1.55s/batch]Batch 2300/3201 Done, mean position loss: 21.677953600883484\n",
      "Training FF1:  71%|██████████████▏     | 2268/3201 [1:08:31<27:09,  1.75s/batch]Batch 2300/3201 Done, mean position loss: 21.59322292804718\n",
      "Training FF1:  70%|█████████████▉      | 2236/3201 [1:08:35<23:46,  1.48s/batch]Batch 2300/3201 Done, mean position loss: 21.242880856990816\n",
      "Training FF1:  71%|██████████████▎     | 2286/3201 [1:08:40<25:23,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 21.68822643995285\n",
      "Training FF1:  71%|██████████████      | 2259/3201 [1:08:41<27:06,  1.73s/batch]Batch 2300/3201 Done, mean position loss: 21.815930211544035\n",
      "Training FF1:  71%|██████████████▏     | 2276/3201 [1:08:41<24:51,  1.61s/batch]Batch 2300/3201 Done, mean position loss: 21.503693945407868\n",
      "Training FF1:  72%|██████████████▎     | 2292/3201 [1:08:43<26:10,  1.73s/batch]Batch 2300/3201 Done, mean position loss: 21.837136383056638\n",
      "Training FF1:  72%|██████████████▍     | 2315/3201 [1:08:47<23:26,  1.59s/batch]Batch 2300/3201 Done, mean position loss: 21.70092625141144\n",
      "Training FF1:  71%|██████████████▎     | 2284/3201 [1:08:50<25:00,  1.64s/batch]Batch 2300/3201 Done, mean position loss: 21.520574479103086\n",
      "Training FF1:  72%|██████████████▍     | 2308/3201 [1:08:58<24:50,  1.67s/batch]Batch 2300/3201 Done, mean position loss: 21.47669248342514\n",
      "Training FF1:  72%|██████████████▎     | 2299/3201 [1:08:58<29:25,  1.96s/batch]Batch 2300/3201 Done, mean position loss: 21.51704743385315\n",
      "Training FF1:  73%|██████████████▌     | 2323/3201 [1:09:02<27:43,  1.89s/batch]Batch 2300/3201 Done, mean position loss: 21.374467544555664\n",
      "Training FF1:  71%|██████████████▎     | 2286/3201 [1:09:02<28:43,  1.88s/batch]Batch 2300/3201 Done, mean position loss: 21.156707625389103\n",
      "Training FF1:  71%|██████████████▏     | 2270/3201 [1:09:02<27:46,  1.79s/batch]Batch 2300/3201 Done, mean position loss: 21.742824997901916\n",
      "Training FF1:  72%|██████████████▍     | 2305/3201 [1:09:04<22:38,  1.52s/batch]Batch 2300/3201 Done, mean position loss: 21.52564813375473\n",
      "Training FF1:  72%|██████████████▍     | 2303/3201 [1:09:05<25:24,  1.70s/batch]Batch 2300/3201 Done, mean position loss: 21.50214970111847\n",
      "Training FF1:  72%|██████████████▍     | 2314/3201 [1:09:05<27:18,  1.85s/batch]Batch 2300/3201 Done, mean position loss: 21.13593428850174\n",
      "Training FF1:  72%|██████████████▍     | 2313/3201 [1:09:07<26:29,  1.79s/batch]Batch 2300/3201 Done, mean position loss: 21.005735690593717\n",
      "Training FF1:  72%|██████████████▍     | 2304/3201 [1:09:07<26:12,  1.75s/batch]Batch 2300/3201 Done, mean position loss: 21.405292735099792\n",
      "Training FF1:  72%|██████████████▎     | 2296/3201 [1:09:09<23:56,  1.59s/batch]Batch 2300/3201 Done, mean position loss: 21.20377324342728\n",
      "Training FF1:  71%|██████████████▎     | 2281/3201 [1:09:09<26:36,  1.74s/batch]Batch 2300/3201 Done, mean position loss: 21.313061127662657\n",
      "Training FF1:  72%|██████████████▍     | 2306/3201 [1:09:14<25:53,  1.74s/batch]Batch 2300/3201 Done, mean position loss: 21.493177886009217\n",
      "Training FF1:  72%|██████████████▍     | 2307/3201 [1:09:19<30:19,  2.04s/batch]Batch 2300/3201 Done, mean position loss: 21.30741368293762\n",
      "Training FF1:  71%|██████████████▎     | 2282/3201 [1:09:28<26:19,  1.72s/batch]Batch 2300/3201 Done, mean position loss: 21.287954897880553\n",
      "Training FF1:  71%|██████████████▎     | 2285/3201 [1:09:28<34:02,  2.23s/batch]Batch 2300/3201 Done, mean position loss: 21.10133641242981\n",
      "Training FF1:  73%|██████████████▌     | 2334/3201 [1:09:32<27:36,  1.91s/batch]Batch 2300/3201 Done, mean position loss: 21.14813566446304\n",
      "Training FF1:  73%|██████████████▌     | 2328/3201 [1:09:32<25:02,  1.72s/batch]Batch 2300/3201 Done, mean position loss: 21.13307516813278\n",
      "Training FF1:  72%|██████████████▍     | 2309/3201 [1:09:40<24:43,  1.66s/batch]Batch 2300/3201 Done, mean position loss: 21.64571418762207\n",
      "Training FF1:  73%|██████████████▌     | 2331/3201 [1:09:43<31:06,  2.15s/batch]Batch 2300/3201 Done, mean position loss: 21.442092010974886\n",
      "Training FF1:  73%|██████████████▋     | 2348/3201 [1:09:49<26:45,  1.88s/batch]Batch 2300/3201 Done, mean position loss: 21.45518096923828\n",
      "Training FF1:  72%|██████████████▍     | 2315/3201 [1:09:52<25:20,  1.72s/batch]Batch 2300/3201 Done, mean position loss: 21.766568486690524\n",
      "Training FF1:  73%|██████████████▋     | 2349/3201 [1:09:55<24:34,  1.73s/batch]Batch 2300/3201 Done, mean position loss: 21.47930988788605\n",
      "Training FF1:  73%|██████████████▌     | 2339/3201 [1:09:58<27:41,  1.93s/batch]Batch 2300/3201 Done, mean position loss: 21.30082170009613\n",
      "Training FF1:  73%|██████████████▌     | 2330/3201 [1:09:59<24:38,  1.70s/batch]Batch 2300/3201 Done, mean position loss: 21.358735868930815\n",
      "Training FF1:  72%|██████████████▍     | 2313/3201 [1:10:01<25:15,  1.71s/batch]Batch 2300/3201 Done, mean position loss: 22.2446720123291\n",
      "Training FF1:  73%|██████████████▌     | 2329/3201 [1:10:01<27:42,  1.91s/batch]Batch 2300/3201 Done, mean position loss: 21.12543786525726\n",
      "Training FF1:  73%|██████████████▌     | 2324/3201 [1:10:35<30:09,  2.06s/batch]Batch 2300/3201 Done, mean position loss: 21.2981742811203\n",
      "Training FF1:  73%|██████████████▌     | 2338/3201 [1:10:38<26:18,  1.83s/batch]Batch 2300/3201 Done, mean position loss: 21.504385828971863\n",
      "Training FF1:  73%|██████████████▋     | 2349/3201 [1:11:15<26:00,  1.83s/batch]Batch 2400/3201 Done, mean position loss: 21.67288683414459\n",
      "Training FF1:  74%|██████████████▊     | 2366/3201 [1:11:18<25:37,  1.84s/batch]Batch 2400/3201 Done, mean position loss: 22.18864096403122\n",
      "Training FF1:  75%|██████████████▉     | 2400/3201 [1:11:21<25:08,  1.88s/batch]Batch 2400/3201 Done, mean position loss: 22.21707183122635\n",
      "Training FF1:  74%|██████████████▉     | 2384/3201 [1:11:24<20:46,  1.53s/batch]Batch 2400/3201 Done, mean position loss: 21.577664854526518\n",
      "Training FF1:  73%|██████████████▌     | 2331/3201 [1:11:32<27:35,  1.90s/batch]Batch 2400/3201 Done, mean position loss: 21.200977687835692\n",
      "Training FF1:  74%|██████████████▊     | 2374/3201 [1:11:38<20:56,  1.52s/batch]Batch 2400/3201 Done, mean position loss: 21.77304848909378\n",
      "Training FF1:  74%|██████████████▊     | 2378/3201 [1:11:40<24:17,  1.77s/batch]Batch 2400/3201 Done, mean position loss: 21.685606818199158\n",
      "Training FF1:  75%|██████████████▉     | 2388/3201 [1:11:41<24:34,  1.81s/batch]Batch 2400/3201 Done, mean position loss: 21.460998606681827\n",
      "Training FF1:  74%|██████████████▊     | 2368/3201 [1:11:43<22:53,  1.65s/batch]Batch 2400/3201 Done, mean position loss: 21.685677380561827\n",
      "Training FF1:  74%|██████████████▊     | 2378/3201 [1:11:44<21:47,  1.59s/batch]Batch 2400/3201 Done, mean position loss: 21.51251742362976\n",
      "Training FF1:  75%|███████████████     | 2416/3201 [1:11:48<22:54,  1.75s/batch]Batch 2400/3201 Done, mean position loss: 21.445381639003756\n",
      "Training FF1:  75%|███████████████     | 2404/3201 [1:11:53<22:20,  1.68s/batch]Batch 2400/3201 Done, mean position loss: 21.493447477817536\n",
      "Training FF1:  75%|███████████████     | 2409/3201 [1:11:53<24:01,  1.82s/batch]Batch 2400/3201 Done, mean position loss: 21.10333726167679\n",
      "Training FF1:  75%|██████████████▉     | 2396/3201 [1:11:55<24:51,  1.85s/batch]Batch 2400/3201 Done, mean position loss: 21.824773111343383\n",
      "Training FF1:  73%|██████████████▋     | 2349/3201 [1:11:57<24:26,  1.72s/batch]Batch 2400/3201 Done, mean position loss: 21.349507596492767\n",
      "Training FF1:  74%|██████████████▊     | 2374/3201 [1:11:59<24:54,  1.81s/batch]Batch 2400/3201 Done, mean position loss: 21.142066826820372\n",
      "Training FF1:  75%|██████████████▉     | 2389/3201 [1:11:59<20:49,  1.54s/batch]Batch 2400/3201 Done, mean position loss: 21.72176139354706\n",
      "Training FF1:  75%|███████████████     | 2404/3201 [1:12:01<19:06,  1.44s/batch]Batch 2400/3201 Done, mean position loss: 21.469744222164152\n",
      "Training FF1:  75%|███████████████     | 2405/3201 [1:12:03<23:09,  1.75s/batch]Batch 2400/3201 Done, mean position loss: 21.31031855106354\n",
      "Training FF1:  74%|██████████████▉     | 2381/3201 [1:12:04<28:08,  2.06s/batch]Batch 2400/3201 Done, mean position loss: 21.187581152915953\n",
      "Training FF1:  75%|███████████████     | 2401/3201 [1:12:04<22:24,  1.68s/batch]Batch 2400/3201 Done, mean position loss: 20.98836974143982\n",
      "Training FF1:  75%|███████████████     | 2416/3201 [1:12:05<24:27,  1.87s/batch]Batch 2400/3201 Done, mean position loss: 21.364701058864593\n",
      "Training FF1:  75%|██████████████▉     | 2397/3201 [1:12:06<22:19,  1.67s/batch]Batch 2400/3201 Done, mean position loss: 21.507151262760164\n",
      "Training FF1:  75%|██████████████▉     | 2386/3201 [1:12:14<27:54,  2.05s/batch]Batch 2400/3201 Done, mean position loss: 21.461863663196564\n",
      "Training FF1:  75%|███████████████     | 2411/3201 [1:12:22<24:57,  1.90s/batch]Batch 2400/3201 Done, mean position loss: 21.294335408210753\n",
      "Training FF1:  74%|██████████████▉     | 2383/3201 [1:12:23<20:07,  1.48s/batch]Batch 2400/3201 Done, mean position loss: 21.22640798330307\n",
      "Training FF1:  76%|███████████████▏    | 2425/3201 [1:12:23<22:38,  1.75s/batch]Batch 2400/3201 Done, mean position loss: 21.12093509197235\n",
      "Training FF1:  76%|███████████████▏    | 2439/3201 [1:12:25<24:05,  1.90s/batch]Batch 2400/3201 Done, mean position loss: 21.097311382293704\n",
      "Training FF1:  75%|██████████████▉     | 2386/3201 [1:12:24<24:45,  1.82s/batch]Batch 2400/3201 Done, mean position loss: 21.133538398742676\n",
      "Training FF1:  76%|███████████████▏    | 2422/3201 [1:12:41<26:52,  2.07s/batch]Batch 2400/3201 Done, mean position loss: 21.626007974147797\n",
      "Training FF1:  75%|██████████████▉     | 2395/3201 [1:12:41<23:17,  1.73s/batch]Batch 2400/3201 Done, mean position loss: 21.457057507038115\n",
      "Training FF1:  75%|███████████████     | 2414/3201 [1:12:48<19:33,  1.49s/batch]Batch 2400/3201 Done, mean position loss: 21.420011548995973\n",
      "Training FF1:  76%|███████████████▏    | 2423/3201 [1:12:50<27:43,  2.14s/batch]Batch 2400/3201 Done, mean position loss: 21.69220260620117\n",
      "Training FF1:  76%|███████████████▎    | 2443/3201 [1:12:51<23:36,  1.87s/batch]Batch 2400/3201 Done, mean position loss: 21.46464479923248\n",
      "Training FF1:  76%|███████████████▏    | 2438/3201 [1:12:52<25:50,  2.03s/batch]Batch 2400/3201 Done, mean position loss: 21.279846861362458\n",
      "Training FF1:  75%|██████████████▉     | 2398/3201 [1:12:52<23:45,  1.78s/batch]Batch 2400/3201 Done, mean position loss: 21.146666524410247\n",
      "Training FF1:  76%|███████████████▎    | 2444/3201 [1:12:53<22:18,  1.77s/batch]Batch 2400/3201 Done, mean position loss: 22.24982944726944\n",
      "Training FF1:  76%|███████████████▏    | 2438/3201 [1:12:59<22:08,  1.74s/batch]Batch 2400/3201 Done, mean position loss: 21.3536211681366\n",
      "Training FF1:  76%|███████████████▏    | 2426/3201 [1:13:31<21:41,  1.68s/batch]Batch 2400/3201 Done, mean position loss: 21.26236320257187\n",
      "Training FF1:  77%|███████████████▍    | 2461/3201 [1:13:42<22:26,  1.82s/batch]Batch 2400/3201 Done, mean position loss: 21.444246213436124\n",
      "Training FF1:  77%|███████████████▍    | 2475/3201 [1:14:10<21:23,  1.77s/batch]Batch 2500/3201 Done, mean position loss: 21.616310391426087\n",
      "Training FF1:  78%|███████████████▌    | 2485/3201 [1:14:12<19:40,  1.65s/batch]Batch 2500/3201 Done, mean position loss: 22.16881945133209\n",
      "Training FF1:  77%|███████████████▍    | 2476/3201 [1:14:14<26:58,  2.23s/batch]Batch 2500/3201 Done, mean position loss: 22.09957993745804\n",
      "Training FF1:  77%|███████████████▎    | 2454/3201 [1:14:17<23:02,  1.85s/batch]Batch 2500/3201 Done, mean position loss: 21.54782131433487\n",
      "Training FF1:  78%|███████████████▌    | 2500/3201 [1:14:25<19:52,  1.70s/batch]Batch 2500/3201 Done, mean position loss: 21.196888756752013\n",
      "Training FF1:  77%|███████████████▎    | 2455/3201 [1:14:27<21:00,  1.69s/batch]Batch 2500/3201 Done, mean position loss: 21.65803839683533\n",
      "Training FF1:  78%|███████████████▌    | 2491/3201 [1:14:38<18:33,  1.57s/batch]Batch 2500/3201 Done, mean position loss: 21.741399705410004\n",
      "Training FF1:  77%|███████████████▍    | 2462/3201 [1:14:40<21:46,  1.77s/batch]Batch 2500/3201 Done, mean position loss: 21.703153281211854\n",
      "Training FF1:  78%|███████████████▋    | 2502/3201 [1:14:41<17:32,  1.51s/batch]Batch 2500/3201 Done, mean position loss: 21.430008306503296\n",
      "Training FF1:  78%|███████████████▌    | 2492/3201 [1:14:42<23:48,  2.01s/batch]Batch 2500/3201 Done, mean position loss: 21.505228197574617\n",
      "Training FF1:  77%|███████████████▍    | 2477/3201 [1:14:44<24:33,  2.04s/batch]Batch 2500/3201 Done, mean position loss: 21.409118454456326\n",
      "Training FF1:  78%|███████████████▌    | 2498/3201 [1:14:49<24:15,  2.07s/batch]Batch 2500/3201 Done, mean position loss: 21.07846221446991\n",
      "Training FF1:  78%|███████████████▌    | 2483/3201 [1:14:51<22:51,  1.91s/batch]Batch 2500/3201 Done, mean position loss: 21.78996200799942\n",
      "Training FF1:  78%|███████████████▌    | 2487/3201 [1:14:52<19:48,  1.66s/batch]Batch 2500/3201 Done, mean position loss: 21.47186460018158\n",
      "Training FF1:  78%|███████████████▌    | 2491/3201 [1:14:52<23:00,  1.94s/batch]Batch 2500/3201 Done, mean position loss: 21.31166040658951\n",
      "Training FF1:  78%|███████████████▌    | 2485/3201 [1:14:54<13:32,  1.13s/batch]Batch 2500/3201 Done, mean position loss: 21.709612560272216\n",
      "Training FF1:  78%|███████████████▌    | 2486/3201 [1:14:56<21:54,  1.84s/batch]Batch 2500/3201 Done, mean position loss: 21.156184375286102\n",
      "Training FF1:  77%|███████████████▍    | 2479/3201 [1:14:56<23:27,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.129915039539338\n",
      "Training FF1:  79%|███████████████▊    | 2529/3201 [1:14:57<21:53,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 20.979515628814696\n",
      "Training FF1:  78%|███████████████▌    | 2493/3201 [1:14:56<24:41,  2.09s/batch]Batch 2500/3201 Done, mean position loss: 21.282730555534364\n",
      "Training FF1:  79%|███████████████▊    | 2525/3201 [1:14:58<18:07,  1.61s/batch]Batch 2500/3201 Done, mean position loss: 21.43946665048599\n",
      "Training FF1:  78%|███████████████▋    | 2504/3201 [1:14:59<23:35,  2.03s/batch]Batch 2500/3201 Done, mean position loss: 21.47110670566559\n",
      "Training FF1:  78%|███████████████▌    | 2494/3201 [1:15:08<19:10,  1.63s/batch]Batch 2500/3201 Done, mean position loss: 21.33578044652939\n",
      "Training FF1:  77%|███████████████▎    | 2459/3201 [1:15:13<24:46,  2.00s/batch]Batch 2500/3201 Done, mean position loss: 21.425721855163573\n",
      "Training FF1:  77%|███████████████▍    | 2479/3201 [1:15:15<20:06,  1.67s/batch]Batch 2500/3201 Done, mean position loss: 21.291132934093476\n",
      "Training FF1:  78%|███████████████▋    | 2508/3201 [1:15:21<21:37,  1.87s/batch]Batch 2500/3201 Done, mean position loss: 21.11373946428299\n",
      "Training FF1:  78%|███████████████▌    | 2485/3201 [1:15:22<21:19,  1.79s/batch]Batch 2500/3201 Done, mean position loss: 21.223971259593963\n",
      "Training FF1:  78%|███████████████▌    | 2483/3201 [1:15:23<21:17,  1.78s/batch]Batch 2500/3201 Done, mean position loss: 21.12477147579193\n",
      "Training FF1:  78%|███████████████▋    | 2510/3201 [1:15:24<20:34,  1.79s/batch]Batch 2500/3201 Done, mean position loss: 21.065688343048095\n",
      "Training FF1:  78%|███████████████▌    | 2491/3201 [1:15:31<21:23,  1.81s/batch]Batch 2500/3201 Done, mean position loss: 21.58462187051773\n",
      "Training FF1:  78%|███████████████▌    | 2494/3201 [1:15:40<21:29,  1.82s/batch]Batch 2500/3201 Done, mean position loss: 21.373693606853486\n",
      "Training FF1:  79%|███████████████▉    | 2541/3201 [1:15:45<17:27,  1.59s/batch]Batch 2500/3201 Done, mean position loss: 21.42609089851379\n",
      "Training FF1:  79%|███████████████▋    | 2519/3201 [1:15:48<20:11,  1.78s/batch]Batch 2500/3201 Done, mean position loss: 21.37343813896179\n",
      "Training FF1:  79%|███████████████▊    | 2538/3201 [1:15:49<23:07,  2.09s/batch]Batch 2500/3201 Done, mean position loss: 21.09793735265732\n",
      "Training FF1:  78%|███████████████▋    | 2504/3201 [1:15:50<18:36,  1.60s/batch]Batch 2500/3201 Done, mean position loss: 22.22332359790802\n",
      "Training FF1:  79%|███████████████▋    | 2518/3201 [1:15:55<23:21,  2.05s/batch]Batch 2500/3201 Done, mean position loss: 21.25925358533859\n",
      "Training FF1:  77%|███████████████▍    | 2472/3201 [1:15:55<23:39,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.327239537239073\n",
      "Training FF1:  78%|███████████████▋    | 2509/3201 [1:15:55<20:43,  1.80s/batch]Batch 2500/3201 Done, mean position loss: 21.67235792636871\n",
      "Training FF1:  80%|████████████████    | 2563/3201 [1:16:34<18:48,  1.77s/batch]Batch 2500/3201 Done, mean position loss: 21.23556315422058\n",
      "Training FF1:  80%|████████████████    | 2571/3201 [1:16:49<22:23,  2.13s/batch]Batch 2500/3201 Done, mean position loss: 21.386635031700134\n",
      "Training FF1:  80%|████████████████    | 2566/3201 [1:17:04<17:07,  1.62s/batch]Batch 2600/3201 Done, mean position loss: 22.13938236951828\n",
      "Training FF1:  80%|████████████████    | 2574/3201 [1:17:08<17:15,  1.65s/batch]Batch 2600/3201 Done, mean position loss: 21.584659140110016\n",
      "Training FF1:  81%|████████████████▎   | 2605/3201 [1:17:12<18:44,  1.89s/batch]Batch 2600/3201 Done, mean position loss: 22.100875074863435\n",
      "Training FF1:  80%|███████████████▉    | 2560/3201 [1:17:12<22:22,  2.09s/batch]Batch 2600/3201 Done, mean position loss: 21.52690680027008\n",
      "Training FF1:  81%|████████████████▏   | 2583/3201 [1:17:27<21:46,  2.11s/batch]Batch 2600/3201 Done, mean position loss: 21.63359023809433\n",
      "Training FF1:  82%|████████████████▎   | 2612/3201 [1:17:27<16:51,  1.72s/batch]Batch 2600/3201 Done, mean position loss: 21.188684222698214\n",
      "Training FF1:  80%|████████████████    | 2576/3201 [1:17:35<19:30,  1.87s/batch]Batch 2600/3201 Done, mean position loss: 21.735818419456482\n",
      "Training FF1:  81%|████████████████▏   | 2599/3201 [1:17:41<17:11,  1.71s/batch]Batch 2600/3201 Done, mean position loss: 21.51019064426422\n",
      "Training FF1:  80%|████████████████    | 2574/3201 [1:17:41<21:40,  2.07s/batch]Batch 2600/3201 Done, mean position loss: 21.396749658584596\n",
      "Training FF1:  81%|████████████████▏   | 2591/3201 [1:17:46<21:30,  2.12s/batch]Batch 2600/3201 Done, mean position loss: 21.630471150875092\n",
      "Training FF1:  82%|████████████████▎   | 2614/3201 [1:17:47<16:27,  1.68s/batch]Batch 2600/3201 Done, mean position loss: 21.076387331485748\n",
      "Training FF1:  81%|████████████████▏   | 2597/3201 [1:17:51<17:59,  1.79s/batch]Batch 2600/3201 Done, mean position loss: 21.443254220485688\n",
      "Training FF1:  80%|████████████████    | 2562/3201 [1:17:51<20:09,  1.89s/batch]Batch 2600/3201 Done, mean position loss: 21.697421381473543\n",
      "Training FF1:  80%|████████████████    | 2568/3201 [1:17:51<19:11,  1.82s/batch]Batch 2600/3201 Done, mean position loss: 21.37855164527893\n",
      "Training FF1:  80%|████████████████    | 2566/3201 [1:17:57<16:46,  1.58s/batch]Batch 2600/3201 Done, mean position loss: 21.735078365802764\n",
      "Training FF1:  80%|████████████████    | 2575/3201 [1:17:59<20:24,  1.96s/batch]Batch 2600/3201 Done, mean position loss: 21.42455389738083\n",
      "Training FF1:  81%|████████████████▎   | 2601/3201 [1:18:00<19:51,  1.99s/batch]Batch 2600/3201 Done, mean position loss: 21.256168558597565\n",
      "Training FF1:  81%|████████████████▎   | 2606/3201 [1:17:59<17:11,  1.73s/batch]Batch 2600/3201 Done, mean position loss: 21.14229011058807\n",
      "Training FF1:  81%|████████████████▎   | 2607/3201 [1:18:01<18:10,  1.84s/batch]Batch 2600/3201 Done, mean position loss: 21.31343138217926\n",
      "Training FF1:  82%|████████████████▍   | 2632/3201 [1:18:02<17:57,  1.89s/batch]Batch 2600/3201 Done, mean position loss: 21.10967197179794\n",
      "Training FF1:  81%|████████████████▏   | 2600/3201 [1:18:03<19:24,  1.94s/batch]Batch 2600/3201 Done, mean position loss: 21.309156880378723\n",
      "Training FF1:  82%|████████████████▎   | 2612/3201 [1:18:03<21:24,  2.18s/batch]Batch 2600/3201 Done, mean position loss: 21.42960160970688\n",
      "Training FF1:  82%|████████████████▍   | 2623/3201 [1:18:04<17:39,  1.83s/batch]Batch 2600/3201 Done, mean position loss: 20.970073130130768\n",
      "Training FF1:  81%|████████████████▏   | 2597/3201 [1:18:13<21:43,  2.16s/batch]Batch 2600/3201 Done, mean position loss: 21.430875251293184\n",
      "Training FF1:  81%|████████████████    | 2579/3201 [1:18:22<20:56,  2.02s/batch]Batch 2600/3201 Done, mean position loss: 21.285183849334715\n",
      "Training FF1:  82%|████████████████▍   | 2621/3201 [1:18:22<20:51,  2.16s/batch]Batch 2600/3201 Done, mean position loss: 21.116485550403596\n",
      "Training FF1:  83%|████████████████▌   | 2644/3201 [1:18:27<16:22,  1.76s/batch]Batch 2600/3201 Done, mean position loss: 21.085526084899904\n",
      "Training FF1:  82%|████████████████▍   | 2635/3201 [1:18:32<14:09,  1.50s/batch]Batch 2600/3201 Done, mean position loss: 21.064466538429258\n",
      "Training FF1:  81%|████████████████▎   | 2601/3201 [1:18:32<21:14,  2.12s/batch]Batch 2600/3201 Done, mean position loss: 21.208841478824617\n",
      "Training FF1:  83%|████████████████▌   | 2648/3201 [1:18:35<18:28,  2.01s/batch]Batch 2600/3201 Done, mean position loss: 21.581923699378965\n",
      "Training FF1:  82%|████████████████▍   | 2624/3201 [1:18:46<19:07,  1.99s/batch]Batch 2600/3201 Done, mean position loss: 21.09064740896225\n",
      "Training FF1:  82%|████████████████▍   | 2634/3201 [1:18:51<14:47,  1.56s/batch]Batch 2600/3201 Done, mean position loss: 21.36309896469116\n",
      "Training FF1:  82%|████████████████▍   | 2637/3201 [1:18:52<16:42,  1.78s/batch]Batch 2600/3201 Done, mean position loss: 21.339294147491454\n",
      "Training FF1:  82%|████████████████▎   | 2613/3201 [1:18:55<16:19,  1.67s/batch]Batch 2600/3201 Done, mean position loss: 21.40189562559128\n",
      "Training FF1:  81%|████████████████▏   | 2598/3201 [1:18:56<19:09,  1.91s/batch]Batch 2600/3201 Done, mean position loss: 21.22688187360764\n",
      "Training FF1:  81%|████████████████▎   | 2604/3201 [1:18:59<17:08,  1.72s/batch]Batch 2600/3201 Done, mean position loss: 21.66334052801132\n",
      "Training FF1:  83%|████████████████▌   | 2642/3201 [1:18:59<15:11,  1.63s/batch]Batch 2600/3201 Done, mean position loss: 21.27189538002014\n",
      "Training FF1:  82%|████████████████▎   | 2618/3201 [1:19:03<19:50,  2.04s/batch]Batch 2600/3201 Done, mean position loss: 22.185565795898437\n",
      "Training FF1:  83%|████████████████▋   | 2662/3201 [1:19:39<16:22,  1.82s/batch]Batch 2600/3201 Done, mean position loss: 21.207101707458495\n",
      "Training FF1:  83%|████████████████▋   | 2667/3201 [1:19:59<16:00,  1.80s/batch]Batch 2600/3201 Done, mean position loss: 21.3760991191864\n",
      "Training FF1:  84%|████████████████▋   | 2673/3201 [1:20:03<14:04,  1.60s/batch]Batch 2700/3201 Done, mean position loss: 22.057667200565337\n",
      "Training FF1:  83%|████████████████▌   | 2651/3201 [1:20:04<15:37,  1.71s/batch]Batch 2700/3201 Done, mean position loss: 22.1727659869194\n",
      "Training FF1:  84%|████████████████▋   | 2678/3201 [1:20:11<13:25,  1.54s/batch]Batch 2700/3201 Done, mean position loss: 21.59118263959885\n",
      "Training FF1:  84%|████████████████▊   | 2684/3201 [1:20:18<12:50,  1.49s/batch]Batch 2700/3201 Done, mean position loss: 21.519160742759702\n",
      "Training FF1:  84%|████████████████▊   | 2693/3201 [1:20:24<12:53,  1.52s/batch]Batch 2700/3201 Done, mean position loss: 21.64198642492294\n",
      "Training FF1:  84%|████████████████▉   | 2704/3201 [1:20:27<11:34,  1.40s/batch]Batch 2700/3201 Done, mean position loss: 21.164418168067932\n",
      "Training FF1:  84%|████████████████▊   | 2689/3201 [1:20:34<14:13,  1.67s/batch]Batch 2700/3201 Done, mean position loss: 21.729604301452635\n",
      "Training FF1:  84%|████████████████▊   | 2690/3201 [1:20:38<12:36,  1.48s/batch]Batch 2700/3201 Done, mean position loss: 21.470756096839906\n",
      "Training FF1:  84%|████████████████▊   | 2694/3201 [1:20:39<17:47,  2.11s/batch]Batch 2700/3201 Done, mean position loss: 21.60671771287918\n",
      "Training FF1:  85%|████████████████▉   | 2714/3201 [1:20:39<14:38,  1.80s/batch]Batch 2700/3201 Done, mean position loss: 21.37575112819672\n",
      "Training FF1:  84%|████████████████▊   | 2683/3201 [1:20:45<16:35,  1.92s/batch]Batch 2700/3201 Done, mean position loss: 21.045877552032472\n",
      "Training FF1:  84%|████████████████▋   | 2678/3201 [1:20:46<13:51,  1.59s/batch]Batch 2700/3201 Done, mean position loss: 21.34984754323959\n",
      "Training FF1:  85%|█████████████████   | 2727/3201 [1:20:49<14:28,  1.83s/batch]Batch 2700/3201 Done, mean position loss: 21.136601090431213\n",
      "Training FF1:  83%|████████████████▌   | 2641/3201 [1:20:51<14:44,  1.58s/batch]Batch 2700/3201 Done, mean position loss: 21.44216603755951\n",
      "Training FF1:  85%|████████████████▉   | 2709/3201 [1:20:51<12:48,  1.56s/batch]Batch 2700/3201 Done, mean position loss: 21.375644116401674\n",
      "Training FF1:  84%|████████████████▋   | 2673/3201 [1:20:54<15:13,  1.73s/batch]Batch 2700/3201 Done, mean position loss: 21.65925982952118\n",
      "Training FF1:  84%|████████████████▊   | 2700/3201 [1:20:54<15:17,  1.83s/batch]Batch 2700/3201 Done, mean position loss: 21.207577641010282\n",
      "Training FF1:  83%|████████████████▋   | 2664/3201 [1:20:55<16:48,  1.88s/batch]Batch 2700/3201 Done, mean position loss: 20.94704476594925\n",
      "Training FF1:  83%|████████████████▌   | 2644/3201 [1:20:56<14:26,  1.56s/batch]Batch 2700/3201 Done, mean position loss: 21.326377775669098\n",
      "Training FF1:  84%|████████████████▊   | 2685/3201 [1:20:59<15:23,  1.79s/batch]Batch 2700/3201 Done, mean position loss: 21.721254496574403\n",
      "Training FF1:  85%|█████████████████   | 2725/3201 [1:20:59<13:43,  1.73s/batch]Batch 2700/3201 Done, mean position loss: 21.080553250312807\n",
      "Training FF1:  84%|████████████████▋   | 2675/3201 [1:21:00<15:23,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 21.27172520160675\n",
      "Training FF1:  85%|████████████████▉   | 2717/3201 [1:21:02<14:47,  1.83s/batch]Batch 2700/3201 Done, mean position loss: 21.408188190460205\n",
      "Training FF1:  84%|████████████████▊   | 2683/3201 [1:21:02<16:45,  1.94s/batch]Batch 2700/3201 Done, mean position loss: 21.400750477313995\n",
      "Training FF1:  85%|█████████████████   | 2734/3201 [1:21:15<14:09,  1.82s/batch]Batch 2700/3201 Done, mean position loss: 21.0835538816452\n",
      "Training FF1:  85%|█████████████████   | 2722/3201 [1:21:16<16:27,  2.06s/batch]Batch 2700/3201 Done, mean position loss: 21.078974545001984\n",
      "Training FF1:  85%|████████████████▉   | 2718/3201 [1:21:19<14:05,  1.75s/batch]Batch 2700/3201 Done, mean position loss: 21.23415306329727\n",
      "Training FF1:  86%|█████████████████▏  | 2748/3201 [1:21:24<13:09,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 21.562462475299835\n",
      "Training FF1:  85%|████████████████▉   | 2718/3201 [1:21:29<13:12,  1.64s/batch]Batch 2700/3201 Done, mean position loss: 21.06524894714355\n",
      "Training FF1:  84%|████████████████▉   | 2703/3201 [1:21:32<13:33,  1.63s/batch]Batch 2700/3201 Done, mean position loss: 21.203911125659943\n",
      "Training FF1:  85%|████████████████▉   | 2708/3201 [1:21:40<12:08,  1.48s/batch]Batch 2700/3201 Done, mean position loss: 21.035980117321014\n",
      "Training FF1:  85%|████████████████▉   | 2714/3201 [1:21:45<13:29,  1.66s/batch]Batch 2700/3201 Done, mean position loss: 21.3264358997345\n",
      "Batch 2700/3201 Done, mean position loss: 21.298777749538424\n",
      "Training FF1:  85%|████████████████▉   | 2712/3201 [1:21:50<12:54,  1.58s/batch]Batch 2700/3201 Done, mean position loss: 21.243757007122042\n",
      "Training FF1:  85%|████████████████▉   | 2713/3201 [1:21:50<13:23,  1.65s/batch]Batch 2700/3201 Done, mean position loss: 21.38836885213852\n",
      "Training FF1:  85%|█████████████████   | 2733/3201 [1:21:52<14:29,  1.86s/batch]Batch 2700/3201 Done, mean position loss: 21.228039243221282\n",
      "Training FF1:  86%|█████████████████▏  | 2752/3201 [1:21:55<13:09,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 22.186206755638125\n",
      "Training FF1:  85%|████████████████▉   | 2710/3201 [1:22:05<14:14,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 21.651878309249877\n",
      "Training FF1:  87%|█████████████████▍  | 2789/3201 [1:22:33<10:12,  1.49s/batch]Batch 2700/3201 Done, mean position loss: 21.210455322265624\n",
      "Training FF1:  86%|█████████████████▏  | 2756/3201 [1:22:51<10:40,  1.44s/batch]Batch 2800/3201 Done, mean position loss: 22.07228945732117\n",
      "Training FF1:  87%|█████████████████▎  | 2769/3201 [1:22:58<13:29,  1.87s/batch]Batch 2800/3201 Done, mean position loss: 21.544357693195344\n",
      "Training FF1:  87%|█████████████████▍  | 2790/3201 [1:23:00<11:39,  1.70s/batch]Batch 2800/3201 Done, mean position loss: 22.129823703765865\n",
      "Training FF1:  87%|█████████████████▍  | 2781/3201 [1:23:01<14:28,  2.07s/batch]Batch 2700/3201 Done, mean position loss: 21.327160604000092\n",
      "Training FF1:  87%|█████████████████▎  | 2778/3201 [1:23:10<12:35,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.528278934955598\n",
      "Training FF1:  88%|█████████████████▌  | 2804/3201 [1:23:15<10:41,  1.62s/batch]Batch 2800/3201 Done, mean position loss: 21.65365464925766\n",
      "Training FF1:  88%|█████████████████▌  | 2804/3201 [1:23:19<09:36,  1.45s/batch]Batch 2800/3201 Done, mean position loss: 21.133568296432493\n",
      "Training FF1:  86%|█████████████████▎  | 2766/3201 [1:23:26<14:05,  1.94s/batch]Batch 2800/3201 Done, mean position loss: 21.715848934650417\n",
      "Training FF1:  87%|█████████████████▍  | 2797/3201 [1:23:33<12:42,  1.89s/batch]Batch 2800/3201 Done, mean position loss: 21.469094123840335\n",
      "Training FF1:  86%|█████████████████▏  | 2750/3201 [1:23:35<16:51,  2.24s/batch]Batch 2800/3201 Done, mean position loss: 21.35802836894989\n",
      "Training FF1:  86%|█████████████████▎  | 2763/3201 [1:23:41<14:19,  1.96s/batch]Batch 2800/3201 Done, mean position loss: 21.60446230649948\n",
      "Training FF1:  87%|█████████████████▍  | 2796/3201 [1:23:41<12:10,  1.80s/batch]Batch 2800/3201 Done, mean position loss: 21.610477957725525\n",
      "Training FF1:  88%|█████████████████▋  | 2827/3201 [1:23:41<08:40,  1.39s/batch]Batch 2800/3201 Done, mean position loss: 21.289493939876557\n",
      "Training FF1:  87%|█████████████████▍  | 2792/3201 [1:23:42<12:18,  1.80s/batch]Batch 2800/3201 Done, mean position loss: 21.03299825191498\n",
      "Training FF1:  88%|█████████████████▌  | 2802/3201 [1:23:42<11:33,  1.74s/batch]Batch 2800/3201 Done, mean position loss: 21.35041131734848\n",
      "Training FF1:  87%|█████████████████▍  | 2787/3201 [1:23:44<12:17,  1.78s/batch]Batch 2800/3201 Done, mean position loss: 21.342632598876953\n",
      "Training FF1:  88%|█████████████████▌  | 2807/3201 [1:23:44<11:07,  1.69s/batch]Batch 2800/3201 Done, mean position loss: 21.42494886159897\n",
      "Training FF1:  88%|█████████████████▌  | 2810/3201 [1:23:50<11:39,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.72123533010483\n",
      "Training FF1:  88%|█████████████████▌  | 2807/3201 [1:23:50<08:50,  1.35s/batch]Batch 2800/3201 Done, mean position loss: 21.111517329216003\n",
      "Training FF1:  87%|█████████████████▍  | 2788/3201 [1:23:51<11:59,  1.74s/batch]Batch 2800/3201 Done, mean position loss: 20.92693943500519\n",
      "Training FF1:  88%|█████████████████▌  | 2806/3201 [1:23:51<11:45,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.211161863803866\n",
      "Training FF1:  89%|█████████████████▋  | 2835/3201 [1:23:52<09:32,  1.56s/batch]Batch 2800/3201 Done, mean position loss: 21.235014634132384\n",
      "Training FF1:  88%|█████████████████▌  | 2811/3201 [1:23:57<10:58,  1.69s/batch]Batch 2800/3201 Done, mean position loss: 21.071064908504486\n",
      "Training FF1:  86%|█████████████████▏  | 2749/3201 [1:23:59<12:37,  1.68s/batch]Batch 2800/3201 Done, mean position loss: 21.400653727054596\n",
      "Training FF1:  87%|█████████████████▍  | 2800/3201 [1:24:06<13:36,  2.04s/batch]Batch 2800/3201 Done, mean position loss: 21.38631446361542\n",
      "Training FF1:  88%|█████████████████▌  | 2811/3201 [1:24:09<11:17,  1.74s/batch]Batch 2800/3201 Done, mean position loss: 21.06927345275879\n",
      "Training FF1:  88%|█████████████████▌  | 2811/3201 [1:24:09<12:39,  1.95s/batch]Batch 2800/3201 Done, mean position loss: 21.080374205112456\n",
      "Training FF1:  89%|█████████████████▊  | 2846/3201 [1:24:15<12:17,  2.08s/batch]Batch 2800/3201 Done, mean position loss: 21.20287451505661\n",
      "Training FF1:  87%|█████████████████▍  | 2790/3201 [1:24:22<12:41,  1.85s/batch]Batch 2800/3201 Done, mean position loss: 21.51969485759735\n",
      "Training FF1:  88%|█████████████████▋  | 2827/3201 [1:24:27<10:40,  1.71s/batch]Batch 2800/3201 Done, mean position loss: 21.216381294727324\n",
      "Training FF1:  87%|█████████████████▎  | 2778/3201 [1:24:30<12:09,  1.72s/batch]Batch 2800/3201 Done, mean position loss: 21.061209111213685\n",
      "Training FF1:  88%|█████████████████▌  | 2819/3201 [1:24:41<13:03,  2.05s/batch]Batch 2800/3201 Done, mean position loss: 21.322922728061677\n",
      "Training FF1:  87%|█████████████████▍  | 2799/3201 [1:24:43<12:32,  1.87s/batch]Batch 2800/3201 Done, mean position loss: 21.291470737457274\n",
      "Training FF1:  88%|█████████████████▋  | 2823/3201 [1:24:44<11:06,  1.76s/batch]Batch 2800/3201 Done, mean position loss: 21.01879175901413\n",
      "Training FF1:  88%|█████████████████▌  | 2812/3201 [1:24:46<11:47,  1.82s/batch]Batch 2800/3201 Done, mean position loss: 21.228774001598357\n",
      "Training FF1:  90%|█████████████████▉  | 2865/3201 [1:24:49<10:00,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.204595007896422\n",
      "Training FF1:  89%|█████████████████▋  | 2838/3201 [1:24:52<11:39,  1.93s/batch]Batch 2800/3201 Done, mean position loss: 21.34669601917267\n",
      "Training FF1:  90%|█████████████████▉  | 2870/3201 [1:24:53<10:25,  1.89s/batch]Batch 2800/3201 Done, mean position loss: 22.160258502960204\n",
      "Training FF1:  88%|█████████████████▋  | 2828/3201 [1:25:13<13:03,  2.10s/batch]Batch 2800/3201 Done, mean position loss: 21.621565473079684\n",
      "Training FF1:  89%|█████████████████▊  | 2855/3201 [1:25:34<10:19,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.17672920465469\n",
      "Training FF1:  89%|█████████████████▋  | 2837/3201 [1:25:46<11:26,  1.89s/batch]Batch 2900/3201 Done, mean position loss: 22.035513377189638\n",
      "Training FF1:  89%|█████████████████▊  | 2841/3201 [1:25:54<11:28,  1.91s/batch]Batch 2900/3201 Done, mean position loss: 21.53216492176056\n",
      "Training FF1:  90%|█████████████████▉  | 2874/3201 [1:25:55<09:08,  1.68s/batch]Batch 2900/3201 Done, mean position loss: 22.154436943531035\n",
      "Training FF1:  90%|█████████████████▉  | 2876/3201 [1:26:08<09:08,  1.69s/batch]Batch 2800/3201 Done, mean position loss: 21.306503298282625\n",
      "Training FF1:  89%|█████████████████▋  | 2837/3201 [1:26:12<11:14,  1.85s/batch]Batch 2900/3201 Done, mean position loss: 21.493661828041077\n",
      "Training FF1:  90%|█████████████████▉  | 2870/3201 [1:26:20<09:05,  1.65s/batch]Batch 2900/3201 Done, mean position loss: 21.66073583126068\n",
      "Training FF1:  90%|█████████████████▉  | 2867/3201 [1:26:25<10:17,  1.85s/batch]Batch 2900/3201 Done, mean position loss: 21.131474587917328\n",
      "Training FF1:  88%|█████████████████▌  | 2811/3201 [1:26:26<11:42,  1.80s/batch]Batch 2900/3201 Done, mean position loss: 21.6402698969841\n",
      "Training FF1:  89%|█████████████████▊  | 2858/3201 [1:26:31<11:38,  2.04s/batch]Batch 2900/3201 Done, mean position loss: 21.457038292884825\n",
      "Training FF1:  90%|██████████████████  | 2881/3201 [1:26:36<09:50,  1.85s/batch]Batch 2900/3201 Done, mean position loss: 21.289845480918885\n",
      "Training FF1:  89%|█████████████████▉  | 2863/3201 [1:26:37<10:03,  1.79s/batch]Batch 2900/3201 Done, mean position loss: 21.355600953102112\n",
      "Training FF1:  91%|██████████████████▎ | 2925/3201 [1:26:43<07:44,  1.68s/batch]Batch 2900/3201 Done, mean position loss: 21.325590314865114\n",
      "Training FF1:  90%|█████████████████▉  | 2868/3201 [1:26:44<10:50,  1.95s/batch]Batch 2900/3201 Done, mean position loss: 21.612752788066864\n",
      "Training FF1:  89%|█████████████████▊  | 2860/3201 [1:26:45<11:32,  2.03s/batch]Batch 2900/3201 Done, mean position loss: 21.009087526798247\n",
      "Batch 2900/3201 Done, mean position loss: 21.30991148471832\n",
      "Training FF1:  90%|█████████████████▉  | 2875/3201 [1:26:46<10:41,  1.97s/batch]Batch 2900/3201 Done, mean position loss: 21.185933957099913\n",
      "Training FF1:  91%|██████████████████▏ | 2901/3201 [1:26:46<08:10,  1.63s/batch]Batch 2900/3201 Done, mean position loss: 21.412690672874447\n",
      "Training FF1:  91%|██████████████████▏ | 2916/3201 [1:26:46<09:01,  1.90s/batch]Batch 2900/3201 Done, mean position loss: 21.602372176647187\n",
      "Training FF1:  90%|█████████████████▉  | 2869/3201 [1:26:47<11:26,  2.07s/batch]Batch 2900/3201 Done, mean position loss: 21.103506429195402\n",
      "Training FF1:  91%|██████████████████▏ | 2904/3201 [1:26:52<09:27,  1.91s/batch]Batch 2900/3201 Done, mean position loss: 21.675857958793642\n",
      "Training FF1:  88%|█████████████████▋  | 2825/3201 [1:26:54<13:19,  2.13s/batch]Batch 2900/3201 Done, mean position loss: 20.917232596874236\n",
      "Training FF1:  90%|█████████████████▉  | 2865/3201 [1:26:55<11:09,  1.99s/batch]Batch 2900/3201 Done, mean position loss: 21.24637389659882\n",
      "Training FF1:  91%|██████████████████▏ | 2916/3201 [1:27:04<09:53,  2.08s/batch]Batch 2900/3201 Done, mean position loss: 21.044850471019743\n",
      "Training FF1:  91%|██████████████████▏ | 2908/3201 [1:27:06<08:18,  1.70s/batch]Batch 2900/3201 Done, mean position loss: 21.376227333545685\n",
      "Training FF1:  90%|██████████████████  | 2896/3201 [1:27:06<08:49,  1.74s/batch]Batch 2900/3201 Done, mean position loss: 21.374693846702577\n",
      "Training FF1:  91%|██████████████████▎ | 2922/3201 [1:27:14<09:16,  1.99s/batch]Batch 2900/3201 Done, mean position loss: 21.05518175840378\n",
      "Training FF1:  90%|██████████████████  | 2893/3201 [1:27:16<10:13,  1.99s/batch]Batch 2900/3201 Done, mean position loss: 21.20369234085083\n",
      "Training FF1:  90%|██████████████████  | 2888/3201 [1:27:20<09:25,  1.81s/batch]Batch 2900/3201 Done, mean position loss: 21.09246370792389\n",
      "Training FF1:  91%|██████████████████▏ | 2914/3201 [1:27:28<07:22,  1.54s/batch]Batch 2900/3201 Done, mean position loss: 21.510218245983125\n",
      "Training FF1:  91%|██████████████████▎ | 2926/3201 [1:27:31<08:06,  1.77s/batch]Batch 2900/3201 Done, mean position loss: 21.19982776641846\n",
      "Training FF1:  92%|██████████████████▍ | 2955/3201 [1:27:31<06:27,  1.58s/batch]Batch 2900/3201 Done, mean position loss: 21.06689119577408\n",
      "Training FF1:  90%|██████████████████  | 2890/3201 [1:27:39<09:01,  1.74s/batch]Batch 2900/3201 Done, mean position loss: 21.254199836254116\n",
      "Training FF1:  92%|██████████████████▎ | 2929/3201 [1:27:42<07:53,  1.74s/batch]Batch 2900/3201 Done, mean position loss: 21.288446326255798\n",
      "Training FF1:  91%|██████████████████▏ | 2914/3201 [1:27:45<10:58,  2.29s/batch]Batch 2900/3201 Done, mean position loss: 21.017734811306\n",
      "Training FF1:  91%|██████████████████▏ | 2911/3201 [1:27:50<08:53,  1.84s/batch]Batch 2900/3201 Done, mean position loss: 21.3099738907814\n",
      "Training FF1:  91%|██████████████████▏ | 2905/3201 [1:27:53<10:19,  2.09s/batch]Batch 2900/3201 Done, mean position loss: 21.19478280067444\n",
      "Training FF1:  92%|██████████████████▎ | 2939/3201 [1:27:55<08:00,  1.84s/batch]Batch 2900/3201 Done, mean position loss: 21.17982965707779\n",
      "Training FF1:  92%|██████████████████▎ | 2930/3201 [1:27:59<09:57,  2.20s/batch]Batch 2900/3201 Done, mean position loss: 22.11435191869736\n",
      "Training FF1:  91%|██████████████████▎ | 2923/3201 [1:28:11<08:58,  1.94s/batch]Batch 2900/3201 Done, mean position loss: 21.5901100564003\n",
      "Training FF1:  92%|██████████████████▎ | 2934/3201 [1:28:38<08:52,  1.99s/batch]Batch 2900/3201 Done, mean position loss: 21.140924284458162\n",
      "Training FF1:  92%|██████████████████▍ | 2957/3201 [1:28:46<07:48,  1.92s/batch]Batch 3000/3201 Done, mean position loss: 21.996100149154664\n",
      "Training FF1:  92%|██████████████████▍ | 2947/3201 [1:28:55<07:08,  1.69s/batch]Batch 3000/3201 Done, mean position loss: 21.481285848617553\n",
      "Training FF1:  94%|██████████████████▋ | 2993/3201 [1:29:07<05:03,  1.46s/batch]Batch 3000/3201 Done, mean position loss: 22.109508895874022\n",
      "Training FF1:  93%|██████████████████▌ | 2976/3201 [1:29:09<07:33,  2.02s/batch]Batch 2900/3201 Done, mean position loss: 21.280750765800477\n",
      "Training FF1:  93%|██████████████████▌ | 2979/3201 [1:29:17<06:27,  1.74s/batch]Batch 3000/3201 Done, mean position loss: 21.488207173347476\n",
      "Training FF1:  93%|██████████████████▌ | 2977/3201 [1:29:22<06:59,  1.87s/batch]Batch 3000/3201 Done, mean position loss: 21.654049782752992\n",
      "Training FF1:  93%|██████████████████▌ | 2961/3201 [1:29:29<08:17,  2.07s/batch]Batch 3000/3201 Done, mean position loss: 21.461694178581237\n",
      "Training FF1:  93%|██████████████████▋ | 2988/3201 [1:29:32<06:43,  1.89s/batch]Batch 3000/3201 Done, mean position loss: 21.124733641147614\n",
      "Training FF1:  92%|██████████████████▍ | 2957/3201 [1:29:32<07:55,  1.95s/batch]Batch 3000/3201 Done, mean position loss: 21.63707013845444\n",
      "Training FF1:  95%|██████████████████▉ | 3026/3201 [1:29:38<04:17,  1.47s/batch]Batch 3000/3201 Done, mean position loss: 21.293815693855286\n",
      "Training FF1:  94%|██████████████████▊ | 3004/3201 [1:29:39<06:51,  2.09s/batch]Batch 3000/3201 Done, mean position loss: 21.340993490219116\n",
      "Training FF1:  95%|██████████████████▉ | 3031/3201 [1:29:40<04:31,  1.60s/batch]Batch 3000/3201 Done, mean position loss: 21.286848249435423\n",
      "Training FF1:  94%|██████████████████▋ | 2998/3201 [1:29:45<07:16,  2.15s/batch]Batch 3000/3201 Done, mean position loss: 21.556735973358155\n",
      "Training FF1:  93%|██████████████████▋ | 2988/3201 [1:29:46<08:12,  2.31s/batch]Batch 3000/3201 Done, mean position loss: 21.376808924674986\n",
      "Training FF1:  93%|██████████████████▌ | 2964/3201 [1:29:47<08:13,  2.08s/batch]Batch 3000/3201 Done, mean position loss: 21.568725020885466\n",
      "Training FF1:  93%|██████████████████▋ | 2992/3201 [1:29:51<06:46,  1.94s/batch]Batch 3000/3201 Done, mean position loss: 21.08535499572754\n",
      "Training FF1:  94%|██████████████████▊ | 3007/3201 [1:29:52<07:51,  2.43s/batch]Batch 3000/3201 Done, mean position loss: 21.280817217826844\n",
      "Training FF1:  92%|██████████████████▍ | 2941/3201 [1:29:53<10:23,  2.40s/batch]Batch 3000/3201 Done, mean position loss: 21.007953412532807\n",
      "Training FF1:  93%|██████████████████▌ | 2969/3201 [1:29:56<06:16,  1.62s/batch]Batch 3000/3201 Done, mean position loss: 21.66988288640976\n",
      "Training FF1:  93%|██████████████████▌ | 2970/3201 [1:29:57<05:24,  1.40s/batch]Batch 3000/3201 Done, mean position loss: 21.184700441360473\n",
      "Training FF1:  93%|██████████████████▌ | 2971/3201 [1:29:58<05:27,  1.43s/batch]Batch 3000/3201 Done, mean position loss: 21.212851510047912\n",
      "Training FF1:  94%|██████████████████▋ | 2995/3201 [1:29:58<06:11,  1.81s/batch]Batch 3000/3201 Done, mean position loss: 20.88851181268692\n",
      "Training FF1:  95%|██████████████████▉ | 3026/3201 [1:30:06<06:40,  2.29s/batch]Batch 3000/3201 Done, mean position loss: 21.35123927593231\n",
      "Training FF1:  94%|██████████████████▊ | 3008/3201 [1:30:11<07:30,  2.33s/batch]Batch 3000/3201 Done, mean position loss: 21.034672343730925\n",
      "Training FF1:  94%|██████████████████▊ | 3003/3201 [1:30:11<06:22,  1.93s/batch]Batch 3000/3201 Done, mean position loss: 21.3521684718132\n",
      "Training FF1:  94%|██████████████████▊ | 3007/3201 [1:30:21<05:30,  1.70s/batch]Batch 3000/3201 Done, mean position loss: 21.06003659248352\n",
      "Training FF1:  93%|██████████████████▋ | 2988/3201 [1:30:23<07:16,  2.05s/batch]Batch 3000/3201 Done, mean position loss: 21.068467769622803\n",
      "Training FF1:  94%|██████████████████▊ | 3004/3201 [1:30:28<05:25,  1.65s/batch]Batch 3000/3201 Done, mean position loss: 21.15614053249359\n",
      "Training FF1:  95%|██████████████████▉ | 3035/3201 [1:30:37<04:37,  1.67s/batch]Batch 3000/3201 Done, mean position loss: 21.163634543418883\n",
      "Training FF1:  95%|██████████████████▉ | 3025/3201 [1:30:39<05:14,  1.79s/batch]Batch 3000/3201 Done, mean position loss: 21.053427343368533\n",
      "Training FF1:  93%|██████████████████▋ | 2991/3201 [1:30:40<07:30,  2.14s/batch]Batch 3000/3201 Done, mean position loss: 21.50992584466934\n",
      "Training FF1:  96%|███████████████████▏| 3066/3201 [1:30:45<04:04,  1.81s/batch]Batch 3000/3201 Done, mean position loss: 21.241804225444795\n",
      "Training FF1:  95%|██████████████████▉ | 3034/3201 [1:30:47<04:49,  1.73s/batch]Batch 3000/3201 Done, mean position loss: 21.273388171195982\n",
      "Training FF1:  95%|███████████████████ | 3048/3201 [1:30:49<04:36,  1.81s/batch]Batch 3000/3201 Done, mean position loss: 21.27666160583496\n",
      "Training FF1:  95%|███████████████████ | 3041/3201 [1:30:55<05:41,  2.13s/batch]Batch 3000/3201 Done, mean position loss: 21.18565654039383\n",
      "Training FF1:  95%|██████████████████▉ | 3033/3201 [1:30:58<04:57,  1.77s/batch]Batch 3000/3201 Done, mean position loss: 21.003281037807465\n",
      "Training FF1:  94%|██████████████████▊ | 3008/3201 [1:31:08<05:02,  1.57s/batch]Batch 3000/3201 Done, mean position loss: 21.167443556785585\n",
      "Training FF1:  95%|███████████████████ | 3052/3201 [1:31:10<04:35,  1.85s/batch]Batch 3000/3201 Done, mean position loss: 22.1218119263649\n",
      "Training FF1:  95%|███████████████████ | 3048/3201 [1:31:20<04:00,  1.57s/batch]Batch 3000/3201 Done, mean position loss: 21.56882051944733\n",
      "Training FF1:  96%|███████████████████▎| 3087/3201 [1:31:47<04:04,  2.14s/batch]Batch 3000/3201 Done, mean position loss: 21.12968154191971\n",
      "Training FF1:  95%|███████████████████ | 3041/3201 [1:31:49<04:22,  1.64s/batch]Batch 3100/3201 Done, mean position loss: 22.03450538635254\n",
      "Training FF1:  95%|██████████████████▉ | 3040/3201 [1:31:56<04:48,  1.79s/batch]Batch 3100/3201 Done, mean position loss: 21.46096872806549\n",
      "Training FF1:  97%|███████████████████▍| 3116/3201 [1:32:15<02:47,  1.97s/batch]Batch 3100/3201 Done, mean position loss: 22.101590495109555\n",
      "Training FF1:  95%|███████████████████ | 3052/3201 [1:32:24<04:35,  1.85s/batch]Batch 3100/3201 Done, mean position loss: 21.484676134586337\n",
      "Training FF1:  95%|███████████████████ | 3049/3201 [1:32:25<04:13,  1.67s/batch]Batch 3000/3201 Done, mean position loss: 21.2784752035141\n",
      "Training FF1:  96%|███████████████████▏| 3066/3201 [1:32:30<04:58,  2.21s/batch]Batch 3100/3201 Done, mean position loss: 21.634442374706268\n",
      "Training FF1:  97%|███████████████████▎| 3089/3201 [1:32:40<04:31,  2.43s/batch]Batch 3100/3201 Done, mean position loss: 21.44232079744339\n",
      "Training FF1:  97%|███████████████████▍| 3108/3201 [1:32:42<02:30,  1.62s/batch]Batch 3100/3201 Done, mean position loss: 21.63755177497864\n",
      "Training FF1:  97%|███████████████████▎| 3100/3201 [1:32:46<02:44,  1.63s/batch]Batch 3100/3201 Done, mean position loss: 21.119762089252472\n",
      "Training FF1:  97%|███████████████████▍| 3102/3201 [1:32:48<03:10,  1.92s/batch]Batch 3100/3201 Done, mean position loss: 21.57358466863632\n",
      "Training FF1:  98%|███████████████████▌| 3129/3201 [1:32:48<02:01,  1.69s/batch]Batch 3100/3201 Done, mean position loss: 21.2631320977211\n",
      "Training FF1:  97%|███████████████████▎| 3095/3201 [1:32:51<03:05,  1.75s/batch]Batch 3100/3201 Done, mean position loss: 21.392553524971007\n",
      "Training FF1:  97%|███████████████████▎| 3092/3201 [1:32:53<03:44,  2.06s/batch]Batch 3100/3201 Done, mean position loss: 21.52128862142563\n",
      "Training FF1:  97%|███████████████████▎| 3100/3201 [1:32:53<02:32,  1.51s/batch]Batch 3100/3201 Done, mean position loss: 21.291321604251863\n",
      "Training FF1:  97%|███████████████████▍| 3114/3201 [1:32:54<02:48,  1.93s/batch]Batch 3100/3201 Done, mean position loss: 21.315847787857052\n",
      "Training FF1:  95%|███████████████████ | 3055/3201 [1:32:56<04:28,  1.84s/batch]Batch 3100/3201 Done, mean position loss: 21.611357946395874\n",
      "Training FF1:  96%|███████████████████▏| 3070/3201 [1:32:58<04:38,  2.13s/batch]Batch 3100/3201 Done, mean position loss: 21.009239199161527\n",
      "Training FF1:  97%|███████████████████▍| 3112/3201 [1:32:59<02:33,  1.72s/batch]Batch 3100/3201 Done, mean position loss: 21.068314225673674\n",
      "Training FF1:  97%|███████████████████▍| 3109/3201 [1:33:01<02:37,  1.71s/batch]Batch 3100/3201 Done, mean position loss: 21.136019475460053\n",
      "Training FF1:  96%|███████████████████▏| 3071/3201 [1:33:04<04:19,  1.99s/batch]Batch 3100/3201 Done, mean position loss: 21.196438724994657\n",
      "Training FF1:  97%|███████████████████▍| 3108/3201 [1:33:06<02:40,  1.72s/batch]Batch 3100/3201 Done, mean position loss: 21.33212786912918\n",
      "Training FF1:  96%|███████████████████▏| 3067/3201 [1:33:09<04:12,  1.89s/batch]Batch 3100/3201 Done, mean position loss: 21.26233055591583\n",
      "Training FF1:  97%|███████████████████▍| 3101/3201 [1:33:09<03:06,  1.86s/batch]Batch 3100/3201 Done, mean position loss: 20.899070730209353\n",
      "Training FF1:  96%|███████████████████▏| 3079/3201 [1:33:12<03:45,  1.84s/batch]Batch 3100/3201 Done, mean position loss: 21.33195147037506\n",
      "Training FF1:  98%|███████████████████▌| 3124/3201 [1:33:25<02:37,  2.05s/batch]Batch 3100/3201 Done, mean position loss: 21.046420748233796\n",
      "Training FF1:  97%|███████████████████▎| 3094/3201 [1:33:31<02:57,  1.66s/batch]Batch 3100/3201 Done, mean position loss: 21.010294775962826\n",
      "Training FF1:  96%|███████████████████▏| 3072/3201 [1:33:35<03:45,  1.74s/batch]Batch 3100/3201 Done, mean position loss: 21.034535760879514\n",
      "Training FF1:  97%|███████████████████▍| 3119/3201 [1:33:37<02:36,  1.91s/batch]Batch 3100/3201 Done, mean position loss: 21.143426549434665\n",
      "Training FF1:  96%|███████████████████▎| 3085/3201 [1:33:41<03:09,  1.64s/batch]Batch 3100/3201 Done, mean position loss: 21.167983300685883\n",
      "Training FF1:  98%|███████████████████▌| 3126/3201 [1:33:44<02:48,  2.25s/batch]Batch 3100/3201 Done, mean position loss: 21.01682202100754\n",
      "Training FF1:  99%|███████████████████▊| 3167/3201 [1:33:53<01:03,  1.88s/batch]Batch 3100/3201 Done, mean position loss: 21.214736983776092\n",
      "Training FF1:  99%|███████████████████▊| 3168/3201 [1:33:54<00:54,  1.65s/batch]Batch 3100/3201 Done, mean position loss: 21.25799524784088\n",
      "Training FF1:  97%|███████████████████▎| 3094/3201 [1:33:57<02:48,  1.58s/batch]Batch 3100/3201 Done, mean position loss: 21.16491733312607\n",
      "Training FF1:  98%|███████████████████▌| 3135/3201 [1:33:59<02:11,  1.99s/batch]Batch 3100/3201 Done, mean position loss: 21.48243614673614\n",
      "Training FF1:  97%|███████████████████▍| 3120/3201 [1:34:00<02:38,  1.96s/batch]Batch 3100/3201 Done, mean position loss: 21.246705696582794\n",
      "Training FF1:  98%|███████████████████▌| 3126/3201 [1:34:08<02:02,  1.63s/batch]Batch 3100/3201 Done, mean position loss: 21.029636471271516\n",
      "Training FF1:  98%|███████████████████▌| 3133/3201 [1:34:08<01:43,  1.52s/batch]Batch 3100/3201 Done, mean position loss: 22.07389967441559\n",
      "Training FF1:  97%|███████████████████▍| 3112/3201 [1:34:18<02:44,  1.85s/batch]Batch 3100/3201 Done, mean position loss: 21.12975292444229\n",
      "Training FF1:  99%|███████████████████▋| 3154/3201 [1:34:29<01:31,  1.95s/batch]Batch 3100/3201 Done, mean position loss: 21.531577467918396\n",
      "Training FF1:  99%|███████████████████▋| 3160/3201 [1:34:43<01:21,  2.00s/batch]Batch 3100/3201 Done, mean position loss: 21.119041118621826\n",
      "Training FF1:  98%|███████████████████▌| 3134/3201 [1:34:48<02:01,  1.81s/batch]Batch 3200/3201 Done, mean position loss: 22.00070126533508\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:34:48<00:00,  1.78s/batch]\n",
      "Done...\n",
      "Training FF1:  96%|███████████████████▏| 3080/3201 [1:34:54<03:59,  1.98s/batch]Batch 3200/3201 Done, mean position loss: 21.477465310096743\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:34:54<00:00,  1.78s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3190/3201 [1:35:20<00:16,  1.54s/batch]Batch 3200/3201 Done, mean position loss: 21.489345307350156\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:19<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1:  98%|███████████████████▌| 3122/3201 [1:35:22<03:00,  2.28s/batch]Batch 3200/3201 Done, mean position loss: 22.105027248859408\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:23<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3161/3201 [1:35:28<01:12,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 21.260527083873747\n",
      "Training FF1:  99%|███████████████████▊| 3163/3201 [1:35:30<01:00,  1.60s/batch]Batch 3200/3201 Done, mean position loss: 21.595273058414456\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:30<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3190/3201 [1:35:38<00:17,  1.58s/batch]Batch 3200/3201 Done, mean position loss: 21.637732751369477\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:37<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1:  98%|███████████████████▋| 3151/3201 [1:35:39<01:12,  1.46s/batch]Batch 3200/3201 Done, mean position loss: 21.432925081253053\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:39<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3161/3201 [1:35:41<01:03,  1.59s/batch]Batch 3200/3201 Done, mean position loss: 21.386655111312866\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:42<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3200/3201 [1:35:43<00:01,  1.26s/batch]Batch 3200/3201 Done, mean position loss: 21.109896972179413\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:43<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3194/3201 [1:35:44<00:10,  1.47s/batch]Batch 3200/3201 Done, mean position loss: 21.272800359725952\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:44<00:00,  1.79s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3187/3201 [1:35:47<00:20,  1.45s/batch]Batch 3200/3201 Done, mean position loss: 21.56112682580948\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:46<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3200/3201 [1:35:47<00:01,  1.20s/batch]Batch 3200/3201 Done, mean position loss: 21.509500217437747\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:48<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3166/3201 [1:35:48<00:47,  1.36s/batch]Batch 3200/3201 Done, mean position loss: 21.11413081407547\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:48<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3200/3201 [1:35:49<00:01,  1.34s/batch]Batch 3200/3201 Done, mean position loss: 21.272887763977053\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:49<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3164/3201 [1:35:51<00:55,  1.50s/batch]Batch 3200/3201 Done, mean position loss: 21.294090099334717\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:51<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▋| 3153/3201 [1:35:51<00:58,  1.23s/batch]Batch 3200/3201 Done, mean position loss: 21.304899275302887\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:51<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3180/3201 [1:35:52<00:22,  1.08s/batch]Batch 3200/3201 Done, mean position loss: 21.050509934425353\n",
      "Training FF1:  99%|███████████████████▋| 3159/3201 [1:35:52<01:01,  1.46s/batch]Batch 3200/3201 Done, mean position loss: 21.62583109855652\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:52<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:52<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  98%|███████████████████▋| 3141/3201 [1:35:52<01:27,  1.45s/batch]Batch 3200/3201 Done, mean position loss: 20.99029578924179\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:53<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3175/3201 [1:35:53<00:29,  1.13s/batch]Batch 3200/3201 Done, mean position loss: 21.177585480213168\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:53<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▋| 3158/3201 [1:35:57<00:45,  1.06s/batch]Batch 3200/3201 Done, mean position loss: 21.311705334186556\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:56<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  98%|███████████████████▌| 3122/3201 [1:35:58<01:15,  1.05batch/s]Batch 3200/3201 Done, mean position loss: 20.887440855503083\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:35:58<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3178/3201 [1:35:59<00:14,  1.63batch/s]Batch 3200/3201 Done, mean position loss: 21.22472805976868\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:00<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3173/3201 [1:36:05<00:22,  1.24batch/s]Batch 3200/3201 Done, mean position loss: 21.025408036708832\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:05<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3192/3201 [1:36:08<00:07,  1.20batch/s]Batch 3200/3201 Done, mean position loss: 20.998832495212554\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:08<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3196/3201 [1:36:10<00:02,  1.91batch/s]Batch 3200/3201 Done, mean position loss: 21.000818300247193\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:10<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3196/3201 [1:36:10<00:02,  1.77batch/s]Batch 3200/3201 Done, mean position loss: 21.12164066314697\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:10<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3188/3201 [1:36:12<00:05,  2.44batch/s]Batch 3200/3201 Done, mean position loss: 21.004116978645325\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:12<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Batch 3200/3201 Done, mean position loss: 21.150939271450042\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:12<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3191/3201 [1:36:14<00:06,  1.51batch/s]Batch 3200/3201 Done, mean position loss: 22.090748500823974\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:14<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3199/3201 [1:36:15<00:00,  3.26batch/s]Batch 3200/3201 Done, mean position loss: 21.14465377092361\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:16<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3169/3201 [1:36:16<00:13,  2.35batch/s]Batch 3200/3201 Done, mean position loss: 21.197044911384584\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:16<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Batch 3200/3201 Done, mean position loss: 21.235229461193086\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:16<00:00,  1.80s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3195/3201 [1:36:17<00:04,  1.32batch/s]Batch 3200/3201 Done, mean position loss: 21.230990624427797\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:18<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3200/3201 [1:36:20<00:00,  2.09batch/s]Batch 3200/3201 Done, mean position loss: 21.48335042238235\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:20<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3167/3201 [1:36:20<00:08,  3.99batch/s]Batch 3200/3201 Done, mean position loss: 20.976652388572695\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:20<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3169/3201 [1:36:21<00:07,  4.03batch/s]Batch 3200/3201 Done, mean position loss: 21.531852614879607\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:21<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▊| 3179/3201 [1:36:23<00:04,  4.45batch/s]Batch 3200/3201 Done, mean position loss: 21.110796518325806\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:23<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1:  99%|███████████████████▉| 3184/3201 [1:36:24<00:03,  4.56batch/s]Batch 3200/3201 Done, mean position loss: 21.112570683956147\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:24<00:00,  1.81s/batch]\n",
      "Done...\n",
      "Training FF1: 100%|███████████████████▉| 3200/3201 [1:36:27<00:00,  5.23batch/s]Batch 3200/3201 Done, mean position loss: 21.240037074089052\n",
      "Training FF1: 100%|████████████████████| 3201/3201 [1:36:27<00:00,  1.81s/batch]\n",
      "Done...\n",
      "model00...\n",
      "model01...\n",
      "model02...\n",
      "model03...\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...\n",
      "model13...\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training NF2:   1%|▏                    | 95/10001 [03:08<4:51:10,  1.76s/batch]Batch 100/10001 Done, mean position loss: 29.023512599468233\n",
      "Training NF2:   1%|▏                    | 95/10001 [03:09<4:26:13,  1.61s/batch]Batch 100/10001 Done, mean position loss: 28.4903089594841\n",
      "Training NF2:   1%|▏                    | 96/10001 [03:11<4:33:23,  1.66s/batch]Batch 100/10001 Done, mean position loss: 29.24479705095291\n",
      "Training NF2:   1%|▏                   | 100/10001 [03:12<4:54:00,  1.78s/batch]Batch 100/10001 Done, mean position loss: 28.439609870910644\n",
      "Training NF2:   1%|▏                   | 101/10001 [03:12<5:08:28,  1.87s/batch]Batch 100/10001 Done, mean position loss: 28.37726750135422\n",
      "Training NF2:   1%|▏                    | 95/10001 [03:12<4:25:50,  1.61s/batch]Batch 100/10001 Done, mean position loss: 30.165880670547487\n",
      "Training NF2:   1%|▏                    | 96/10001 [03:12<4:16:30,  1.55s/batch]Batch 100/10001 Done, mean position loss: 29.189612245559694\n",
      "Training NF2:   1%|▏                   | 100/10001 [03:13<5:23:44,  1.96s/batch]Batch 100/10001 Done, mean position loss: 27.55652131557465\n",
      "Training NF2:   1%|▏                    | 94/10001 [03:13<6:29:43,  2.36s/batch]Batch 100/10001 Done, mean position loss: 30.082508127689362\n",
      "Training NF2:   1%|▏                   | 101/10001 [03:13<4:37:11,  1.68s/batch]Batch 100/10001 Done, mean position loss: 29.149116861820218\n",
      "Training NF2:   1%|▏                   | 100/10001 [03:14<5:11:14,  1.89s/batch]Batch 100/10001 Done, mean position loss: 28.818590784072878\n",
      "Training NF2:   1%|▏                    | 98/10001 [03:14<4:25:36,  1.61s/batch]Batch 100/10001 Done, mean position loss: 28.624671382904054\n",
      "Training NF2:   1%|▏                    | 98/10001 [03:15<4:44:22,  1.72s/batch]Batch 100/10001 Done, mean position loss: 28.14345379829407\n",
      "Training NF2:   1%|▏                    | 95/10001 [03:16<4:59:29,  1.81s/batch]Batch 100/10001 Done, mean position loss: 29.629839704036712\n",
      "Training NF2:   1%|▏                    | 96/10001 [03:16<5:10:34,  1.88s/batch]Batch 100/10001 Done, mean position loss: 29.070945601463315\n",
      "Training NF2:   1%|▏                   | 106/10001 [03:16<4:47:01,  1.74s/batch]Batch 100/10001 Done, mean position loss: 30.400542566776274\n",
      "Training NF2:   1%|▏                    | 99/10001 [03:16<4:41:11,  1.70s/batch]Batch 100/10001 Done, mean position loss: 28.712285947799682\n",
      "Training NF2:   1%|▏                   | 105/10001 [03:17<4:24:56,  1.61s/batch]Batch 100/10001 Done, mean position loss: 28.23833033323288\n",
      "Training NF2:   1%|▏                   | 103/10001 [03:18<5:21:46,  1.95s/batch]Batch 100/10001 Done, mean position loss: 29.185922813415527\n",
      "Training NF2:   1%|▏                   | 100/10001 [03:18<5:25:22,  1.97s/batch]Batch 100/10001 Done, mean position loss: 28.05654107093811\n",
      "Training NF2:   1%|▏                   | 106/10001 [03:18<3:57:53,  1.44s/batch]Batch 100/10001 Done, mean position loss: 29.538000378608704\n",
      "Training NF2:   1%|▏                   | 108/10001 [03:19<3:59:32,  1.45s/batch]Batch 100/10001 Done, mean position loss: 29.13923300266266\n",
      "Batch 100/10001 Done, mean position loss: 29.69560028553009\n",
      "Training NF2:   1%|▏                   | 103/10001 [03:19<3:41:09,  1.34s/batch]Batch 100/10001 Done, mean position loss: 27.31849804162979\n",
      "Training NF2:   1%|▏                   | 101/10001 [03:20<4:51:40,  1.77s/batch]Batch 100/10001 Done, mean position loss: 29.831964585781098\n",
      "Training NF2:   1%|▏                   | 101/10001 [03:20<4:40:41,  1.70s/batch]Batch 100/10001 Done, mean position loss: 28.32569579362869\n",
      "Training NF2:   1%|▏                   | 105/10001 [03:19<4:50:51,  1.76s/batch]Batch 100/10001 Done, mean position loss: 30.109491684436797\n",
      "Training NF2:   1%|▏                   | 105/10001 [03:20<5:16:06,  1.92s/batch]Batch 100/10001 Done, mean position loss: 29.898884928226472\n",
      "Training NF2:   1%|▏                   | 102/10001 [03:21<4:31:27,  1.65s/batch]Batch 100/10001 Done, mean position loss: 29.489616241455078\n",
      "Training NF2:   1%|▏                   | 108/10001 [03:22<3:50:07,  1.40s/batch]Batch 100/10001 Done, mean position loss: 28.31412560224533\n",
      "Training NF2:   1%|▏                   | 108/10001 [03:23<4:19:53,  1.58s/batch]Batch 100/10001 Done, mean position loss: 28.911810529232028\n",
      "Training NF2:   1%|▏                   | 103/10001 [03:23<4:47:39,  1.74s/batch]Batch 100/10001 Done, mean position loss: 29.894612340927125\n",
      "Training NF2:   1%|▏                   | 108/10001 [03:24<5:14:08,  1.91s/batch]Batch 100/10001 Done, mean position loss: 30.613658940792085\n",
      "Training NF2:   1%|▏                   | 106/10001 [03:24<4:35:39,  1.67s/batch]Batch 100/10001 Done, mean position loss: 28.67388243675232\n",
      "Training NF2:   1%|▏                   | 104/10001 [03:24<4:25:46,  1.61s/batch]Batch 100/10001 Done, mean position loss: 28.705152339935303\n",
      "Training NF2:   1%|▏                   | 100/10001 [03:25<5:11:47,  1.89s/batch]Batch 100/10001 Done, mean position loss: 29.715043637752533\n",
      "Training NF2:   1%|▏                   | 102/10001 [03:25<4:35:44,  1.67s/batch]Batch 100/10001 Done, mean position loss: 28.98576499938965\n",
      "Training NF2:   1%|▏                   | 103/10001 [03:27<5:19:23,  1.94s/batch]Batch 100/10001 Done, mean position loss: 28.680706577301024\n",
      "Training NF2:   1%|▏                   | 102/10001 [03:27<4:23:14,  1.60s/batch]Batch 100/10001 Done, mean position loss: 27.74739542007446\n",
      "Training NF2:   1%|▏                   | 112/10001 [03:27<4:51:39,  1.77s/batch]Batch 100/10001 Done, mean position loss: 29.11568887710571\n",
      "Training NF2:   2%|▍                   | 192/10001 [06:03<4:58:58,  1.83s/batch]Batch 200/10001 Done, mean position loss: 25.55024867773056\n",
      "Training NF2:   2%|▎                   | 187/10001 [06:05<4:46:14,  1.75s/batch]Batch 200/10001 Done, mean position loss: 25.320193178653717\n",
      "Training NF2:   2%|▍                   | 194/10001 [06:08<5:03:41,  1.86s/batch]Batch 200/10001 Done, mean position loss: 24.935622913837435\n",
      "Training NF2:   2%|▍                   | 193/10001 [06:08<4:53:11,  1.79s/batch]Batch 200/10001 Done, mean position loss: 24.998917574882505\n",
      "Training NF2:   2%|▍                   | 204/10001 [06:08<4:56:11,  1.81s/batch]Batch 200/10001 Done, mean position loss: 26.741499099731442\n",
      "Training NF2:   2%|▍                   | 189/10001 [06:09<5:06:29,  1.87s/batch]Batch 200/10001 Done, mean position loss: 25.788516497612\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:09<4:09:34,  1.53s/batch]Batch 200/10001 Done, mean position loss: 24.83055551290512\n",
      "Training NF2:   2%|▍                   | 204/10001 [06:10<5:07:17,  1.88s/batch]Batch 200/10001 Done, mean position loss: 26.10890955209732\n",
      "Training NF2:   2%|▍                   | 197/10001 [06:12<5:01:07,  1.84s/batch]Batch 200/10001 Done, mean position loss: 25.71919186115265\n",
      "Training NF2:   2%|▍                   | 195/10001 [06:12<5:36:47,  2.06s/batch]Batch 200/10001 Done, mean position loss: 26.596747236251833\n",
      "Training NF2:   2%|▍                   | 196/10001 [06:13<5:21:52,  1.97s/batch]Batch 200/10001 Done, mean position loss: 25.782648320198057\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:14<3:53:25,  1.43s/batch]Batch 200/10001 Done, mean position loss: 26.140948884487152\n",
      "Training NF2:   2%|▍                   | 204/10001 [06:14<4:58:20,  1.83s/batch]Batch 200/10001 Done, mean position loss: 24.591615641117095\n",
      "Training NF2:   2%|▍                   | 194/10001 [06:14<4:08:09,  1.52s/batch]Batch 200/10001 Done, mean position loss: 23.89080265760422\n",
      "Training NF2:   2%|▍                   | 206/10001 [06:17<5:01:26,  1.85s/batch]Batch 200/10001 Done, mean position loss: 25.333316860198973\n",
      "Training NF2:   2%|▍                   | 200/10001 [06:18<5:38:50,  2.07s/batch]Batch 200/10001 Done, mean position loss: 25.757247507572174\n",
      "Training NF2:   2%|▍                   | 196/10001 [06:18<6:43:26,  2.47s/batch]Batch 200/10001 Done, mean position loss: 25.483088307380676\n",
      "Training NF2:   2%|▍                   | 204/10001 [06:19<4:55:50,  1.81s/batch]Batch 200/10001 Done, mean position loss: 24.177677450180056\n",
      "Training NF2:   2%|▍                   | 194/10001 [06:19<5:37:58,  2.07s/batch]Batch 200/10001 Done, mean position loss: 25.37498656511307\n",
      "Training NF2:   2%|▍                   | 206/10001 [06:20<5:07:05,  1.88s/batch]Batch 200/10001 Done, mean position loss: 24.583126833438875\n",
      "Training NF2:   2%|▍                   | 206/10001 [06:20<4:13:41,  1.55s/batch]Batch 200/10001 Done, mean position loss: 25.846450901031496\n",
      "Training NF2:   2%|▍                   | 200/10001 [06:20<4:44:37,  1.74s/batch]Batch 200/10001 Done, mean position loss: 25.204218740463254\n",
      "Training NF2:   2%|▍                   | 205/10001 [06:21<4:51:36,  1.79s/batch]Batch 200/10001 Done, mean position loss: 24.93258848667145\n",
      "Training NF2:   2%|▍                   | 208/10001 [06:21<4:42:23,  1.73s/batch]Batch 200/10001 Done, mean position loss: 25.125399558544157\n",
      "Training NF2:   2%|▍                   | 203/10001 [06:22<5:27:21,  2.00s/batch]Batch 200/10001 Done, mean position loss: 26.977373592853546\n",
      "Training NF2:   2%|▍                   | 196/10001 [06:22<5:30:44,  2.02s/batch]Batch 200/10001 Done, mean position loss: 25.964944577217103\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:22<5:04:15,  1.86s/batch]Batch 200/10001 Done, mean position loss: 25.44448912382126\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:23<4:42:00,  1.73s/batch]Batch 200/10001 Done, mean position loss: 24.275283567905426\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:24<4:53:29,  1.80s/batch]Batch 200/10001 Done, mean position loss: 25.29907092809677\n",
      "Training NF2:   2%|▍                   | 197/10001 [06:24<5:54:04,  2.17s/batch]Batch 200/10001 Done, mean position loss: 27.014503786563875\n",
      "Training NF2:   2%|▍                   | 200/10001 [06:25<4:12:39,  1.55s/batch]Batch 200/10001 Done, mean position loss: 25.571976563930512\n",
      "Training NF2:   2%|▍                   | 200/10001 [06:25<5:14:48,  1.93s/batch]Batch 200/10001 Done, mean position loss: 25.340234532356263\n",
      "Training NF2:   2%|▍                   | 204/10001 [06:26<4:57:36,  1.82s/batch]Batch 200/10001 Done, mean position loss: 24.85864590406418\n",
      "Training NF2:   2%|▍                   | 208/10001 [06:28<5:16:37,  1.94s/batch]Batch 200/10001 Done, mean position loss: 24.544933166503906\n",
      "Training NF2:   2%|▍                   | 203/10001 [06:28<5:49:10,  2.14s/batch]Batch 200/10001 Done, mean position loss: 25.345412693023682\n",
      "Training NF2:   2%|▍                   | 202/10001 [06:29<4:56:58,  1.82s/batch]Batch 200/10001 Done, mean position loss: 25.001488397121427\n",
      "Training NF2:   2%|▍                   | 207/10001 [06:30<5:01:15,  1.85s/batch]Batch 200/10001 Done, mean position loss: 26.40560570001602\n",
      "Training NF2:   2%|▍                   | 211/10001 [06:31<4:30:27,  1.66s/batch]Batch 200/10001 Done, mean position loss: 25.080163741111758\n",
      "Training NF2:   2%|▍                   | 203/10001 [06:31<5:31:01,  2.03s/batch]Batch 200/10001 Done, mean position loss: 25.677324504852294\n",
      "Training NF2:   2%|▍                   | 215/10001 [06:34<5:08:18,  1.89s/batch]Batch 200/10001 Done, mean position loss: 24.718302147388457\n",
      "Training NF2:   3%|▌                   | 292/10001 [09:00<4:31:54,  1.68s/batch]Batch 300/10001 Done, mean position loss: 24.076295220851897\n",
      "Training NF2:   3%|▌                   | 294/10001 [09:03<4:54:54,  1.82s/batch]Batch 300/10001 Done, mean position loss: 22.816597480773925\n",
      "Training NF2:   3%|▌                   | 285/10001 [09:02<4:41:27,  1.74s/batch]Batch 300/10001 Done, mean position loss: 23.996558327674865\n",
      "Training NF2:   3%|▌                   | 289/10001 [09:06<6:02:22,  2.24s/batch]Batch 300/10001 Done, mean position loss: 24.572197206020356\n",
      "Training NF2:   3%|▌                   | 299/10001 [09:08<4:39:13,  1.73s/batch]Batch 300/10001 Done, mean position loss: 23.291868019104\n",
      "Training NF2:   3%|▌                   | 295/10001 [09:09<5:34:45,  2.07s/batch]Batch 300/10001 Done, mean position loss: 23.32037025928497\n",
      "Training NF2:   3%|▌                   | 288/10001 [09:09<6:12:24,  2.30s/batch]Batch 300/10001 Done, mean position loss: 24.539684309959412\n",
      "Training NF2:   3%|▌                   | 290/10001 [09:09<4:17:57,  1.59s/batch]Batch 300/10001 Done, mean position loss: 23.661214146614075\n",
      "Training NF2:   3%|▌                   | 294/10001 [09:10<4:14:26,  1.57s/batch]Batch 300/10001 Done, mean position loss: 23.266148273944857\n",
      "Training NF2:   3%|▌                   | 288/10001 [09:11<4:52:03,  1.80s/batch]Batch 300/10001 Done, mean position loss: 24.91323660850525\n",
      "Training NF2:   3%|▌                   | 291/10001 [09:14<5:23:06,  2.00s/batch]Batch 300/10001 Done, mean position loss: 23.80036790370941\n",
      "Training NF2:   3%|▌                   | 291/10001 [09:16<5:41:05,  2.11s/batch]Batch 300/10001 Done, mean position loss: 24.103136849403377\n",
      "Training NF2:   3%|▌                   | 295/10001 [09:16<4:53:48,  1.82s/batch]Batch 300/10001 Done, mean position loss: 24.362544541358947\n",
      "Training NF2:   3%|▌                   | 292/10001 [09:17<5:44:41,  2.13s/batch]Batch 300/10001 Done, mean position loss: 23.70873281955719\n",
      "Training NF2:   3%|▌                   | 298/10001 [09:17<4:57:30,  1.84s/batch]Batch 300/10001 Done, mean position loss: 23.391755921840666\n",
      "Training NF2:   3%|▌                   | 298/10001 [09:17<5:11:44,  1.93s/batch]Batch 300/10001 Done, mean position loss: 23.25584102630615\n",
      "Training NF2:   3%|▌                   | 307/10001 [09:18<4:29:43,  1.67s/batch]Batch 300/10001 Done, mean position loss: 22.498802733421325\n",
      "Training NF2:   3%|▌                   | 294/10001 [09:18<5:46:23,  2.14s/batch]Batch 300/10001 Done, mean position loss: 24.13227173089981\n",
      "Training NF2:   3%|▌                   | 309/10001 [09:20<4:30:44,  1.68s/batch]Batch 300/10001 Done, mean position loss: 22.278401844501495\n",
      "Training NF2:   3%|▌                   | 310/10001 [09:20<5:17:48,  1.97s/batch]Batch 300/10001 Done, mean position loss: 23.04387589454651\n",
      "Training NF2:   3%|▌                   | 309/10001 [09:20<4:09:25,  1.54s/batch]Batch 300/10001 Done, mean position loss: 23.578784222602845\n",
      "Training NF2:   3%|▌                   | 304/10001 [09:20<4:38:49,  1.73s/batch]Batch 300/10001 Done, mean position loss: 23.911863446235657\n",
      "Training NF2:   3%|▌                   | 306/10001 [09:22<4:08:18,  1.54s/batch]Batch 300/10001 Done, mean position loss: 25.233722600936886\n",
      "Training NF2:   3%|▌                   | 309/10001 [09:23<4:35:07,  1.70s/batch]Batch 300/10001 Done, mean position loss: 23.07073522567749\n",
      "Training NF2:   3%|▌                   | 303/10001 [09:23<4:13:44,  1.57s/batch]Batch 300/10001 Done, mean position loss: 24.215784969329835\n",
      "Training NF2:   3%|▌                   | 296/10001 [09:26<5:21:03,  1.98s/batch]Batch 300/10001 Done, mean position loss: 24.66590764760971\n",
      "Training NF2:   3%|▌                   | 303/10001 [09:27<5:19:21,  1.98s/batch]Batch 300/10001 Done, mean position loss: 22.980534784793853\n",
      "Training NF2:   3%|▌                   | 298/10001 [09:27<5:19:47,  1.98s/batch]Batch 300/10001 Done, mean position loss: 23.563613231182096\n",
      "Training NF2:   3%|▋                   | 314/10001 [09:29<4:49:07,  1.79s/batch]Batch 300/10001 Done, mean position loss: 23.176064846515658\n",
      "Training NF2:   3%|▌                   | 302/10001 [09:29<4:52:55,  1.81s/batch]Batch 300/10001 Done, mean position loss: 23.570748479366305\n",
      "Training NF2:   3%|▌                   | 297/10001 [09:30<5:19:17,  1.97s/batch]Batch 300/10001 Done, mean position loss: 23.959188096523285\n",
      "Training NF2:   3%|▌                   | 300/10001 [09:32<5:42:39,  2.12s/batch]Batch 300/10001 Done, mean position loss: 23.56509331703186\n",
      "Training NF2:   3%|▌                   | 305/10001 [09:32<5:57:46,  2.21s/batch]Batch 300/10001 Done, mean position loss: 23.276108758449553\n",
      "Training NF2:   3%|▌                   | 308/10001 [09:32<4:39:14,  1.73s/batch]Batch 300/10001 Done, mean position loss: 24.591596333980558\n",
      "Training NF2:   3%|▌                   | 304/10001 [09:32<5:05:16,  1.89s/batch]Batch 300/10001 Done, mean position loss: 23.93515303850174\n",
      "Training NF2:   3%|▋                   | 315/10001 [09:32<4:43:30,  1.76s/batch]Batch 300/10001 Done, mean position loss: 23.928660807609557\n",
      "Training NF2:   3%|▌                   | 304/10001 [09:33<4:29:42,  1.67s/batch]Batch 300/10001 Done, mean position loss: 23.036395146846772\n",
      "Training NF2:   3%|▌                   | 310/10001 [09:33<5:30:46,  2.05s/batch]Batch 300/10001 Done, mean position loss: 23.722743723392483\n",
      "Training NF2:   3%|▋                   | 315/10001 [09:35<5:28:37,  2.04s/batch]Batch 300/10001 Done, mean position loss: 23.01288210391998\n",
      "Training NF2:   3%|▌                   | 303/10001 [09:38<5:32:32,  2.06s/batch]Batch 300/10001 Done, mean position loss: 22.79303678750992\n",
      "Training NF2:   4%|▊                   | 388/10001 [11:54<4:53:55,  1.83s/batch]Batch 400/10001 Done, mean position loss: 22.89488404512405\n",
      "Training NF2:   4%|▊                   | 386/10001 [11:54<4:47:39,  1.80s/batch]Batch 400/10001 Done, mean position loss: 23.268516359329222\n",
      "Training NF2:   4%|▊                   | 387/10001 [11:55<4:52:21,  1.82s/batch]Batch 400/10001 Done, mean position loss: 21.93401583433151\n",
      "Training NF2:   4%|▊                   | 382/10001 [12:03<5:01:27,  1.88s/batch]Batch 400/10001 Done, mean position loss: 23.961038591861726\n",
      "Training NF2:   4%|▊                   | 393/10001 [12:03<4:58:50,  1.87s/batch]Batch 400/10001 Done, mean position loss: 22.59516582250595\n",
      "Training NF2:   4%|▊                   | 393/10001 [12:04<4:11:39,  1.57s/batch]Batch 400/10001 Done, mean position loss: 22.504488320350646\n",
      "Training NF2:   4%|▊                   | 394/10001 [12:13<5:00:51,  1.88s/batch]Batch 400/10001 Done, mean position loss: 23.207794744968414\n",
      "Training NF2:   4%|▊                   | 391/10001 [12:13<4:50:38,  1.81s/batch]Batch 400/10001 Done, mean position loss: 22.847148742675778\n",
      "Training NF2:   4%|▊                   | 399/10001 [12:15<5:02:24,  1.89s/batch]Batch 400/10001 Done, mean position loss: 22.355771327018736\n",
      "Training NF2:   4%|▊                   | 400/10001 [12:18<5:44:26,  2.15s/batch]Batch 400/10001 Done, mean position loss: 23.86967308282852\n",
      "Training NF2:   4%|▊                   | 393/10001 [12:18<4:44:14,  1.78s/batch]Batch 400/10001 Done, mean position loss: 23.3270484995842\n",
      "Training NF2:   4%|▊                   | 396/10001 [12:18<5:19:53,  2.00s/batch]Batch 400/10001 Done, mean position loss: 22.348619587421418\n",
      "Training NF2:   4%|▊                   | 397/10001 [12:18<5:10:15,  1.94s/batch]Batch 400/10001 Done, mean position loss: 22.319618623256684\n",
      "Training NF2:   4%|▊                   | 393/10001 [12:20<4:50:24,  1.81s/batch]Batch 400/10001 Done, mean position loss: 23.273443212509157\n",
      "Training NF2:   4%|▊                   | 397/10001 [12:20<5:25:58,  2.04s/batch]Batch 400/10001 Done, mean position loss: 22.76388123989105\n",
      "Training NF2:   4%|▊                   | 390/10001 [12:20<6:09:05,  2.30s/batch]Batch 400/10001 Done, mean position loss: 23.221875107288362\n",
      "Training NF2:   4%|▊                   | 399/10001 [12:20<5:02:35,  1.89s/batch]Batch 400/10001 Done, mean position loss: 22.446922180652617\n",
      "Training NF2:   4%|▊                   | 394/10001 [12:20<5:40:41,  2.13s/batch]Batch 400/10001 Done, mean position loss: 22.654137203693388\n",
      "Training NF2:   4%|▊                   | 401/10001 [12:20<5:10:18,  1.94s/batch]Batch 400/10001 Done, mean position loss: 21.755663318634035\n",
      "Training NF2:   4%|▊                   | 398/10001 [12:22<4:13:15,  1.58s/batch]Batch 400/10001 Done, mean position loss: 21.96263761281967\n",
      "Training NF2:   4%|▊                   | 397/10001 [12:24<4:21:26,  1.63s/batch]Batch 400/10001 Done, mean position loss: 21.88825306892395\n",
      "Batch 400/10001 Done, mean position loss: 23.403226311206815\n",
      "Training NF2:   4%|▊                   | 405/10001 [12:24<4:14:37,  1.59s/batch]Batch 400/10001 Done, mean position loss: 24.36281209945679\n",
      "Training NF2:   4%|▊                   | 401/10001 [12:24<5:16:51,  1.98s/batch]Batch 400/10001 Done, mean position loss: 22.930869977474213\n",
      "Training NF2:   4%|▊                   | 404/10001 [12:27<5:56:52,  2.23s/batch]Batch 400/10001 Done, mean position loss: 22.776939029693605\n",
      "Training NF2:   4%|▊                   | 410/10001 [12:26<4:39:25,  1.75s/batch]Batch 400/10001 Done, mean position loss: 22.093155460357668\n",
      "Training NF2:   4%|▊                   | 403/10001 [12:27<4:41:10,  1.76s/batch]Batch 400/10001 Done, mean position loss: 22.759137361049653\n",
      "Training NF2:   4%|▊                   | 402/10001 [12:29<4:09:10,  1.56s/batch]Batch 400/10001 Done, mean position loss: 23.826553280353547\n",
      "Training NF2:   4%|▊                   | 407/10001 [12:29<4:13:12,  1.58s/batch]Batch 400/10001 Done, mean position loss: 22.697250797748566\n",
      "Training NF2:   4%|▊                   | 399/10001 [12:31<3:42:00,  1.39s/batch]Batch 400/10001 Done, mean position loss: 22.926779124736782\n",
      "Training NF2:   4%|▊                   | 400/10001 [12:31<4:34:07,  1.71s/batch]Batch 400/10001 Done, mean position loss: 22.49499626636505\n",
      "Training NF2:   4%|▊                   | 410/10001 [12:31<5:28:27,  2.05s/batch]Batch 400/10001 Done, mean position loss: 23.01231250047684\n",
      "Training NF2:   4%|▊                   | 403/10001 [12:33<4:44:27,  1.78s/batch]Batch 400/10001 Done, mean position loss: 22.633386018276212\n",
      "Training NF2:   4%|▊                   | 411/10001 [12:33<5:36:52,  2.11s/batch]Batch 400/10001 Done, mean position loss: 22.946772387027742\n",
      "Batch 400/10001 Done, mean position loss: 22.284341089725494\n",
      "Training NF2:   4%|▊                   | 408/10001 [12:33<5:09:25,  1.94s/batch]Batch 400/10001 Done, mean position loss: 23.48568749666214\n",
      "Training NF2:   4%|▊                   | 402/10001 [12:35<4:46:29,  1.79s/batch]Batch 400/10001 Done, mean position loss: 22.691974678039553\n",
      "Training NF2:   4%|▊                   | 397/10001 [12:36<6:02:36,  2.27s/batch]Batch 400/10001 Done, mean position loss: 22.237210960388182\n",
      "Training NF2:   4%|▊                   | 411/10001 [12:41<4:40:07,  1.75s/batch]Batch 400/10001 Done, mean position loss: 22.94848102807999\n",
      "Training NF2:   4%|▊                   | 405/10001 [12:43<4:29:57,  1.69s/batch]Batch 400/10001 Done, mean position loss: 21.63150058031082\n",
      "Training NF2:   5%|▉                   | 474/10001 [14:48<5:47:57,  2.19s/batch]Batch 500/10001 Done, mean position loss: 22.68054001569748\n",
      "Training NF2:   5%|▉                   | 494/10001 [14:57<4:12:13,  1.59s/batch]Batch 500/10001 Done, mean position loss: 22.061047649383546\n",
      "Training NF2:   5%|▉                   | 495/10001 [15:00<4:20:31,  1.64s/batch]Batch 500/10001 Done, mean position loss: 21.470816836357116\n",
      "Training NF2:   5%|▉                   | 496/10001 [15:00<4:40:56,  1.77s/batch]Batch 500/10001 Done, mean position loss: 22.252462751865387\n",
      "Training NF2:   5%|▉                   | 497/10001 [15:05<4:58:52,  1.89s/batch]Batch 500/10001 Done, mean position loss: 22.163282041549685\n",
      "Training NF2:   5%|▉                   | 488/10001 [15:07<5:37:54,  2.13s/batch]Batch 500/10001 Done, mean position loss: 23.416867048740386\n",
      "Training NF2:   5%|▉                   | 495/10001 [15:10<4:22:44,  1.66s/batch]Batch 500/10001 Done, mean position loss: 22.448551585674288\n",
      "Training NF2:   5%|▉                   | 490/10001 [15:10<5:23:35,  2.04s/batch]Batch 500/10001 Done, mean position loss: 21.78821666717529\n",
      "Training NF2:   5%|▉                   | 494/10001 [15:11<4:52:37,  1.85s/batch]Batch 500/10001 Done, mean position loss: 22.580276048183443\n",
      "Training NF2:   5%|▉                   | 496/10001 [15:14<3:56:53,  1.50s/batch]Batch 500/10001 Done, mean position loss: 22.127859172821047\n",
      "Training NF2:   5%|█                   | 517/10001 [15:15<4:40:29,  1.77s/batch]Batch 500/10001 Done, mean position loss: 22.4279336810112\n",
      "Training NF2:   5%|▉                   | 490/10001 [15:16<4:55:54,  1.87s/batch]Batch 500/10001 Done, mean position loss: 22.48693159341812\n",
      "Training NF2:   5%|█                   | 518/10001 [15:17<4:43:06,  1.79s/batch]Batch 500/10001 Done, mean position loss: 23.199556677341462\n",
      "Training NF2:   5%|▉                   | 492/10001 [15:19<4:40:21,  1.77s/batch]Batch 500/10001 Done, mean position loss: 21.830931899547576\n",
      "Training NF2:   5%|█                   | 508/10001 [15:21<4:55:04,  1.86s/batch]Batch 500/10001 Done, mean position loss: 21.506527013778687\n",
      "Training NF2:   5%|▉                   | 500/10001 [15:21<4:26:45,  1.68s/batch]Batch 500/10001 Done, mean position loss: 21.85190040588379\n",
      "Training NF2:   5%|▉                   | 493/10001 [15:22<5:06:04,  1.93s/batch]Batch 500/10001 Done, mean position loss: 21.42566269397736\n",
      "Training NF2:   5%|▉                   | 493/10001 [15:21<4:54:23,  1.86s/batch]Batch 500/10001 Done, mean position loss: 22.815259737968447\n",
      "Training NF2:   5%|█                   | 512/10001 [15:23<4:24:27,  1.67s/batch]Batch 500/10001 Done, mean position loss: 21.405288245677948\n",
      "Training NF2:   5%|▉                   | 489/10001 [15:22<4:37:29,  1.75s/batch]Batch 500/10001 Done, mean position loss: 22.76553148984909\n",
      "Training NF2:   5%|▉                   | 489/10001 [15:23<4:36:41,  1.75s/batch]Batch 500/10001 Done, mean position loss: 21.995435779094695\n",
      "Training NF2:   5%|█                   | 508/10001 [15:23<4:48:37,  1.82s/batch]Batch 500/10001 Done, mean position loss: 23.7510063624382\n",
      "Training NF2:   5%|█                   | 516/10001 [15:23<4:38:15,  1.76s/batch]Batch 500/10001 Done, mean position loss: 22.006388771533967\n",
      "Training NF2:   5%|█                   | 513/10001 [15:25<4:31:42,  1.72s/batch]Batch 500/10001 Done, mean position loss: 22.180376353263856\n",
      "Training NF2:   5%|▉                   | 495/10001 [15:24<4:13:27,  1.60s/batch]Batch 500/10001 Done, mean position loss: 22.595876867771146\n",
      "Training NF2:   5%|█                   | 518/10001 [15:26<4:30:21,  1.71s/batch]Batch 500/10001 Done, mean position loss: 22.093469479084014\n",
      "Training NF2:   5%|█                   | 515/10001 [15:27<4:41:31,  1.78s/batch]Batch 500/10001 Done, mean position loss: 22.2134174156189\n",
      "Training NF2:   5%|█                   | 505/10001 [15:27<4:48:42,  1.82s/batch]Batch 500/10001 Done, mean position loss: 22.15946196079254\n",
      "Training NF2:   5%|▉                   | 497/10001 [15:29<4:41:10,  1.78s/batch]Batch 500/10001 Done, mean position loss: 22.145418293476105\n",
      "Training NF2:   5%|█                   | 505/10001 [15:30<4:50:06,  1.83s/batch]Batch 500/10001 Done, mean position loss: 22.464644091129305\n",
      "Training NF2:   5%|█                   | 505/10001 [15:32<4:56:25,  1.87s/batch]Batch 500/10001 Done, mean position loss: 21.710411043167113\n",
      "Training NF2:   5%|▉                   | 494/10001 [15:33<5:12:59,  1.98s/batch]Batch 500/10001 Done, mean position loss: 23.29543955564499\n",
      "Training NF2:   5%|█                   | 509/10001 [15:35<4:05:51,  1.55s/batch]Batch 500/10001 Done, mean position loss: 22.37215542793274\n",
      "Training NF2:   5%|█                   | 510/10001 [15:35<3:51:06,  1.46s/batch]Batch 500/10001 Done, mean position loss: 21.70040272951126\n",
      "Training NF2:   5%|█                   | 511/10001 [15:35<4:37:53,  1.76s/batch]Batch 500/10001 Done, mean position loss: 21.68709118366241\n",
      "Training NF2:   5%|█                   | 527/10001 [15:35<4:51:35,  1.85s/batch]Batch 500/10001 Done, mean position loss: 22.85261808872223\n",
      "Training NF2:   5%|█                   | 518/10001 [15:38<4:21:45,  1.66s/batch]Batch 500/10001 Done, mean position loss: 22.024802832603456\n",
      "Training NF2:   5%|█                   | 515/10001 [15:43<4:00:00,  1.52s/batch]Batch 500/10001 Done, mean position loss: 22.305209236145018\n",
      "Training NF2:   5%|█                   | 507/10001 [15:46<4:41:31,  1.78s/batch]Batch 500/10001 Done, mean position loss: 22.289696650505064\n",
      "Training NF2:   5%|█                   | 519/10001 [15:48<4:17:04,  1.63s/batch]Batch 500/10001 Done, mean position loss: 21.116024506092074\n",
      "Training NF2:   6%|█▏                  | 585/10001 [17:50<4:48:50,  1.84s/batch]Batch 600/10001 Done, mean position loss: 22.265926823616027\n",
      "Training NF2:   6%|█▏                  | 581/10001 [17:55<5:31:06,  2.11s/batch]Batch 600/10001 Done, mean position loss: 21.85908677339554\n",
      "Training NF2:   6%|█▏                  | 588/10001 [17:54<5:23:22,  2.06s/batch]Batch 600/10001 Done, mean position loss: 21.792014551162723\n",
      "Training NF2:   6%|█▏                  | 577/10001 [17:56<6:02:39,  2.31s/batch]Batch 600/10001 Done, mean position loss: 21.17710359573364\n",
      "Training NF2:   6%|█▏                  | 592/10001 [18:00<4:26:59,  1.70s/batch]Batch 600/10001 Done, mean position loss: 22.10183576107025\n",
      "Training NF2:   6%|█▏                  | 588/10001 [18:01<4:31:41,  1.73s/batch]Batch 600/10001 Done, mean position loss: 21.57675178527832\n",
      "Training NF2:   6%|█▏                  | 596/10001 [18:09<4:42:26,  1.80s/batch]Batch 600/10001 Done, mean position loss: 21.476213383674622\n",
      "Training NF2:   6%|█▏                  | 581/10001 [18:10<4:17:01,  1.64s/batch]Batch 600/10001 Done, mean position loss: 22.09770311832428\n",
      "Training NF2:   6%|█▏                  | 594/10001 [18:11<5:28:34,  2.10s/batch]Batch 600/10001 Done, mean position loss: 21.753902535438538\n",
      "Training NF2:   6%|█▏                  | 581/10001 [18:15<5:12:21,  1.99s/batch]Batch 600/10001 Done, mean position loss: 21.35988618850708\n",
      "Training NF2:   6%|█▏                  | 594/10001 [18:15<4:36:14,  1.76s/batch]Batch 600/10001 Done, mean position loss: 21.835162394046783\n",
      "Training NF2:   6%|█▏                  | 603/10001 [18:17<3:12:25,  1.23s/batch]Batch 600/10001 Done, mean position loss: 22.65624148845673\n",
      "Training NF2:   6%|█▏                  | 595/10001 [18:17<4:10:56,  1.60s/batch]Batch 600/10001 Done, mean position loss: 21.44469176530838\n",
      "Training NF2:   6%|█▏                  | 593/10001 [18:17<4:32:00,  1.73s/batch]Batch 600/10001 Done, mean position loss: 22.366145412921906\n",
      "Training NF2:   6%|█▏                  | 590/10001 [18:17<5:24:41,  2.07s/batch]Batch 600/10001 Done, mean position loss: 21.824555702209473\n",
      "Training NF2:   6%|█▏                  | 597/10001 [18:19<5:27:24,  2.09s/batch]Batch 600/10001 Done, mean position loss: 21.824220116138456\n",
      "Training NF2:   6%|█▏                  | 584/10001 [18:19<6:32:13,  2.50s/batch]Batch 600/10001 Done, mean position loss: 22.950589356422423\n",
      "Training NF2:   6%|█▏                  | 602/10001 [18:19<3:57:35,  1.52s/batch]Batch 600/10001 Done, mean position loss: 21.336532576084135\n",
      "Training NF2:   6%|█▏                  | 598/10001 [18:19<4:44:35,  1.82s/batch]Batch 600/10001 Done, mean position loss: 22.02410446882248\n",
      "Training NF2:   6%|█▏                  | 609/10001 [18:21<4:08:45,  1.59s/batch]Batch 600/10001 Done, mean position loss: 21.073122296333313\n",
      "Training NF2:   6%|█▏                  | 599/10001 [18:23<5:40:05,  2.17s/batch]Batch 600/10001 Done, mean position loss: 21.740347237586974\n",
      "Training NF2:   6%|█▏                  | 597/10001 [18:24<4:40:09,  1.79s/batch]Batch 600/10001 Done, mean position loss: 23.230512702465056\n",
      "Training NF2:   6%|█▏                  | 597/10001 [18:23<4:21:45,  1.67s/batch]Batch 600/10001 Done, mean position loss: 21.764890966415408\n",
      "Training NF2:   6%|█▏                  | 616/10001 [18:26<5:15:03,  2.01s/batch]Batch 600/10001 Done, mean position loss: 22.332075228691103\n",
      "Training NF2:   6%|█▏                  | 600/10001 [18:26<4:20:15,  1.66s/batch]Batch 600/10001 Done, mean position loss: 22.02469281435013\n",
      "Training NF2:   6%|█▏                  | 604/10001 [18:27<3:20:49,  1.28s/batch]Batch 600/10001 Done, mean position loss: 21.190281376838684\n",
      "Training NF2:   6%|█▏                  | 599/10001 [18:28<4:07:00,  1.58s/batch]Batch 600/10001 Done, mean position loss: 21.74182107925415\n",
      "Training NF2:   6%|█▏                  | 600/10001 [18:29<4:22:30,  1.68s/batch]Batch 600/10001 Done, mean position loss: 22.853221118450165\n",
      "Training NF2:   6%|█▏                  | 595/10001 [18:29<4:42:10,  1.80s/batch]Batch 600/10001 Done, mean position loss: 22.138388178348542\n",
      "Training NF2:   6%|█▏                  | 621/10001 [18:29<4:21:58,  1.68s/batch]Batch 600/10001 Done, mean position loss: 21.776904125213623\n",
      "Training NF2:   6%|█▏                  | 605/10001 [18:29<3:37:35,  1.39s/batch]Batch 600/10001 Done, mean position loss: 21.302085797786713\n",
      "Training NF2:   6%|█▏                  | 602/10001 [18:29<4:14:10,  1.62s/batch]Batch 600/10001 Done, mean position loss: 21.74184415578842\n",
      "Training NF2:   6%|█▏                  | 606/10001 [18:31<4:21:19,  1.67s/batch]Batch 600/10001 Done, mean position loss: 21.453327431678773\n",
      "Training NF2:   6%|█▏                  | 608/10001 [18:33<4:18:56,  1.65s/batch]Batch 600/10001 Done, mean position loss: 21.564106349945067\n",
      "Training NF2:   6%|█▏                  | 604/10001 [18:36<4:30:58,  1.73s/batch]Batch 600/10001 Done, mean position loss: 21.923356907367705\n",
      "Training NF2:   6%|█▏                  | 623/10001 [18:39<4:21:24,  1.67s/batch]Batch 600/10001 Done, mean position loss: 21.402070713043212\n",
      "Training NF2:   6%|█▎                  | 629/10001 [18:38<5:26:44,  2.09s/batch]Batch 600/10001 Done, mean position loss: 22.43253621816635\n",
      "Training NF2:   6%|█▏                  | 612/10001 [18:52<6:16:52,  2.41s/batch]Batch 600/10001 Done, mean position loss: 21.809213008880615\n",
      "Training NF2:   6%|█▎                  | 633/10001 [18:53<5:23:49,  2.07s/batch]Batch 600/10001 Done, mean position loss: 21.863342583179474\n",
      "Training NF2:   6%|█▏                  | 617/10001 [18:58<4:35:34,  1.76s/batch]Batch 600/10001 Done, mean position loss: 20.72028235912323\n",
      "Training NF2:   7%|█▎                  | 687/10001 [20:44<4:12:17,  1.63s/batch]Batch 700/10001 Done, mean position loss: 21.927464439868928\n",
      "Training NF2:   7%|█▎                  | 679/10001 [20:49<4:44:58,  1.83s/batch]Batch 700/10001 Done, mean position loss: 21.59101373195648\n",
      "Training NF2:   7%|█▎                  | 682/10001 [20:52<5:29:25,  2.12s/batch]Batch 700/10001 Done, mean position loss: 21.616726303100585\n",
      "Training NF2:   7%|█▎                  | 669/10001 [20:55<5:08:05,  1.98s/batch]Batch 700/10001 Done, mean position loss: 20.994813442230225\n",
      "Training NF2:   7%|█▍                  | 699/10001 [20:57<3:59:00,  1.54s/batch]Batch 700/10001 Done, mean position loss: 21.204177343845366\n",
      "Training NF2:   7%|█▎                  | 682/10001 [20:58<4:09:35,  1.61s/batch]Batch 700/10001 Done, mean position loss: 21.288614029884336\n",
      "Training NF2:   7%|█▍                  | 708/10001 [21:01<4:36:05,  1.78s/batch]Batch 700/10001 Done, mean position loss: 21.84243436336517\n",
      "Training NF2:   7%|█▎                  | 685/10001 [21:06<4:08:03,  1.60s/batch]Batch 700/10001 Done, mean position loss: 21.39563935518265\n",
      "Training NF2:   7%|█▍                  | 689/10001 [21:07<4:07:35,  1.60s/batch]Batch 700/10001 Done, mean position loss: 21.868642818927764\n",
      "Training NF2:   7%|█▍                  | 699/10001 [21:08<4:49:44,  1.87s/batch]Batch 700/10001 Done, mean position loss: 21.542816915512084\n",
      "Training NF2:   7%|█▍                  | 688/10001 [21:09<4:05:36,  1.58s/batch]Batch 700/10001 Done, mean position loss: 21.22266701698303\n",
      "Training NF2:   7%|█▍                  | 694/10001 [21:11<4:19:05,  1.67s/batch]Batch 700/10001 Done, mean position loss: 21.197194886207583\n",
      "Training NF2:   7%|█▍                  | 717/10001 [21:11<4:16:12,  1.66s/batch]Batch 700/10001 Done, mean position loss: 22.594733591079713\n",
      "Training NF2:   7%|█▍                  | 712/10001 [21:13<5:17:22,  2.05s/batch]Batch 700/10001 Done, mean position loss: 21.495061845779418\n",
      "Training NF2:   7%|█▍                  | 691/10001 [21:13<4:10:06,  1.61s/batch]Batch 700/10001 Done, mean position loss: 21.781401503086087\n",
      "Training NF2:   7%|█▍                  | 714/10001 [21:15<4:01:20,  1.56s/batch]Batch 700/10001 Done, mean position loss: 20.89238557100296\n",
      "Training NF2:   7%|█▍                  | 714/10001 [21:16<4:43:55,  1.83s/batch]Batch 700/10001 Done, mean position loss: 21.078225700855256\n",
      "Training NF2:   7%|█▍                  | 710/10001 [21:17<4:56:57,  1.92s/batch]Batch 700/10001 Done, mean position loss: 22.34947553396225\n",
      "Training NF2:   7%|█▍                  | 693/10001 [21:17<3:57:42,  1.53s/batch]Batch 700/10001 Done, mean position loss: 21.086720402240754\n",
      "Training NF2:   7%|█▍                  | 693/10001 [21:18<4:45:20,  1.84s/batch]Batch 700/10001 Done, mean position loss: 22.753438222408295\n",
      "Training NF2:   7%|█▍                  | 696/10001 [21:19<4:13:21,  1.63s/batch]Batch 700/10001 Done, mean position loss: 22.058186538219452\n",
      "Training NF2:   7%|█▍                  | 695/10001 [21:21<4:21:37,  1.69s/batch]Batch 700/10001 Done, mean position loss: 22.452243223190308\n",
      "Training NF2:   7%|█▍                  | 704/10001 [21:22<3:42:05,  1.43s/batch]Batch 700/10001 Done, mean position loss: 21.462852480411527\n",
      "Training NF2:   7%|█▍                  | 708/10001 [21:23<4:36:25,  1.78s/batch]Batch 700/10001 Done, mean position loss: 21.716268405914306\n",
      "Training NF2:   7%|█▍                  | 697/10001 [21:23<3:59:07,  1.54s/batch]Batch 700/10001 Done, mean position loss: 21.809850754737855\n",
      "Training NF2:   7%|█▍                  | 718/10001 [21:24<5:09:33,  2.00s/batch]Batch 700/10001 Done, mean position loss: 22.010546777248383\n",
      "Training NF2:   7%|█▍                  | 701/10001 [21:24<4:46:35,  1.85s/batch]Batch 700/10001 Done, mean position loss: 21.598712284564975\n",
      "Training NF2:   7%|█▍                  | 707/10001 [21:26<4:36:05,  1.78s/batch]Batch 700/10001 Done, mean position loss: 21.547851736545564\n",
      "Training NF2:   7%|█▍                  | 714/10001 [21:27<4:31:06,  1.75s/batch]Batch 700/10001 Done, mean position loss: 21.103975458145143\n",
      "Training NF2:   7%|█▍                  | 710/10001 [21:28<4:19:51,  1.68s/batch]Batch 700/10001 Done, mean position loss: 21.458789949417117\n",
      "Training NF2:   7%|█▍                  | 710/10001 [21:30<4:29:39,  1.74s/batch]Batch 700/10001 Done, mean position loss: 21.511961534023285\n",
      "Training NF2:   7%|█▍                  | 709/10001 [21:30<4:08:37,  1.61s/batch]Batch 700/10001 Done, mean position loss: 21.17140260696411\n",
      "Batch 700/10001 Done, mean position loss: 21.29659441232681\n",
      "Training NF2:   7%|█▍                  | 705/10001 [21:31<4:24:25,  1.71s/batch]Batch 700/10001 Done, mean position loss: 21.2613005900383\n",
      "Training NF2:   7%|█▍                  | 721/10001 [21:32<5:03:56,  1.97s/batch]Batch 700/10001 Done, mean position loss: 21.710321922302246\n",
      "Training NF2:   7%|█▍                  | 719/10001 [21:33<4:10:02,  1.62s/batch]Batch 700/10001 Done, mean position loss: 22.100354187488556\n",
      "Training NF2:   7%|█▍                  | 723/10001 [21:33<4:20:03,  1.68s/batch]Batch 700/10001 Done, mean position loss: 21.52176926136017\n",
      "Training NF2:   7%|█▍                  | 721/10001 [21:50<3:45:59,  1.46s/batch]Batch 700/10001 Done, mean position loss: 21.593504996299743\n",
      "Training NF2:   7%|█▍                  | 717/10001 [21:53<4:36:34,  1.79s/batch]Batch 700/10001 Done, mean position loss: 21.43744607925415\n",
      "Training NF2:   7%|█▍                  | 726/10001 [22:00<4:42:24,  1.83s/batch]Batch 700/10001 Done, mean position loss: 20.537834033966064\n",
      "Training NF2:   8%|█▌                  | 781/10001 [23:36<4:31:08,  1.76s/batch]Batch 800/10001 Done, mean position loss: 21.408345403671262\n",
      "Training NF2:   8%|█▌                  | 782/10001 [23:39<4:42:05,  1.84s/batch]Batch 800/10001 Done, mean position loss: 21.708866348266604\n",
      "Training NF2:   8%|█▌                  | 789/10001 [23:46<4:56:56,  1.93s/batch]Batch 800/10001 Done, mean position loss: 20.85967758178711\n",
      "Training NF2:   8%|█▌                  | 778/10001 [23:48<4:56:37,  1.93s/batch]Batch 800/10001 Done, mean position loss: 20.999882433414456\n",
      "Training NF2:   8%|█▌                  | 779/10001 [23:52<4:18:24,  1.68s/batch]Batch 800/10001 Done, mean position loss: 21.50180673122406\n",
      "Training NF2:   8%|█▌                  | 783/10001 [23:52<4:30:23,  1.76s/batch]Batch 800/10001 Done, mean position loss: 21.163695805072784\n",
      "Training NF2:   8%|█▌                  | 780/10001 [23:56<4:39:38,  1.82s/batch]Batch 800/10001 Done, mean position loss: 21.333536615371706\n",
      "Training NF2:   8%|█▌                  | 783/10001 [23:58<3:51:41,  1.51s/batch]Batch 800/10001 Done, mean position loss: 21.64593209505081\n",
      "Training NF2:   8%|█▌                  | 792/10001 [23:59<4:55:03,  1.92s/batch]Batch 800/10001 Done, mean position loss: 21.592626988887787\n",
      "Training NF2:   8%|█▌                  | 787/10001 [24:01<4:31:15,  1.77s/batch]Batch 800/10001 Done, mean position loss: 21.092959430217743\n",
      "Training NF2:   8%|█▌                  | 798/10001 [24:02<4:59:02,  1.95s/batch]Batch 800/10001 Done, mean position loss: 22.359023735523223\n",
      "Training NF2:   8%|█▌                  | 796/10001 [24:06<5:04:01,  1.98s/batch]Batch 800/10001 Done, mean position loss: 21.07200606584549\n",
      "Training NF2:   8%|█▋                  | 817/10001 [24:07<4:12:44,  1.65s/batch]Batch 800/10001 Done, mean position loss: 21.64975960969925\n",
      "Training NF2:   8%|█▌                  | 798/10001 [24:09<4:36:11,  1.80s/batch]Batch 800/10001 Done, mean position loss: 21.0722306227684\n",
      "Training NF2:   8%|█▌                  | 792/10001 [24:09<4:48:41,  1.88s/batch]Batch 800/10001 Done, mean position loss: 21.87344858646393\n",
      "Training NF2:   8%|█▋                  | 815/10001 [24:11<4:05:13,  1.60s/batch]Batch 800/10001 Done, mean position loss: 20.762285525798795\n",
      "Training NF2:   8%|█▌                  | 805/10001 [24:13<5:05:53,  2.00s/batch]Batch 800/10001 Done, mean position loss: 22.391993119716645\n",
      "Training NF2:   8%|█▌                  | 798/10001 [24:14<4:37:40,  1.81s/batch]Batch 800/10001 Done, mean position loss: 20.902762801647185\n",
      "Batch 800/10001 Done, mean position loss: 22.09827004432678\n",
      "Training NF2:   8%|█▌                  | 798/10001 [24:15<5:15:30,  2.06s/batch]Batch 800/10001 Done, mean position loss: 21.285338752269745\n",
      "Training NF2:   8%|█▌                  | 792/10001 [24:15<4:42:09,  1.84s/batch]Batch 800/10001 Done, mean position loss: 20.926568117141723\n",
      "Training NF2:   8%|█▌                  | 794/10001 [24:15<4:02:09,  1.58s/batch]Batch 800/10001 Done, mean position loss: 22.039555711746214\n",
      "Training NF2:   8%|█▌                  | 802/10001 [24:16<3:53:59,  1.53s/batch]Batch 800/10001 Done, mean position loss: 21.448493161201476\n",
      "Training NF2:   8%|█▌                  | 795/10001 [24:19<4:32:08,  1.77s/batch]Batch 800/10001 Done, mean position loss: 21.72298674821854\n",
      "Training NF2:   8%|█▌                  | 804/10001 [24:21<3:24:44,  1.34s/batch]Batch 800/10001 Done, mean position loss: 21.303948800563813\n",
      "Training NF2:   8%|█▌                  | 806/10001 [24:23<5:23:19,  2.11s/batch]Batch 800/10001 Done, mean position loss: 21.4890332865715\n",
      "Training NF2:   8%|█▌                  | 796/10001 [24:24<4:54:54,  1.92s/batch]Batch 800/10001 Done, mean position loss: 21.613521213531495\n",
      "Training NF2:   8%|█▌                  | 799/10001 [24:25<5:03:21,  1.98s/batch]Batch 800/10001 Done, mean position loss: 21.16250949382782\n",
      "Training NF2:   8%|█▌                  | 807/10001 [24:25<5:32:17,  2.17s/batch]Batch 800/10001 Done, mean position loss: 21.01771532297134\n",
      "Training NF2:   8%|█▌                  | 797/10001 [24:26<5:04:08,  1.98s/batch]Batch 800/10001 Done, mean position loss: 21.06999678373337\n",
      "Training NF2:   8%|█▌                  | 799/10001 [24:27<5:05:32,  1.99s/batch]Batch 800/10001 Done, mean position loss: 21.28907881975174\n",
      "Training NF2:   8%|█▌                  | 802/10001 [24:27<4:37:03,  1.81s/batch]Batch 800/10001 Done, mean position loss: 21.32279913187027\n",
      "Training NF2:   8%|█▌                  | 803/10001 [24:29<4:57:10,  1.94s/batch]Batch 800/10001 Done, mean position loss: 21.836007776260374\n",
      "Training NF2:   8%|█▌                  | 783/10001 [24:30<5:21:40,  2.09s/batch]Batch 800/10001 Done, mean position loss: 21.21624740600586\n",
      "Training NF2:   8%|█▋                  | 825/10001 [24:30<4:47:37,  1.88s/batch]Batch 800/10001 Done, mean position loss: 21.36307903766632\n",
      "Training NF2:   8%|█▋                  | 815/10001 [24:32<4:13:07,  1.65s/batch]Batch 800/10001 Done, mean position loss: 21.702886366844176\n",
      "Training NF2:   8%|█▌                  | 789/10001 [24:34<4:37:57,  1.81s/batch]Batch 800/10001 Done, mean position loss: 21.08710395812988\n",
      "Training NF2:   8%|█▋                  | 835/10001 [24:55<5:23:39,  2.12s/batch]Batch 800/10001 Done, mean position loss: 21.384443316459652\n",
      "Training NF2:   8%|█▋                  | 823/10001 [24:56<4:48:38,  1.89s/batch]Batch 800/10001 Done, mean position loss: 21.184539971351622\n",
      "Training NF2:   8%|█▋                  | 835/10001 [25:04<4:53:55,  1.92s/batch]Batch 800/10001 Done, mean position loss: 20.448023808002475\n",
      "Training NF2:   9%|█▋                  | 867/10001 [26:33<4:54:25,  1.93s/batch]Batch 900/10001 Done, mean position loss: 21.320469691753388\n",
      "Training NF2:   9%|█▋                  | 856/10001 [26:42<5:07:55,  2.02s/batch]Batch 900/10001 Done, mean position loss: 21.50313386440277\n",
      "Training NF2:   9%|█▊                  | 878/10001 [26:49<4:11:26,  1.65s/batch]Batch 900/10001 Done, mean position loss: 20.74068467617035\n",
      "Training NF2:   9%|█▊                  | 889/10001 [26:52<4:28:14,  1.77s/batch]Batch 900/10001 Done, mean position loss: 20.910778319835664\n",
      "Training NF2:   9%|█▊                  | 894/10001 [26:55<4:29:35,  1.78s/batch]Batch 900/10001 Done, mean position loss: 21.102766127586364\n",
      "Training NF2:   9%|█▊                  | 883/10001 [26:59<4:30:07,  1.78s/batch]Batch 900/10001 Done, mean position loss: 21.41787900686264\n",
      "Training NF2:   9%|█▊                  | 892/10001 [26:59<3:43:49,  1.47s/batch]Batch 900/10001 Done, mean position loss: 21.508225948810576\n",
      "Training NF2:   9%|█▊                  | 905/10001 [27:05<3:45:30,  1.49s/batch]Batch 900/10001 Done, mean position loss: 21.23313390493393\n",
      "Training NF2:   9%|█▊                  | 897/10001 [27:06<4:21:09,  1.72s/batch]Batch 900/10001 Done, mean position loss: 20.953185131549834\n",
      "Training NF2:   9%|█▊                  | 896/10001 [27:06<4:41:44,  1.86s/batch]Batch 900/10001 Done, mean position loss: 21.45047152996063\n",
      "Training NF2:   9%|█▊                  | 888/10001 [27:09<4:49:36,  1.91s/batch]Batch 900/10001 Done, mean position loss: 21.05768089056015\n",
      "Training NF2:   9%|█▊                  | 906/10001 [27:14<4:02:12,  1.60s/batch]Batch 900/10001 Done, mean position loss: 21.710032956600188\n",
      "Training NF2:   9%|█▊                  | 890/10001 [27:13<4:32:53,  1.80s/batch]Batch 900/10001 Done, mean position loss: 21.15246374607086\n",
      "Training NF2:   9%|█▊                  | 901/10001 [27:15<4:56:07,  1.95s/batch]Batch 900/10001 Done, mean position loss: 21.551477165222167\n",
      "Training NF2:   9%|█▊                  | 900/10001 [27:16<4:45:41,  1.88s/batch]Batch 900/10001 Done, mean position loss: 21.36089962720871\n",
      "Training NF2:   9%|█▋                  | 875/10001 [27:16<4:49:53,  1.91s/batch]Batch 900/10001 Done, mean position loss: 21.85512745857239\n",
      "Training NF2:   9%|█▊                  | 908/10001 [27:17<4:33:11,  1.80s/batch]Batch 900/10001 Done, mean position loss: 22.136003561019898\n",
      "Training NF2:   9%|█▊                  | 903/10001 [27:18<4:27:17,  1.76s/batch]Batch 900/10001 Done, mean position loss: 21.007840917110446\n",
      "Training NF2:   9%|█▊                  | 899/10001 [27:18<4:45:43,  1.88s/batch]Batch 900/10001 Done, mean position loss: 22.105992426872255\n",
      "Training NF2:   9%|█▊                  | 877/10001 [27:18<4:41:08,  1.85s/batch]Batch 900/10001 Done, mean position loss: 20.681355571746828\n",
      "Training NF2:   9%|█▊                  | 893/10001 [27:20<4:55:56,  1.95s/batch]Batch 900/10001 Done, mean position loss: 21.665455574989316\n",
      "Training NF2:   9%|█▊                  | 897/10001 [27:21<5:12:14,  2.06s/batch]Batch 900/10001 Done, mean position loss: 20.83191344499588\n",
      "Training NF2:   9%|█▊                  | 898/10001 [27:23<4:51:08,  1.92s/batch]Batch 900/10001 Done, mean position loss: 21.47010322570801\n",
      "Training NF2:   9%|█▊                  | 880/10001 [27:24<4:43:24,  1.86s/batch]Batch 900/10001 Done, mean position loss: 20.806560034751893\n",
      "Training NF2:   9%|█▊                  | 906/10001 [27:28<4:03:26,  1.61s/batch]Batch 900/10001 Done, mean position loss: 20.88728179693222\n",
      "Training NF2:   9%|█▊                  | 915/10001 [27:29<5:16:07,  2.09s/batch]Batch 900/10001 Done, mean position loss: 21.276764681339262\n",
      "Training NF2:   9%|█▊                  | 883/10001 [27:30<4:48:59,  1.90s/batch]Batch 900/10001 Done, mean position loss: 21.6277720117569\n",
      "Training NF2:   9%|█▊                  | 927/10001 [27:30<4:30:50,  1.79s/batch]Batch 900/10001 Done, mean position loss: 21.20772748231888\n",
      "Training NF2:   9%|█▊                  | 916/10001 [27:31<4:05:55,  1.62s/batch]Batch 900/10001 Done, mean position loss: 21.225241565704344\n",
      "Training NF2:   9%|█▊                  | 917/10001 [27:33<4:35:40,  1.82s/batch]Batch 900/10001 Done, mean position loss: 21.21056272268295\n",
      "Training NF2:   9%|█▊                  | 910/10001 [27:33<4:32:58,  1.80s/batch]Batch 900/10001 Done, mean position loss: 20.980652663707733\n",
      "Training NF2:   9%|█▊                  | 904/10001 [27:34<4:42:50,  1.87s/batch]Batch 900/10001 Done, mean position loss: 21.503439428806303\n",
      "Training NF2:   9%|█▊                  | 914/10001 [27:35<4:08:50,  1.64s/batch]Batch 900/10001 Done, mean position loss: 21.18715437412262\n",
      "Training NF2:   9%|█▊                  | 936/10001 [27:36<4:33:31,  1.81s/batch]Batch 900/10001 Done, mean position loss: 21.603571834564207\n",
      "Training NF2:   9%|█▊                  | 912/10001 [27:36<4:15:05,  1.68s/batch]Batch 900/10001 Done, mean position loss: 21.070322268009186\n",
      "Training NF2:   9%|█▊                  | 921/10001 [27:37<4:10:31,  1.66s/batch]Batch 900/10001 Done, mean position loss: 21.072201669216156\n",
      "Training NF2:   9%|█▊                  | 919/10001 [27:37<5:21:35,  2.12s/batch]Batch 900/10001 Done, mean position loss: 21.02192621946335\n",
      "Training NF2:   9%|█▊                  | 931/10001 [27:59<4:35:02,  1.82s/batch]Batch 900/10001 Done, mean position loss: 21.22075459718704\n",
      "Training NF2:   9%|█▊                  | 925/10001 [28:03<4:43:34,  1.87s/batch]Batch 900/10001 Done, mean position loss: 21.011618592739104\n",
      "Training NF2:   9%|█▊                  | 901/10001 [28:03<4:51:12,  1.92s/batch]Batch 900/10001 Done, mean position loss: 20.391632397174835\n",
      "Training NF2:  10%|█▉                  | 978/10001 [29:28<5:06:20,  2.04s/batch]Batch 1000/10001 Done, mean position loss: 21.20044466495514\n",
      "Training NF2:  10%|█▉                  | 994/10001 [29:49<4:24:31,  1.76s/batch]Batch 1000/10001 Done, mean position loss: 21.42392099380493\n",
      "Training NF2:  10%|█▉                  | 958/10001 [29:50<4:46:57,  1.90s/batch]Batch 1000/10001 Done, mean position loss: 20.63639430522919\n",
      "Training NF2:  10%|█▉                  | 987/10001 [29:51<3:39:46,  1.46s/batch]Batch 1000/10001 Done, mean position loss: 21.0100919342041\n",
      "Training NF2:  10%|█▉                  | 972/10001 [29:52<4:44:30,  1.89s/batch]Batch 1000/10001 Done, mean position loss: 20.854339926242826\n",
      "Training NF2:  10%|█▉                  | 991/10001 [29:53<3:43:20,  1.49s/batch]Batch 1000/10001 Done, mean position loss: 21.343257277011872\n",
      "Training NF2:  10%|█▉                  | 986/10001 [30:02<4:15:39,  1.70s/batch]Batch 1000/10001 Done, mean position loss: 21.128307139873506\n",
      "Batch 1000/10001 Done, mean position loss: 21.38327842950821\n",
      "Training NF2:  10%|█▉                  | 990/10001 [30:07<4:04:19,  1.63s/batch]Batch 1000/10001 Done, mean position loss: 21.280139014720916\n",
      "Training NF2:  10%|█▉                  | 996/10001 [30:09<4:23:36,  1.76s/batch]Batch 1000/10001 Done, mean position loss: 20.917859601974488\n",
      "Training NF2:  10%|█▉                  | 989/10001 [30:12<4:56:54,  1.98s/batch]Batch 1000/10001 Done, mean position loss: 21.06902743577957\n",
      "Training NF2:  10%|█▉                 | 1000/10001 [30:12<4:51:19,  1.94s/batch]Batch 1000/10001 Done, mean position loss: 21.01390312433243\n",
      "Training NF2:  10%|█▉                  | 972/10001 [30:13<4:33:38,  1.82s/batch]Batch 1000/10001 Done, mean position loss: 22.013556611537933\n",
      "Training NF2:  10%|█▉                  | 971/10001 [30:13<4:46:05,  1.90s/batch]Batch 1000/10001 Done, mean position loss: 21.54706456184387\n",
      "Training NF2:  10%|█▉                  | 990/10001 [30:17<4:35:45,  1.84s/batch]Batch 1000/10001 Done, mean position loss: 21.303157489299775\n",
      "Training NF2:  10%|█▉                 | 1001/10001 [30:17<4:25:14,  1.77s/batch]Batch 1000/10001 Done, mean position loss: 21.404507641792296\n",
      "Training NF2:  10%|█▉                  | 996/10001 [30:19<4:24:41,  1.76s/batch]Batch 1000/10001 Done, mean position loss: 21.885734014511108\n",
      "Training NF2:  10%|█▉                 | 1015/10001 [30:19<4:28:03,  1.79s/batch]Batch 1000/10001 Done, mean position loss: 20.678426678180696\n",
      "Training NF2:  10%|█▉                 | 1006/10001 [30:20<3:49:26,  1.53s/batch]Batch 1000/10001 Done, mean position loss: 21.508898229598998\n",
      "Training NF2:  10%|█▉                  | 993/10001 [30:20<3:59:39,  1.60s/batch]Batch 1000/10001 Done, mean position loss: 21.7296537899971\n",
      "Training NF2:  10%|█▉                 | 1012/10001 [30:21<4:14:55,  1.70s/batch]Batch 1000/10001 Done, mean position loss: 20.949370374679567\n",
      "Training NF2:  10%|█▉                  | 996/10001 [30:26<4:21:03,  1.74s/batch]Batch 1000/10001 Done, mean position loss: 20.807172708511352\n",
      "Training NF2:  10%|█▉                  | 979/10001 [30:27<4:59:05,  1.99s/batch]Batch 1000/10001 Done, mean position loss: 21.30456867694855\n",
      "Training NF2:  10%|█▉                 | 1016/10001 [30:27<4:32:57,  1.82s/batch]Batch 1000/10001 Done, mean position loss: 20.767506759166714\n",
      "Training NF2:  10%|█▉                 | 1010/10001 [30:28<4:47:24,  1.92s/batch]Batch 1000/10001 Done, mean position loss: 21.12163555622101\n",
      "Training NF2:  10%|█▉                 | 1017/10001 [30:30<5:16:04,  2.11s/batch]Batch 1000/10001 Done, mean position loss: 21.55231382369995\n",
      "Training NF2:  10%|█▉                 | 1004/10001 [30:32<4:25:05,  1.77s/batch]Batch 1000/10001 Done, mean position loss: 20.728864369392397\n",
      "Training NF2:  10%|█▉                 | 1024/10001 [30:35<4:20:10,  1.74s/batch]Batch 1000/10001 Done, mean position loss: 20.920771403312685\n",
      "Training NF2:  10%|█▉                 | 1019/10001 [30:34<4:28:13,  1.79s/batch]Batch 1000/10001 Done, mean position loss: 20.8936696434021\n",
      "Training NF2:  10%|█▉                 | 1005/10001 [30:34<5:22:50,  2.15s/batch]Batch 1000/10001 Done, mean position loss: 21.161446137428285\n",
      "Training NF2:  10%|█▉                 | 1025/10001 [30:34<3:47:15,  1.52s/batch]Batch 1000/10001 Done, mean position loss: 21.389269688129424\n",
      "Training NF2:  10%|█▉                 | 1002/10001 [30:35<3:38:58,  1.46s/batch]Batch 1000/10001 Done, mean position loss: 21.214688777923584\n",
      "Training NF2:  10%|█▉                 | 1002/10001 [30:37<4:09:17,  1.66s/batch]Batch 1000/10001 Done, mean position loss: 21.43036088466644\n",
      "Training NF2:  10%|█▉                 | 1011/10001 [30:37<5:17:02,  2.12s/batch]Batch 1000/10001 Done, mean position loss: 21.009280741214752\n",
      "Training NF2:  10%|█▉                 | 1017/10001 [30:42<4:49:05,  1.93s/batch]Batch 1000/10001 Done, mean position loss: 21.065864973068237\n",
      "Training NF2:  10%|█▉                 | 1007/10001 [30:43<4:09:54,  1.67s/batch]Batch 1000/10001 Done, mean position loss: 21.0459060382843\n",
      "Training NF2:  10%|█▉                 | 1008/10001 [30:48<4:22:24,  1.75s/batch]Batch 1000/10001 Done, mean position loss: 21.16103217124939\n",
      "Training NF2:  10%|█▉                 | 1024/10001 [31:01<4:37:19,  1.85s/batch]Batch 1000/10001 Done, mean position loss: 21.032068893909454\n",
      "Training NF2:  10%|█▉                 | 1022/10001 [31:08<5:03:43,  2.03s/batch]Batch 1000/10001 Done, mean position loss: 20.3678629899025\n",
      "Training NF2:  10%|█▉                 | 1021/10001 [31:10<4:14:32,  1.70s/batch]Batch 1000/10001 Done, mean position loss: 20.86571288347244\n",
      "Training NF2:  11%|██                 | 1064/10001 [32:24<4:07:22,  1.66s/batch]Batch 1100/10001 Done, mean position loss: 21.026580560207368\n",
      "Training NF2:  11%|██                 | 1063/10001 [32:40<4:03:22,  1.63s/batch]Batch 1100/10001 Done, mean position loss: 21.353096828460693\n",
      "Training NF2:  11%|██                 | 1086/10001 [32:47<4:31:21,  1.83s/batch]Batch 1100/10001 Done, mean position loss: 20.961252386569974\n",
      "Training NF2:  11%|██                 | 1076/10001 [32:49<4:23:37,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 20.831556067466735\n",
      "Training NF2:  11%|██                 | 1061/10001 [32:55<4:32:18,  1.83s/batch]Batch 1100/10001 Done, mean position loss: 20.57873862504959\n",
      "Training NF2:  11%|██                 | 1094/10001 [32:57<5:12:49,  2.11s/batch]Batch 1100/10001 Done, mean position loss: 21.274080727100372\n",
      "Training NF2:  11%|██                 | 1090/10001 [32:58<4:21:54,  1.76s/batch]Batch 1100/10001 Done, mean position loss: 21.260328600406645\n",
      "Training NF2:  11%|██                 | 1113/10001 [33:08<3:56:28,  1.60s/batch]Batch 1100/10001 Done, mean position loss: 21.05879263162613\n",
      "Training NF2:  11%|██                 | 1110/10001 [33:09<3:43:06,  1.51s/batch]Batch 1100/10001 Done, mean position loss: 21.165888128280642\n",
      "Training NF2:  11%|██                 | 1098/10001 [33:10<4:31:12,  1.83s/batch]Batch 1100/10001 Done, mean position loss: 20.859696505069735\n",
      "Training NF2:  11%|██                 | 1107/10001 [33:10<4:36:39,  1.87s/batch]Batch 1100/10001 Done, mean position loss: 21.039604115486146\n",
      "Training NF2:  11%|██                 | 1096/10001 [33:16<4:59:34,  2.02s/batch]Batch 1100/10001 Done, mean position loss: 21.424487755298614\n",
      "Training NF2:  11%|██                 | 1070/10001 [33:15<5:14:05,  2.11s/batch]Batch 1100/10001 Done, mean position loss: 21.729676067829132\n",
      "Training NF2:  11%|██                 | 1118/10001 [33:17<4:17:57,  1.74s/batch]Batch 1100/10001 Done, mean position loss: 21.939207015037535\n",
      "Training NF2:  11%|██                 | 1097/10001 [33:18<4:31:04,  1.83s/batch]Batch 1100/10001 Done, mean position loss: 21.191098420619966\n",
      "Training NF2:  11%|██                 | 1104/10001 [33:20<4:29:07,  1.81s/batch]Batch 1100/10001 Done, mean position loss: 21.672783217430116\n",
      "Training NF2:  11%|██                 | 1085/10001 [33:22<4:16:22,  1.73s/batch]Batch 1100/10001 Done, mean position loss: 21.02053822994232\n",
      "Training NF2:  11%|██                 | 1092/10001 [33:22<4:23:27,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 20.648557369709017\n",
      "Training NF2:  11%|██                 | 1096/10001 [33:24<4:02:41,  1.64s/batch]Batch 1100/10001 Done, mean position loss: 20.927651546001435\n",
      "Training NF2:  11%|██                 | 1104/10001 [33:26<4:23:02,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 21.275248165130616\n",
      "Training NF2:  11%|██                 | 1117/10001 [33:27<4:53:42,  1.98s/batch]Batch 1100/10001 Done, mean position loss: 21.441936993598937\n",
      "Training NF2:  11%|██                 | 1108/10001 [33:30<4:02:52,  1.64s/batch]Batch 1100/10001 Done, mean position loss: 20.712785892486572\n",
      "Training NF2:  11%|██▏                | 1129/10001 [33:32<5:07:29,  2.08s/batch]Batch 1100/10001 Done, mean position loss: 21.504697198867795\n",
      "Training NF2:  11%|██                 | 1110/10001 [33:33<4:12:20,  1.70s/batch]Batch 1100/10001 Done, mean position loss: 21.179888570308684\n",
      "Training NF2:  11%|██                 | 1108/10001 [33:35<4:23:03,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 21.060838701725004\n",
      "Training NF2:  11%|██                 | 1102/10001 [33:35<4:22:25,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 20.7058812713623\n",
      "Training NF2:  11%|██                 | 1082/10001 [33:35<4:18:25,  1.74s/batch]Batch 1100/10001 Done, mean position loss: 20.84156831741333\n",
      "Training NF2:  11%|██                 | 1106/10001 [33:37<5:22:34,  2.18s/batch]Batch 1100/10001 Done, mean position loss: 21.338115456104276\n",
      "Training NF2:  11%|██                 | 1100/10001 [33:39<4:31:19,  1.83s/batch]Batch 1100/10001 Done, mean position loss: 21.35173247337341\n",
      "Training NF2:  11%|██                 | 1096/10001 [33:39<4:38:22,  1.88s/batch]Batch 1100/10001 Done, mean position loss: 20.749346857070925\n",
      "Training NF2:  11%|██                 | 1114/10001 [33:40<4:22:11,  1.77s/batch]Batch 1100/10001 Done, mean position loss: 21.071703405380248\n",
      "Training NF2:  11%|██                 | 1101/10001 [33:41<4:35:12,  1.86s/batch]Batch 1100/10001 Done, mean position loss: 21.0798445391655\n",
      "Training NF2:  11%|██                 | 1106/10001 [33:42<4:25:58,  1.79s/batch]Batch 1100/10001 Done, mean position loss: 20.97184552669525\n",
      "Training NF2:  11%|██                 | 1098/10001 [33:42<4:09:03,  1.68s/batch]Batch 1100/10001 Done, mean position loss: 20.903121869564057\n",
      "Training NF2:  11%|██▏                | 1133/10001 [33:47<5:00:00,  2.03s/batch]Batch 1100/10001 Done, mean position loss: 20.943674228191377\n",
      "Training NF2:  11%|██▏                | 1122/10001 [33:47<4:19:17,  1.75s/batch]Batch 1100/10001 Done, mean position loss: 21.04397933244705\n",
      "Training NF2:  11%|██                 | 1113/10001 [33:52<4:06:57,  1.67s/batch]Batch 1100/10001 Done, mean position loss: 21.11084517240524\n",
      "Training NF2:  11%|██                 | 1100/10001 [34:06<3:59:48,  1.62s/batch]Batch 1100/10001 Done, mean position loss: 20.92784011602402\n",
      "Training NF2:  11%|██                 | 1110/10001 [34:08<4:12:51,  1.71s/batch]Batch 1100/10001 Done, mean position loss: 20.35655368566513\n",
      "Training NF2:  11%|██▏                | 1131/10001 [34:08<4:24:34,  1.79s/batch]Batch 1100/10001 Done, mean position loss: 20.792737600803374\n",
      "Training NF2:  12%|██▏                | 1164/10001 [35:21<4:58:24,  2.03s/batch]Batch 1200/10001 Done, mean position loss: 20.895406379699708\n",
      "Training NF2:  12%|██▏                | 1165/10001 [35:40<5:37:32,  2.29s/batch]Batch 1200/10001 Done, mean position loss: 21.309254138469697\n",
      "Training NF2:  12%|██▎                | 1189/10001 [35:44<3:52:36,  1.58s/batch]Batch 1200/10001 Done, mean position loss: 20.53255539655685\n",
      "Training NF2:  12%|██▏                | 1175/10001 [35:46<4:35:46,  1.87s/batch]Batch 1200/10001 Done, mean position loss: 20.923168275356296\n",
      "Training NF2:  12%|██▏                | 1153/10001 [35:51<6:30:48,  2.65s/batch]Batch 1200/10001 Done, mean position loss: 20.78696899652481\n",
      "Training NF2:  12%|██▎                | 1190/10001 [36:03<4:13:28,  1.73s/batch]Batch 1200/10001 Done, mean position loss: 21.0244268655777\n",
      "Training NF2:  12%|██▏                | 1161/10001 [36:04<4:21:52,  1.78s/batch]Batch 1200/10001 Done, mean position loss: 21.208916013240817\n",
      "Training NF2:  12%|██▎                | 1202/10001 [36:05<4:06:47,  1.68s/batch]Batch 1200/10001 Done, mean position loss: 20.982170195579528\n",
      "Training NF2:  12%|██▏                | 1163/10001 [36:08<4:39:29,  1.90s/batch]Batch 1200/10001 Done, mean position loss: 21.088198692798613\n",
      "Training NF2:  12%|██▏                | 1179/10001 [36:08<4:19:47,  1.77s/batch]Batch 1200/10001 Done, mean position loss: 21.022299485206602\n",
      "Training NF2:  12%|██▎                | 1204/10001 [36:10<4:45:14,  1.95s/batch]Batch 1200/10001 Done, mean position loss: 21.580204677581786\n",
      "Training NF2:  12%|██▎                | 1187/10001 [36:12<4:08:01,  1.69s/batch]Batch 1200/10001 Done, mean position loss: 21.352273604869843\n",
      "Training NF2:  12%|██▏                | 1166/10001 [36:14<4:41:36,  1.91s/batch]Batch 1200/10001 Done, mean position loss: 20.834162487983704\n",
      "Training NF2:  12%|██▏                | 1184/10001 [36:13<4:44:31,  1.94s/batch]Batch 1200/10001 Done, mean position loss: 21.546357905864717\n",
      "Training NF2:  12%|██▎                | 1208/10001 [36:17<4:12:15,  1.72s/batch]Batch 1200/10001 Done, mean position loss: 21.073601686954497\n",
      "Training NF2:  12%|██▎                | 1210/10001 [36:19<3:44:41,  1.53s/batch]Batch 1200/10001 Done, mean position loss: 21.87632960319519\n",
      "Training NF2:  12%|██▎                | 1218/10001 [36:22<4:09:49,  1.71s/batch]Batch 1200/10001 Done, mean position loss: 20.976731343269346\n",
      "Training NF2:  12%|██▎                | 1207/10001 [36:23<3:42:17,  1.52s/batch]Batch 1200/10001 Done, mean position loss: 20.647186830043793\n",
      "Training NF2:  12%|██▎                | 1190/10001 [36:31<4:30:07,  1.84s/batch]Batch 1200/10001 Done, mean position loss: 20.891616895198823\n",
      "Training NF2:  12%|██▎                | 1190/10001 [36:32<4:40:45,  1.91s/batch]Batch 1200/10001 Done, mean position loss: 21.378265261650085\n",
      "Training NF2:  12%|██▎                | 1193/10001 [36:33<3:57:53,  1.62s/batch]Batch 1200/10001 Done, mean position loss: 20.676325175762173\n",
      "Training NF2:  12%|██▎                | 1229/10001 [36:34<4:45:06,  1.95s/batch]Batch 1200/10001 Done, mean position loss: 21.26444395542145\n",
      "Training NF2:  12%|██▎                | 1216/10001 [36:34<4:15:31,  1.75s/batch]Batch 1200/10001 Done, mean position loss: 21.45378960609436\n",
      "Training NF2:  12%|██▎                | 1210/10001 [36:35<4:15:28,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 20.817689907550815\n",
      "Training NF2:  12%|██▎                | 1203/10001 [36:38<5:09:06,  2.11s/batch]Batch 1200/10001 Done, mean position loss: 20.660509841442106\n",
      "Training NF2:  12%|██▎                | 1234/10001 [36:41<4:06:24,  1.69s/batch]Batch 1200/10001 Done, mean position loss: 21.02207991361618\n",
      "Training NF2:  12%|██▎                | 1187/10001 [36:41<4:15:03,  1.74s/batch]Batch 1200/10001 Done, mean position loss: 21.28033130645752\n",
      "Training NF2:  12%|██▎                | 1235/10001 [36:43<4:07:58,  1.70s/batch]Batch 1200/10001 Done, mean position loss: 20.723292243480685\n",
      "Training NF2:  12%|██▎                | 1220/10001 [36:45<4:05:04,  1.67s/batch]Batch 1200/10001 Done, mean position loss: 20.971222732067105\n",
      "Training NF2:  12%|██▎                | 1226/10001 [36:45<3:39:28,  1.50s/batch]Batch 1200/10001 Done, mean position loss: 21.13183598279953\n",
      "Training NF2:  12%|██▎                | 1209/10001 [36:46<3:44:56,  1.54s/batch]Batch 1200/10001 Done, mean position loss: 21.273275737762454\n",
      "Training NF2:  12%|██▎                | 1210/10001 [36:47<4:08:15,  1.69s/batch]Batch 1200/10001 Done, mean position loss: 21.042858021259306\n",
      "Training NF2:  12%|██▎                | 1222/10001 [36:48<4:00:26,  1.64s/batch]Batch 1200/10001 Done, mean position loss: 21.019423258304595\n",
      "Training NF2:  12%|██▎                | 1226/10001 [36:50<3:39:14,  1.50s/batch]Batch 1200/10001 Done, mean position loss: 20.88249487876892\n",
      "Training NF2:  12%|██▎                | 1200/10001 [36:50<4:46:49,  1.96s/batch]Batch 1200/10001 Done, mean position loss: 20.854126274585724\n",
      "Training NF2:  12%|██▎                | 1238/10001 [36:50<4:48:36,  1.98s/batch]Batch 1200/10001 Done, mean position loss: 21.05426411628723\n",
      "Training NF2:  12%|██▎                | 1202/10001 [36:51<4:01:21,  1.65s/batch]Batch 1200/10001 Done, mean position loss: 21.082787375450135\n",
      "Training NF2:  12%|██▎                | 1233/10001 [37:07<3:50:43,  1.58s/batch]Batch 1200/10001 Done, mean position loss: 20.34324579000473\n",
      "Training NF2:  12%|██▎                | 1230/10001 [37:11<4:49:20,  1.98s/batch]Batch 1200/10001 Done, mean position loss: 20.700623404979705\n",
      "Training NF2:  12%|██▎                | 1248/10001 [37:13<3:57:54,  1.63s/batch]Batch 1200/10001 Done, mean position loss: 20.86118418455124\n",
      "Training NF2:  13%|██▍                | 1264/10001 [38:23<3:55:32,  1.62s/batch]Batch 1300/10001 Done, mean position loss: 20.800018877983092\n",
      "Training NF2:  13%|██▍                | 1257/10001 [38:32<4:23:38,  1.81s/batch]Batch 1300/10001 Done, mean position loss: 20.91133021593094\n",
      "Training NF2:  13%|██▍                | 1273/10001 [38:38<4:20:08,  1.79s/batch]Batch 1300/10001 Done, mean position loss: 20.493885703086853\n",
      "Training NF2:  13%|██▍                | 1266/10001 [38:39<3:35:36,  1.48s/batch]Batch 1300/10001 Done, mean position loss: 21.247918965816496\n",
      "Training NF2:  13%|██▍                | 1273/10001 [38:47<4:14:08,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 20.75228452205658\n",
      "Training NF2:  13%|██▍                | 1277/10001 [38:49<4:38:02,  1.91s/batch]Batch 1300/10001 Done, mean position loss: 21.152578723430636\n",
      "Training NF2:  13%|██▍                | 1314/10001 [38:54<4:13:29,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 21.010489215850832\n",
      "Training NF2:  13%|██▍                | 1283/10001 [38:56<4:26:45,  1.84s/batch]Batch 1300/10001 Done, mean position loss: 20.97632866382599\n",
      "Training NF2:  13%|██▍                | 1276/10001 [39:01<4:04:10,  1.68s/batch]Batch 1300/10001 Done, mean position loss: 20.931773009300233\n",
      "Batch 1300/10001 Done, mean position loss: 20.90862625837326\n",
      "Training NF2:  13%|██▍                | 1314/10001 [39:03<4:12:01,  1.74s/batch]Batch 1300/10001 Done, mean position loss: 21.419391601085664\n",
      "Training NF2:  13%|██▍                | 1286/10001 [39:05<4:22:41,  1.81s/batch]Batch 1300/10001 Done, mean position loss: 21.325440325737\n",
      "Training NF2:  13%|██▍                | 1280/10001 [39:10<4:15:28,  1.76s/batch]Batch 1300/10001 Done, mean position loss: 20.801282501220705\n",
      "Training NF2:  13%|██▍                | 1307/10001 [39:10<4:08:10,  1.71s/batch]Batch 1300/10001 Done, mean position loss: 20.990951554775236\n",
      "Training NF2:  13%|██▍                | 1306/10001 [39:11<4:04:03,  1.68s/batch]Batch 1300/10001 Done, mean position loss: 21.48117788314819\n",
      "Training NF2:  13%|██▍                | 1287/10001 [39:13<4:19:11,  1.78s/batch]Batch 1300/10001 Done, mean position loss: 20.922485206127167\n",
      "Training NF2:  13%|██▍                | 1287/10001 [39:16<4:56:13,  2.04s/batch]Batch 1300/10001 Done, mean position loss: 20.65356005907059\n",
      "Training NF2:  13%|██▍                | 1271/10001 [39:17<4:30:07,  1.86s/batch]Batch 1300/10001 Done, mean position loss: 21.817544565200805\n",
      "Training NF2:  13%|██▌                | 1325/10001 [39:18<3:54:00,  1.62s/batch]Batch 1300/10001 Done, mean position loss: 20.646030745506287\n",
      "Training NF2:  13%|██▍                | 1295/10001 [39:23<3:44:24,  1.55s/batch]Batch 1300/10001 Done, mean position loss: 21.301281914710998\n",
      "Training NF2:  13%|██▍                | 1295/10001 [39:27<4:09:35,  1.72s/batch]Batch 1300/10001 Done, mean position loss: 20.857989404201504\n",
      "Training NF2:  13%|██▍                | 1301/10001 [39:27<4:16:10,  1.77s/batch]Batch 1300/10001 Done, mean position loss: 21.22342766046524\n",
      "Training NF2:  13%|██▍                | 1298/10001 [39:28<4:48:50,  1.99s/batch]Batch 1300/10001 Done, mean position loss: 21.37446579694748\n",
      "Training NF2:  13%|██▍                | 1282/10001 [39:33<2:56:25,  1.21s/batch]Batch 1300/10001 Done, mean position loss: 20.802976617813112\n",
      "Training NF2:  13%|██▍                | 1298/10001 [39:34<4:30:15,  1.86s/batch]Batch 1300/10001 Done, mean position loss: 21.19287102937698\n",
      "Training NF2:  13%|██▍                | 1292/10001 [39:34<4:40:10,  1.93s/batch]Batch 1300/10001 Done, mean position loss: 20.934163763523102\n",
      "Training NF2:  13%|██▌                | 1316/10001 [39:37<4:44:42,  1.97s/batch]Batch 1300/10001 Done, mean position loss: 21.002053756713867\n",
      "Training NF2:  13%|██▍                | 1303/10001 [39:37<3:54:27,  1.62s/batch]Batch 1300/10001 Done, mean position loss: 21.228674492836\n",
      "Training NF2:  13%|██▌                | 1317/10001 [39:37<4:13:42,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 20.9859800863266\n",
      "Training NF2:  13%|██▌                | 1327/10001 [39:37<4:24:26,  1.83s/batch]Batch 1300/10001 Done, mean position loss: 20.70666087150574\n",
      "Training NF2:  13%|██▌                | 1318/10001 [39:39<4:18:51,  1.79s/batch]Batch 1300/10001 Done, mean position loss: 20.643198044300078\n",
      "Training NF2:  13%|██▍                | 1300/10001 [39:42<4:40:26,  1.93s/batch]Batch 1300/10001 Done, mean position loss: 20.749182522296906\n",
      "Training NF2:  13%|██▍                | 1298/10001 [39:44<3:32:12,  1.46s/batch]Batch 1300/10001 Done, mean position loss: 20.86317270517349\n",
      "Training NF2:  13%|██▍                | 1303/10001 [39:47<4:10:27,  1.73s/batch]Batch 1300/10001 Done, mean position loss: 21.068145577907565\n",
      "Training NF2:  13%|██▌                | 1322/10001 [39:47<4:12:48,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 20.998068690299988\n",
      "Training NF2:  13%|██▌                | 1329/10001 [39:48<3:35:28,  1.49s/batch]Batch 1300/10001 Done, mean position loss: 21.056355254650114\n",
      "Training NF2:  13%|██▌                | 1326/10001 [39:56<5:15:11,  2.18s/batch]Batch 1300/10001 Done, mean position loss: 21.02002143621445\n",
      "Training NF2:  13%|██▍                | 1312/10001 [40:01<4:20:51,  1.80s/batch]Batch 1300/10001 Done, mean position loss: 20.350096096992495\n",
      "Training NF2:  13%|██▌                | 1334/10001 [40:02<4:44:17,  1.97s/batch]Batch 1300/10001 Done, mean position loss: 20.831497292518616\n",
      "Training NF2:  14%|██▌                | 1363/10001 [40:06<4:12:31,  1.75s/batch]Batch 1300/10001 Done, mean position loss: 20.645878467559815\n",
      "Training NF2:  14%|██▌                | 1359/10001 [41:13<4:31:31,  1.89s/batch]Batch 1400/10001 Done, mean position loss: 20.716489152908323\n",
      "Training NF2:  14%|██▋                | 1407/10001 [41:25<4:08:00,  1.73s/batch]Batch 1400/10001 Done, mean position loss: 20.873081121444702\n",
      "Training NF2:  14%|██▌                | 1368/10001 [41:31<4:43:06,  1.97s/batch]Batch 1400/10001 Done, mean position loss: 21.208173596858977\n",
      "Training NF2:  14%|██▋                | 1401/10001 [41:31<4:09:03,  1.74s/batch]Batch 1400/10001 Done, mean position loss: 20.46971709012985\n",
      "Training NF2:  14%|██▌                | 1357/10001 [41:44<4:27:09,  1.85s/batch]Batch 1400/10001 Done, mean position loss: 20.745319724082947\n",
      "Training NF2:  14%|██▋                | 1414/10001 [41:47<4:55:30,  2.06s/batch]Batch 1400/10001 Done, mean position loss: 21.032386362552643\n",
      "Training NF2:  14%|██▌                | 1376/10001 [41:47<4:24:23,  1.84s/batch]Batch 1400/10001 Done, mean position loss: 21.09987404346466\n",
      "Training NF2:  14%|██▋                | 1384/10001 [41:48<3:57:15,  1.65s/batch]Batch 1400/10001 Done, mean position loss: 20.88141939163208\n",
      "Training NF2:  14%|██▋                | 1405/10001 [41:51<3:52:50,  1.63s/batch]Batch 1400/10001 Done, mean position loss: 20.822177760601043\n",
      "Training NF2:  14%|██▋                | 1405/10001 [41:54<4:03:37,  1.70s/batch]Batch 1400/10001 Done, mean position loss: 21.3068203663826\n",
      "Training NF2:  14%|██▋                | 1418/10001 [41:54<4:43:01,  1.98s/batch]Batch 1400/10001 Done, mean position loss: 20.894262845516202\n",
      "Training NF2:  14%|██▋                | 1391/10001 [42:03<4:21:39,  1.82s/batch]Batch 1400/10001 Done, mean position loss: 21.27519714832306\n",
      "Training NF2:  14%|██▋                | 1412/10001 [42:07<4:22:07,  1.83s/batch]Batch 1400/10001 Done, mean position loss: 21.380920898914336\n",
      "Training NF2:  14%|██▋                | 1414/10001 [42:11<4:36:15,  1.93s/batch]Batch 1400/10001 Done, mean position loss: 20.63033589363098\n",
      "Training NF2:  14%|██▋                | 1387/10001 [42:12<5:02:47,  2.11s/batch]Batch 1400/10001 Done, mean position loss: 20.793702161312105\n",
      "Training NF2:  14%|██▋                | 1388/10001 [42:15<6:02:18,  2.52s/batch]Batch 1400/10001 Done, mean position loss: 20.878227922916412\n",
      "Training NF2:  14%|██▋                | 1399/10001 [42:19<4:50:40,  2.03s/batch]Batch 1400/10001 Done, mean position loss: 20.870898261070252\n",
      "Training NF2:  14%|██▋                | 1392/10001 [42:21<5:33:33,  2.32s/batch]Batch 1400/10001 Done, mean position loss: 21.731256158351897\n",
      "Training NF2:  14%|██▋                | 1405/10001 [42:22<4:15:51,  1.79s/batch]Batch 1400/10001 Done, mean position loss: 20.654435501098632\n",
      "Training NF2:  14%|██▋                | 1406/10001 [42:24<4:14:41,  1.78s/batch]Batch 1400/10001 Done, mean position loss: 21.290261068344115\n",
      "Training NF2:  14%|██▋                | 1405/10001 [42:25<3:59:15,  1.67s/batch]Batch 1400/10001 Done, mean position loss: 20.68358798980713\n",
      "Training NF2:  14%|██▋                | 1389/10001 [42:25<4:11:38,  1.75s/batch]Batch 1400/10001 Done, mean position loss: 20.785588862895963\n",
      "Training NF2:  14%|██▋                | 1401/10001 [42:25<4:59:27,  2.09s/batch]Batch 1400/10001 Done, mean position loss: 21.341948163509368\n",
      "Training NF2:  14%|██▋                | 1390/10001 [42:27<4:18:48,  1.80s/batch]Batch 1400/10001 Done, mean position loss: 21.223855137825012\n",
      "Training NF2:  14%|██▋                | 1395/10001 [42:30<3:52:06,  1.62s/batch]Batch 1400/10001 Done, mean position loss: 21.139724404811858\n",
      "Training NF2:  14%|██▋                | 1432/10001 [42:31<3:45:04,  1.58s/batch]Batch 1400/10001 Done, mean position loss: 20.82559079408646\n",
      "Training NF2:  14%|██▋                | 1439/10001 [42:36<4:12:25,  1.77s/batch]Batch 1400/10001 Done, mean position loss: 20.71788148403168\n",
      "Training NF2:  14%|██▋                | 1420/10001 [42:37<4:14:53,  1.78s/batch]Batch 1400/10001 Done, mean position loss: 20.943962039947507\n",
      "Training NF2:  14%|██▋                | 1419/10001 [42:39<4:44:44,  1.99s/batch]Batch 1400/10001 Done, mean position loss: 20.878156945705413\n",
      "Training NF2:  14%|██▋                | 1432/10001 [42:39<4:20:20,  1.82s/batch]Batch 1400/10001 Done, mean position loss: 21.18912144899368\n",
      "Training NF2:  14%|██▋                | 1440/10001 [42:39<4:31:22,  1.90s/batch]Batch 1400/10001 Done, mean position loss: 20.835789053440095\n",
      "Training NF2:  14%|██▋                | 1412/10001 [42:42<4:24:38,  1.85s/batch]Batch 1400/10001 Done, mean position loss: 20.960163259506224\n",
      "Training NF2:  14%|██▋                | 1404/10001 [42:44<4:03:45,  1.70s/batch]Batch 1400/10001 Done, mean position loss: 20.605525381565094\n",
      "Training NF2:  14%|██▋                | 1432/10001 [42:48<4:35:15,  1.93s/batch]Batch 1400/10001 Done, mean position loss: 21.02706442117691\n",
      "Training NF2:  14%|██▋                | 1407/10001 [42:49<5:05:50,  2.14s/batch]Batch 1400/10001 Done, mean position loss: 20.976772239208223\n",
      "Training NF2:  14%|██▋                | 1416/10001 [42:58<4:41:36,  1.97s/batch]Batch 1400/10001 Done, mean position loss: 20.988710463047028\n",
      "Batch 1400/10001 Done, mean position loss: 21.05479695558548\n",
      "Training NF2:  14%|██▋                | 1411/10001 [43:04<3:24:21,  1.43s/batch]Batch 1400/10001 Done, mean position loss: 20.336701672077176\n",
      "Training NF2:  14%|██▋                | 1442/10001 [43:08<4:15:20,  1.79s/batch]Batch 1400/10001 Done, mean position loss: 20.604605782032014\n",
      "Training NF2:  15%|██▊                | 1462/10001 [43:17<3:44:06,  1.57s/batch]Batch 1400/10001 Done, mean position loss: 20.78151186466217\n",
      "Training NF2:  14%|██▋                | 1442/10001 [44:14<4:12:10,  1.77s/batch]Batch 1500/10001 Done, mean position loss: 20.713297970294953\n",
      "Training NF2:  15%|██▊                | 1451/10001 [44:28<4:03:39,  1.71s/batch]Batch 1500/10001 Done, mean position loss: 20.867244839668274\n",
      "Training NF2:  15%|██▊                | 1481/10001 [44:29<4:17:42,  1.81s/batch]Batch 1500/10001 Done, mean position loss: 21.20646042108536\n",
      "Training NF2:  15%|██▊                | 1467/10001 [44:38<4:01:41,  1.70s/batch]Batch 1500/10001 Done, mean position loss: 20.447819714546206\n",
      "Training NF2:  15%|██▊                | 1510/10001 [44:44<4:15:38,  1.81s/batch]Batch 1500/10001 Done, mean position loss: 20.735821869373325\n",
      "Training NF2:  15%|██▊                | 1511/10001 [44:46<4:15:52,  1.81s/batch]Batch 1500/10001 Done, mean position loss: 21.042986917495725\n",
      "Training NF2:  15%|██▊                | 1481/10001 [44:48<4:07:17,  1.74s/batch]Batch 1500/10001 Done, mean position loss: 21.024832248687744\n",
      "Training NF2:  15%|██▊                | 1475/10001 [44:50<4:40:37,  1.97s/batch]Batch 1500/10001 Done, mean position loss: 21.188230323791505\n",
      "Training NF2:  15%|██▊                | 1495/10001 [44:53<3:55:33,  1.66s/batch]Batch 1500/10001 Done, mean position loss: 20.84176900625229\n",
      "Training NF2:  15%|██▊                | 1471/10001 [44:53<3:19:40,  1.40s/batch]Batch 1500/10001 Done, mean position loss: 20.74404569864273\n",
      "Training NF2:  15%|██▊                | 1478/10001 [45:04<3:52:53,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 20.781642475128173\n",
      "Training NF2:  15%|██▊                | 1513/10001 [45:06<4:19:56,  1.84s/batch]Batch 1500/10001 Done, mean position loss: 21.287349817752837\n",
      "Training NF2:  15%|██▊                | 1478/10001 [45:09<4:27:55,  1.89s/batch]Batch 1500/10001 Done, mean position loss: 20.84031768321991\n",
      "Training NF2:  15%|██▉                | 1515/10001 [45:10<4:15:00,  1.80s/batch]Batch 1500/10001 Done, mean position loss: 20.6172926402092\n",
      "Training NF2:  15%|██▊                | 1506/10001 [45:13<4:09:27,  1.76s/batch]Batch 1500/10001 Done, mean position loss: 20.842105720043183\n",
      "Training NF2:  15%|██▊                | 1498/10001 [45:14<4:13:41,  1.79s/batch]Batch 1500/10001 Done, mean position loss: 21.317192940711976\n",
      "Training NF2:  15%|██▊                | 1507/10001 [45:19<3:48:07,  1.61s/batch]Batch 1500/10001 Done, mean position loss: 21.728254988193513\n",
      "Training NF2:  15%|██▉                | 1523/10001 [45:20<3:52:13,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 20.78467780590057\n",
      "Training NF2:  15%|██▊                | 1489/10001 [45:22<4:59:33,  2.11s/batch]Batch 1500/10001 Done, mean position loss: 20.63012208938599\n",
      "Training NF2:  15%|██▉                | 1522/10001 [45:24<4:37:36,  1.96s/batch]Batch 1500/10001 Done, mean position loss: 20.83320205926895\n",
      "Training NF2:  15%|██▊                | 1493/10001 [45:26<4:19:22,  1.83s/batch]Batch 1500/10001 Done, mean position loss: 21.242337951660154\n",
      "Training NF2:  15%|██▊                | 1481/10001 [45:26<4:02:34,  1.71s/batch]Batch 1500/10001 Done, mean position loss: 20.663138358592985\n",
      "Training NF2:  15%|██▊                | 1492/10001 [45:28<4:59:19,  2.11s/batch]Batch 1500/10001 Done, mean position loss: 21.240144393444062\n",
      "Training NF2:  15%|██▊                | 1495/10001 [45:30<4:06:13,  1.74s/batch]Batch 1500/10001 Done, mean position loss: 21.133177299499515\n",
      "Training NF2:  15%|██▉                | 1528/10001 [45:36<4:42:54,  2.00s/batch]Batch 1500/10001 Done, mean position loss: 21.30496233224869\n",
      "Training NF2:  15%|██▊                | 1496/10001 [45:36<4:34:01,  1.93s/batch]Batch 1500/10001 Done, mean position loss: 20.825383944511415\n",
      "Training NF2:  15%|██▉                | 1527/10001 [45:38<3:38:35,  1.55s/batch]Batch 1500/10001 Done, mean position loss: 20.66648068189621\n",
      "Training NF2:  15%|██▊                | 1486/10001 [45:40<3:51:30,  1.63s/batch]Batch 1500/10001 Done, mean position loss: 20.822701470851896\n",
      "Training NF2:  15%|██▉                | 1528/10001 [45:41<3:52:39,  1.65s/batch]Batch 1500/10001 Done, mean position loss: 20.935460793972016\n",
      "Training NF2:  15%|██▊                | 1511/10001 [45:42<4:24:02,  1.87s/batch]Batch 1500/10001 Done, mean position loss: 20.945130374431606\n",
      "Training NF2:  15%|██▉                | 1545/10001 [45:44<4:19:53,  1.84s/batch]Batch 1500/10001 Done, mean position loss: 21.128117074966433\n",
      "Training NF2:  15%|██▉                | 1521/10001 [45:45<4:40:24,  1.98s/batch]Batch 1500/10001 Done, mean position loss: 20.88968794584274\n",
      "Training NF2:  15%|██▉                | 1543/10001 [45:45<4:43:17,  2.01s/batch]Batch 1500/10001 Done, mean position loss: 21.00098004102707\n",
      "Training NF2:  15%|██▉                | 1519/10001 [45:47<4:21:18,  1.85s/batch]Batch 1500/10001 Done, mean position loss: 20.952143123149874\n",
      "Training NF2:  15%|██▉                | 1518/10001 [45:55<4:26:30,  1.89s/batch]Batch 1500/10001 Done, mean position loss: 20.57634911775589\n",
      "Training NF2:  16%|██▉                | 1553/10001 [46:00<4:47:13,  2.04s/batch]Batch 1500/10001 Done, mean position loss: 20.334838631153104\n",
      "Training NF2:  15%|██▊                | 1510/10001 [46:01<3:44:15,  1.58s/batch]Batch 1500/10001 Done, mean position loss: 21.022798280715943\n",
      "Training NF2:  15%|██▉                | 1533/10001 [46:05<3:51:16,  1.64s/batch]Batch 1500/10001 Done, mean position loss: 20.962983598709105\n",
      "Training NF2:  15%|██▊                | 1503/10001 [46:08<4:31:51,  1.92s/batch]Batch 1500/10001 Done, mean position loss: 20.557767822742463\n",
      "Training NF2:  15%|██▉                | 1528/10001 [46:26<3:53:53,  1.66s/batch]Batch 1500/10001 Done, mean position loss: 20.76196136713028\n",
      "Training NF2:  16%|██▉                | 1567/10001 [47:20<3:36:37,  1.54s/batch]Batch 1600/10001 Done, mean position loss: 20.641741001605986\n",
      "Training NF2:  16%|███                | 1589/10001 [47:29<3:42:57,  1.59s/batch]Batch 1600/10001 Done, mean position loss: 21.148746967315674\n",
      "Training NF2:  16%|███                | 1589/10001 [47:31<4:30:56,  1.93s/batch]Batch 1600/10001 Done, mean position loss: 20.84248033761978\n",
      "Training NF2:  16%|██▉                | 1568/10001 [47:41<3:52:00,  1.65s/batch]Batch 1600/10001 Done, mean position loss: 20.43042488336563\n",
      "Training NF2:  16%|███                | 1587/10001 [47:44<4:31:01,  1.93s/batch]Batch 1600/10001 Done, mean position loss: 21.03449890136719\n",
      "Training NF2:  16%|██▉                | 1579/10001 [47:49<3:59:41,  1.71s/batch]Batch 1600/10001 Done, mean position loss: 21.011669206619263\n",
      "Training NF2:  15%|██▉                | 1545/10001 [47:50<4:37:26,  1.97s/batch]Batch 1600/10001 Done, mean position loss: 20.711274030208585\n",
      "Training NF2:  16%|███                | 1602/10001 [47:51<4:10:14,  1.79s/batch]Batch 1600/10001 Done, mean position loss: 20.70515894412994\n",
      "Training NF2:  16%|██▉                | 1572/10001 [47:52<3:36:17,  1.54s/batch]Batch 1600/10001 Done, mean position loss: 21.164492361545562\n",
      "Training NF2:  16%|███                | 1619/10001 [47:59<3:39:28,  1.57s/batch]Batch 1600/10001 Done, mean position loss: 20.832917239665985\n",
      "Training NF2:  16%|██▉                | 1571/10001 [48:09<3:45:31,  1.61s/batch]Batch 1600/10001 Done, mean position loss: 21.262368848323824\n",
      "Training NF2:  16%|███                | 1581/10001 [48:09<4:16:03,  1.82s/batch]Batch 1600/10001 Done, mean position loss: 20.77722449541092\n",
      "Training NF2:  16%|███                | 1608/10001 [48:12<4:02:23,  1.73s/batch]Batch 1600/10001 Done, mean position loss: 20.639237093925473\n",
      "Training NF2:  16%|███                | 1599/10001 [48:16<4:06:13,  1.76s/batch]Batch 1600/10001 Done, mean position loss: 21.673235175609587\n",
      "Training NF2:  16%|███                | 1584/10001 [48:17<5:36:27,  2.40s/batch]Batch 1600/10001 Done, mean position loss: 20.810762581825255\n",
      "Training NF2:  16%|███                | 1627/10001 [48:19<4:41:31,  2.02s/batch]Batch 1600/10001 Done, mean position loss: 20.82931886196136\n",
      "Training NF2:  16%|██▉                | 1578/10001 [48:20<4:02:41,  1.73s/batch]Batch 1600/10001 Done, mean position loss: 20.784234297275543\n",
      "Training NF2:  16%|███                | 1592/10001 [48:23<4:36:32,  1.97s/batch]Batch 1600/10001 Done, mean position loss: 21.234104406833648\n",
      "Training NF2:  16%|███                | 1590/10001 [48:26<4:04:01,  1.74s/batch]Batch 1600/10001 Done, mean position loss: 20.786350955963133\n",
      "Training NF2:  16%|███                | 1584/10001 [48:28<4:15:53,  1.82s/batch]Batch 1600/10001 Done, mean position loss: 21.24713462352753\n",
      "Training NF2:  16%|███                | 1602/10001 [48:29<3:57:20,  1.70s/batch]Batch 1600/10001 Done, mean position loss: 20.619099106788635\n",
      "Training NF2:  16%|███                | 1623/10001 [48:30<3:59:24,  1.71s/batch]Batch 1600/10001 Done, mean position loss: 20.652332057952883\n",
      "Training NF2:  16%|███                | 1635/10001 [48:31<4:14:28,  1.83s/batch]Batch 1600/10001 Done, mean position loss: 21.249148173332216\n",
      "Training NF2:  16%|███                | 1608/10001 [48:39<4:31:46,  1.94s/batch]Batch 1600/10001 Done, mean position loss: 20.634802412986755\n",
      "Training NF2:  16%|██▉                | 1572/10001 [48:41<4:26:35,  1.90s/batch]Batch 1600/10001 Done, mean position loss: 20.802020699977877\n",
      "Training NF2:  16%|███                | 1628/10001 [48:41<4:09:51,  1.79s/batch]Batch 1600/10001 Done, mean position loss: 21.111897139549257\n",
      "Training NF2:  16%|███                | 1613/10001 [48:42<3:53:05,  1.67s/batch]Batch 1600/10001 Done, mean position loss: 20.922800989151\n",
      "Training NF2:  16%|███                | 1618/10001 [48:43<4:39:17,  2.00s/batch]Batch 1600/10001 Done, mean position loss: 20.811879651546477\n",
      "Training NF2:  16%|███                | 1594/10001 [48:44<4:40:27,  2.00s/batch]Batch 1600/10001 Done, mean position loss: 21.109609053134918\n",
      "Training NF2:  16%|███                | 1618/10001 [48:46<4:19:09,  1.85s/batch]Batch 1600/10001 Done, mean position loss: 20.878729207515715\n",
      "Training NF2:  16%|███                | 1587/10001 [48:46<4:27:19,  1.91s/batch]Batch 1600/10001 Done, mean position loss: 20.84900686264038\n",
      "Training NF2:  16%|███                | 1614/10001 [48:49<4:30:25,  1.93s/batch]Batch 1600/10001 Done, mean position loss: 21.00680026292801\n",
      "Training NF2:  16%|██▉                | 1577/10001 [48:49<4:06:19,  1.75s/batch]Batch 1600/10001 Done, mean position loss: 21.28903209686279\n",
      "Training NF2:  16%|███                | 1600/10001 [48:56<4:21:01,  1.86s/batch]Batch 1600/10001 Done, mean position loss: 20.937293169498442\n",
      "Training NF2:  16%|███                | 1606/10001 [48:58<4:02:06,  1.73s/batch]Batch 1600/10001 Done, mean position loss: 20.57496325492859\n",
      "Training NF2:  16%|███                | 1614/10001 [49:04<4:05:32,  1.76s/batch]Batch 1600/10001 Done, mean position loss: 20.332008969783786\n",
      "Training NF2:  16%|███▏               | 1646/10001 [49:08<4:18:47,  1.86s/batch]Batch 1600/10001 Done, mean position loss: 20.916860423088075\n",
      "Training NF2:  17%|███▏               | 1657/10001 [49:13<4:24:00,  1.90s/batch]Batch 1600/10001 Done, mean position loss: 20.53006582736969\n",
      "Training NF2:  17%|███▏               | 1659/10001 [49:17<4:40:24,  2.02s/batch]Batch 1600/10001 Done, mean position loss: 21.010816259384157\n",
      "Training NF2:  16%|███                | 1639/10001 [49:36<3:58:56,  1.71s/batch]Batch 1600/10001 Done, mean position loss: 20.732184009552\n",
      "Training NF2:  17%|███▏               | 1684/10001 [50:25<4:13:22,  1.83s/batch]Batch 1700/10001 Done, mean position loss: 20.597001872062684\n",
      "Training NF2:  17%|███▏               | 1691/10001 [50:37<3:52:52,  1.68s/batch]Batch 1700/10001 Done, mean position loss: 21.181885619163516\n",
      "Training NF2:  17%|███▏               | 1679/10001 [50:40<3:34:38,  1.55s/batch]Batch 1700/10001 Done, mean position loss: 20.827150764465333\n",
      "Training NF2:  16%|███                | 1638/10001 [50:48<4:33:31,  1.96s/batch]Batch 1700/10001 Done, mean position loss: 21.011354956626892\n",
      "Training NF2:  17%|███▏               | 1688/10001 [50:48<3:42:51,  1.61s/batch]Batch 1700/10001 Done, mean position loss: 20.426914508342744\n",
      "Training NF2:  17%|███▏               | 1676/10001 [50:52<4:42:36,  2.04s/batch]Batch 1700/10001 Done, mean position loss: 21.133835730552672\n",
      "Training NF2:  17%|███▏               | 1658/10001 [50:56<4:40:37,  2.02s/batch]Batch 1700/10001 Done, mean position loss: 20.693547737598422\n",
      "Training NF2:  17%|███▏               | 1706/10001 [50:57<4:11:13,  1.82s/batch]Batch 1700/10001 Done, mean position loss: 20.96091829776764\n",
      "Training NF2:  17%|███▏               | 1677/10001 [50:59<4:18:56,  1.87s/batch]Batch 1700/10001 Done, mean position loss: 20.67284248113632\n",
      "Training NF2:  17%|███▏               | 1667/10001 [51:04<3:45:43,  1.63s/batch]Batch 1700/10001 Done, mean position loss: 20.80547863006592\n",
      "Training NF2:  17%|███▏               | 1709/10001 [51:13<4:00:43,  1.74s/batch]Batch 1700/10001 Done, mean position loss: 20.63239163637161\n",
      "Training NF2:  17%|███▎               | 1727/10001 [51:15<4:48:17,  2.09s/batch]Batch 1700/10001 Done, mean position loss: 20.78216346502304\n",
      "Training NF2:  17%|███▏               | 1652/10001 [51:15<4:39:27,  2.01s/batch]Batch 1700/10001 Done, mean position loss: 21.209233675003052\n",
      "Training NF2:  17%|███▏               | 1686/10001 [51:19<4:42:54,  2.04s/batch]Batch 1700/10001 Done, mean position loss: 20.788829298019408\n",
      "Training NF2:  17%|███▏               | 1671/10001 [51:20<4:08:02,  1.79s/batch]Batch 1700/10001 Done, mean position loss: 21.68239214897156\n",
      "Training NF2:  17%|███▏               | 1685/10001 [51:19<3:56:16,  1.70s/batch]Batch 1700/10001 Done, mean position loss: 20.788807258605956\n",
      "Training NF2:  17%|███▏               | 1687/10001 [51:24<4:00:36,  1.74s/batch]Batch 1700/10001 Done, mean position loss: 21.197854137420656\n",
      "Training NF2:  17%|███▏               | 1704/10001 [51:24<4:00:10,  1.74s/batch]Batch 1700/10001 Done, mean position loss: 20.748597087860105\n",
      "Training NF2:  17%|███▏               | 1709/10001 [51:26<3:32:48,  1.54s/batch]Batch 1700/10001 Done, mean position loss: 20.803766968250276\n",
      "Training NF2:  17%|███▏               | 1709/10001 [51:34<4:35:56,  2.00s/batch]Batch 1700/10001 Done, mean position loss: 21.218161478042603\n",
      "Training NF2:  17%|███▎               | 1711/10001 [51:37<3:50:34,  1.67s/batch]Batch 1700/10001 Done, mean position loss: 21.23343281030655\n",
      "Training NF2:  17%|███▏               | 1677/10001 [51:40<4:00:52,  1.74s/batch]Batch 1700/10001 Done, mean position loss: 20.61979487657547\n",
      "Training NF2:  17%|███▎               | 1725/10001 [51:40<4:42:22,  2.05s/batch]Batch 1700/10001 Done, mean position loss: 20.611491367816924\n",
      "Training NF2:  17%|███▏               | 1686/10001 [51:41<4:36:47,  2.00s/batch]Batch 1700/10001 Done, mean position loss: 20.769833195209504\n",
      "Training NF2:  17%|███▎               | 1711/10001 [51:43<4:09:52,  1.81s/batch]Batch 1700/10001 Done, mean position loss: 21.1131943488121\n",
      "Training NF2:  17%|███▏               | 1699/10001 [51:45<3:51:46,  1.68s/batch]Batch 1700/10001 Done, mean position loss: 20.58492528915405\n",
      "Training NF2:  17%|███▏               | 1704/10001 [51:46<4:29:01,  1.95s/batch]Batch 1700/10001 Done, mean position loss: 20.831714522838592\n",
      "Training NF2:  17%|███▏               | 1699/10001 [51:46<4:06:16,  1.78s/batch]Batch 1700/10001 Done, mean position loss: 20.810254330635072\n",
      "Training NF2:  17%|███▏               | 1705/10001 [51:47<4:09:37,  1.81s/batch]Batch 1700/10001 Done, mean position loss: 20.892674334049225\n",
      "Training NF2:  17%|███▎               | 1739/10001 [51:48<4:05:56,  1.79s/batch]Batch 1700/10001 Done, mean position loss: 21.08281378507614\n",
      "Training NF2:  17%|███▏               | 1688/10001 [51:48<3:51:31,  1.67s/batch]Batch 1700/10001 Done, mean position loss: 20.86857491016388\n",
      "Training NF2:  17%|███▏               | 1707/10001 [51:50<3:56:11,  1.71s/batch]Batch 1700/10001 Done, mean position loss: 20.990626854896547\n",
      "Training NF2:  17%|███▎               | 1728/10001 [51:55<3:09:14,  1.37s/batch]Batch 1700/10001 Done, mean position loss: 20.911334078311917\n",
      "Training NF2:  17%|███▏               | 1708/10001 [51:56<3:13:31,  1.40s/batch]Batch 1700/10001 Done, mean position loss: 21.246418812274932\n",
      "Training NF2:  17%|███▏               | 1706/10001 [52:06<4:55:26,  2.14s/batch]Batch 1700/10001 Done, mean position loss: 20.556384406089784\n",
      "Training NF2:  18%|███▎               | 1753/10001 [52:09<3:51:20,  1.68s/batch]Batch 1700/10001 Done, mean position loss: 20.322462420463562\n",
      "Training NF2:  17%|███▎               | 1748/10001 [52:10<4:12:52,  1.84s/batch]Batch 1700/10001 Done, mean position loss: 20.846638417243955\n",
      "Training NF2:  17%|███▏               | 1708/10001 [52:19<3:17:58,  1.43s/batch]Batch 1700/10001 Done, mean position loss: 20.50384102344513\n",
      "Training NF2:  17%|███▎               | 1737/10001 [52:26<4:01:40,  1.75s/batch]Batch 1700/10001 Done, mean position loss: 20.97331181526184\n",
      "Training NF2:  18%|███▎               | 1766/10001 [52:46<3:52:06,  1.69s/batch]Batch 1700/10001 Done, mean position loss: 20.722637779712677\n",
      "Training NF2:  18%|███▎               | 1754/10001 [53:35<4:44:09,  2.07s/batch]Batch 1800/10001 Done, mean position loss: 20.587280280590058\n",
      "Training NF2:  18%|███▎               | 1762/10001 [53:40<3:54:03,  1.70s/batch]Batch 1800/10001 Done, mean position loss: 20.80324102163315\n",
      "Training NF2:  18%|███▎               | 1770/10001 [53:44<3:54:17,  1.71s/batch]Batch 1800/10001 Done, mean position loss: 21.086922411918643\n",
      "Training NF2:  18%|███▍               | 1789/10001 [53:46<3:34:55,  1.57s/batch]Batch 1800/10001 Done, mean position loss: 21.16549743413925\n",
      "Training NF2:  17%|███▎               | 1740/10001 [53:50<4:18:07,  1.87s/batch]Batch 1800/10001 Done, mean position loss: 21.051393637657164\n",
      "Training NF2:  18%|███▎               | 1775/10001 [53:53<3:12:22,  1.40s/batch]Batch 1800/10001 Done, mean position loss: 20.409071485996247\n",
      "Training NF2:  18%|███▎               | 1774/10001 [53:55<4:00:42,  1.76s/batch]Batch 1800/10001 Done, mean position loss: 20.953994081020355\n",
      "Training NF2:  18%|███▍               | 1792/10001 [53:59<3:18:40,  1.45s/batch]Batch 1800/10001 Done, mean position loss: 20.759610829353335\n",
      "Training NF2:  18%|███▍               | 1804/10001 [54:01<4:45:07,  2.09s/batch]Batch 1800/10001 Done, mean position loss: 20.664959905147555\n",
      "Training NF2:  18%|███▍               | 1790/10001 [54:02<3:25:49,  1.50s/batch]Batch 1800/10001 Done, mean position loss: 20.70294322490692\n",
      "Training NF2:  18%|███▍               | 1813/10001 [54:10<3:38:07,  1.60s/batch]Batch 1800/10001 Done, mean position loss: 20.78930601835251\n",
      "Training NF2:  18%|███▍               | 1788/10001 [54:16<4:41:42,  2.06s/batch]Batch 1800/10001 Done, mean position loss: 21.21127391576767\n",
      "Training NF2:  18%|███▍               | 1809/10001 [54:18<4:31:53,  1.99s/batch]Batch 1800/10001 Done, mean position loss: 20.763055639266966\n",
      "Training NF2:  18%|███▍               | 1803/10001 [54:22<4:20:09,  1.90s/batch]Batch 1800/10001 Done, mean position loss: 20.766557610034944\n",
      "Training NF2:  18%|███▍               | 1790/10001 [54:22<4:56:01,  2.16s/batch]Batch 1800/10001 Done, mean position loss: 20.65077602624893\n",
      "Training NF2:  18%|███▍               | 1819/10001 [54:23<4:49:13,  2.12s/batch]Batch 1800/10001 Done, mean position loss: 21.672833354473113\n",
      "Training NF2:  18%|███▍               | 1823/10001 [54:24<4:48:49,  2.12s/batch]Batch 1800/10001 Done, mean position loss: 20.710484833717345\n",
      "Training NF2:  18%|███▍               | 1790/10001 [54:26<4:26:18,  1.95s/batch]Batch 1800/10001 Done, mean position loss: 20.786540977954864\n",
      "Training NF2:  18%|███▍               | 1826/10001 [54:30<4:42:37,  2.07s/batch]Batch 1800/10001 Done, mean position loss: 21.150522124767303\n",
      "Training NF2:  18%|███▍               | 1798/10001 [54:37<4:13:00,  1.85s/batch]Batch 1800/10001 Done, mean position loss: 21.05077633857727\n",
      "Training NF2:  18%|███▍               | 1798/10001 [54:38<5:02:30,  2.21s/batch]Batch 1800/10001 Done, mean position loss: 20.74136570215225\n",
      "Training NF2:  18%|███▍               | 1825/10001 [54:38<3:55:09,  1.73s/batch]Batch 1800/10001 Done, mean position loss: 20.82476683139801\n",
      "Training NF2:  18%|███▍               | 1778/10001 [54:41<3:52:02,  1.69s/batch]Batch 1800/10001 Done, mean position loss: 20.601710872650145\n",
      "Training NF2:  18%|███▍               | 1825/10001 [54:42<3:51:58,  1.70s/batch]Batch 1800/10001 Done, mean position loss: 20.795314378738404\n",
      "Training NF2:  18%|███▍               | 1811/10001 [54:42<3:47:03,  1.66s/batch]Batch 1800/10001 Done, mean position loss: 21.182062101364135\n",
      "Training NF2:  18%|███▍               | 1813/10001 [54:43<3:38:52,  1.60s/batch]Batch 1800/10001 Done, mean position loss: 20.58011173009872\n",
      "Training NF2:  18%|███▍               | 1826/10001 [54:43<3:42:38,  1.63s/batch]Batch 1800/10001 Done, mean position loss: 20.78433249235153\n",
      "Training NF2:  18%|███▍               | 1787/10001 [54:45<3:57:50,  1.74s/batch]Batch 1800/10001 Done, mean position loss: 20.87894543170929\n",
      "Training NF2:  18%|███▍               | 1829/10001 [54:46<4:09:18,  1.83s/batch]Batch 1800/10001 Done, mean position loss: 21.121870260238648\n",
      "Training NF2:  18%|███▍               | 1817/10001 [54:47<3:55:44,  1.73s/batch]Batch 1800/10001 Done, mean position loss: 20.622383525371554\n",
      "Training NF2:  18%|███▍               | 1816/10001 [54:53<4:22:41,  1.93s/batch]Batch 1800/10001 Done, mean position loss: 21.075319290161133\n",
      "Training NF2:  18%|███▍               | 1808/10001 [54:57<3:50:43,  1.69s/batch]Batch 1800/10001 Done, mean position loss: 20.978164014816283\n",
      "Training NF2:  18%|███▍               | 1820/10001 [55:04<3:31:51,  1.55s/batch]Batch 1800/10001 Done, mean position loss: 21.23887465953827\n",
      "Training NF2:  18%|███▍               | 1815/10001 [55:06<3:47:40,  1.67s/batch]Batch 1800/10001 Done, mean position loss: 20.818668100833893\n",
      "Training NF2:  18%|███▍               | 1840/10001 [55:07<4:05:04,  1.80s/batch]Batch 1800/10001 Done, mean position loss: 20.545796670913695\n",
      "Training NF2:  18%|███▌               | 1845/10001 [55:09<4:02:56,  1.79s/batch]Batch 1800/10001 Done, mean position loss: 20.315269579887392\n",
      "Training NF2:  18%|███▍               | 1821/10001 [55:12<4:30:00,  1.98s/batch]Batch 1800/10001 Done, mean position loss: 20.86981494188309\n",
      "Training NF2:  18%|███▍               | 1824/10001 [55:22<3:58:33,  1.75s/batch]Batch 1800/10001 Done, mean position loss: 20.4794282746315\n",
      "Training NF2:  19%|███▌               | 1866/10001 [55:32<3:01:56,  1.34s/batch]Batch 1800/10001 Done, mean position loss: 20.96451945066452\n",
      "Training NF2:  18%|███▍               | 1839/10001 [55:40<4:33:06,  2.01s/batch]Batch 1800/10001 Done, mean position loss: 20.713951334953308\n",
      "Training NF2:  19%|███▌               | 1868/10001 [56:31<3:21:46,  1.49s/batch]Batch 1900/10001 Done, mean position loss: 20.551784694194794\n",
      "Training NF2:  19%|███▌               | 1867/10001 [56:35<3:11:47,  1.41s/batch]Batch 1900/10001 Done, mean position loss: 20.78621057510376\n",
      "Training NF2:  19%|███▌               | 1881/10001 [56:44<4:01:11,  1.78s/batch]Batch 1900/10001 Done, mean position loss: 21.107502665519714\n",
      "Training NF2:  19%|███▌               | 1857/10001 [56:46<4:02:24,  1.79s/batch]Batch 1900/10001 Done, mean position loss: 21.074942426681517\n",
      "Training NF2:  19%|███▌               | 1892/10001 [56:47<3:56:50,  1.75s/batch]Batch 1900/10001 Done, mean position loss: 21.14230775117874\n",
      "Training NF2:  19%|███▌               | 1875/10001 [56:50<4:23:35,  1.95s/batch]Batch 1900/10001 Done, mean position loss: 20.39028846502304\n",
      "Training NF2:  19%|███▌               | 1872/10001 [56:50<4:07:13,  1.82s/batch]Batch 1900/10001 Done, mean position loss: 20.919767224788668\n",
      "Training NF2:  19%|███▌               | 1879/10001 [56:58<4:02:25,  1.79s/batch]Batch 1900/10001 Done, mean position loss: 20.69638613462448\n",
      "Training NF2:  19%|███▌               | 1861/10001 [57:00<4:37:23,  2.04s/batch]Batch 1900/10001 Done, mean position loss: 20.663407204151156\n",
      "Training NF2:  19%|███▌               | 1891/10001 [57:00<5:04:25,  2.25s/batch]Batch 1900/10001 Done, mean position loss: 20.739200427532197\n",
      "Training NF2:  19%|███▋               | 1916/10001 [57:08<4:44:44,  2.11s/batch]Batch 1900/10001 Done, mean position loss: 20.769284737110137\n",
      "Training NF2:  19%|███▌               | 1900/10001 [57:15<3:24:28,  1.51s/batch]Batch 1900/10001 Done, mean position loss: 21.214174394607543\n",
      "Training NF2:  19%|███▌               | 1902/10001 [57:16<3:08:06,  1.39s/batch]Batch 1900/10001 Done, mean position loss: 20.61940465927124\n",
      "Training NF2:  19%|███▌               | 1894/10001 [57:16<4:12:33,  1.87s/batch]Batch 1900/10001 Done, mean position loss: 20.756502511501314\n",
      "Training NF2:  19%|███▌               | 1901/10001 [57:16<3:37:51,  1.61s/batch]Batch 1900/10001 Done, mean position loss: 20.783380982875826\n",
      "Training NF2:  19%|███▌               | 1882/10001 [57:17<3:57:06,  1.75s/batch]Batch 1900/10001 Done, mean position loss: 20.74756332874298\n",
      "Training NF2:  19%|███▌               | 1895/10001 [57:18<4:18:55,  1.92s/batch]Batch 1900/10001 Done, mean position loss: 21.638695130348204\n",
      "Training NF2:  19%|███▌               | 1894/10001 [57:27<4:07:39,  1.83s/batch]Batch 1900/10001 Done, mean position loss: 20.688295853137973\n",
      "Training NF2:  19%|███▌               | 1900/10001 [57:28<4:24:08,  1.96s/batch]Batch 1900/10001 Done, mean position loss: 21.09395143508911\n",
      "Batch 1900/10001 Done, mean position loss: 20.596230187416076\n",
      "Training NF2:  19%|███▌               | 1881/10001 [57:29<4:00:53,  1.78s/batch]Batch 1900/10001 Done, mean position loss: 20.730942051410675\n",
      "Training NF2:  19%|███▌               | 1905/10001 [57:34<3:32:49,  1.58s/batch]Batch 1900/10001 Done, mean position loss: 21.070527708530427\n",
      "Training NF2:  19%|███▋               | 1933/10001 [57:34<4:44:57,  2.12s/batch]Batch 1900/10001 Done, mean position loss: 20.792256293296813\n",
      "Training NF2:  19%|███▌               | 1906/10001 [57:35<3:33:38,  1.58s/batch]Batch 1900/10001 Done, mean position loss: 20.817332923412323\n",
      "Training NF2:  19%|███▌               | 1901/10001 [57:35<4:13:13,  1.88s/batch]Batch 1900/10001 Done, mean position loss: 20.80676183462143\n",
      "Training NF2:  19%|███▌               | 1895/10001 [57:39<4:00:16,  1.78s/batch]Batch 1900/10001 Done, mean position loss: 21.10785007953644\n",
      "Training NF2:  19%|███▋               | 1929/10001 [57:40<3:51:07,  1.72s/batch]Batch 1900/10001 Done, mean position loss: 20.547194764614105\n",
      "Training NF2:  19%|███▌               | 1899/10001 [57:41<4:49:27,  2.14s/batch]Batch 1900/10001 Done, mean position loss: 20.86935687541962\n",
      "Training NF2:  19%|███▌               | 1899/10001 [57:44<3:28:46,  1.55s/batch]Batch 1900/10001 Done, mean position loss: 20.608103585243228\n",
      "Training NF2:  19%|███▌               | 1908/10001 [57:47<3:53:28,  1.73s/batch]Batch 1900/10001 Done, mean position loss: 21.188950428962706\n",
      "Training NF2:  19%|███▌               | 1908/10001 [57:50<3:44:38,  1.67s/batch]Batch 1900/10001 Done, mean position loss: 20.94188643693924\n",
      "Training NF2:  19%|███▌               | 1890/10001 [57:51<4:11:33,  1.86s/batch]Batch 1900/10001 Done, mean position loss: 21.05047508955002\n",
      "Training NF2:  19%|███▋               | 1949/10001 [58:01<4:23:15,  1.96s/batch]Batch 1900/10001 Done, mean position loss: 21.214092671871185\n",
      "Training NF2:  19%|███▌               | 1892/10001 [58:02<4:22:39,  1.94s/batch]Batch 1900/10001 Done, mean position loss: 20.528443949222563\n",
      "Training NF2:  19%|███▋               | 1921/10001 [58:02<4:11:09,  1.87s/batch]Batch 1900/10001 Done, mean position loss: 20.304016432762147\n",
      "Training NF2:  19%|███▋               | 1933/10001 [58:07<3:47:09,  1.69s/batch]Batch 1900/10001 Done, mean position loss: 20.795957138538363\n",
      "Training NF2:  20%|███▋               | 1951/10001 [58:13<4:35:43,  2.06s/batch]Batch 1900/10001 Done, mean position loss: 20.83131863594055\n",
      "Training NF2:  19%|███▋               | 1936/10001 [58:20<3:21:58,  1.50s/batch]Batch 1900/10001 Done, mean position loss: 20.469781002998353\n",
      "Training NF2:  19%|███▋               | 1912/10001 [58:25<4:10:52,  1.86s/batch]Batch 1900/10001 Done, mean position loss: 20.952663626670837\n",
      "Training NF2:  19%|███▋               | 1921/10001 [58:37<4:04:02,  1.81s/batch]Batch 1900/10001 Done, mean position loss: 20.693002331256864\n",
      "Training NF2:  19%|███▋               | 1945/10001 [59:30<4:07:53,  1.85s/batch]Batch 2000/10001 Done, mean position loss: 20.791315286159517\n",
      "Training NF2:  20%|███▋               | 1967/10001 [59:31<3:33:10,  1.59s/batch]Batch 2000/10001 Done, mean position loss: 20.523077259063722\n",
      "Training NF2:  20%|███▋               | 1956/10001 [59:39<3:56:55,  1.77s/batch]Batch 2000/10001 Done, mean position loss: 21.155322148799897\n",
      "Training NF2:  20%|███▋               | 1957/10001 [59:41<4:23:12,  1.96s/batch]Batch 2000/10001 Done, mean position loss: 21.08201954603195\n",
      "Training NF2:  20%|███▊               | 1995/10001 [59:46<3:52:22,  1.74s/batch]Batch 2000/10001 Done, mean position loss: 20.91218337059021\n",
      "Training NF2:  20%|███▋               | 1964/10001 [59:48<4:45:28,  2.13s/batch]Batch 2000/10001 Done, mean position loss: 20.39849178314209\n",
      "Training NF2:  20%|███▋               | 1962/10001 [59:49<4:04:36,  1.83s/batch]Batch 2000/10001 Done, mean position loss: 21.075310561656952\n",
      "Training NF2:  20%|███▊               | 1998/10001 [59:52<4:07:40,  1.86s/batch]Batch 2000/10001 Done, mean position loss: 20.71112596035004\n",
      "Training NF2:  20%|███▊               | 1978/10001 [59:56<4:26:36,  1.99s/batch]Batch 2000/10001 Done, mean position loss: 20.65259680747986\n",
      "Training NF2:  20%|███▊               | 2016/10001 [59:58<4:01:03,  1.81s/batch]Batch 2000/10001 Done, mean position loss: 20.69748207807541\n",
      "Training NF2:  20%|███▍             | 1997/10001 [1:00:00<3:37:28,  1.63s/batch]Batch 2000/10001 Done, mean position loss: 20.756855294704437\n",
      "Training NF2:  20%|███▎             | 1960/10001 [1:00:02<4:12:35,  1.88s/batch]Batch 2000/10001 Done, mean position loss: 21.138558540344242\n",
      "Training NF2:  20%|███▍             | 1997/10001 [1:00:07<4:06:38,  1.85s/batch]Batch 2000/10001 Done, mean position loss: 20.626115777492522\n",
      "Training NF2:  20%|███▍             | 1989/10001 [1:00:10<3:23:44,  1.53s/batch]Batch 2000/10001 Done, mean position loss: 21.630534126758576\n",
      "Training NF2:  20%|███▍             | 1991/10001 [1:00:13<4:02:21,  1.82s/batch]Batch 2000/10001 Done, mean position loss: 20.717252230644227\n",
      "Training NF2:  20%|███▍             | 2010/10001 [1:00:14<4:34:41,  2.06s/batch]Batch 2000/10001 Done, mean position loss: 20.784851179122924\n",
      "Training NF2:  20%|███▍             | 1998/10001 [1:00:14<3:37:01,  1.63s/batch]Batch 2000/10001 Done, mean position loss: 20.740373272895813\n",
      "Training NF2:  20%|███▎             | 1977/10001 [1:00:15<3:33:29,  1.60s/batch]Batch 2000/10001 Done, mean position loss: 20.569942524433138\n",
      "Training NF2:  20%|███▍             | 2011/10001 [1:00:19<4:15:07,  1.92s/batch]Batch 2000/10001 Done, mean position loss: 20.72299144268036\n",
      "Training NF2:  20%|███▍             | 2011/10001 [1:00:27<4:49:21,  2.17s/batch]Batch 2000/10001 Done, mean position loss: 21.08613545894623\n",
      "Training NF2:  20%|███▍             | 2020/10001 [1:00:31<3:50:38,  1.73s/batch]Batch 2000/10001 Done, mean position loss: 20.802508034706115\n",
      "Training NF2:  20%|███▍             | 2030/10001 [1:00:32<3:49:49,  1.73s/batch]Batch 2000/10001 Done, mean position loss: 21.051985590457917\n",
      "Training NF2:  20%|███▍             | 1995/10001 [1:00:34<3:17:16,  1.48s/batch]Batch 2000/10001 Done, mean position loss: 20.661395049095155\n",
      "Training NF2:  20%|███▍             | 2022/10001 [1:00:38<4:36:55,  2.08s/batch]Batch 2000/10001 Done, mean position loss: 20.78295803308487\n",
      "Training NF2:  20%|███▎             | 1983/10001 [1:00:39<4:21:30,  1.96s/batch]Batch 2000/10001 Done, mean position loss: 20.596542861461636\n",
      "Training NF2:  20%|███▍             | 2005/10001 [1:00:39<4:05:42,  1.84s/batch]Batch 2000/10001 Done, mean position loss: 21.06118928670883\n",
      "Training NF2:  20%|███▍             | 2020/10001 [1:00:42<3:22:44,  1.52s/batch]Batch 2000/10001 Done, mean position loss: 20.856356935501097\n",
      "Training NF2:  20%|███▍             | 2033/10001 [1:00:44<4:21:49,  1.97s/batch]Batch 2000/10001 Done, mean position loss: 20.51784411907196\n",
      "Training NF2:  20%|███▍             | 2016/10001 [1:00:44<3:46:22,  1.70s/batch]Batch 2000/10001 Done, mean position loss: 20.830451993942262\n",
      "Training NF2:  20%|███▎             | 1975/10001 [1:00:46<4:35:31,  2.06s/batch]Batch 2000/10001 Done, mean position loss: 21.1560329413414\n",
      "Training NF2:  20%|███▍             | 2039/10001 [1:00:54<4:02:20,  1.83s/batch]Batch 2000/10001 Done, mean position loss: 21.06263235569\n",
      "Training NF2:  20%|███▍             | 2025/10001 [1:00:56<3:57:15,  1.78s/batch]Batch 2000/10001 Done, mean position loss: 20.559353559017183\n",
      "Training NF2:  20%|███▍             | 2036/10001 [1:01:00<4:20:53,  1.97s/batch]Batch 2000/10001 Done, mean position loss: 20.93496650218964\n",
      "Training NF2:  20%|███▍             | 2043/10001 [1:01:00<3:53:58,  1.76s/batch]Batch 2000/10001 Done, mean position loss: 20.328425097465516\n",
      "Training NF2:  20%|███▍             | 2045/10001 [1:01:02<3:25:16,  1.55s/batch]Batch 2000/10001 Done, mean position loss: 21.176433820724487\n",
      "Training NF2:  20%|███▍             | 2034/10001 [1:01:15<4:08:35,  1.87s/batch]Batch 2000/10001 Done, mean position loss: 20.77060764312744\n",
      "Training NF2:  20%|███▍             | 2047/10001 [1:01:21<3:29:18,  1.58s/batch]Batch 2000/10001 Done, mean position loss: 20.814170026779177\n",
      "Training NF2:  21%|███▍             | 2058/10001 [1:01:22<4:27:42,  2.02s/batch]Batch 2000/10001 Done, mean position loss: 20.46623287677765\n",
      "Training NF2:  20%|███▍             | 2035/10001 [1:01:36<4:01:14,  1.82s/batch]Batch 2000/10001 Done, mean position loss: 20.939784982204436\n",
      "Training NF2:  20%|███▍             | 2024/10001 [1:01:40<4:17:10,  1.93s/batch]Batch 2000/10001 Done, mean position loss: 20.683333649635316\n",
      "Training NF2:  20%|███▍             | 2044/10001 [1:02:30<3:31:25,  1.59s/batch]Batch 2100/10001 Done, mean position loss: 20.789486036300662\n",
      "Training NF2:  21%|███▌             | 2085/10001 [1:02:33<3:06:33,  1.41s/batch]Batch 2100/10001 Done, mean position loss: 20.509758501052854\n",
      "Training NF2:  21%|███▌             | 2079/10001 [1:02:42<4:38:20,  2.11s/batch]Batch 2100/10001 Done, mean position loss: 21.09654129266739\n",
      "Training NF2:  21%|███▌             | 2072/10001 [1:02:43<4:14:35,  1.93s/batch]Batch 2100/10001 Done, mean position loss: 20.394752922058107\n",
      "Training NF2:  21%|███▌             | 2093/10001 [1:02:47<4:07:29,  1.88s/batch]Batch 2100/10001 Done, mean position loss: 21.109671652317047\n",
      "Training NF2:  21%|███▌             | 2063/10001 [1:02:47<4:10:53,  1.90s/batch]Batch 2100/10001 Done, mean position loss: 20.885709402561186\n",
      "Training NF2:  21%|███▌             | 2077/10001 [1:02:54<4:35:32,  2.09s/batch]Batch 2100/10001 Done, mean position loss: 21.10023984193802\n",
      "Training NF2:  21%|███▌             | 2100/10001 [1:02:56<3:32:57,  1.62s/batch]Batch 2100/10001 Done, mean position loss: 20.723810474872586\n",
      "Training NF2:  21%|███▌             | 2114/10001 [1:02:58<4:36:00,  2.10s/batch]Batch 2100/10001 Done, mean position loss: 20.6933332157135\n",
      "Training NF2:  21%|███▌             | 2105/10001 [1:03:02<3:54:53,  1.78s/batch]Batch 2100/10001 Done, mean position loss: 21.189229054450987\n",
      "Training NF2:  21%|███▌             | 2091/10001 [1:03:04<4:13:19,  1.92s/batch]Batch 2100/10001 Done, mean position loss: 20.643345017433166\n",
      "Training NF2:  21%|███▌             | 2075/10001 [1:03:04<4:50:39,  2.20s/batch]Batch 2100/10001 Done, mean position loss: 20.764175999164582\n",
      "Training NF2:  20%|███▍             | 2050/10001 [1:03:14<5:00:38,  2.27s/batch]Batch 2100/10001 Done, mean position loss: 20.626564135551455\n",
      "Training NF2:  21%|███▌             | 2081/10001 [1:03:18<3:47:16,  1.72s/batch]Batch 2100/10001 Done, mean position loss: 20.779927918910982\n",
      "Training NF2:  21%|███▍             | 2054/10001 [1:03:22<4:41:56,  2.13s/batch]Batch 2100/10001 Done, mean position loss: 21.614100527763366\n",
      "Training NF2:  21%|███▌             | 2111/10001 [1:03:23<4:16:00,  1.95s/batch]Batch 2100/10001 Done, mean position loss: 20.56085598707199\n",
      "Training NF2:  21%|███▌             | 2082/10001 [1:03:25<4:48:20,  2.18s/batch]Batch 2100/10001 Done, mean position loss: 20.708355801105498\n",
      "Training NF2:  21%|███▌             | 2066/10001 [1:03:25<4:45:26,  2.16s/batch]Batch 2100/10001 Done, mean position loss: 20.710015563964845\n",
      "Training NF2:  21%|███▌             | 2127/10001 [1:03:25<5:44:13,  2.62s/batch]Batch 2100/10001 Done, mean position loss: 20.72790387392044\n",
      "Training NF2:  21%|███▌             | 2123/10001 [1:03:38<4:31:36,  2.07s/batch]Batch 2100/10001 Done, mean position loss: 21.07885040044785\n",
      "Training NF2:  21%|███▌             | 2080/10001 [1:03:38<3:33:02,  1.61s/batch]Batch 2100/10001 Done, mean position loss: 20.770267627239228\n",
      "Training NF2:  21%|███▌             | 2109/10001 [1:03:38<3:41:40,  1.69s/batch]Batch 2100/10001 Done, mean position loss: 20.780272517204285\n",
      "Training NF2:  21%|███▋             | 2135/10001 [1:03:42<4:13:35,  1.93s/batch]Batch 2100/10001 Done, mean position loss: 21.028970806598664\n",
      "Training NF2:  21%|███▌             | 2111/10001 [1:03:43<4:26:44,  2.03s/batch]Batch 2100/10001 Done, mean position loss: 20.658958315849304\n",
      "Training NF2:  21%|███▌             | 2132/10001 [1:03:44<3:51:50,  1.77s/batch]Batch 2100/10001 Done, mean position loss: 20.589556019306183\n",
      "Training NF2:  21%|███▌             | 2087/10001 [1:03:45<4:05:09,  1.86s/batch]Batch 2100/10001 Done, mean position loss: 21.02404263496399\n",
      "Training NF2:  21%|███▌             | 2096/10001 [1:03:52<4:31:30,  2.06s/batch]Batch 2100/10001 Done, mean position loss: 20.837434051036837\n",
      "Training NF2:  21%|███▌             | 2128/10001 [1:03:54<3:30:07,  1.60s/batch]Batch 2100/10001 Done, mean position loss: 20.794771020412448\n",
      "Training NF2:  21%|███▌             | 2108/10001 [1:03:55<3:47:12,  1.73s/batch]Batch 2100/10001 Done, mean position loss: 21.14082657337189\n",
      "Training NF2:  21%|███▋             | 2136/10001 [1:03:57<5:09:56,  2.36s/batch]Batch 2100/10001 Done, mean position loss: 20.556526079177857\n",
      "Training NF2:  21%|███▌             | 2121/10001 [1:03:58<3:48:01,  1.74s/batch]Batch 2100/10001 Done, mean position loss: 21.02618993520737\n",
      "Training NF2:  21%|███▌             | 2125/10001 [1:04:02<4:38:42,  2.12s/batch]Batch 2100/10001 Done, mean position loss: 20.508221719264984\n",
      "Training NF2:  21%|███▌             | 2106/10001 [1:04:02<4:07:00,  1.88s/batch]Batch 2100/10001 Done, mean position loss: 20.33194572687149\n",
      "Training NF2:  21%|███▌             | 2076/10001 [1:04:04<4:43:29,  2.15s/batch]Batch 2100/10001 Done, mean position loss: 20.915923137664794\n",
      "Training NF2:  21%|███▌             | 2122/10001 [1:04:16<3:28:11,  1.59s/batch]Batch 2100/10001 Done, mean position loss: 21.154072134494783\n",
      "Training NF2:  21%|███▋             | 2147/10001 [1:04:19<4:18:19,  1.97s/batch]Batch 2100/10001 Done, mean position loss: 20.780378255844116\n",
      "Training NF2:  21%|███▋             | 2149/10001 [1:04:30<4:09:49,  1.91s/batch]Batch 2100/10001 Done, mean position loss: 20.45319006681442\n",
      "Training NF2:  22%|███▋             | 2158/10001 [1:04:30<3:26:42,  1.58s/batch]Batch 2100/10001 Done, mean position loss: 20.795732574462892\n",
      "Training NF2:  21%|███▌             | 2126/10001 [1:04:42<4:46:45,  2.18s/batch]Batch 2100/10001 Done, mean position loss: 20.91320858716965\n",
      "Training NF2:  22%|███▋             | 2164/10001 [1:04:50<3:55:28,  1.80s/batch]Batch 2100/10001 Done, mean position loss: 20.683871629238126\n",
      "Training NF2:  21%|███▋             | 2135/10001 [1:05:43<2:49:19,  1.29s/batch]Batch 2200/10001 Done, mean position loss: 21.103032619953154\n",
      "Training NF2:  22%|███▋             | 2180/10001 [1:05:43<3:07:44,  1.44s/batch]Batch 2200/10001 Done, mean position loss: 20.750378885269164\n",
      "Training NF2:  22%|███▋             | 2189/10001 [1:05:44<3:14:40,  1.50s/batch]Batch 2200/10001 Done, mean position loss: 20.512471611499787\n",
      "Training NF2:  21%|███▋             | 2144/10001 [1:05:50<4:09:20,  1.90s/batch]Batch 2200/10001 Done, mean position loss: 20.382365896701813\n",
      "Training NF2:  21%|███▋             | 2135/10001 [1:05:55<4:47:35,  2.19s/batch]Batch 2200/10001 Done, mean position loss: 21.098065500259402\n",
      "Training NF2:  22%|███▋             | 2178/10001 [1:05:59<3:30:38,  1.62s/batch]Batch 2200/10001 Done, mean position loss: 20.873692796230316\n",
      "Training NF2:  22%|███▋             | 2206/10001 [1:06:02<3:36:43,  1.67s/batch]Batch 2200/10001 Done, mean position loss: 21.061907835006714\n",
      "Training NF2:  22%|███▋             | 2180/10001 [1:06:04<4:17:32,  1.98s/batch]Batch 2200/10001 Done, mean position loss: 20.697274656295775\n",
      "Training NF2:  22%|███▋             | 2189/10001 [1:06:05<4:24:57,  2.03s/batch]Batch 2200/10001 Done, mean position loss: 20.612435126304625\n",
      "Training NF2:  22%|███▋             | 2190/10001 [1:06:09<4:29:08,  2.07s/batch]Batch 2200/10001 Done, mean position loss: 21.15270259141922\n",
      "Training NF2:  22%|███▋             | 2202/10001 [1:06:11<3:48:58,  1.76s/batch]Batch 2200/10001 Done, mean position loss: 20.697331669330598\n",
      "Training NF2:  21%|███▋             | 2143/10001 [1:06:12<4:28:50,  2.05s/batch]Batch 2200/10001 Done, mean position loss: 20.764878194332123\n",
      "Training NF2:  22%|███▋             | 2188/10001 [1:06:20<4:12:10,  1.94s/batch]Batch 2200/10001 Done, mean position loss: 20.69578778028488\n",
      "Training NF2:  22%|███▋             | 2191/10001 [1:06:20<4:15:08,  1.96s/batch]Batch 2200/10001 Done, mean position loss: 20.77883850812912\n",
      "Training NF2:  22%|███▊             | 2209/10001 [1:06:24<3:53:35,  1.80s/batch]Batch 2200/10001 Done, mean position loss: 20.61973743915558\n",
      "Training NF2:  22%|███▋             | 2165/10001 [1:06:29<4:01:51,  1.85s/batch]Batch 2200/10001 Done, mean position loss: 20.719035465717315\n",
      "Training NF2:  22%|███▋             | 2175/10001 [1:06:32<4:33:37,  2.10s/batch]Batch 2200/10001 Done, mean position loss: 20.5520841050148\n",
      "Training NF2:  22%|███▊             | 2207/10001 [1:06:34<3:31:07,  1.63s/batch]Batch 2200/10001 Done, mean position loss: 21.612908141613005\n",
      "Training NF2:  22%|███▋             | 2185/10001 [1:06:38<4:04:30,  1.88s/batch]Batch 2200/10001 Done, mean position loss: 20.707979006767275\n",
      "Training NF2:  22%|███▋             | 2172/10001 [1:06:41<3:39:03,  1.68s/batch]Batch 2200/10001 Done, mean position loss: 20.976474409103393\n",
      "Training NF2:  22%|███▊             | 2220/10001 [1:06:43<4:05:36,  1.89s/batch]Batch 2200/10001 Done, mean position loss: 20.57776395082474\n",
      "Training NF2:  22%|███▋             | 2175/10001 [1:06:45<3:53:03,  1.79s/batch]Batch 2200/10001 Done, mean position loss: 20.770127799510956\n",
      "Training NF2:  22%|███▋             | 2202/10001 [1:06:46<3:31:20,  1.63s/batch]Batch 2200/10001 Done, mean position loss: 21.046573538780212\n",
      "Training NF2:  22%|███▋             | 2175/10001 [1:06:46<3:46:32,  1.74s/batch]Batch 2200/10001 Done, mean position loss: 20.757149381637575\n",
      "Training NF2:  22%|███▊             | 2236/10001 [1:06:47<3:39:23,  1.70s/batch]Batch 2200/10001 Done, mean position loss: 20.63408470392227\n",
      "Training NF2:  22%|███▊             | 2211/10001 [1:06:52<3:37:56,  1.68s/batch]Batch 2200/10001 Done, mean position loss: 20.81544865131378\n",
      "Training NF2:  22%|███▊             | 2207/10001 [1:06:56<3:46:52,  1.75s/batch]Batch 2200/10001 Done, mean position loss: 21.00843432188034\n",
      "Training NF2:  22%|███▊             | 2215/10001 [1:06:58<3:28:22,  1.61s/batch]Batch 2200/10001 Done, mean position loss: 21.119789996147155\n",
      "Training NF2:  22%|███▊             | 2220/10001 [1:07:00<4:59:58,  2.31s/batch]Batch 2200/10001 Done, mean position loss: 20.79383428096771\n",
      "Training NF2:  22%|███▊             | 2212/10001 [1:07:04<3:53:45,  1.80s/batch]Batch 2200/10001 Done, mean position loss: 21.044785304069517\n",
      "Training NF2:  22%|███▋             | 2202/10001 [1:07:05<3:36:55,  1.67s/batch]Batch 2200/10001 Done, mean position loss: 20.543512172698975\n",
      "Training NF2:  22%|███▋             | 2195/10001 [1:07:07<3:21:44,  1.55s/batch]Batch 2200/10001 Done, mean position loss: 20.886428298950197\n",
      "Training NF2:  22%|███▊             | 2235/10001 [1:07:10<4:16:58,  1.99s/batch]Batch 2200/10001 Done, mean position loss: 20.480313889980316\n",
      "Training NF2:  22%|███▊             | 2250/10001 [1:07:16<4:35:53,  2.14s/batch]Batch 2200/10001 Done, mean position loss: 20.335877149105073\n",
      "Training NF2:  22%|███▊             | 2210/10001 [1:07:19<4:46:56,  2.21s/batch]Batch 2200/10001 Done, mean position loss: 21.130088727474217\n",
      "Training NF2:  22%|███▊             | 2210/10001 [1:07:23<3:55:22,  1.81s/batch]Batch 2200/10001 Done, mean position loss: 20.734399709701542\n",
      "Training NF2:  22%|███▋             | 2187/10001 [1:07:33<4:21:46,  2.01s/batch]Batch 2200/10001 Done, mean position loss: 20.44633036136627\n",
      "Training NF2:  23%|███▊             | 2263/10001 [1:07:37<4:11:09,  1.95s/batch]Batch 2200/10001 Done, mean position loss: 20.791573803424836\n",
      "Training NF2:  23%|███▊             | 2269/10001 [1:07:47<3:34:20,  1.66s/batch]Batch 2200/10001 Done, mean position loss: 20.899922091960907\n",
      "Training NF2:  23%|███▊             | 2275/10001 [1:07:56<3:11:31,  1.49s/batch]Batch 2200/10001 Done, mean position loss: 20.68958510875702\n",
      "Training NF2:  23%|███▊             | 2274/10001 [1:08:44<3:40:12,  1.71s/batch]Batch 2300/10001 Done, mean position loss: 21.099873847961426\n",
      "Training NF2:  23%|███▉             | 2304/10001 [1:08:49<3:49:03,  1.79s/batch]Batch 2300/10001 Done, mean position loss: 20.744599223136902\n",
      "Training NF2:  23%|███▊             | 2273/10001 [1:08:51<4:27:25,  2.08s/batch]Batch 2300/10001 Done, mean position loss: 20.508213942050933\n",
      "Training NF2:  23%|███▉             | 2284/10001 [1:08:55<4:35:41,  2.14s/batch]Batch 2300/10001 Done, mean position loss: 21.11844531297684\n",
      "Training NF2:  23%|███▊             | 2263/10001 [1:08:57<4:24:23,  2.05s/batch]Batch 2300/10001 Done, mean position loss: 20.384939987659454\n",
      "Training NF2:  23%|███▊             | 2265/10001 [1:09:01<3:24:47,  1.59s/batch]Batch 2300/10001 Done, mean position loss: 21.042526416778564\n",
      "Training NF2:  22%|███▊             | 2249/10001 [1:09:05<4:12:56,  1.96s/batch]Batch 2300/10001 Done, mean position loss: 20.863856890201568\n",
      "Training NF2:  23%|███▉             | 2291/10001 [1:09:06<3:32:28,  1.65s/batch]Batch 2300/10001 Done, mean position loss: 20.675097069740296\n",
      "Training NF2:  23%|███▉             | 2309/10001 [1:09:10<4:09:54,  1.95s/batch]Batch 2300/10001 Done, mean position loss: 20.607361788749696\n",
      "Training NF2:  23%|███▉             | 2315/10001 [1:09:15<3:32:50,  1.66s/batch]Batch 2300/10001 Done, mean position loss: 21.161695284843447\n",
      "Training NF2:  23%|███▉             | 2283/10001 [1:09:17<3:44:13,  1.74s/batch]Batch 2300/10001 Done, mean position loss: 20.695377495288845\n",
      "Training NF2:  23%|███▉             | 2309/10001 [1:09:20<3:59:26,  1.87s/batch]Batch 2300/10001 Done, mean position loss: 20.758949859142305\n",
      "Training NF2:  23%|███▊             | 2276/10001 [1:09:22<3:29:10,  1.62s/batch]Batch 2300/10001 Done, mean position loss: 20.70543951034546\n",
      "Training NF2:  23%|███▉             | 2285/10001 [1:09:24<4:10:39,  1.95s/batch]Batch 2300/10001 Done, mean position loss: 20.738710463047028\n",
      "Training NF2:  23%|███▉             | 2306/10001 [1:09:29<4:16:19,  2.00s/batch]Batch 2300/10001 Done, mean position loss: 20.617111184597015\n",
      "Training NF2:  23%|███▉             | 2282/10001 [1:09:31<3:57:22,  1.85s/batch]Batch 2300/10001 Done, mean position loss: 21.572740190029144\n",
      "Training NF2:  23%|███▉             | 2305/10001 [1:09:37<3:59:45,  1.87s/batch]Batch 2300/10001 Done, mean position loss: 20.537288877964016\n",
      "Training NF2:  23%|███▉             | 2292/10001 [1:09:39<3:55:05,  1.83s/batch]Batch 2300/10001 Done, mean position loss: 20.77630774497986\n",
      "Training NF2:  23%|███▉             | 2313/10001 [1:09:39<3:52:01,  1.81s/batch]Batch 2300/10001 Done, mean position loss: 20.935692250728607\n",
      "Training NF2:  23%|███▉             | 2287/10001 [1:09:41<3:47:51,  1.77s/batch]Batch 2300/10001 Done, mean position loss: 20.7103226017952\n",
      "Training NF2:  23%|███▉             | 2315/10001 [1:09:44<4:24:52,  2.07s/batch]Batch 2300/10001 Done, mean position loss: 20.668478779792785\n",
      "Training NF2:  23%|███▉             | 2306/10001 [1:09:49<4:26:50,  2.08s/batch]Batch 2300/10001 Done, mean position loss: 20.741877608299255\n",
      "Training NF2:  23%|███▉             | 2297/10001 [1:09:49<3:38:58,  1.71s/batch]Batch 2300/10001 Done, mean position loss: 21.033266704082486\n",
      "Training NF2:  23%|███▉             | 2336/10001 [1:09:52<3:32:32,  1.66s/batch]Batch 2300/10001 Done, mean position loss: 20.569366772174835\n",
      "Training NF2:  23%|███▉             | 2294/10001 [1:09:55<3:38:31,  1.70s/batch]Batch 2300/10001 Done, mean position loss: 20.626447024345396\n",
      "Training NF2:  23%|███▉             | 2288/10001 [1:09:55<5:12:42,  2.43s/batch]Batch 2300/10001 Done, mean position loss: 21.138021206855775\n",
      "Training NF2:  23%|███▉             | 2296/10001 [1:09:57<3:41:26,  1.72s/batch]Batch 2300/10001 Done, mean position loss: 20.80019399404526\n",
      "Training NF2:  23%|███▉             | 2344/10001 [1:10:06<3:47:09,  1.78s/batch]Batch 2300/10001 Done, mean position loss: 21.031255252361298\n",
      "Training NF2:  23%|███▉             | 2289/10001 [1:10:06<4:05:36,  1.91s/batch]Batch 2300/10001 Done, mean position loss: 20.540388629436492\n",
      "Training NF2:  23%|███▉             | 2300/10001 [1:10:08<3:33:34,  1.66s/batch]Batch 2300/10001 Done, mean position loss: 20.48196230649948\n",
      "Training NF2:  23%|███▉             | 2341/10001 [1:10:10<4:12:05,  1.97s/batch]Batch 2300/10001 Done, mean position loss: 21.01194432973862\n",
      "Training NF2:  23%|███▉             | 2335/10001 [1:10:14<4:14:12,  1.99s/batch]Batch 2300/10001 Done, mean position loss: 20.876157324314118\n",
      "Training NF2:  23%|███▉             | 2305/10001 [1:10:14<4:13:24,  1.98s/batch]Batch 2300/10001 Done, mean position loss: 20.788193621635436\n",
      "Training NF2:  23%|███▉             | 2314/10001 [1:10:18<3:43:54,  1.75s/batch]Batch 2300/10001 Done, mean position loss: 20.332418627738953\n",
      "Training NF2:  23%|███▉             | 2305/10001 [1:10:26<4:10:14,  1.95s/batch]Batch 2300/10001 Done, mean position loss: 21.095274484157564\n",
      "Training NF2:  23%|███▉             | 2313/10001 [1:10:33<4:13:27,  1.98s/batch]Batch 2300/10001 Done, mean position loss: 20.716702153682707\n",
      "Training NF2:  23%|███▉             | 2336/10001 [1:10:36<3:57:46,  1.86s/batch]Batch 2300/10001 Done, mean position loss: 20.77398148775101\n",
      "Training NF2:  24%|████             | 2362/10001 [1:10:39<4:28:17,  2.11s/batch]Batch 2300/10001 Done, mean position loss: 20.44453774929047\n",
      "Training NF2:  24%|███▉             | 2353/10001 [1:10:53<4:02:14,  1.90s/batch]Batch 2300/10001 Done, mean position loss: 20.89388121128082\n",
      "Training NF2:  23%|███▉             | 2329/10001 [1:10:59<3:19:52,  1.56s/batch]Batch 2300/10001 Done, mean position loss: 20.676690180301666\n",
      "Training NF2:  24%|████             | 2376/10001 [1:11:44<3:41:06,  1.74s/batch]Batch 2400/10001 Done, mean position loss: 20.492613055706023\n",
      "Training NF2:  24%|████             | 2358/10001 [1:11:47<3:58:47,  1.87s/batch]Batch 2400/10001 Done, mean position loss: 20.739057743549346\n",
      "Training NF2:  24%|████             | 2365/10001 [1:11:52<4:33:45,  2.15s/batch]Batch 2400/10001 Done, mean position loss: 21.08876264810562\n",
      "Training NF2:  24%|████             | 2366/10001 [1:11:54<4:14:03,  2.00s/batch]Batch 2400/10001 Done, mean position loss: 21.044094135761263\n",
      "Training NF2:  24%|████             | 2367/10001 [1:12:03<3:05:09,  1.46s/batch]Batch 2400/10001 Done, mean position loss: 21.09986276388168\n",
      "Training NF2:  24%|████             | 2401/10001 [1:12:03<4:17:13,  2.03s/batch]Batch 2400/10001 Done, mean position loss: 20.675192565917968\n",
      "Training NF2:  24%|████             | 2393/10001 [1:12:07<3:38:42,  1.72s/batch]Batch 2400/10001 Done, mean position loss: 20.37693487405777\n",
      "Training NF2:  24%|████             | 2358/10001 [1:12:12<3:30:11,  1.65s/batch]Batch 2400/10001 Done, mean position loss: 20.858024258613586\n",
      "Training NF2:  23%|███▉             | 2344/10001 [1:12:16<3:45:47,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 20.743488123416903\n",
      "Training NF2:  24%|████             | 2380/10001 [1:12:20<4:09:31,  1.96s/batch]Batch 2400/10001 Done, mean position loss: 20.594959721565246\n",
      "Training NF2:  24%|████             | 2378/10001 [1:12:20<3:34:02,  1.68s/batch]Batch 2400/10001 Done, mean position loss: 21.150678789615633\n",
      "Training NF2:  24%|████             | 2420/10001 [1:12:22<4:28:25,  2.12s/batch]Batch 2400/10001 Done, mean position loss: 20.685877406597136\n",
      "Training NF2:  24%|████             | 2421/10001 [1:12:24<4:19:11,  2.05s/batch]Batch 2400/10001 Done, mean position loss: 20.744510567188264\n",
      "Training NF2:  24%|████             | 2388/10001 [1:12:26<3:26:09,  1.62s/batch]Batch 2400/10001 Done, mean position loss: 20.67820360660553\n",
      "Training NF2:  24%|████             | 2425/10001 [1:12:30<3:35:28,  1.71s/batch]Batch 2400/10001 Done, mean position loss: 20.606549875736235\n",
      "Training NF2:  24%|████             | 2397/10001 [1:12:35<4:23:19,  2.08s/batch]Batch 2400/10001 Done, mean position loss: 21.560904762744904\n",
      "Training NF2:  24%|████             | 2356/10001 [1:12:38<3:45:10,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 20.5185640501976\n",
      "Training NF2:  24%|████             | 2410/10001 [1:12:42<4:05:11,  1.94s/batch]Batch 2400/10001 Done, mean position loss: 20.74103158712387\n",
      "Training NF2:  24%|████             | 2422/10001 [1:12:41<3:51:16,  1.83s/batch]Batch 2400/10001 Done, mean position loss: 20.915537915229798\n",
      "Training NF2:  24%|████             | 2411/10001 [1:12:42<3:43:50,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 20.692805790901183\n",
      "Training NF2:  24%|████             | 2385/10001 [1:12:52<5:11:53,  2.46s/batch]Batch 2400/10001 Done, mean position loss: 20.623947658538818\n",
      "Training NF2:  24%|████             | 2418/10001 [1:12:53<3:55:08,  1.86s/batch]Batch 2400/10001 Done, mean position loss: 20.713589529991147\n",
      "Training NF2:  24%|████             | 2408/10001 [1:12:52<3:08:51,  1.49s/batch]Batch 2400/10001 Done, mean position loss: 20.647769515514373\n",
      "Training NF2:  24%|████▏            | 2440/10001 [1:12:58<4:35:19,  2.18s/batch]Batch 2400/10001 Done, mean position loss: 21.020548520088198\n",
      "Training NF2:  24%|████             | 2397/10001 [1:13:00<3:58:32,  1.88s/batch]Batch 2400/10001 Done, mean position loss: 20.5650492310524\n",
      "Training NF2:  24%|████             | 2388/10001 [1:13:00<4:13:45,  2.00s/batch]Batch 2400/10001 Done, mean position loss: 20.79924554824829\n",
      "Training NF2:  24%|████             | 2379/10001 [1:13:02<4:01:58,  1.90s/batch]Batch 2400/10001 Done, mean position loss: 20.47235868215561\n",
      "Training NF2:  24%|████▏            | 2429/10001 [1:13:02<4:14:48,  2.02s/batch]Batch 2400/10001 Done, mean position loss: 21.13691791296005\n",
      "Training NF2:  24%|████             | 2406/10001 [1:13:06<3:37:57,  1.72s/batch]Batch 2400/10001 Done, mean position loss: 20.998858692646028\n",
      "Training NF2:  24%|████             | 2416/10001 [1:13:07<3:34:24,  1.70s/batch]Batch 2400/10001 Done, mean position loss: 21.03266278505325\n",
      "Training NF2:  24%|████             | 2426/10001 [1:13:08<3:43:03,  1.77s/batch]Batch 2400/10001 Done, mean position loss: 20.53872165441513\n",
      "Training NF2:  24%|████             | 2413/10001 [1:13:18<3:46:38,  1.79s/batch]Batch 2400/10001 Done, mean position loss: 20.87138416290283\n",
      "Training NF2:  24%|████             | 2416/10001 [1:13:20<4:07:03,  1.95s/batch]Batch 2400/10001 Done, mean position loss: 20.772587583065032\n",
      "Training NF2:  24%|████             | 2411/10001 [1:13:23<4:00:18,  1.90s/batch]Batch 2400/10001 Done, mean position loss: 20.334443981647492\n",
      "Training NF2:  24%|████             | 2413/10001 [1:13:28<4:30:43,  2.14s/batch]Batch 2400/10001 Done, mean position loss: 21.10892202615738\n",
      "Training NF2:  24%|████▏            | 2438/10001 [1:13:30<3:07:51,  1.49s/batch]Batch 2400/10001 Done, mean position loss: 20.675471320152283\n",
      "Training NF2:  24%|████▏            | 2448/10001 [1:13:39<3:55:08,  1.87s/batch]Batch 2400/10001 Done, mean position loss: 20.76460161447525\n",
      "Training NF2:  24%|████▏            | 2435/10001 [1:13:46<4:13:45,  2.01s/batch]Batch 2400/10001 Done, mean position loss: 20.41913591146469\n",
      "Training NF2:  24%|████▏            | 2436/10001 [1:14:00<3:46:43,  1.80s/batch]Batch 2400/10001 Done, mean position loss: 20.67510385274887\n",
      "Training NF2:  24%|████▏            | 2444/10001 [1:14:01<4:05:33,  1.95s/batch]Batch 2400/10001 Done, mean position loss: 20.882058782577516\n",
      "Training NF2:  25%|████▏            | 2479/10001 [1:14:49<3:17:21,  1.57s/batch]Batch 2500/10001 Done, mean position loss: 20.733794775009155\n",
      "Training NF2:  25%|████▏            | 2466/10001 [1:14:52<3:43:04,  1.78s/batch]Batch 2500/10001 Done, mean position loss: 20.4766504073143\n",
      "Training NF2:  25%|████▏            | 2483/10001 [1:14:54<3:45:15,  1.80s/batch]Batch 2500/10001 Done, mean position loss: 21.123456699848173\n",
      "Training NF2:  25%|████▏            | 2498/10001 [1:14:58<3:16:27,  1.57s/batch]Batch 2500/10001 Done, mean position loss: 20.649591002464295\n",
      "Training NF2:  25%|████▎            | 2509/10001 [1:15:02<3:45:10,  1.80s/batch]Batch 2500/10001 Done, mean position loss: 21.056307744979858\n",
      "Training NF2:  25%|████▏            | 2475/10001 [1:15:03<3:26:05,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 21.120861668586727\n",
      "Training NF2:  25%|████▎            | 2506/10001 [1:15:05<3:00:50,  1.45s/batch]Batch 2500/10001 Done, mean position loss: 20.376632702350616\n",
      "Training NF2:  25%|████▏            | 2469/10001 [1:15:09<3:24:44,  1.63s/batch]Batch 2500/10001 Done, mean position loss: 20.72572108745575\n",
      "Training NF2:  25%|████▏            | 2479/10001 [1:15:15<3:35:50,  1.72s/batch]Batch 2500/10001 Done, mean position loss: 20.85288635969162\n",
      "Training NF2:  25%|████▎            | 2515/10001 [1:15:21<3:33:53,  1.71s/batch]Batch 2500/10001 Done, mean position loss: 20.578186378479003\n",
      "Training NF2:  25%|████▏            | 2469/10001 [1:15:23<3:26:14,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 20.76117326974869\n",
      "Training NF2:  25%|████▏            | 2492/10001 [1:15:24<3:25:47,  1.64s/batch]Batch 2500/10001 Done, mean position loss: 21.174263706207277\n",
      "Training NF2:  25%|████▎            | 2518/10001 [1:15:24<3:58:13,  1.91s/batch]Batch 2500/10001 Done, mean position loss: 20.673707320690156\n",
      "Training NF2:  25%|████▏            | 2468/10001 [1:15:26<3:53:01,  1.86s/batch]Batch 2500/10001 Done, mean position loss: 20.61824200153351\n",
      "Training NF2:  25%|████▏            | 2493/10001 [1:15:26<3:57:07,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 20.673689625263215\n",
      "Training NF2:  25%|████▎            | 2505/10001 [1:15:32<3:17:53,  1.58s/batch]Batch 2500/10001 Done, mean position loss: 21.558751676082608\n",
      "Training NF2:  25%|████▎            | 2509/10001 [1:15:37<3:13:55,  1.55s/batch]Batch 2500/10001 Done, mean position loss: 20.697565681934357\n",
      "Training NF2:  25%|████▏            | 2487/10001 [1:15:38<3:07:40,  1.50s/batch]Batch 2500/10001 Done, mean position loss: 20.86471311569214\n",
      "Training NF2:  25%|████▏            | 2488/10001 [1:15:40<3:29:01,  1.67s/batch]Batch 2500/10001 Done, mean position loss: 20.499070327281952\n",
      "Training NF2:  25%|████▏            | 2478/10001 [1:15:45<3:35:44,  1.72s/batch]Batch 2500/10001 Done, mean position loss: 20.757380101680756\n",
      "Training NF2:  25%|████▏            | 2464/10001 [1:15:51<3:50:14,  1.83s/batch]Batch 2500/10001 Done, mean position loss: 20.737841522693635\n",
      "Training NF2:  25%|████▏            | 2480/10001 [1:15:54<3:33:16,  1.70s/batch]Batch 2500/10001 Done, mean position loss: 20.78591626405716\n",
      "Training NF2:  25%|████▎            | 2510/10001 [1:15:55<3:43:22,  1.79s/batch]Batch 2500/10001 Done, mean position loss: 20.654658174514772\n",
      "Training NF2:  25%|████▎            | 2503/10001 [1:15:56<4:14:44,  2.04s/batch]Batch 2500/10001 Done, mean position loss: 20.992947275638578\n",
      "Training NF2:  25%|████▎            | 2520/10001 [1:15:56<2:50:38,  1.37s/batch]Batch 2500/10001 Done, mean position loss: 20.586643788814545\n",
      "Training NF2:  25%|████▎            | 2501/10001 [1:15:56<3:45:53,  1.81s/batch]Batch 2500/10001 Done, mean position loss: 20.55558612346649\n",
      "Training NF2:  25%|████▎            | 2504/10001 [1:16:01<3:38:06,  1.75s/batch]Batch 2500/10001 Done, mean position loss: 20.95421191930771\n",
      "Training NF2:  25%|████▏            | 2488/10001 [1:16:01<3:36:55,  1.73s/batch]Batch 2500/10001 Done, mean position loss: 21.120419642925263\n",
      "Training NF2:  25%|████▎            | 2530/10001 [1:16:02<2:54:41,  1.40s/batch]Batch 2500/10001 Done, mean position loss: 20.497409343719482\n",
      "Training NF2:  25%|████▏            | 2476/10001 [1:16:02<3:32:37,  1.70s/batch]Batch 2500/10001 Done, mean position loss: 21.02135528087616\n",
      "Training NF2:  25%|████▏            | 2489/10001 [1:16:06<3:57:36,  1.90s/batch]Batch 2500/10001 Done, mean position loss: 20.529104907512668\n",
      "Training NF2:  25%|████▎            | 2533/10001 [1:16:17<3:28:17,  1.67s/batch]Batch 2500/10001 Done, mean position loss: 20.75313750743866\n",
      "Training NF2:  25%|████▎            | 2540/10001 [1:16:20<4:00:23,  1.93s/batch]Batch 2500/10001 Done, mean position loss: 20.872362554073334\n",
      "Training NF2:  25%|████▎            | 2522/10001 [1:16:22<3:28:51,  1.68s/batch]Batch 2500/10001 Done, mean position loss: 20.330709178447727\n",
      "Training NF2:  25%|████▎            | 2537/10001 [1:16:25<3:15:30,  1.57s/batch]Batch 2500/10001 Done, mean position loss: 20.683939740657806\n",
      "Training NF2:  25%|████▏            | 2484/10001 [1:16:26<3:15:04,  1.56s/batch]Batch 2500/10001 Done, mean position loss: 21.144529905319217\n",
      "Training NF2:  26%|████▎            | 2562/10001 [1:16:29<3:35:52,  1.74s/batch]Batch 2500/10001 Done, mean position loss: 20.729570474624637\n",
      "Training NF2:  26%|████▎            | 2563/10001 [1:16:48<3:53:53,  1.89s/batch]Batch 2500/10001 Done, mean position loss: 20.417829899787904\n",
      "Training NF2:  25%|████▎            | 2518/10001 [1:16:58<3:36:56,  1.74s/batch]Batch 2500/10001 Done, mean position loss: 20.864459471702574\n",
      "Training NF2:  26%|████▎            | 2562/10001 [1:17:00<3:42:32,  1.79s/batch]Batch 2500/10001 Done, mean position loss: 20.674401915073396\n",
      "Training NF2:  26%|████▎            | 2555/10001 [1:17:37<3:53:19,  1.88s/batch]Batch 2600/10001 Done, mean position loss: 20.730228555202483\n",
      "Training NF2:  26%|████▎            | 2563/10001 [1:17:42<3:13:54,  1.56s/batch]Batch 2600/10001 Done, mean position loss: 21.138332235813138\n",
      "Training NF2:  26%|████▍            | 2593/10001 [1:17:46<3:46:26,  1.83s/batch]Batch 2600/10001 Done, mean position loss: 20.461534445285796\n",
      "Training NF2:  26%|████▍            | 2606/10001 [1:17:51<4:20:21,  2.11s/batch]Batch 2600/10001 Done, mean position loss: 20.62535256624222\n",
      "Training NF2:  26%|████▎            | 2554/10001 [1:17:58<3:34:33,  1.73s/batch]Batch 2600/10001 Done, mean position loss: 20.70595909833908\n",
      "Training NF2:  26%|████▍            | 2598/10001 [1:18:01<4:06:10,  2.00s/batch]Batch 2600/10001 Done, mean position loss: 21.07015609025955\n",
      "Training NF2:  26%|████▎            | 2570/10001 [1:18:01<3:54:05,  1.89s/batch]Batch 2600/10001 Done, mean position loss: 21.054213321208955\n",
      "Training NF2:  26%|████▍            | 2613/10001 [1:18:07<3:41:58,  1.80s/batch]Batch 2600/10001 Done, mean position loss: 20.838236520290373\n",
      "Training NF2:  26%|████▍            | 2576/10001 [1:18:10<3:24:09,  1.65s/batch]Batch 2600/10001 Done, mean position loss: 20.35783581495285\n",
      "Training NF2:  26%|████▍            | 2609/10001 [1:18:12<3:33:45,  1.74s/batch]Batch 2600/10001 Done, mean position loss: 20.611138439178468\n",
      "Training NF2:  26%|████▍            | 2608/10001 [1:18:15<4:12:08,  2.05s/batch]Batch 2600/10001 Done, mean position loss: 20.75400219678879\n",
      "Training NF2:  26%|████▍            | 2591/10001 [1:18:16<3:44:00,  1.81s/batch]Batch 2600/10001 Done, mean position loss: 21.11731812953949\n",
      "Training NF2:  26%|████▍            | 2606/10001 [1:18:16<4:21:55,  2.13s/batch]Batch 2600/10001 Done, mean position loss: 20.564391419887542\n",
      "Training NF2:  26%|████▍            | 2576/10001 [1:18:20<3:50:30,  1.86s/batch]Batch 2600/10001 Done, mean position loss: 21.553052518367767\n",
      "Training NF2:  25%|████▎            | 2548/10001 [1:18:21<3:32:47,  1.71s/batch]Batch 2600/10001 Done, mean position loss: 20.655271637439725\n",
      "Training NF2:  26%|████▍            | 2614/10001 [1:18:22<3:19:11,  1.62s/batch]Batch 2600/10001 Done, mean position loss: 20.681996665000916\n",
      "Training NF2:  26%|████▍            | 2582/10001 [1:18:28<3:54:57,  1.90s/batch]Batch 2600/10001 Done, mean position loss: 20.83290822267532\n",
      "Training NF2:  26%|████▍            | 2614/10001 [1:18:32<4:08:09,  2.02s/batch]Batch 2600/10001 Done, mean position loss: 20.497100670337677\n",
      "Training NF2:  26%|████▍            | 2579/10001 [1:18:33<3:29:07,  1.69s/batch]Batch 2600/10001 Done, mean position loss: 20.684885501861572\n",
      "Training NF2:  26%|████▍            | 2591/10001 [1:18:38<3:45:58,  1.83s/batch]Batch 2600/10001 Done, mean position loss: 20.745282375812533\n",
      "Training NF2:  26%|████▍            | 2615/10001 [1:18:43<3:08:18,  1.53s/batch]Batch 2600/10001 Done, mean position loss: 20.780014760494232\n",
      "Training NF2:  26%|████▍            | 2587/10001 [1:18:48<3:57:11,  1.92s/batch]Batch 2600/10001 Done, mean position loss: 20.654540870189667\n",
      "Training NF2:  26%|████▍            | 2582/10001 [1:18:48<3:48:50,  1.85s/batch]Batch 2600/10001 Done, mean position loss: 20.594708762168885\n",
      "Training NF2:  26%|████▍            | 2624/10001 [1:18:48<3:19:01,  1.62s/batch]Batch 2600/10001 Done, mean position loss: 20.552347619533542\n",
      "Training NF2:  26%|████▍            | 2623/10001 [1:18:52<3:21:33,  1.64s/batch]Batch 2600/10001 Done, mean position loss: 20.9619845867157\n",
      "Training NF2:  26%|████▍            | 2621/10001 [1:18:53<3:24:31,  1.66s/batch]Batch 2600/10001 Done, mean position loss: 20.480604536533356\n",
      "Training NF2:  26%|████▍            | 2595/10001 [1:18:55<3:37:15,  1.76s/batch]Batch 2600/10001 Done, mean position loss: 21.11132283449173\n",
      "Training NF2:  26%|████▍            | 2638/10001 [1:18:57<3:42:02,  1.81s/batch]Batch 2600/10001 Done, mean position loss: 20.69058227777481\n",
      "Training NF2:  26%|████▎            | 2569/10001 [1:18:58<4:13:55,  2.05s/batch]Batch 2600/10001 Done, mean position loss: 21.021439678668976\n",
      "Training NF2:  26%|████▍            | 2626/10001 [1:19:02<4:41:16,  2.29s/batch]Batch 2600/10001 Done, mean position loss: 20.519396009445188\n",
      "Training NF2:  26%|████▍            | 2629/10001 [1:19:07<4:09:30,  2.03s/batch]Batch 2600/10001 Done, mean position loss: 20.958718087673184\n",
      "Training NF2:  26%|████▍            | 2582/10001 [1:19:13<4:32:51,  2.21s/batch]Batch 2600/10001 Done, mean position loss: 20.73979950904846\n",
      "Training NF2:  26%|████▍            | 2626/10001 [1:19:18<4:12:53,  2.06s/batch]Batch 2600/10001 Done, mean position loss: 20.856322672367096\n",
      "Training NF2:  26%|████▍            | 2606/10001 [1:19:22<4:01:12,  1.96s/batch]Batch 2600/10001 Done, mean position loss: 21.127715604305266\n",
      "Training NF2:  26%|████▍            | 2619/10001 [1:19:23<4:25:08,  2.16s/batch]Batch 2600/10001 Done, mean position loss: 20.674394307136534\n",
      "Training NF2:  26%|████▍            | 2621/10001 [1:19:30<3:40:53,  1.80s/batch]Batch 2600/10001 Done, mean position loss: 20.355906617641452\n",
      "Training NF2:  26%|████▍            | 2605/10001 [1:19:36<3:32:00,  1.72s/batch]Batch 2600/10001 Done, mean position loss: 20.72395278453827\n",
      "Training NF2:  26%|████▍            | 2643/10001 [1:19:49<4:33:00,  2.23s/batch]Batch 2600/10001 Done, mean position loss: 20.44058115720749\n",
      "Training NF2:  27%|████▌            | 2654/10001 [1:19:59<4:02:31,  1.98s/batch]Batch 2600/10001 Done, mean position loss: 20.864728879928588\n",
      "Training NF2:  27%|████▌            | 2662/10001 [1:20:08<4:29:47,  2.21s/batch]Batch 2600/10001 Done, mean position loss: 20.673119282722475\n",
      "Training NF2:  27%|████▌            | 2689/10001 [1:20:39<4:05:11,  2.01s/batch]Batch 2700/10001 Done, mean position loss: 20.724161553382874\n",
      "Training NF2:  27%|████▌            | 2663/10001 [1:20:47<4:31:29,  2.22s/batch]Batch 2700/10001 Done, mean position loss: 21.13625020503998\n",
      "Training NF2:  27%|████▌            | 2668/10001 [1:20:51<4:00:08,  1.96s/batch]Batch 2700/10001 Done, mean position loss: 20.454864327907565\n",
      "Training NF2:  26%|████▍            | 2632/10001 [1:20:58<4:06:45,  2.01s/batch]Batch 2700/10001 Done, mean position loss: 20.64415014028549\n",
      "Training NF2:  27%|████▌            | 2661/10001 [1:21:01<3:14:18,  1.59s/batch]Batch 2700/10001 Done, mean position loss: 21.055217146873474\n",
      "Training NF2:  26%|████▍            | 2635/10001 [1:21:03<4:00:24,  1.96s/batch]Batch 2700/10001 Done, mean position loss: 20.715755848884584\n",
      "Training NF2:  27%|████▌            | 2707/10001 [1:21:07<3:24:27,  1.68s/batch]Batch 2700/10001 Done, mean position loss: 21.054017703533173\n",
      "Training NF2:  27%|████▌            | 2716/10001 [1:21:12<3:44:04,  1.85s/batch]Batch 2700/10001 Done, mean position loss: 20.603302309513094\n",
      "Training NF2:  27%|████▌            | 2678/10001 [1:21:12<4:22:24,  2.15s/batch]Batch 2700/10001 Done, mean position loss: 20.365303473472594\n",
      "Training NF2:  27%|████▌            | 2687/10001 [1:21:12<3:57:00,  1.94s/batch]Batch 2700/10001 Done, mean position loss: 20.754114668369297\n",
      "Training NF2:  27%|████▌            | 2707/10001 [1:21:22<3:34:38,  1.77s/batch]Batch 2700/10001 Done, mean position loss: 21.547534058094023\n",
      "Training NF2:  27%|████▋            | 2726/10001 [1:21:23<3:28:11,  1.72s/batch]Batch 2700/10001 Done, mean position loss: 20.64782638311386\n",
      "Training NF2:  27%|████▌            | 2679/10001 [1:21:24<4:06:27,  2.02s/batch]Batch 2700/10001 Done, mean position loss: 21.10870958328247\n",
      "Training NF2:  27%|████▌            | 2662/10001 [1:21:26<4:36:33,  2.26s/batch]Batch 2700/10001 Done, mean position loss: 20.818203547000884\n",
      "Training NF2:  27%|████▌            | 2691/10001 [1:21:26<4:01:01,  1.98s/batch]Batch 2700/10001 Done, mean position loss: 20.547388565540313\n",
      "Training NF2:  27%|████▌            | 2681/10001 [1:21:27<3:49:55,  1.88s/batch]Batch 2700/10001 Done, mean position loss: 20.679314348697662\n",
      "Training NF2:  27%|████▌            | 2705/10001 [1:21:30<3:55:14,  1.93s/batch]Batch 2700/10001 Done, mean position loss: 20.47080350160599\n",
      "Training NF2:  27%|████▌            | 2709/10001 [1:21:35<3:05:43,  1.53s/batch]Batch 2700/10001 Done, mean position loss: 20.854418046474457\n",
      "Training NF2:  27%|████▌            | 2651/10001 [1:21:43<4:14:26,  2.08s/batch]Batch 2700/10001 Done, mean position loss: 20.64566022157669\n",
      "Training NF2:  27%|████▌            | 2713/10001 [1:21:45<3:39:27,  1.81s/batch]Batch 2700/10001 Done, mean position loss: 20.73998025894165\n",
      "Training NF2:  27%|████▌            | 2687/10001 [1:21:48<3:57:01,  1.94s/batch]Batch 2700/10001 Done, mean position loss: 20.65180176734924\n",
      "Training NF2:  27%|████▌            | 2716/10001 [1:21:53<3:20:07,  1.65s/batch]Batch 2700/10001 Done, mean position loss: 20.573456432819366\n",
      "Batch 2700/10001 Done, mean position loss: 20.769190654754638\n",
      "Training NF2:  27%|████▋            | 2725/10001 [1:21:53<4:01:03,  1.99s/batch]Batch 2700/10001 Done, mean position loss: 20.472667899131775\n",
      "Training NF2:  27%|████▌            | 2696/10001 [1:21:55<3:33:16,  1.75s/batch]Batch 2700/10001 Done, mean position loss: 20.558672919273377\n",
      "Training NF2:  27%|████▋            | 2730/10001 [1:22:03<3:37:29,  1.79s/batch]Batch 2700/10001 Done, mean position loss: 20.513133006095885\n",
      "Training NF2:  27%|████▋            | 2729/10001 [1:22:04<3:56:47,  1.95s/batch]Batch 2700/10001 Done, mean position loss: 21.102208933830262\n",
      "Training NF2:  27%|████▌            | 2696/10001 [1:22:06<4:17:27,  2.11s/batch]Batch 2700/10001 Done, mean position loss: 20.968219027519226\n",
      "Training NF2:  27%|████▌            | 2708/10001 [1:22:07<4:13:33,  2.09s/batch]Batch 2700/10001 Done, mean position loss: 20.996998026371003\n",
      "Training NF2:  27%|████▌            | 2715/10001 [1:22:10<3:46:43,  1.87s/batch]Batch 2700/10001 Done, mean position loss: 20.68529588699341\n",
      "Training NF2:  27%|████▌            | 2686/10001 [1:22:15<4:22:36,  2.15s/batch]Batch 2700/10001 Done, mean position loss: 20.750131528377533\n",
      "Training NF2:  27%|████▌            | 2712/10001 [1:22:16<3:52:31,  1.91s/batch]Batch 2700/10001 Done, mean position loss: 20.947296714782716\n",
      "Training NF2:  27%|████▌            | 2694/10001 [1:22:26<3:18:53,  1.63s/batch]Batch 2700/10001 Done, mean position loss: 21.093727414608\n",
      "Training NF2:  27%|████▌            | 2720/10001 [1:22:31<3:20:22,  1.65s/batch]Batch 2700/10001 Done, mean position loss: 20.86312976360321\n",
      "Training NF2:  27%|████▌            | 2720/10001 [1:22:39<3:29:16,  1.72s/batch]Batch 2700/10001 Done, mean position loss: 20.67165117740631\n",
      "Training NF2:  27%|████▋            | 2747/10001 [1:22:39<3:20:56,  1.66s/batch]Batch 2700/10001 Done, mean position loss: 20.34178892374039\n",
      "Training NF2:  27%|████▋            | 2739/10001 [1:22:41<4:01:14,  1.99s/batch]Batch 2700/10001 Done, mean position loss: 20.72331094264984\n",
      "Training NF2:  27%|████▋            | 2747/10001 [1:22:53<3:44:35,  1.86s/batch]Batch 2700/10001 Done, mean position loss: 20.430793855190277\n",
      "Training NF2:  28%|████▋            | 2767/10001 [1:23:06<4:06:45,  2.05s/batch]Batch 2700/10001 Done, mean position loss: 20.853190705776214\n",
      "Training NF2:  27%|████▋            | 2738/10001 [1:23:15<4:13:25,  2.09s/batch]Batch 2700/10001 Done, mean position loss: 20.68984082221985\n",
      "Training NF2:  27%|████▋            | 2721/10001 [1:23:40<3:20:50,  1.66s/batch]Batch 2800/10001 Done, mean position loss: 20.7145960688591\n",
      "Training NF2:  28%|████▋            | 2786/10001 [1:23:52<3:48:04,  1.90s/batch]Batch 2800/10001 Done, mean position loss: 21.170376834869387\n",
      "Training NF2:  28%|████▋            | 2793/10001 [1:24:00<3:19:08,  1.66s/batch]Batch 2800/10001 Done, mean position loss: 20.614643630981448\n",
      "Training NF2:  27%|████▋            | 2747/10001 [1:24:02<3:38:10,  1.80s/batch]Batch 2800/10001 Done, mean position loss: 20.453731110095976\n",
      "Training NF2:  28%|████▋            | 2753/10001 [1:24:05<3:09:39,  1.57s/batch]Batch 2800/10001 Done, mean position loss: 21.041098165512086\n",
      "Training NF2:  28%|████▋            | 2756/10001 [1:24:06<3:44:21,  1.86s/batch]Batch 2800/10001 Done, mean position loss: 20.721634583473204\n",
      "Training NF2:  28%|████▋            | 2767/10001 [1:24:08<3:33:09,  1.77s/batch]Batch 2800/10001 Done, mean position loss: 20.36386291980743\n",
      "Training NF2:  28%|████▊            | 2804/10001 [1:24:15<3:58:26,  1.99s/batch]Batch 2800/10001 Done, mean position loss: 20.596179854869842\n",
      "Training NF2:  28%|████▋            | 2780/10001 [1:24:17<4:13:15,  2.10s/batch]Batch 2800/10001 Done, mean position loss: 20.993389894962313\n",
      "Training NF2:  28%|████▊            | 2812/10001 [1:24:21<3:11:58,  1.60s/batch]Batch 2800/10001 Done, mean position loss: 20.76965528011322\n",
      "Training NF2:  28%|████▊            | 2796/10001 [1:24:22<3:50:22,  1.92s/batch]Batch 2800/10001 Done, mean position loss: 21.520625133514404\n",
      "Training NF2:  28%|████▊            | 2827/10001 [1:24:24<3:41:07,  1.85s/batch]Batch 2800/10001 Done, mean position loss: 20.659355545043944\n",
      "Training NF2:  28%|████▊            | 2806/10001 [1:24:28<4:25:46,  2.22s/batch]Batch 2800/10001 Done, mean position loss: 20.545039484500883\n",
      "Training NF2:  28%|████▋            | 2778/10001 [1:24:28<3:47:26,  1.89s/batch]Batch 2800/10001 Done, mean position loss: 20.678419165611267\n",
      "Training NF2:  28%|████▋            | 2775/10001 [1:24:31<4:02:43,  2.02s/batch]Batch 2800/10001 Done, mean position loss: 20.80009660720825\n",
      "Batch 2800/10001 Done, mean position loss: 21.148697576522828\n",
      "Training NF2:  28%|████▋            | 2781/10001 [1:24:32<3:20:53,  1.67s/batch]Batch 2800/10001 Done, mean position loss: 20.4682004737854\n",
      "Training NF2:  28%|████▋            | 2783/10001 [1:24:47<3:43:37,  1.86s/batch]Batch 2800/10001 Done, mean position loss: 20.843445000648497\n",
      "Training NF2:  28%|████▊            | 2824/10001 [1:24:53<3:51:26,  1.93s/batch]Batch 2800/10001 Done, mean position loss: 20.744456424713135\n",
      "Training NF2:  28%|████▊            | 2826/10001 [1:24:53<4:02:26,  2.03s/batch]Batch 2800/10001 Done, mean position loss: 20.629052753448487\n",
      "Training NF2:  28%|████▊            | 2821/10001 [1:24:56<3:10:30,  1.59s/batch]Batch 2800/10001 Done, mean position loss: 20.48638520717621\n",
      "Training NF2:  28%|████▋            | 2794/10001 [1:24:57<3:37:39,  1.81s/batch]Batch 2800/10001 Done, mean position loss: 20.652416343688962\n",
      "Training NF2:  28%|████▋            | 2773/10001 [1:24:59<4:06:54,  2.05s/batch]Batch 2800/10001 Done, mean position loss: 20.771289477348326\n",
      "Training NF2:  28%|████▊            | 2805/10001 [1:25:01<3:53:18,  1.95s/batch]Batch 2800/10001 Done, mean position loss: 20.589973242282866\n",
      "Training NF2:  28%|████▊            | 2819/10001 [1:25:03<3:30:26,  1.76s/batch]Batch 2800/10001 Done, mean position loss: 20.555188877582548\n",
      "Training NF2:  28%|████▊            | 2836/10001 [1:25:09<3:59:05,  2.00s/batch]Batch 2800/10001 Done, mean position loss: 20.94264964580536\n",
      "Training NF2:  28%|████▊            | 2827/10001 [1:25:09<3:25:00,  1.71s/batch]Batch 2800/10001 Done, mean position loss: 20.6856493639946\n",
      "Training NF2:  28%|████▊            | 2828/10001 [1:25:09<3:22:53,  1.70s/batch]Batch 2800/10001 Done, mean position loss: 20.99479977369308\n",
      "Training NF2:  28%|████▋            | 2779/10001 [1:25:11<3:14:44,  1.62s/batch]Batch 2800/10001 Done, mean position loss: 20.506005578041076\n",
      "Training NF2:  28%|████▋            | 2780/10001 [1:25:12<3:59:54,  1.99s/batch]Batch 2800/10001 Done, mean position loss: 21.098416218757627\n",
      "Training NF2:  28%|████▊            | 2826/10001 [1:25:18<3:42:31,  1.86s/batch]Batch 2800/10001 Done, mean position loss: 20.920314853191375\n",
      "Training NF2:  28%|████▊            | 2826/10001 [1:25:21<3:47:19,  1.90s/batch]Batch 2800/10001 Done, mean position loss: 20.75247857093811\n",
      "Training NF2:  28%|████▊            | 2817/10001 [1:25:31<4:10:31,  2.09s/batch]Batch 2800/10001 Done, mean position loss: 21.09153085231781\n",
      "Training NF2:  28%|████▋            | 2785/10001 [1:25:39<3:43:19,  1.86s/batch]Batch 2800/10001 Done, mean position loss: 20.85468635559082\n",
      "Training NF2:  28%|████▊            | 2825/10001 [1:25:46<3:51:50,  1.94s/batch]Batch 2800/10001 Done, mean position loss: 20.346306953430172\n",
      "Training NF2:  29%|████▊            | 2852/10001 [1:25:49<3:27:56,  1.75s/batch]Batch 2800/10001 Done, mean position loss: 20.707611634731293\n",
      "Training NF2:  29%|████▊            | 2856/10001 [1:25:50<3:42:57,  1.87s/batch]Batch 2800/10001 Done, mean position loss: 20.42686398744583\n",
      "Training NF2:  28%|████▊            | 2802/10001 [1:25:51<3:51:19,  1.93s/batch]Batch 2800/10001 Done, mean position loss: 20.67080539941788\n",
      "Training NF2:  28%|████▊            | 2833/10001 [1:26:07<3:42:59,  1.87s/batch]Batch 2800/10001 Done, mean position loss: 20.869330899715422\n",
      "Training NF2:  29%|████▊            | 2862/10001 [1:26:23<3:27:54,  1.75s/batch]Batch 2800/10001 Done, mean position loss: 20.67088847875595\n",
      "Training NF2:  29%|████▊            | 2858/10001 [1:26:36<3:27:41,  1.74s/batch]Batch 2900/10001 Done, mean position loss: 20.71902436494827\n",
      "Training NF2:  29%|████▉            | 2886/10001 [1:26:56<3:07:54,  1.58s/batch]Batch 2900/10001 Done, mean position loss: 21.13627378463745\n",
      "Training NF2:  29%|████▉            | 2898/10001 [1:27:04<3:46:53,  1.92s/batch]Batch 2900/10001 Done, mean position loss: 20.441563999652864\n",
      "Training NF2:  29%|████▊            | 2866/10001 [1:27:07<3:59:04,  2.01s/batch]Batch 2900/10001 Done, mean position loss: 20.633625330924986\n",
      "Training NF2:  29%|████▉            | 2887/10001 [1:27:11<3:14:09,  1.64s/batch]Batch 2900/10001 Done, mean position loss: 21.06385354042053\n",
      "Training NF2:  29%|████▉            | 2895/10001 [1:27:11<4:15:27,  2.16s/batch]Batch 2900/10001 Done, mean position loss: 20.695965201854705\n",
      "Training NF2:  29%|████▊            | 2856/10001 [1:27:15<3:29:18,  1.76s/batch]Batch 2900/10001 Done, mean position loss: 20.356619553565977\n",
      "Training NF2:  29%|████▉            | 2883/10001 [1:27:19<3:46:58,  1.91s/batch]Batch 2900/10001 Done, mean position loss: 20.6074289727211\n",
      "Training NF2:  29%|████▉            | 2905/10001 [1:27:22<3:34:36,  1.81s/batch]Batch 2900/10001 Done, mean position loss: 21.555784668922424\n",
      "Training NF2:  29%|████▉            | 2904/10001 [1:27:23<3:18:16,  1.68s/batch]Batch 2900/10001 Done, mean position loss: 20.631546065807342\n",
      "Training NF2:  29%|████▉            | 2884/10001 [1:27:29<3:55:38,  1.99s/batch]Batch 2900/10001 Done, mean position loss: 21.00949491262436\n",
      "Training NF2:  29%|████▉            | 2879/10001 [1:27:29<3:16:58,  1.66s/batch]Batch 2900/10001 Done, mean position loss: 20.661233315467836\n",
      "Training NF2:  29%|████▉            | 2872/10001 [1:27:33<3:28:54,  1.76s/batch]Batch 2900/10001 Done, mean position loss: 20.546610152721406\n",
      "Training NF2:  29%|████▊            | 2856/10001 [1:27:33<3:16:53,  1.65s/batch]Batch 2900/10001 Done, mean position loss: 20.756502351760865\n",
      "Training NF2:  29%|████▉            | 2890/10001 [1:27:35<3:01:52,  1.53s/batch]Batch 2900/10001 Done, mean position loss: 21.119796707630158\n",
      "Training NF2:  29%|████▉            | 2891/10001 [1:27:38<3:57:15,  2.00s/batch]Batch 2900/10001 Done, mean position loss: 20.796462259292603\n",
      "Training NF2:  29%|████▉            | 2902/10001 [1:27:40<3:32:00,  1.79s/batch]Batch 2900/10001 Done, mean position loss: 20.46292856454849\n",
      "Training NF2:  29%|████▉            | 2921/10001 [1:27:51<3:38:34,  1.85s/batch]Batch 2900/10001 Done, mean position loss: 20.800126399993896\n",
      "Training NF2:  29%|████▉            | 2921/10001 [1:27:54<3:20:14,  1.70s/batch]Batch 2900/10001 Done, mean position loss: 20.731851699352266\n",
      "Training NF2:  29%|████▉            | 2868/10001 [1:27:56<3:38:43,  1.84s/batch]Batch 2900/10001 Done, mean position loss: 20.584677612781526\n",
      "Training NF2:  29%|████▉            | 2898/10001 [1:28:01<3:47:46,  1.92s/batch]Batch 2900/10001 Done, mean position loss: 20.617804932594296\n",
      "Training NF2:  29%|████▉            | 2920/10001 [1:28:02<3:33:01,  1.81s/batch]Batch 2900/10001 Done, mean position loss: 20.77128108501434\n",
      "Training NF2:  29%|████▉            | 2918/10001 [1:28:04<3:31:52,  1.79s/batch]Batch 2900/10001 Done, mean position loss: 20.64218391418457\n",
      "Training NF2:  29%|████▉            | 2886/10001 [1:28:05<2:47:49,  1.42s/batch]Batch 2900/10001 Done, mean position loss: 20.55091355085373\n",
      "Training NF2:  29%|████▉            | 2876/10001 [1:28:09<3:18:07,  1.67s/batch]Batch 2900/10001 Done, mean position loss: 20.48734458208084\n",
      "Training NF2:  29%|████▉            | 2911/10001 [1:28:10<3:14:16,  1.64s/batch]Batch 2900/10001 Done, mean position loss: 20.522308416366577\n",
      "Training NF2:  29%|████▉            | 2907/10001 [1:28:15<3:48:36,  1.93s/batch]Batch 2900/10001 Done, mean position loss: 21.052198615074158\n",
      "Training NF2:  29%|████▉            | 2899/10001 [1:28:17<4:09:24,  2.11s/batch]Batch 2900/10001 Done, mean position loss: 20.697692029476165\n",
      "Training NF2:  29%|████▉            | 2884/10001 [1:28:20<3:23:26,  1.72s/batch]Batch 2900/10001 Done, mean position loss: 20.920370852947237\n",
      "Training NF2:  29%|█████            | 2949/10001 [1:28:20<3:38:50,  1.86s/batch]Batch 2900/10001 Done, mean position loss: 20.98615207195282\n",
      "Training NF2:  29%|████▉            | 2932/10001 [1:28:21<3:33:54,  1.82s/batch]Batch 2900/10001 Done, mean position loss: 20.89642599821091\n",
      "Training NF2:  29%|████▉            | 2905/10001 [1:28:23<3:01:05,  1.53s/batch]Batch 2900/10001 Done, mean position loss: 20.746706285476684\n",
      "Training NF2:  30%|█████            | 2964/10001 [1:28:34<3:51:17,  1.97s/batch]Batch 2900/10001 Done, mean position loss: 21.071531171798703\n",
      "Training NF2:  29%|████▉            | 2925/10001 [1:28:41<3:41:48,  1.88s/batch]Batch 2900/10001 Done, mean position loss: 20.844495375156406\n",
      "Training NF2:  29%|████▉            | 2927/10001 [1:28:52<2:56:30,  1.50s/batch]Batch 2900/10001 Done, mean position loss: 20.340991857051847\n",
      "Training NF2:  29%|█████            | 2948/10001 [1:28:53<3:46:14,  1.92s/batch]Batch 2900/10001 Done, mean position loss: 20.415624763965607\n",
      "Training NF2:  30%|█████            | 2970/10001 [1:28:57<2:59:31,  1.53s/batch]Batch 2900/10001 Done, mean position loss: 20.64618904590607\n",
      "Training NF2:  29%|█████            | 2946/10001 [1:29:00<3:32:35,  1.81s/batch]Batch 2900/10001 Done, mean position loss: 20.712862436771392\n",
      "Training NF2:  29%|████▉            | 2939/10001 [1:29:10<3:17:51,  1.68s/batch]Batch 2900/10001 Done, mean position loss: 20.861015853881838\n",
      "Training NF2:  30%|█████            | 2971/10001 [1:29:23<3:06:13,  1.59s/batch]Batch 2900/10001 Done, mean position loss: 20.676610043048857\n",
      "Training NF2:  30%|█████            | 2968/10001 [1:29:40<3:52:08,  1.98s/batch]Batch 3000/10001 Done, mean position loss: 20.699141001701356\n",
      "Training NF2:  30%|█████            | 2977/10001 [1:29:48<4:24:18,  2.26s/batch]Batch 3000/10001 Done, mean position loss: 21.132858850955962\n",
      "Training NF2:  29%|█████            | 2946/10001 [1:30:06<3:54:30,  1.99s/batch]Batch 3000/10001 Done, mean position loss: 20.42381679534912\n",
      "Training NF2:  30%|█████            | 2964/10001 [1:30:14<3:14:55,  1.66s/batch]Batch 3000/10001 Done, mean position loss: 21.06584467172623\n",
      "Training NF2:  30%|█████            | 2971/10001 [1:30:16<3:22:19,  1.73s/batch]Batch 3000/10001 Done, mean position loss: 20.624004123210906\n",
      "Training NF2:  30%|█████            | 2973/10001 [1:30:19<3:12:42,  1.65s/batch]Batch 3000/10001 Done, mean position loss: 20.708631215095522\n",
      "Training NF2:  30%|█████            | 2999/10001 [1:30:21<3:38:42,  1.87s/batch]Batch 3000/10001 Done, mean position loss: 21.52510117530823\n",
      "Training NF2:  30%|█████            | 2974/10001 [1:30:22<3:38:00,  1.86s/batch]Batch 3000/10001 Done, mean position loss: 20.591413309574126\n",
      "Training NF2:  30%|█████            | 3006/10001 [1:30:26<2:49:18,  1.45s/batch]Batch 3000/10001 Done, mean position loss: 20.369689750671384\n",
      "Training NF2:  30%|█████            | 2993/10001 [1:30:27<3:18:02,  1.70s/batch]Batch 3000/10001 Done, mean position loss: 21.001153149604797\n",
      "Training NF2:  30%|█████            | 2987/10001 [1:30:28<3:05:25,  1.59s/batch]Batch 3000/10001 Done, mean position loss: 20.619791789054872\n",
      "Training NF2:  30%|█████            | 2996/10001 [1:30:33<3:46:45,  1.94s/batch]Batch 3000/10001 Done, mean position loss: 20.539174919128417\n",
      "Training NF2:  30%|█████            | 2955/10001 [1:30:33<4:11:12,  2.14s/batch]Batch 3000/10001 Done, mean position loss: 20.643081200122836\n",
      "Training NF2:  30%|█████            | 3010/10001 [1:30:44<3:57:46,  2.04s/batch]Batch 3000/10001 Done, mean position loss: 20.43472310781479\n",
      "Training NF2:  30%|█████            | 2961/10001 [1:30:44<4:11:56,  2.15s/batch]Batch 3000/10001 Done, mean position loss: 21.13493363380432\n",
      "Training NF2:  30%|█████            | 3003/10001 [1:30:47<3:53:26,  2.00s/batch]Batch 3000/10001 Done, mean position loss: 20.794761967658996\n",
      "Training NF2:  30%|█████            | 2981/10001 [1:30:47<4:12:34,  2.16s/batch]Batch 3000/10001 Done, mean position loss: 20.768412156105043\n",
      "Training NF2:  30%|█████            | 2992/10001 [1:30:52<3:29:36,  1.79s/batch]Batch 3000/10001 Done, mean position loss: 20.719104886054993\n",
      "Training NF2:  30%|█████            | 2960/10001 [1:30:56<2:49:16,  1.44s/batch]Batch 3000/10001 Done, mean position loss: 20.563145067691806\n",
      "Training NF2:  30%|█████▏           | 3017/10001 [1:30:57<3:58:18,  2.05s/batch]Batch 3000/10001 Done, mean position loss: 20.61375890493393\n",
      "Training NF2:  30%|█████▏           | 3026/10001 [1:30:59<3:43:54,  1.93s/batch]Batch 3000/10001 Done, mean position loss: 20.62128749370575\n",
      "Training NF2:  30%|█████            | 2970/10001 [1:31:02<3:41:43,  1.89s/batch]Batch 3000/10001 Done, mean position loss: 20.787858827114107\n",
      "Training NF2:  30%|█████            | 3012/10001 [1:31:08<3:17:55,  1.70s/batch]Batch 3000/10001 Done, mean position loss: 20.507826313972473\n",
      "Training NF2:  30%|█████▏           | 3033/10001 [1:31:10<2:33:17,  1.32s/batch]Batch 3000/10001 Done, mean position loss: 20.54231411933899\n",
      "Training NF2:  30%|█████            | 3000/10001 [1:31:11<3:36:14,  1.85s/batch]Batch 3000/10001 Done, mean position loss: 20.453498935699464\n",
      "Training NF2:  30%|█████▏           | 3048/10001 [1:31:12<2:42:48,  1.40s/batch]Batch 3000/10001 Done, mean position loss: 20.78184351205826\n",
      "Training NF2:  30%|█████            | 3003/10001 [1:31:13<3:07:44,  1.61s/batch]Batch 3000/10001 Done, mean position loss: 20.67428355693817\n",
      "Training NF2:  30%|█████▏           | 3021/10001 [1:31:22<2:59:17,  1.54s/batch]Batch 3000/10001 Done, mean position loss: 20.896761307716368\n",
      "Training NF2:  30%|█████▏           | 3023/10001 [1:31:24<3:23:53,  1.75s/batch]Batch 3000/10001 Done, mean position loss: 21.058975026607513\n",
      "Training NF2:  30%|█████            | 3007/10001 [1:31:25<4:04:22,  2.10s/batch]Batch 3000/10001 Done, mean position loss: 20.97990898132324\n",
      "Training NF2:  30%|█████▏           | 3034/10001 [1:31:27<3:24:58,  1.77s/batch]Batch 3000/10001 Done, mean position loss: 20.88619567632675\n",
      "Training NF2:  30%|█████            | 2981/10001 [1:31:33<3:14:21,  1.66s/batch]Batch 3000/10001 Done, mean position loss: 20.73482277393341\n",
      "Training NF2:  30%|█████            | 2999/10001 [1:31:37<4:20:14,  2.23s/batch]Batch 3000/10001 Done, mean position loss: 20.83725734233856\n",
      "Training NF2:  30%|█████▏           | 3042/10001 [1:31:42<3:11:06,  1.65s/batch]Batch 3000/10001 Done, mean position loss: 21.07289040565491\n",
      "Training NF2:  30%|█████▏           | 3038/10001 [1:31:55<3:52:54,  2.01s/batch]Batch 3000/10001 Done, mean position loss: 20.338953750133513\n",
      "Training NF2:  30%|█████▏           | 3043/10001 [1:32:00<2:54:28,  1.50s/batch]Batch 3000/10001 Done, mean position loss: 20.65090743780136\n",
      "Training NF2:  30%|█████▏           | 3031/10001 [1:32:02<3:49:04,  1.97s/batch]Batch 3000/10001 Done, mean position loss: 20.428105356693266\n",
      "Training NF2:  31%|█████▏           | 3080/10001 [1:32:09<3:08:20,  1.63s/batch]Batch 3000/10001 Done, mean position loss: 20.86010692358017\n",
      "Training NF2:  31%|█████▏           | 3059/10001 [1:32:12<3:00:42,  1.56s/batch]Batch 3000/10001 Done, mean position loss: 20.692577061653136\n",
      "Training NF2:  31%|█████▏           | 3072/10001 [1:32:27<3:07:00,  1.62s/batch]Batch 3000/10001 Done, mean position loss: 20.677354378700258\n",
      "Training NF2:  31%|█████▏           | 3066/10001 [1:32:45<3:10:57,  1.65s/batch]Batch 3100/10001 Done, mean position loss: 21.146280643939974\n",
      "Training NF2:  30%|█████▏           | 3050/10001 [1:32:52<2:51:08,  1.48s/batch]Batch 3100/10001 Done, mean position loss: 20.708415474891662\n",
      "Training NF2:  31%|█████▏           | 3053/10001 [1:33:06<3:10:39,  1.65s/batch]Batch 3100/10001 Done, mean position loss: 20.434895474910736\n",
      "Training NF2:  31%|█████▏           | 3077/10001 [1:33:09<3:54:35,  2.03s/batch]Batch 3100/10001 Done, mean position loss: 20.68943246126175\n",
      "Training NF2:  31%|█████▏           | 3076/10001 [1:33:12<3:02:44,  1.58s/batch]Batch 3100/10001 Done, mean position loss: 21.046313877105714\n",
      "Training NF2:  31%|█████▎           | 3093/10001 [1:33:13<3:06:48,  1.62s/batch]Batch 3100/10001 Done, mean position loss: 20.594367091655734\n",
      "Training NF2:  31%|█████▎           | 3100/10001 [1:33:18<2:57:25,  1.54s/batch]Batch 3100/10001 Done, mean position loss: 20.6304629945755\n",
      "Training NF2:  30%|█████▏           | 3049/10001 [1:33:19<3:36:42,  1.87s/batch]Batch 3100/10001 Done, mean position loss: 20.52406367778778\n",
      "Training NF2:  31%|█████▏           | 3074/10001 [1:33:20<3:41:58,  1.92s/batch]Batch 3100/10001 Done, mean position loss: 21.525011513233185\n",
      "Training NF2:  30%|█████▏           | 3041/10001 [1:33:20<3:04:18,  1.59s/batch]Batch 3100/10001 Done, mean position loss: 20.366660273075105\n",
      "Training NF2:  31%|█████▎           | 3105/10001 [1:33:27<3:49:01,  1.99s/batch]Batch 3100/10001 Done, mean position loss: 20.961211178302765\n",
      "Training NF2:  31%|█████▎           | 3097/10001 [1:33:29<3:59:40,  2.08s/batch]Batch 3100/10001 Done, mean position loss: 20.618116319179535\n",
      "Training NF2:  31%|█████▎           | 3111/10001 [1:33:37<3:36:46,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 20.66196895599365\n",
      "Training NF2:  31%|█████▏           | 3087/10001 [1:33:43<3:40:07,  1.91s/batch]Batch 3100/10001 Done, mean position loss: 20.425808284282684\n",
      "Training NF2:  31%|█████▏           | 3055/10001 [1:33:43<3:38:02,  1.88s/batch]Batch 3100/10001 Done, mean position loss: 21.149995350837706\n",
      "Training NF2:  31%|█████▏           | 3072/10001 [1:33:47<3:52:49,  2.02s/batch]Batch 3100/10001 Done, mean position loss: 20.744276804924013\n",
      "Training NF2:  31%|█████▏           | 3068/10001 [1:33:53<3:02:38,  1.58s/batch]Batch 3100/10001 Done, mean position loss: 20.76470533132553\n",
      "Training NF2:  31%|█████▏           | 3069/10001 [1:33:55<3:06:31,  1.61s/batch]Batch 3100/10001 Done, mean position loss: 20.716278686523438\n",
      "Training NF2:  31%|█████▎           | 3094/10001 [1:33:57<4:15:43,  2.22s/batch]Batch 3100/10001 Done, mean position loss: 20.763340990543366\n",
      "Training NF2:  31%|█████▎           | 3095/10001 [1:33:59<4:12:24,  2.19s/batch]Batch 3100/10001 Done, mean position loss: 20.61238658428192\n",
      "Training NF2:  31%|█████▎           | 3110/10001 [1:33:59<3:34:45,  1.87s/batch]Batch 3100/10001 Done, mean position loss: 20.616553535461428\n",
      "Batch 3100/10001 Done, mean position loss: 20.5597483754158\n",
      "Training NF2:  31%|█████▎           | 3114/10001 [1:34:09<2:56:30,  1.54s/batch]Batch 3100/10001 Done, mean position loss: 20.77842489004135\n",
      "Training NF2:  31%|█████▎           | 3133/10001 [1:34:10<3:52:11,  2.03s/batch]Batch 3100/10001 Done, mean position loss: 20.4690281867981\n",
      "Training NF2:  31%|█████▏           | 3072/10001 [1:34:11<3:37:01,  1.88s/batch]Batch 3100/10001 Done, mean position loss: 20.660190825462344\n",
      "Training NF2:  31%|█████▎           | 3093/10001 [1:34:10<3:25:30,  1.79s/batch]Batch 3100/10001 Done, mean position loss: 20.498026506900786\n",
      "Training NF2:  31%|█████▏           | 3071/10001 [1:34:13<4:18:52,  2.24s/batch]Batch 3100/10001 Done, mean position loss: 20.540816018581392\n",
      "Training NF2:  31%|█████▏           | 3072/10001 [1:34:19<3:42:44,  1.93s/batch]Batch 3100/10001 Done, mean position loss: 20.888957912921907\n",
      "Training NF2:  31%|█████▏           | 3082/10001 [1:34:23<3:37:54,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 21.0642196393013\n",
      "Training NF2:  31%|█████▎           | 3110/10001 [1:34:26<3:35:21,  1.88s/batch]Batch 3100/10001 Done, mean position loss: 20.968577828407287\n",
      "Training NF2:  31%|█████▎           | 3113/10001 [1:34:31<3:02:14,  1.59s/batch]Batch 3100/10001 Done, mean position loss: 20.897229101657867\n",
      "Training NF2:  31%|█████▎           | 3091/10001 [1:34:38<3:31:36,  1.84s/batch]Batch 3100/10001 Done, mean position loss: 20.73415325164795\n",
      "Training NF2:  31%|█████▎           | 3100/10001 [1:34:38<3:37:51,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 20.81425024986267\n",
      "Training NF2:  31%|█████▎           | 3122/10001 [1:34:40<3:25:44,  1.79s/batch]Batch 3100/10001 Done, mean position loss: 21.056746554374698\n",
      "Training NF2:  31%|█████▎           | 3130/10001 [1:34:53<3:13:45,  1.69s/batch]Batch 3100/10001 Done, mean position loss: 20.33287625551224\n",
      "Training NF2:  32%|█████▍           | 3164/10001 [1:34:56<3:45:55,  1.98s/batch]Batch 3100/10001 Done, mean position loss: 20.61497360229492\n",
      "Training NF2:  32%|█████▍           | 3168/10001 [1:34:57<3:35:31,  1.89s/batch]Batch 3100/10001 Done, mean position loss: 20.403821728229524\n",
      "Training NF2:  31%|█████▎           | 3113/10001 [1:35:00<3:18:57,  1.73s/batch]Batch 3100/10001 Done, mean position loss: 20.858794147968293\n",
      "Training NF2:  31%|█████▎           | 3112/10001 [1:35:17<3:23:17,  1.77s/batch]Batch 3100/10001 Done, mean position loss: 20.684081301689147\n",
      "Training NF2:  32%|█████▍           | 3172/10001 [1:35:24<3:18:48,  1.75s/batch]Batch 3100/10001 Done, mean position loss: 20.69196561574936\n",
      "Training NF2:  31%|█████▎           | 3136/10001 [1:35:40<3:32:55,  1.86s/batch]Batch 3200/10001 Done, mean position loss: 21.17177099227905\n",
      "Training NF2:  32%|█████▎           | 3153/10001 [1:35:57<3:01:02,  1.59s/batch]Batch 3200/10001 Done, mean position loss: 20.702956500053403\n",
      "Training NF2:  32%|█████▍           | 3193/10001 [1:35:59<3:01:14,  1.60s/batch]Batch 3200/10001 Done, mean position loss: 20.42531051635742\n",
      "Training NF2:  32%|█████▍           | 3165/10001 [1:36:10<3:21:00,  1.76s/batch]Batch 3200/10001 Done, mean position loss: 20.702918260097505\n",
      "Training NF2:  32%|█████▍           | 3187/10001 [1:36:12<3:17:23,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 20.51761920452118\n",
      "Training NF2:  32%|█████▎           | 3154/10001 [1:36:13<3:11:03,  1.67s/batch]Batch 3200/10001 Done, mean position loss: 20.595213339328765\n",
      "Training NF2:  32%|█████▍           | 3198/10001 [1:36:21<3:46:15,  2.00s/batch]Batch 3200/10001 Done, mean position loss: 21.022612092494963\n",
      "Training NF2:  32%|█████▍           | 3175/10001 [1:36:22<3:17:43,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 20.60818613767624\n",
      "Training NF2:  32%|█████▎           | 3159/10001 [1:36:22<3:22:47,  1.78s/batch]Batch 3200/10001 Done, mean position loss: 20.383847351074216\n",
      "Training NF2:  32%|█████▍           | 3170/10001 [1:36:26<4:39:21,  2.45s/batch]Batch 3200/10001 Done, mean position loss: 21.527327337265014\n",
      "Training NF2:  32%|█████▍           | 3168/10001 [1:36:29<3:28:33,  1.83s/batch]Batch 3200/10001 Done, mean position loss: 20.976261599063875\n",
      "Training NF2:  32%|█████▍           | 3167/10001 [1:36:36<3:17:30,  1.73s/batch]Batch 3200/10001 Done, mean position loss: 20.611609098911288\n",
      "Training NF2:  32%|█████▍           | 3222/10001 [1:36:39<3:25:55,  1.82s/batch]Batch 3200/10001 Done, mean position loss: 20.64469881296158\n",
      "Training NF2:  32%|█████▍           | 3167/10001 [1:36:44<3:43:26,  1.96s/batch]Batch 3200/10001 Done, mean position loss: 20.418367819786074\n",
      "Training NF2:  32%|█████▍           | 3173/10001 [1:36:47<3:29:27,  1.84s/batch]Batch 3200/10001 Done, mean position loss: 21.10950716495514\n",
      "Training NF2:  32%|█████▍           | 3197/10001 [1:36:52<3:49:05,  2.02s/batch]Batch 3200/10001 Done, mean position loss: 20.594854934215547\n",
      "Training NF2:  32%|█████▍           | 3215/10001 [1:36:53<4:00:20,  2.12s/batch]Batch 3200/10001 Done, mean position loss: 20.717141127586366\n",
      "Training NF2:  32%|█████▍           | 3192/10001 [1:36:59<3:54:14,  2.06s/batch]Batch 3200/10001 Done, mean position loss: 20.74023509502411\n",
      "Training NF2:  32%|█████▍           | 3198/10001 [1:36:59<3:22:01,  1.78s/batch]Batch 3200/10001 Done, mean position loss: 20.77493489742279\n",
      "Training NF2:  32%|█████▍           | 3219/10001 [1:37:01<3:44:27,  1.99s/batch]Batch 3200/10001 Done, mean position loss: 20.56234031200409\n",
      "Training NF2:  32%|█████▍           | 3216/10001 [1:37:05<3:39:47,  1.94s/batch]Batch 3200/10001 Done, mean position loss: 20.716766750812532\n",
      "Training NF2:  32%|█████▍           | 3212/10001 [1:37:06<3:37:17,  1.92s/batch]Batch 3200/10001 Done, mean position loss: 20.633195855617522\n",
      "Training NF2:  32%|█████▌           | 3238/10001 [1:37:10<3:16:01,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 20.46555124282837\n",
      "Training NF2:  32%|█████▍           | 3228/10001 [1:37:15<3:31:39,  1.88s/batch]Batch 3200/10001 Done, mean position loss: 20.50097447395325\n",
      "Training NF2:  32%|█████▍           | 3201/10001 [1:37:16<4:00:30,  2.12s/batch]Batch 3200/10001 Done, mean position loss: 20.52925633430481\n",
      "Training NF2:  32%|█████▍           | 3176/10001 [1:37:16<3:26:41,  1.82s/batch]Batch 3200/10001 Done, mean position loss: 20.6325682759285\n",
      "Training NF2:  32%|█████▍           | 3200/10001 [1:37:23<3:42:56,  1.97s/batch]Batch 3200/10001 Done, mean position loss: 20.762739911079407\n",
      "Training NF2:  32%|█████▌           | 3246/10001 [1:37:24<2:57:43,  1.58s/batch]Batch 3200/10001 Done, mean position loss: 20.868862154483796\n",
      "Training NF2:  32%|█████▍           | 3216/10001 [1:37:32<3:36:41,  1.92s/batch]Batch 3200/10001 Done, mean position loss: 20.946531636714937\n",
      "Training NF2:  32%|█████▍           | 3212/10001 [1:37:33<3:16:30,  1.74s/batch]Batch 3200/10001 Done, mean position loss: 20.9132758808136\n",
      "Training NF2:  32%|█████▍           | 3206/10001 [1:37:33<3:20:33,  1.77s/batch]Batch 3200/10001 Done, mean position loss: 21.03231001377106\n",
      "Training NF2:  32%|█████▍           | 3215/10001 [1:37:45<3:27:20,  1.83s/batch]Batch 3200/10001 Done, mean position loss: 20.804886181354522\n",
      "Training NF2:  32%|█████▌           | 3250/10001 [1:37:47<3:06:19,  1.66s/batch]Batch 3200/10001 Done, mean position loss: 20.739334650039673\n",
      "Training NF2:  33%|█████▌           | 3251/10001 [1:37:51<2:48:31,  1.50s/batch]Batch 3200/10001 Done, mean position loss: 21.06099250793457\n",
      "Training NF2:  32%|█████▍           | 3232/10001 [1:38:05<3:22:14,  1.79s/batch]Batch 3200/10001 Done, mean position loss: 20.61306277513504\n",
      "Training NF2:  32%|█████▌           | 3238/10001 [1:38:05<2:43:37,  1.45s/batch]Batch 3200/10001 Done, mean position loss: 20.330469591617586\n",
      "Training NF2:  32%|█████▍           | 3235/10001 [1:38:05<3:55:01,  2.08s/batch]Batch 3200/10001 Done, mean position loss: 20.866395728588103\n",
      "Training NF2:  32%|█████▌           | 3247/10001 [1:38:09<3:25:20,  1.82s/batch]Batch 3200/10001 Done, mean position loss: 20.41030163526535\n",
      "Training NF2:  32%|█████▍           | 3211/10001 [1:38:23<3:00:57,  1.60s/batch]Batch 3200/10001 Done, mean position loss: 20.685566444396972\n",
      "Training NF2:  33%|█████▌           | 3279/10001 [1:38:30<3:19:02,  1.78s/batch]Batch 3200/10001 Done, mean position loss: 20.69424526453018\n",
      "Training NF2:  32%|█████▌           | 3245/10001 [1:38:51<3:48:08,  2.03s/batch]Batch 3300/10001 Done, mean position loss: 21.151974215507508\n",
      "Training NF2:  32%|█████▍           | 3228/10001 [1:38:57<3:39:46,  1.95s/batch]Batch 3300/10001 Done, mean position loss: 20.424859664440156\n",
      "Training NF2:  32%|█████▍           | 3232/10001 [1:39:03<3:24:37,  1.81s/batch]Batch 3300/10001 Done, mean position loss: 20.6778480052948\n",
      "Training NF2:  33%|█████▌           | 3267/10001 [1:39:10<3:46:29,  2.02s/batch]Batch 3300/10001 Done, mean position loss: 20.524623377323152\n",
      "Training NF2:  33%|█████▌           | 3259/10001 [1:39:11<3:45:35,  2.01s/batch]Batch 3300/10001 Done, mean position loss: 20.686402945518495\n",
      "Training NF2:  33%|█████▌           | 3280/10001 [1:39:21<3:30:11,  1.88s/batch]Batch 3300/10001 Done, mean position loss: 20.57455770492554\n",
      "Training NF2:  33%|█████▌           | 3307/10001 [1:39:21<3:22:26,  1.81s/batch]Batch 3300/10001 Done, mean position loss: 20.618301696777344\n",
      "Training NF2:  33%|█████▌           | 3290/10001 [1:39:23<3:41:20,  1.98s/batch]Batch 3300/10001 Done, mean position loss: 20.994718296527864\n",
      "Training NF2:  33%|█████▌           | 3304/10001 [1:39:26<3:11:59,  1.72s/batch]Batch 3300/10001 Done, mean position loss: 20.374353618621825\n",
      "Training NF2:  32%|█████▌           | 3247/10001 [1:39:31<3:37:53,  1.94s/batch]Batch 3300/10001 Done, mean position loss: 21.52268562555313\n",
      "Training NF2:  33%|█████▌           | 3273/10001 [1:39:38<3:24:28,  1.82s/batch]Batch 3300/10001 Done, mean position loss: 20.98381377696991\n",
      "Training NF2:  33%|█████▌           | 3288/10001 [1:39:40<3:16:26,  1.76s/batch]Batch 3300/10001 Done, mean position loss: 20.597517940998074\n",
      "Training NF2:  33%|█████▋           | 3319/10001 [1:39:43<3:17:18,  1.77s/batch]Batch 3300/10001 Done, mean position loss: 20.650750782489776\n",
      "Training NF2:  33%|█████▌           | 3309/10001 [1:39:45<3:21:13,  1.80s/batch]Batch 3300/10001 Done, mean position loss: 20.417598667144773\n",
      "Training NF2:  33%|█████▋           | 3321/10001 [1:39:57<3:12:42,  1.73s/batch]Batch 3300/10001 Done, mean position loss: 20.59436904668808\n",
      "Training NF2:  33%|█████▌           | 3283/10001 [1:39:59<3:58:03,  2.13s/batch]Batch 3300/10001 Done, mean position loss: 20.74411257982254\n",
      "Training NF2:  32%|█████▌           | 3247/10001 [1:40:00<4:12:16,  2.24s/batch]Batch 3300/10001 Done, mean position loss: 21.158833417892456\n",
      "Training NF2:  32%|█████▌           | 3250/10001 [1:40:06<3:56:37,  2.10s/batch]Batch 3300/10001 Done, mean position loss: 20.728578815460207\n",
      "Training NF2:  33%|█████▌           | 3274/10001 [1:40:06<4:16:19,  2.29s/batch]Batch 3300/10001 Done, mean position loss: 20.758904836177827\n",
      "Training NF2:  33%|█████▋           | 3326/10001 [1:40:07<3:42:44,  2.00s/batch]Batch 3300/10001 Done, mean position loss: 20.58249237060547\n",
      "Training NF2:  33%|█████▋           | 3335/10001 [1:40:09<2:43:53,  1.48s/batch]Batch 3300/10001 Done, mean position loss: 20.70882581949234\n",
      "Training NF2:  33%|█████▌           | 3266/10001 [1:40:10<3:12:17,  1.71s/batch]Batch 3300/10001 Done, mean position loss: 20.45774031162262\n",
      "Training NF2:  33%|█████▋           | 3321/10001 [1:40:15<3:26:46,  1.86s/batch]Batch 3300/10001 Done, mean position loss: 20.62846618413925\n",
      "Training NF2:  33%|█████▋           | 3338/10001 [1:40:19<3:01:15,  1.63s/batch]Batch 3300/10001 Done, mean position loss: 20.623829629421234\n",
      "Training NF2:  33%|█████▋           | 3342/10001 [1:40:21<2:48:32,  1.52s/batch]Batch 3300/10001 Done, mean position loss: 20.51829770565033\n",
      "Training NF2:  33%|█████▌           | 3296/10001 [1:40:22<3:43:57,  2.00s/batch]Batch 3300/10001 Done, mean position loss: 20.48876679897308\n",
      "Training NF2:  33%|█████▌           | 3278/10001 [1:40:29<3:12:02,  1.71s/batch]Batch 3300/10001 Done, mean position loss: 20.88214909553528\n",
      "Training NF2:  34%|█████▋           | 3355/10001 [1:40:30<3:18:10,  1.79s/batch]Batch 3300/10001 Done, mean position loss: 20.745326266288757\n",
      "Training NF2:  33%|█████▌           | 3264/10001 [1:40:33<3:49:41,  2.05s/batch]Batch 3300/10001 Done, mean position loss: 20.92848818540573\n",
      "Training NF2:  33%|█████▋           | 3315/10001 [1:40:34<3:31:42,  1.90s/batch]Batch 3300/10001 Done, mean position loss: 20.886664934158325\n",
      "Training NF2:  33%|█████▋           | 3343/10001 [1:40:38<4:13:57,  2.29s/batch]Batch 3300/10001 Done, mean position loss: 21.009315264225005\n",
      "Training NF2:  33%|█████▋           | 3342/10001 [1:40:51<3:12:59,  1.74s/batch]Batch 3300/10001 Done, mean position loss: 20.79344945907593\n",
      "Training NF2:  33%|█████▌           | 3274/10001 [1:40:53<3:56:13,  2.11s/batch]Batch 3300/10001 Done, mean position loss: 20.72316573143005\n",
      "Training NF2:  33%|█████▌           | 3293/10001 [1:40:57<3:20:26,  1.79s/batch]Batch 3300/10001 Done, mean position loss: 21.053002729415894\n",
      "Training NF2:  33%|█████▋           | 3334/10001 [1:41:03<3:55:02,  2.12s/batch]Batch 3300/10001 Done, mean position loss: 20.33101095199585\n",
      "Training NF2:  33%|█████▋           | 3332/10001 [1:41:12<3:31:01,  1.90s/batch]Batch 3300/10001 Done, mean position loss: 20.405076873302463\n",
      "Batch 3300/10001 Done, mean position loss: 20.867635571956633\n",
      "Training NF2:  33%|█████▋           | 3323/10001 [1:41:13<3:24:26,  1.84s/batch]Batch 3300/10001 Done, mean position loss: 20.62713873386383\n",
      "Training NF2:  34%|█████▋           | 3377/10001 [1:41:30<3:39:07,  1.98s/batch]Batch 3300/10001 Done, mean position loss: 20.669914791584013\n",
      "Training NF2:  34%|█████▊           | 3386/10001 [1:41:47<3:18:13,  1.80s/batch]Batch 3300/10001 Done, mean position loss: 20.694333846569062\n",
      "Training NF2:  34%|█████▋           | 3369/10001 [1:41:55<3:11:22,  1.73s/batch]Batch 3400/10001 Done, mean position loss: 21.155468027591702\n",
      "Training NF2:  34%|█████▋           | 3369/10001 [1:42:07<3:16:38,  1.78s/batch]Batch 3400/10001 Done, mean position loss: 20.40247895002365\n",
      "Training NF2:  33%|█████▋           | 3336/10001 [1:42:09<3:09:36,  1.71s/batch]Batch 3400/10001 Done, mean position loss: 20.503287692070007\n",
      "Training NF2:  34%|█████▋           | 3359/10001 [1:42:13<3:26:47,  1.87s/batch]Batch 3400/10001 Done, mean position loss: 20.67296482563019\n",
      "Training NF2:  33%|█████▋           | 3337/10001 [1:42:16<2:48:28,  1.52s/batch]Batch 3400/10001 Done, mean position loss: 20.681056008338928\n",
      "Training NF2:  33%|█████▋           | 3346/10001 [1:42:24<3:16:30,  1.77s/batch]Batch 3400/10001 Done, mean position loss: 20.379919176101687\n",
      "Training NF2:  34%|█████▋           | 3363/10001 [1:42:25<3:18:22,  1.79s/batch]Batch 3400/10001 Done, mean position loss: 20.630163111686706\n",
      "Training NF2:  34%|█████▋           | 3361/10001 [1:42:26<2:58:38,  1.61s/batch]Batch 3400/10001 Done, mean position loss: 20.578574681282042\n",
      "Training NF2:  34%|█████▊           | 3390/10001 [1:42:33<3:33:15,  1.94s/batch]Batch 3400/10001 Done, mean position loss: 21.02880515098572\n",
      "Training NF2:  34%|█████▋           | 3368/10001 [1:42:38<3:01:40,  1.64s/batch]Batch 3400/10001 Done, mean position loss: 21.499514262676236\n",
      "Training NF2:  34%|█████▋           | 3375/10001 [1:42:40<3:08:51,  1.71s/batch]Batch 3400/10001 Done, mean position loss: 20.976379570961\n",
      "Training NF2:  34%|█████▊           | 3384/10001 [1:42:51<3:37:27,  1.97s/batch]Batch 3400/10001 Done, mean position loss: 20.757980036735535\n",
      "Training NF2:  34%|█████▋           | 3352/10001 [1:42:51<3:30:12,  1.90s/batch]Batch 3400/10001 Done, mean position loss: 20.422838261127474\n",
      "Training NF2:  34%|█████▋           | 3356/10001 [1:42:51<3:47:23,  2.05s/batch]Batch 3400/10001 Done, mean position loss: 20.58662502527237\n",
      "Training NF2:  34%|█████▋           | 3375/10001 [1:42:53<3:32:36,  1.93s/batch]Batch 3400/10001 Done, mean position loss: 20.65480487585068\n",
      "Training NF2:  33%|█████▋           | 3340/10001 [1:43:02<2:47:36,  1.51s/batch]Batch 3400/10001 Done, mean position loss: 20.753550357818604\n",
      "Training NF2:  34%|█████▋           | 3374/10001 [1:43:09<3:30:04,  1.90s/batch]Batch 3400/10001 Done, mean position loss: 20.58637915849686\n",
      "Training NF2:  33%|█████▋           | 3344/10001 [1:43:09<3:14:48,  1.76s/batch]Batch 3400/10001 Done, mean position loss: 21.12706314086914\n",
      "Training NF2:  34%|█████▊           | 3400/10001 [1:43:13<3:45:37,  2.05s/batch]Batch 3400/10001 Done, mean position loss: 20.578512258529663\n",
      "Training NF2:  34%|█████▊           | 3443/10001 [1:43:14<3:21:20,  1.84s/batch]Batch 3400/10001 Done, mean position loss: 20.69509762763977\n",
      "Training NF2:  34%|█████▊           | 3415/10001 [1:43:18<3:23:57,  1.86s/batch]Batch 3400/10001 Done, mean position loss: 20.531772401332855\n",
      "Training NF2:  34%|█████▊           | 3427/10001 [1:43:19<3:12:33,  1.76s/batch]Batch 3400/10001 Done, mean position loss: 20.695513105392457\n",
      "Training NF2:  34%|█████▋           | 3372/10001 [1:43:21<3:23:14,  1.84s/batch]Batch 3400/10001 Done, mean position loss: 20.435764796733856\n",
      "Training NF2:  34%|█████▊           | 3391/10001 [1:43:22<2:59:50,  1.63s/batch]Batch 3400/10001 Done, mean position loss: 20.58882801055908\n",
      "Training NF2:  34%|█████▋           | 3377/10001 [1:43:23<3:03:21,  1.66s/batch]Batch 3400/10001 Done, mean position loss: 20.88626878976822\n",
      "Training NF2:  34%|█████▊           | 3407/10001 [1:43:28<3:06:28,  1.70s/batch]Batch 3400/10001 Done, mean position loss: 20.486900010108947\n",
      "Training NF2:  34%|█████▋           | 3378/10001 [1:43:32<3:19:28,  1.81s/batch]Batch 3400/10001 Done, mean position loss: 20.612869980335233\n",
      "Training NF2:  34%|█████▋           | 3381/10001 [1:43:34<3:40:55,  2.00s/batch]Batch 3400/10001 Done, mean position loss: 20.73905247449875\n",
      "Training NF2:  34%|█████▋           | 3376/10001 [1:43:38<3:39:13,  1.99s/batch]Batch 3400/10001 Done, mean position loss: 21.00936831712723\n",
      "Training NF2:  34%|█████▊           | 3449/10001 [1:43:40<3:28:55,  1.91s/batch]Batch 3400/10001 Done, mean position loss: 20.92266858577728\n",
      "Training NF2:  34%|█████▊           | 3392/10001 [1:43:42<3:28:24,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.868930354118348\n",
      "Training NF2:  34%|█████▊           | 3392/10001 [1:43:54<3:41:40,  2.01s/batch]Batch 3400/10001 Done, mean position loss: 20.784256920814514\n",
      "Training NF2:  34%|█████▊           | 3386/10001 [1:43:58<3:28:03,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.718000090122224\n",
      "Training NF2:  34%|█████▊           | 3429/10001 [1:44:11<3:04:40,  1.69s/batch]Batch 3400/10001 Done, mean position loss: 20.33719049692154\n",
      "Training NF2:  34%|█████▊           | 3403/10001 [1:44:14<3:20:20,  1.82s/batch]Batch 3400/10001 Done, mean position loss: 20.87199163913727\n",
      "Training NF2:  34%|█████▊           | 3423/10001 [1:44:15<3:28:52,  1.91s/batch]Batch 3400/10001 Done, mean position loss: 21.022780957221983\n",
      "Training NF2:  35%|█████▉           | 3471/10001 [1:44:17<3:19:45,  1.84s/batch]Batch 3400/10001 Done, mean position loss: 20.378583579063417\n",
      "Training NF2:  34%|█████▊           | 3433/10001 [1:44:24<3:27:07,  1.89s/batch]Batch 3400/10001 Done, mean position loss: 20.62284558773041\n",
      "Training NF2:  34%|█████▊           | 3415/10001 [1:44:34<3:02:20,  1.66s/batch]Batch 3400/10001 Done, mean position loss: 20.649301273822786\n",
      "Training NF2:  34%|█████▊           | 3411/10001 [1:44:51<2:51:57,  1.57s/batch]Batch 3400/10001 Done, mean position loss: 20.680854806900026\n",
      "Training NF2:  34%|█████▊           | 3430/10001 [1:45:03<3:30:27,  1.92s/batch]Batch 3500/10001 Done, mean position loss: 21.15089151620865\n",
      "Training NF2:  35%|█████▉           | 3498/10001 [1:45:12<3:02:00,  1.68s/batch]Batch 3500/10001 Done, mean position loss: 20.401158707141875\n",
      "Training NF2:  35%|█████▊           | 3455/10001 [1:45:13<3:27:55,  1.91s/batch]Batch 3500/10001 Done, mean position loss: 20.528677423000335\n",
      "Training NF2:  35%|█████▉           | 3466/10001 [1:45:16<3:18:39,  1.82s/batch]Batch 3500/10001 Done, mean position loss: 20.681140697002412\n",
      "Training NF2:  35%|█████▉           | 3463/10001 [1:45:19<3:15:24,  1.79s/batch]Batch 3500/10001 Done, mean position loss: 20.369932060241702\n",
      "Training NF2:  35%|█████▉           | 3505/10001 [1:45:19<3:23:09,  1.88s/batch]Batch 3500/10001 Done, mean position loss: 20.67751327514648\n",
      "Training NF2:  34%|█████▊           | 3440/10001 [1:45:28<2:39:30,  1.46s/batch]Batch 3500/10001 Done, mean position loss: 20.611336200237275\n",
      "Training NF2:  35%|█████▊           | 3453/10001 [1:45:31<2:48:43,  1.55s/batch]Batch 3500/10001 Done, mean position loss: 21.04763306617737\n",
      "Training NF2:  35%|█████▉           | 3489/10001 [1:45:34<2:59:34,  1.65s/batch]Batch 3500/10001 Done, mean position loss: 20.558919067382813\n",
      "Training NF2:  34%|█████▊           | 3444/10001 [1:45:39<3:36:22,  1.98s/batch]Batch 3500/10001 Done, mean position loss: 20.975858173370362\n",
      "Training NF2:  35%|█████▉           | 3464/10001 [1:45:42<3:13:36,  1.78s/batch]Batch 3500/10001 Done, mean position loss: 21.509758820533754\n",
      "Training NF2:  35%|█████▊           | 3455/10001 [1:45:53<3:18:13,  1.82s/batch]Batch 3500/10001 Done, mean position loss: 20.74547861099243\n",
      "Training NF2:  35%|█████▉           | 3499/10001 [1:45:56<4:02:35,  2.24s/batch]Batch 3500/10001 Done, mean position loss: 20.414000244140624\n",
      "Training NF2:  34%|█████▊           | 3447/10001 [1:46:00<3:50:01,  2.11s/batch]Batch 3500/10001 Done, mean position loss: 20.63339781284332\n",
      "Training NF2:  35%|█████▉           | 3498/10001 [1:46:06<3:03:11,  1.69s/batch]Batch 3500/10001 Done, mean position loss: 20.582701930999754\n",
      "Training NF2:  35%|█████▉           | 3459/10001 [1:46:06<2:45:43,  1.52s/batch]Batch 3500/10001 Done, mean position loss: 20.747417500019075\n",
      "Training NF2:  35%|█████▉           | 3485/10001 [1:46:12<3:25:54,  1.90s/batch]Batch 3500/10001 Done, mean position loss: 21.122133359909057\n",
      "Training NF2:  35%|█████▉           | 3498/10001 [1:46:13<2:54:11,  1.61s/batch]Batch 3500/10001 Done, mean position loss: 20.687056691646575\n",
      "Training NF2:  35%|██████           | 3530/10001 [1:46:13<2:44:05,  1.52s/batch]Batch 3500/10001 Done, mean position loss: 20.57723448753357\n",
      "Training NF2:  35%|█████▉           | 3460/10001 [1:46:19<3:54:17,  2.15s/batch]Batch 3500/10001 Done, mean position loss: 20.655261912345885\n",
      "Training NF2:  35%|█████▉           | 3496/10001 [1:46:19<4:05:38,  2.27s/batch]Batch 3500/10001 Done, mean position loss: 20.520543942451475\n",
      "Training NF2:  35%|█████▉           | 3473/10001 [1:46:19<3:04:45,  1.70s/batch]Batch 3500/10001 Done, mean position loss: 20.57509538173676\n",
      "Training NF2:  35%|█████▉           | 3511/10001 [1:46:25<3:30:04,  1.94s/batch]Batch 3500/10001 Done, mean position loss: 20.486777131557464\n",
      "Training NF2:  35%|██████           | 3542/10001 [1:46:25<2:41:10,  1.50s/batch]Batch 3500/10001 Done, mean position loss: 20.438887977600096\n",
      "Training NF2:  35%|█████▉           | 3465/10001 [1:46:29<3:43:29,  2.05s/batch]Batch 3500/10001 Done, mean position loss: 20.870581119060517\n",
      "Training NF2:  35%|█████▉           | 3511/10001 [1:46:32<3:23:57,  1.89s/batch]Batch 3500/10001 Done, mean position loss: 20.580745446681973\n",
      "Training NF2:  35%|█████▉           | 3502/10001 [1:46:34<3:13:04,  1.78s/batch]Batch 3500/10001 Done, mean position loss: 20.73832352399826\n",
      "Training NF2:  35%|█████▉           | 3506/10001 [1:46:34<3:14:02,  1.79s/batch]Batch 3500/10001 Done, mean position loss: 20.58004310369492\n",
      "Training NF2:  35%|█████▉           | 3513/10001 [1:46:41<2:52:50,  1.60s/batch]Batch 3500/10001 Done, mean position loss: 20.988296542167667\n",
      "Training NF2:  35%|█████▉           | 3510/10001 [1:46:49<3:20:16,  1.85s/batch]Batch 3500/10001 Done, mean position loss: 20.935550134181977\n",
      "Training NF2:  35%|█████▉           | 3511/10001 [1:46:51<2:59:20,  1.66s/batch]Batch 3500/10001 Done, mean position loss: 20.89116323709488\n",
      "Training NF2:  35%|██████           | 3535/10001 [1:46:52<2:49:32,  1.57s/batch]Batch 3500/10001 Done, mean position loss: 20.77836812019348\n",
      "Training NF2:  35%|█████▉           | 3523/10001 [1:47:03<2:52:59,  1.60s/batch]Batch 3500/10001 Done, mean position loss: 20.721712439060212\n",
      "Training NF2:  35%|█████▉           | 3525/10001 [1:47:07<3:10:15,  1.76s/batch]Batch 3500/10001 Done, mean position loss: 20.33758508682251\n",
      "Training NF2:  36%|██████           | 3570/10001 [1:47:19<2:58:09,  1.66s/batch]Batch 3500/10001 Done, mean position loss: 21.029937496185305\n",
      "Batch 3500/10001 Done, mean position loss: 20.836346611976623\n",
      "Training NF2:  35%|██████           | 3539/10001 [1:47:23<2:55:19,  1.63s/batch]Batch 3500/10001 Done, mean position loss: 20.406249074935914\n",
      "Training NF2:  35%|██████           | 3533/10001 [1:47:36<2:45:09,  1.53s/batch]Batch 3500/10001 Done, mean position loss: 20.615562467575074\n",
      "Training NF2:  35%|█████▉           | 3522/10001 [1:47:45<3:24:39,  1.90s/batch]Batch 3500/10001 Done, mean position loss: 20.656941866874696\n",
      "Training NF2:  35%|██████           | 3531/10001 [1:47:55<2:34:19,  1.43s/batch]Batch 3500/10001 Done, mean position loss: 20.689414608478547\n",
      "Training NF2:  36%|██████           | 3558/10001 [1:48:07<3:10:38,  1.78s/batch]Batch 3600/10001 Done, mean position loss: 21.149322597980497\n",
      "Training NF2:  36%|██████           | 3556/10001 [1:48:16<3:21:05,  1.87s/batch]Batch 3600/10001 Done, mean position loss: 20.6813178396225\n",
      "Training NF2:  36%|██████           | 3596/10001 [1:48:17<3:32:21,  1.99s/batch]Batch 3600/10001 Done, mean position loss: 20.39307501077652\n",
      "Training NF2:  35%|██████           | 3549/10001 [1:48:19<3:44:08,  2.08s/batch]Batch 3600/10001 Done, mean position loss: 20.504651250839235\n",
      "Training NF2:  36%|██████           | 3577/10001 [1:48:23<3:26:43,  1.93s/batch]Batch 3600/10001 Done, mean position loss: 20.695923111438752\n",
      "Training NF2:  36%|██████▏          | 3607/10001 [1:48:25<3:01:51,  1.71s/batch]Batch 3600/10001 Done, mean position loss: 20.3835797333717361:42,  2.06s/batch]\n",
      "Training NF2:  36%|██████           | 3578/10001 [1:48:34<4:16:20,  2.39s/batch]Batch 3600/10001 Done, mean position loss: 21.078743765354155\n",
      "Training NF2:  36%|██████           | 3567/10001 [1:48:35<3:52:52,  2.17s/batch]Batch 3600/10001 Done, mean position loss: 20.554853496551516\n",
      "Training NF2:  36%|██████           | 3565/10001 [1:48:36<4:00:13,  2.24s/batch]Batch 3600/10001 Done, mean position loss: 20.604123458862304\n",
      "Training NF2:  36%|██████▏          | 3612/10001 [1:48:42<3:53:32,  2.19s/batch]Batch 3600/10001 Done, mean position loss: 20.749147534370422\n",
      "\n",
      "Training NF2:  36%|██████▏          | 3609/10001 [1:48:50<3:50:33,  2.16s/batch]Batch 3600/10001 Done, mean position loss: 21.487777750492096\n",
      "Training NF2:  36%|██████▏          | 3617/10001 [1:49:01<2:31:29,  1.42s/batch]Batch 3600/10001 Done, mean position loss: 20.393723828792574\n",
      "Training NF2:  36%|██████▏          | 3615/10001 [1:49:05<2:36:18,  1.47s/batch]Batch 3600/10001 Done, mean position loss: 21.13628523826599\n",
      "Training NF2:  36%|██████           | 3597/10001 [1:49:07<3:11:20,  1.79s/batch]Batch 3600/10001 Done, mean position loss: 20.617049190998078\n",
      "Batch 3600/10001 Done, mean position loss: 20.752592499256135\n",
      "Training NF2:  36%|██████           | 3571/10001 [1:49:11<3:36:42,  2.02s/batch]Batch 3600/10001 Done, mean position loss: 20.576094191074368\n",
      "Training NF2:  36%|██████▏          | 3618/10001 [1:49:14<3:01:16,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 20.523791875839233\n",
      "Training NF2:  36%|██████           | 3599/10001 [1:49:18<3:25:36,  1.93s/batch]Batch 3600/10001 Done, mean position loss: 20.685533013343814\n",
      "Training NF2:  36%|██████▏          | 3610/10001 [1:49:22<3:08:46,  1.77s/batch]Batch 3600/10001 Done, mean position loss: 20.56696996688843\n",
      "Training NF2:  36%|██████           | 3599/10001 [1:49:23<3:07:42,  1.76s/batch]Batch 3600/10001 Done, mean position loss: 20.450145194530485\n",
      "Training NF2:  36%|██████▏          | 3611/10001 [1:49:23<2:59:05,  1.68s/batch]Batch 3600/10001 Done, mean position loss: 20.65832822561264\n",
      "Training NF2:  36%|██████▏          | 3635/10001 [1:49:24<3:22:27,  1.91s/batch]Batch 3600/10001 Done, mean position loss: 20.5656924700737\n",
      "Training NF2:  36%|██████▏          | 3641/10001 [1:49:26<3:08:27,  1.78s/batch]Batch 3600/10001 Done, mean position loss: 20.494511983394624\n",
      "Training NF2:  36%|██████▏          | 3604/10001 [1:49:31<2:55:00,  1.64s/batch]Batch 3600/10001 Done, mean position loss: 20.853967270851133\n",
      "Training NF2:  36%|██████           | 3574/10001 [1:49:33<2:51:14,  1.60s/batch]Batch 3600/10001 Done, mean position loss: 20.58131662607193\n",
      "Training NF2:  36%|██████▏          | 3616/10001 [1:49:34<3:41:12,  2.08s/batch]Batch 3600/10001 Done, mean position loss: 20.592934594154357\n",
      "Training NF2:  36%|██████▏          | 3607/10001 [1:49:34<3:38:19,  2.05s/batch]Batch 3600/10001 Done, mean position loss: 21.005104148387908\n",
      "Training NF2:  36%|██████▏          | 3650/10001 [1:49:44<3:33:10,  2.01s/batch]Batch 3600/10001 Done, mean position loss: 20.719051344394686\n",
      "Training NF2:  36%|██████▏          | 3646/10001 [1:49:52<3:00:32,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 20.892657234668732\n",
      "Training NF2:  36%|██████           | 3566/10001 [1:49:55<3:17:24,  1.84s/batch]Batch 3600/10001 Done, mean position loss: 20.952415747642515\n",
      "Training NF2:  36%|██████           | 3576/10001 [1:49:57<3:01:42,  1.70s/batch]Batch 3600/10001 Done, mean position loss: 20.796055800914765\n",
      "Training NF2:  36%|██████▏          | 3629/10001 [1:50:08<3:02:46,  1.72s/batch]Batch 3600/10001 Done, mean position loss: 20.727390944957733\n",
      "Training NF2:  36%|██████▏          | 3638/10001 [1:50:08<3:20:06,  1.89s/batch]Batch 3600/10001 Done, mean position loss: 20.333496367931367\n",
      "Training NF2:  37%|██████▏          | 3663/10001 [1:50:23<2:43:16,  1.55s/batch]Batch 3600/10001 Done, mean position loss: 20.988259742259977\n",
      "Training NF2:  37%|██████▏          | 3672/10001 [1:50:29<3:03:58,  1.74s/batch]Batch 3600/10001 Done, mean position loss: 20.83801590681076\n",
      "Training NF2:  36%|██████▏          | 3622/10001 [1:50:31<3:21:54,  1.90s/batch]Batch 3600/10001 Done, mean position loss: 20.385448241233824\n",
      "Training NF2:  37%|██████▏          | 3670/10001 [1:50:35<2:55:57,  1.67s/batch]Batch 3600/10001 Done, mean position loss: 20.608060870170593\n",
      "Training NF2:  36%|██████▏          | 3637/10001 [1:50:41<3:39:59,  2.07s/batch]Batch 3600/10001 Done, mean position loss: 20.650707831382753\n",
      "Training NF2:  36%|██████▏          | 3621/10001 [1:50:58<3:17:57,  1.86s/batch]Batch 3600/10001 Done, mean position loss: 20.67830608844757\n",
      "Training NF2:  37%|██████▎          | 3679/10001 [1:51:03<2:46:29,  1.58s/batch]Batch 3700/10001 Done, mean position loss: 21.148072469234464\n",
      "Training NF2:  37%|██████▏          | 3665/10001 [1:51:11<2:30:02,  1.42s/batch]Batch 3700/10001 Done, mean position loss: 20.667260065078736\n",
      "Training NF2:  37%|██████▎          | 3685/10001 [1:51:12<3:00:57,  1.72s/batch]Batch 3700/10001 Done, mean position loss: 20.41470988035202\n",
      "Training NF2:  37%|██████▏          | 3660/10001 [1:51:21<3:11:07,  1.81s/batch]Batch 3700/10001 Done, mean position loss: 20.500195667743682\n",
      "Training NF2:  37%|██████▏          | 3664/10001 [1:51:22<2:32:46,  1.45s/batch]Batch 3700/10001 Done, mean position loss: 20.38879061937332\n",
      "Training NF2:  37%|██████▏          | 3668/10001 [1:51:24<2:48:59,  1.60s/batch]Batch 3700/10001 Done, mean position loss: 20.713858237266543\n",
      "Training NF2:  36%|██████▏          | 3648/10001 [1:51:30<3:02:56,  1.73s/batch]Batch 3700/10001 Done, mean position loss: 20.553620569705963\n",
      "Training NF2:  36%|██████▏          | 3620/10001 [1:51:33<3:35:19,  2.02s/batch]Batch 3700/10001 Done, mean position loss: 20.614046037197113\n",
      "Training NF2:  37%|██████▏          | 3658/10001 [1:51:36<2:36:03,  1.48s/batch]Batch 3700/10001 Done, mean position loss: 20.931394085884094\n",
      "Training NF2:  36%|██████▏          | 3648/10001 [1:51:36<3:34:05,  2.02s/batch]Batch 3700/10001 Done, mean position loss: 21.01694408893585\n",
      "Training NF2:  37%|██████▎          | 3691/10001 [1:51:39<3:22:18,  1.92s/batch]Batch 3700/10001 Done, mean position loss: 20.745315036773682\n",
      "Training NF2:  37%|██████▎          | 3683/10001 [1:51:41<2:52:38,  1.64s/batch]Batch 3700/10001 Done, mean position loss: 21.492267193794248\n",
      "Training NF2:  37%|██████▏          | 3665/10001 [1:51:56<2:51:11,  1.62s/batch]Batch 3700/10001 Done, mean position loss: 20.392797226905824\n",
      "Training NF2:  36%|██████▏          | 3649/10001 [1:52:03<3:21:35,  1.90s/batch]Batch 3700/10001 Done, mean position loss: 21.13061862707138\n",
      "Training NF2:  37%|██████▎          | 3719/10001 [1:52:05<3:07:22,  1.79s/batch]Batch 3700/10001 Done, mean position loss: 20.6198113656044\n",
      "Training NF2:  37%|██████▎          | 3698/10001 [1:52:10<3:10:49,  1.82s/batch]Batch 3700/10001 Done, mean position loss: 20.740518786907195\n",
      "Training NF2:  37%|██████▏          | 3674/10001 [1:52:13<3:33:08,  2.02s/batch]Batch 3700/10001 Done, mean position loss: 20.524301614761356\n",
      "Training NF2:  37%|██████▎          | 3724/10001 [1:52:15<2:52:03,  1.64s/batch]Batch 3700/10001 Done, mean position loss: 20.581076517105103\n",
      "Training NF2:  37%|██████▎          | 3695/10001 [1:52:17<3:00:13,  1.71s/batch]Batch 3700/10001 Done, mean position loss: 20.573040196895597\n",
      "Training NF2:  37%|██████▎          | 3733/10001 [1:52:19<2:52:18,  1.65s/batch]Batch 3700/10001 Done, mean position loss: 20.427495110034943\n",
      "Training NF2:  37%|██████▎          | 3725/10001 [1:52:23<2:44:26,  1.57s/batch]Batch 3700/10001 Done, mean position loss: 20.703395838737485\n",
      "Training NF2:  37%|██████▎          | 3689/10001 [1:52:23<3:11:35,  1.82s/batch]Batch 3700/10001 Done, mean position loss: 20.573423800468444\n",
      "Training NF2:  37%|██████▎          | 3713/10001 [1:52:25<2:36:50,  1.50s/batch]Batch 3700/10001 Done, mean position loss: 20.477715144157408\n",
      "Training NF2:  37%|██████▎          | 3706/10001 [1:52:25<3:18:30,  1.89s/batch]Batch 3700/10001 Done, mean position loss: 20.629409778118134\n",
      "Training NF2:  37%|██████▎          | 3679/10001 [1:52:27<3:01:53,  1.73s/batch]Batch 3700/10001 Done, mean position loss: 20.878753788471222\n",
      "Training NF2:  37%|██████▎          | 3711/10001 [1:52:27<2:58:47,  1.71s/batch]Batch 3700/10001 Done, mean position loss: 20.60944723367691\n",
      "Training NF2:  37%|██████▎          | 3716/10001 [1:52:31<3:18:14,  1.89s/batch]Batch 3700/10001 Done, mean position loss: 20.995140433311462\n",
      "Training NF2:  37%|██████▎          | 3746/10001 [1:52:32<2:47:55,  1.61s/batch]Batch 3700/10001 Done, mean position loss: 20.588092415332795\n",
      "Training NF2:  37%|██████▎          | 3697/10001 [1:52:43<3:07:01,  1.78s/batch]Batch 3700/10001 Done, mean position loss: 20.736225583553313\n",
      "Training NF2:  37%|██████▎          | 3706/10001 [1:52:52<3:11:44,  1.83s/batch]Batch 3700/10001 Done, mean position loss: 20.932152280807493\n",
      "Training NF2:  37%|██████▎          | 3684/10001 [1:52:53<3:12:15,  1.83s/batch]Batch 3700/10001 Done, mean position loss: 20.885100636482242\n",
      "Training NF2:  37%|██████▎          | 3716/10001 [1:52:59<3:17:30,  1.89s/batch]Batch 3700/10001 Done, mean position loss: 20.78527117967606\n",
      "Training NF2:  38%|██████▍          | 3759/10001 [1:53:05<2:49:19,  1.63s/batch]Batch 3700/10001 Done, mean position loss: 20.316378703117373\n",
      "Training NF2:  38%|██████▍          | 3769/10001 [1:53:07<2:42:18,  1.56s/batch]Batch 3700/10001 Done, mean position loss: 20.725575864315033\n",
      "Training NF2:  37%|██████▎          | 3738/10001 [1:53:22<3:02:28,  1.75s/batch]Batch 3700/10001 Done, mean position loss: 21.025462605953216\n",
      "Training NF2:  37%|██████▎          | 3733/10001 [1:53:26<3:25:34,  1.97s/batch]Batch 3700/10001 Done, mean position loss: 20.835919058322908\n",
      "Training NF2:  38%|██████▍          | 3783/10001 [1:53:32<2:50:16,  1.64s/batch]Batch 3700/10001 Done, mean position loss: 20.399718849658964\n",
      "Training NF2:  37%|██████▎          | 3748/10001 [1:53:34<3:13:06,  1.85s/batch]Batch 3700/10001 Done, mean position loss: 20.658968040943144\n",
      "Training NF2:  37%|██████▎          | 3736/10001 [1:53:38<3:42:52,  2.13s/batch]Batch 3700/10001 Done, mean position loss: 20.614863722324372\n",
      "Training NF2:  38%|██████▍          | 3763/10001 [1:53:58<3:48:41,  2.20s/batch]Batch 3800/10001 Done, mean position loss: 21.120486707687377\n",
      "Training NF2:  38%|██████▍          | 3788/10001 [1:53:59<3:57:00,  2.29s/batch]:14:09,  1.87s/batch]Batch 3700/10001 Done, mean position loss: 20.6924564743042\n",
      "Training NF2:  37%|██████▎          | 3738/10001 [1:54:04<4:18:52,  2.48s/batch]Batch 3800/10001 Done, mean position loss: 20.66813470363617\n",
      "Training NF2:  38%|██████▍          | 3756/10001 [1:54:08<2:38:01,  1.52s/batch]Batch 3800/10001 Done, mean position loss: 20.405876605510713\n",
      "Training NF2:  37%|██████▎          | 3745/10001 [1:54:19<2:59:12,  1.72s/batch]Batch 3800/10001 Done, mean position loss: 20.696106300354003\n",
      "Training NF2:  38%|██████▍          | 3766/10001 [1:54:20<2:52:43,  1.66s/batch]Batch 3800/10001 Done, mean position loss: 20.50598407745361\n",
      "Training NF2:  37%|██████▎          | 3737/10001 [1:54:24<2:38:13,  1.52s/batch]Batch 3800/10001 Done, mean position loss: 20.386452910900115\n",
      "Training NF2:  38%|██████▍          | 3801/10001 [1:54:24<3:20:24,  1.94s/batch]Batch 3800/10001 Done, mean position loss: 20.537311339378356\n",
      "Training NF2:  38%|██████▍          | 3814/10001 [1:54:33<3:43:28,  2.17s/batch]Batch 3800/10001 Done, mean position loss: 20.599266152381894\n",
      "Training NF2:  38%|██████▍          | 3754/10001 [1:54:35<3:42:52,  2.14s/batch]Batch 3800/10001 Done, mean position loss: 20.988562884330747\n",
      "Training NF2:  38%|██████▍          | 3768/10001 [1:54:38<3:36:15,  2.08s/batch]Batch 3800/10001 Done, mean position loss: 20.925780103206634\n",
      "Training NF2:  38%|██████▍          | 3777/10001 [1:54:41<3:02:22,  1.76s/batch]Batch 3800/10001 Done, mean position loss: 21.454755611419678\n",
      "Training NF2:  38%|██████▍          | 3779/10001 [1:54:42<3:28:17,  2.01s/batch]Batch 3800/10001 Done, mean position loss: 20.751771793365478\n",
      "Training NF2:  38%|██████▍          | 3809/10001 [1:54:57<3:33:10,  2.07s/batch]Batch 3800/10001 Done, mean position loss: 20.379930019378662\n",
      "Training NF2:  38%|██████▍          | 3795/10001 [1:55:08<3:18:09,  1.92s/batch]Batch 3800/10001 Done, mean position loss: 20.74071734189987\n",
      "Training NF2:  38%|██████▍          | 3788/10001 [1:55:08<3:15:12,  1.89s/batch]Batch 3800/10001 Done, mean position loss: 20.612087547779083\n",
      "Training NF2:  37%|██████▎          | 3739/10001 [1:55:09<3:05:13,  1.77s/batch]Batch 3800/10001 Done, mean position loss: 21.113142721652984\n",
      "Training NF2:  38%|██████▍          | 3790/10001 [1:55:12<3:04:39,  1.78s/batch]Batch 3800/10001 Done, mean position loss: 20.50795453310013\n",
      "Training NF2:  38%|██████▍          | 3819/10001 [1:55:18<3:48:12,  2.21s/batch]Batch 3800/10001 Done, mean position loss: 20.57790099143982\n",
      "Training NF2:  38%|██████▍          | 3800/10001 [1:55:19<2:41:43,  1.56s/batch]Batch 3800/10001 Done, mean position loss: 20.43435893058777\n",
      "Training NF2:  38%|██████▍          | 3799/10001 [1:55:21<2:54:09,  1.68s/batch]Batch 3800/10001 Done, mean position loss: 20.63272935152054\n",
      "Training NF2:  38%|██████▍          | 3779/10001 [1:55:24<3:50:34,  2.22s/batch]Batch 3800/10001 Done, mean position loss: 20.484163858890533\n",
      "Training NF2:  37%|██████▎          | 3747/10001 [1:55:24<3:18:03,  1.90s/batch]Batch 3800/10001 Done, mean position loss: 20.577843532562255\n",
      "Training NF2:  38%|██████▍          | 3794/10001 [1:55:25<3:30:41,  2.04s/batch]Batch 3800/10001 Done, mean position loss: 20.561103801727292\n",
      "Training NF2:  38%|██████▌          | 3831/10001 [1:55:33<3:20:30,  1.95s/batch]Batch 3800/10001 Done, mean position loss: 20.84867374181747\n",
      "Training NF2:  38%|██████▍          | 3814/10001 [1:55:33<3:54:43,  2.28s/batch]Batch 3800/10001 Done, mean position loss: 20.609700179100038\n",
      "Training NF2:  38%|██████▌          | 3827/10001 [1:55:33<3:32:52,  2.07s/batch]Batch 3800/10001 Done, mean position loss: 20.683626105785372\n",
      "Training NF2:  38%|██████▍          | 3791/10001 [1:55:35<3:18:20,  1.92s/batch]Batch 3800/10001 Done, mean position loss: 20.976254930496218\n",
      "Training NF2:  38%|██████▍          | 3817/10001 [1:55:39<3:16:28,  1.91s/batch]Batch 3800/10001 Done, mean position loss: 20.562549092769622\n",
      "Training NF2:  39%|██████▌          | 3858/10001 [1:55:43<3:01:13,  1.77s/batch]Batch 3800/10001 Done, mean position loss: 20.735442044734953\n",
      "Training NF2:  38%|██████▌          | 3827/10001 [1:55:55<2:35:08,  1.51s/batch]Batch 3800/10001 Done, mean position loss: 20.956438853740693\n",
      "Training NF2:  38%|██████▍          | 3818/10001 [1:56:04<3:04:09,  1.79s/batch]Batch 3800/10001 Done, mean position loss: 20.76908231973648\n",
      "Training NF2:  38%|██████▌          | 3836/10001 [1:56:04<3:16:59,  1.92s/batch]Batch 3800/10001 Done, mean position loss: 20.863965420722963\n",
      "Training NF2:  39%|██████▌          | 3851/10001 [1:56:10<3:16:49,  1.92s/batch]Batch 3800/10001 Done, mean position loss: 20.331806688308713\n",
      "Training NF2:  39%|██████▌          | 3874/10001 [1:56:14<3:09:45,  1.86s/batch]Batch 3800/10001 Done, mean position loss: 20.71720626592636\n",
      "Training NF2:  39%|██████▌          | 3870/10001 [1:56:27<2:46:16,  1.63s/batch]Batch 3800/10001 Done, mean position loss: 21.036215703487397\n",
      "Training NF2:  39%|██████▌          | 3869/10001 [1:56:28<2:56:43,  1.73s/batch]Batch 3800/10001 Done, mean position loss: 20.83060599088669\n",
      "Training NF2:  39%|██████▌          | 3866/10001 [1:56:36<3:21:43,  1.97s/batch]Batch 3800/10001 Done, mean position loss: 20.380133616924287\n",
      "Training NF2:  38%|██████▌          | 3835/10001 [1:56:40<3:15:35,  1.90s/batch]Batch 3800/10001 Done, mean position loss: 20.61509848356247\n",
      "Training NF2:  38%|██████▌          | 3841/10001 [1:56:47<3:33:21,  2.08s/batch]Batch 3800/10001 Done, mean position loss: 20.63690399646759\n",
      "Training NF2:  39%|██████▌          | 3855/10001 [1:57:01<3:38:54,  2.14s/batch]Batch 3900/10001 Done, mean position loss: 21.118962714672087\n",
      "Training NF2:  39%|██████▌          | 3889/10001 [1:57:05<3:34:10,  2.10s/batch]Batch 3800/10001 Done, mean position loss: 20.6808726978302\n",
      "Training NF2:  39%|██████▌          | 3893/10001 [1:57:13<3:13:19,  1.90s/batch]Batch 3900/10001 Done, mean position loss: 20.407286524772644\n",
      "Training NF2:  38%|██████▌          | 3840/10001 [1:57:14<2:47:09,  1.63s/batch]Batch 3900/10001 Done, mean position loss: 20.659591462612152\n",
      "Training NF2:  39%|██████▌          | 3896/10001 [1:57:19<3:04:31,  1.81s/batch]Batch 3900/10001 Done, mean position loss: 20.540244276523588\n",
      "Training NF2:  38%|██████▌          | 3848/10001 [1:57:24<3:14:37,  1.90s/batch]Batch 3900/10001 Done, mean position loss: 20.70208368062973\n",
      "Training NF2:  39%|██████▌          | 3892/10001 [1:57:28<3:05:24,  1.82s/batch]Batch 3900/10001 Done, mean position loss: 20.50261703968048\n",
      "Training NF2:  39%|██████▌          | 3872/10001 [1:57:31<2:43:30,  1.60s/batch]Batch 3900/10001 Done, mean position loss: 20.372655947208404\n",
      "Training NF2:  39%|██████▌          | 3852/10001 [1:57:35<2:33:31,  1.50s/batch]Batch 3900/10001 Done, mean position loss: 20.981210062503813\n",
      "Training NF2:  38%|██████▍          | 3820/10001 [1:57:45<4:16:19,  2.49s/batch]Batch 3900/10001 Done, mean position loss: 20.584324369430544\n",
      "Training NF2:  39%|██████▌          | 3875/10001 [1:57:45<2:36:22,  1.53s/batch]Batch 3900/10001 Done, mean position loss: 21.45792191028595\n",
      "Training NF2:  39%|██████▌          | 3878/10001 [1:57:45<2:32:23,  1.49s/batch]Batch 3900/10001 Done, mean position loss: 20.92705893278122\n",
      "Training NF2:  39%|██████▌          | 3885/10001 [1:57:51<3:30:13,  2.06s/batch]Batch 3900/10001 Done, mean position loss: 20.74794986486435\n",
      "Training NF2:  38%|██████▌          | 3832/10001 [1:58:07<3:19:50,  1.94s/batch]Batch 3900/10001 Done, mean position loss: 20.388521239757537\n",
      "Training NF2:  39%|██████▋          | 3903/10001 [1:58:11<3:37:21,  2.14s/batch]Batch 3900/10001 Done, mean position loss: 20.73667334794998\n",
      "Training NF2:  39%|██████▌          | 3867/10001 [1:58:18<2:56:27,  1.73s/batch]Batch 3900/10001 Done, mean position loss: 20.595131974220276\n",
      "Training NF2:  38%|██████▌          | 3840/10001 [1:58:22<3:06:21,  1.81s/batch]Batch 3900/10001 Done, mean position loss: 20.496910903453827\n",
      "Training NF2:  39%|██████▌          | 3864/10001 [1:58:23<2:43:02,  1.59s/batch]Batch 3900/10001 Done, mean position loss: 21.11100152492523\n",
      "Training NF2:  39%|██████▋          | 3931/10001 [1:58:27<2:57:21,  1.75s/batch]Batch 3900/10001 Done, mean position loss: 20.420249507427215\n",
      "Training NF2:  39%|██████▋          | 3932/10001 [1:58:29<3:33:20,  2.11s/batch]Batch 3900/10001 Done, mean position loss: 20.476919734477995\n",
      "Training NF2:  39%|██████▋          | 3937/10001 [1:58:29<2:43:04,  1.61s/batch]Batch 3900/10001 Done, mean position loss: 20.555220184326174\n",
      "Training NF2:  39%|██████▌          | 3860/10001 [1:58:30<2:58:04,  1.74s/batch]Batch 3900/10001 Done, mean position loss: 20.59415699481964\n",
      "Training NF2:  39%|██████▌          | 3893/10001 [1:58:32<3:16:37,  1.93s/batch]Batch 3900/10001 Done, mean position loss: 20.56879254579544\n",
      "Training NF2:  39%|██████▋          | 3900/10001 [1:58:36<3:17:59,  1.95s/batch]Batch 3900/10001 Done, mean position loss: 20.828577439785\n",
      "Training NF2:  39%|██████▋          | 3909/10001 [1:58:38<3:32:57,  2.10s/batch]Batch 3900/10001 Done, mean position loss: 20.974992978572843\n",
      "Training NF2:  39%|██████▋          | 3899/10001 [1:58:37<3:39:02,  2.15s/batch]Batch 3900/10001 Done, mean position loss: 20.595543863773344\n",
      "Training NF2:  39%|██████▋          | 3906/10001 [1:58:41<3:25:55,  2.03s/batch]Batch 3900/10001 Done, mean position loss: 20.568313663005828\n",
      "Training NF2:  39%|██████▋          | 3948/10001 [1:58:43<3:09:08,  1.87s/batch]Batch 3900/10001 Done, mean position loss: 20.686279430389405\n",
      "Training NF2:  39%|██████▋          | 3909/10001 [1:58:47<3:14:03,  1.91s/batch]Batch 3900/10001 Done, mean position loss: 20.727863001823422\n",
      "Training NF2:  39%|██████▌          | 3886/10001 [1:58:52<3:05:52,  1.82s/batch]Batch 3900/10001 Done, mean position loss: 20.56918006896973\n",
      "Training NF2:  39%|██████▋          | 3939/10001 [1:58:55<2:43:58,  1.62s/batch]Batch 3900/10001 Done, mean position loss: 20.91969301700592\n",
      "Training NF2:  40%|██████▋          | 3961/10001 [1:59:08<3:03:02,  1.82s/batch]Batch 3900/10001 Done, mean position loss: 20.881457703113554\n",
      "Training NF2:  39%|██████▋          | 3917/10001 [1:59:11<3:12:16,  1.90s/batch]Batch 3900/10001 Done, mean position loss: 20.774757025241854\n",
      "Training NF2:  40%|██████▋          | 3960/10001 [1:59:17<2:32:20,  1.51s/batch]Batch 3900/10001 Done, mean position loss: 20.713848445415497\n",
      "Training NF2:  39%|██████▋          | 3949/10001 [1:59:19<3:25:53,  2.04s/batch]Batch 3900/10001 Done, mean position loss: 20.340320928096773\n",
      "Training NF2:  40%|██████▊          | 3978/10001 [1:59:34<3:04:13,  1.84s/batch]Batch 3900/10001 Done, mean position loss: 20.965442364215853\n",
      "Batch 3900/10001 Done, mean position loss: 20.821486570835113\n",
      "Training NF2:  40%|██████▊          | 3982/10001 [1:59:45<2:36:58,  1.56s/batch]Batch 3900/10001 Done, mean position loss: 20.393666121959686\n",
      "Training NF2:  40%|██████▋          | 3966/10001 [1:59:51<3:04:04,  1.83s/batch]Batch 3900/10001 Done, mean position loss: 20.598850524425508\n",
      "Training NF2:  40%|██████▋          | 3970/10001 [1:59:59<3:23:38,  2.03s/batch]Batch 3900/10001 Done, mean position loss: 20.632340447902678\n",
      "Training NF2:  40%|██████▋          | 3964/10001 [2:00:05<3:46:30,  2.25s/batch]Batch 4000/10001 Done, mean position loss: 21.13621300935745\n",
      "Training NF2:  40%|██████▊          | 3987/10001 [2:00:15<3:18:21,  1.98s/batch]Batch 4000/10001 Done, mean position loss: 20.398655650615694\n",
      "Training NF2:  39%|██████▋          | 3914/10001 [2:00:18<3:46:35,  2.23s/batch]Batch 4000/10001 Done, mean position loss: 20.529735369682314\n",
      "Training NF2:  40%|██████▋          | 3966/10001 [2:00:18<2:42:29,  1.62s/batch]Batch 3900/10001 Done, mean position loss: 20.670903646945952\n",
      "Training NF2:  39%|██████▋          | 3914/10001 [2:00:26<3:43:29,  2.20s/batch]Batch 4000/10001 Done, mean position loss: 20.659816689491272\n",
      "Training NF2:  40%|██████▊          | 3991/10001 [2:00:29<2:27:43,  1.47s/batch]Batch 4000/10001 Done, mean position loss: 20.690704030990602\n",
      "Training NF2:  40%|██████▋          | 3951/10001 [2:00:32<3:17:14,  1.96s/batch]Batch 4000/10001 Done, mean position loss: 21.018825356960296\n",
      "Training NF2:  39%|██████▋          | 3934/10001 [2:00:34<2:36:55,  1.55s/batch]Batch 4000/10001 Done, mean position loss: 20.493174769878387\n",
      "Training NF2:  40%|██████▊          | 3982/10001 [2:00:39<2:58:10,  1.78s/batch]Batch 4000/10001 Done, mean position loss: 20.37087682723999\n",
      "Training NF2:  40%|██████▊          | 4017/10001 [2:00:45<3:15:23,  1.96s/batch]Batch 4000/10001 Done, mean position loss: 20.587695178985598\n",
      "Training NF2:  40%|██████▊          | 4012/10001 [2:00:50<2:35:26,  1.56s/batch]Batch 4000/10001 Done, mean position loss: 20.933295257091523\n",
      "Training NF2:  39%|██████▋          | 3947/10001 [2:00:57<2:42:50,  1.61s/batch]Batch 4000/10001 Done, mean position loss: 20.715408539772035\n",
      "Training NF2:  40%|██████▊          | 4035/10001 [2:01:07<2:58:51,  1.80s/batch]Batch 4000/10001 Done, mean position loss: 21.467384634017943\n",
      "Training NF2:  39%|██████▋          | 3942/10001 [2:01:08<3:14:36,  1.93s/batch]Batch 4000/10001 Done, mean position loss: 20.724214751720428\n",
      "Training NF2:  40%|██████▋          | 3956/10001 [2:01:14<3:40:14,  2.19s/batch]Batch 4000/10001 Done, mean position loss: 20.36850852251053\n",
      "Training NF2:  40%|██████▊          | 3987/10001 [2:01:22<2:59:36,  1.79s/batch]Batch 4000/10001 Done, mean position loss: 20.50856120109558\n",
      "Training NF2:  40%|██████▋          | 3961/10001 [2:01:23<3:09:07,  1.88s/batch]Batch 4000/10001 Done, mean position loss: 21.112094187736513\n",
      "Training NF2:  40%|██████▊          | 3990/10001 [2:01:27<2:52:27,  1.72s/batch]Batch 4000/10001 Done, mean position loss: 20.586266541481017\n",
      "Training NF2:  40%|██████▊          | 4005/10001 [2:01:33<2:46:16,  1.66s/batch]Batch 4000/10001 Done, mean position loss: 20.430139262676242\n",
      "Training NF2:  40%|██████▊          | 3995/10001 [2:01:36<2:47:35,  1.67s/batch]Batch 4000/10001 Done, mean position loss: 20.564424805641174\n",
      "Training NF2:  40%|██████▊          | 4028/10001 [2:01:36<3:49:44,  2.31s/batch]Batch 4000/10001 Done, mean position loss: 20.54565943002701\n",
      "Training NF2:  40%|██████▊          | 4002/10001 [2:01:38<2:36:14,  1.56s/batch]Batch 4000/10001 Done, mean position loss: 20.60898682355881\n",
      "Training NF2:  40%|██████▊          | 4036/10001 [2:01:44<3:24:39,  2.06s/batch]Batch 4000/10001 Done, mean position loss: 20.49101319074631\n",
      "Training NF2:  40%|██████▊          | 3996/10001 [2:01:43<3:06:02,  1.86s/batch]Batch 4000/10001 Done, mean position loss: 20.950127389431\n",
      "Training NF2:  39%|██████▋          | 3943/10001 [2:01:43<3:22:08,  2.00s/batch]Batch 4000/10001 Done, mean position loss: 20.820992612838744\n",
      "Training NF2:  40%|██████▊          | 4030/10001 [2:01:48<2:59:54,  1.81s/batch]Batch 4000/10001 Done, mean position loss: 20.60094505548477\n",
      "Training NF2:  40%|██████▊          | 3982/10001 [2:01:51<3:12:20,  1.92s/batch]Batch 4000/10001 Done, mean position loss: 20.57556294441223\n",
      "Training NF2:  40%|██████▊          | 4019/10001 [2:01:53<2:47:06,  1.68s/batch]Batch 4000/10001 Done, mean position loss: 20.73227012872696\n",
      "Training NF2:  41%|██████▉          | 4057/10001 [2:01:55<3:24:20,  2.06s/batch]Batch 4000/10001 Done, mean position loss: 20.562744476795196\n",
      "Training NF2:  41%|██████▉          | 4062/10001 [2:01:56<3:03:17,  1.85s/batch]Batch 4000/10001 Done, mean position loss: 20.680942599773406\n",
      "Training NF2:  40%|██████▊          | 3994/10001 [2:02:04<2:40:47,  1.61s/batch]Batch 4000/10001 Done, mean position loss: 20.937751214504242\n",
      "Training NF2:  41%|██████▉          | 4060/10001 [2:02:17<3:24:04,  2.06s/batch]Batch 4000/10001 Done, mean position loss: 20.86500262975693\n",
      "Training NF2:  40%|██████▊          | 4027/10001 [2:02:23<2:40:18,  1.61s/batch]Batch 4000/10001 Done, mean position loss: 20.32379145860672\n",
      "Training NF2:  40%|██████▊          | 3994/10001 [2:02:25<2:52:30,  1.72s/batch]Batch 4000/10001 Done, mean position loss: 20.767715275287628\n",
      "Training NF2:  40%|██████▊          | 4043/10001 [2:02:26<3:13:04,  1.94s/batch]Batch 4000/10001 Done, mean position loss: 20.698550822734834\n",
      "Training NF2:  41%|██████▉          | 4058/10001 [2:02:36<3:10:15,  1.92s/batch]Batch 4000/10001 Done, mean position loss: 21.003677313327792\n",
      "Training NF2:  40%|██████▊          | 4024/10001 [2:02:38<3:16:02,  1.97s/batch]Batch 4000/10001 Done, mean position loss: 20.836638996601103\n",
      "Training NF2:  41%|██████▉          | 4060/10001 [2:02:57<3:07:29,  1.89s/batch]Batch 4000/10001 Done, mean position loss: 20.39550208568573\n",
      "Training NF2:  40%|██████▊          | 4044/10001 [2:03:04<2:33:09,  1.54s/batch]Batch 4100/10001 Done, mean position loss: 21.141207048892973\n",
      "Training NF2:  41%|██████▉          | 4051/10001 [2:03:09<3:03:08,  1.85s/batch]Batch 4000/10001 Done, mean position loss: 20.62354842424393\n",
      "Training NF2:  40%|██████▊          | 4030/10001 [2:03:10<3:26:22,  2.07s/batch]Batch 4000/10001 Done, mean position loss: 20.60624990224838\n",
      "Training NF2:  40%|██████▊          | 4006/10001 [2:03:18<2:48:16,  1.68s/batch]Batch 4100/10001 Done, mean position loss: 20.37776422262192\n",
      "Training NF2:  41%|██████▉          | 4061/10001 [2:03:18<2:46:19,  1.68s/batch]Batch 4100/10001 Done, mean position loss: 20.53946342945099\n",
      "Training NF2:  41%|██████▉          | 4066/10001 [2:03:30<2:57:20,  1.79s/batch]Batch 4100/10001 Done, mean position loss: 20.98671988725662\n",
      "Training NF2:  41%|██████▉          | 4051/10001 [2:03:30<2:36:33,  1.58s/batch]Batch 4100/10001 Done, mean position loss: 20.68250571727753\n",
      "Training NF2:  40%|██████▊          | 4037/10001 [2:03:33<2:54:16,  1.75s/batch]Batch 4000/10001 Done, mean position loss: 20.677147386074065\n",
      "Training NF2:  41%|██████▉          | 4111/10001 [2:03:35<2:35:59,  1.59s/batch]Batch 4100/10001 Done, mean position loss: 20.654539761543273\n",
      "Training NF2:  41%|██████▉          | 4073/10001 [2:03:39<3:08:44,  1.91s/batch]Batch 4100/10001 Done, mean position loss: 20.49290723323822\n",
      "Training NF2:  41%|███████          | 4121/10001 [2:03:41<3:10:50,  1.95s/batch]Batch 4100/10001 Done, mean position loss: 20.37465220928192\n",
      "Training NF2:  41%|██████▉          | 4078/10001 [2:03:49<2:41:56,  1.64s/batch]Batch 4100/10001 Done, mean position loss: 20.62773005962372\n",
      "Training NF2:  40%|██████▉          | 4048/10001 [2:03:56<3:16:26,  1.98s/batch]Batch 4100/10001 Done, mean position loss: 20.91888921499252\n",
      "Training NF2:  41%|██████▉          | 4102/10001 [2:03:58<3:05:12,  1.88s/batch]Batch 4100/10001 Done, mean position loss: 20.714071712493897\n",
      "Training NF2:  41%|██████▉          | 4099/10001 [2:04:10<3:01:47,  1.85s/batch]Batch 4100/10001 Done, mean position loss: 20.726869616508484\n",
      "Training NF2:  41%|██████▉          | 4092/10001 [2:04:13<2:49:31,  1.72s/batch]Batch 4100/10001 Done, mean position loss: 20.496645700931552\n",
      "Training NF2:  41%|███████          | 4140/10001 [2:04:13<2:40:49,  1.65s/batch]Batch 4100/10001 Done, mean position loss: 20.375011827945706\n",
      "Training NF2:  41%|██████▉          | 4079/10001 [2:04:15<3:31:14,  2.14s/batch]Batch 4100/10001 Done, mean position loss: 21.464536609649656\n",
      "Training NF2:  41%|███████          | 4147/10001 [2:04:27<3:29:08,  2.14s/batch]Batch 4100/10001 Done, mean position loss: 20.604293971061708\n",
      "Training NF2:  41%|██████▉          | 4090/10001 [2:04:28<3:29:44,  2.13s/batch]Batch 4100/10001 Done, mean position loss: 21.083449952602386\n",
      "Training NF2:  41%|██████▉          | 4090/10001 [2:04:33<2:29:51,  1.52s/batch]Batch 4100/10001 Done, mean position loss: 20.597863166332246\n",
      "Training NF2:  41%|██████▉          | 4094/10001 [2:04:38<3:49:54,  2.34s/batch]Batch 4100/10001 Done, mean position loss: 20.417469804286956\n",
      "Training NF2:  41%|███████          | 4146/10001 [2:04:39<2:47:29,  1.72s/batch]Batch 4100/10001 Done, mean position loss: 20.549247174263\n",
      "Training NF2:  41%|██████▉          | 4056/10001 [2:04:42<3:47:10,  2.29s/batch]Batch 4100/10001 Done, mean position loss: 20.542446937561035\n",
      "Training NF2:  41%|██████▉          | 4092/10001 [2:04:45<2:48:23,  1.71s/batch]Batch 4100/10001 Done, mean position loss: 20.949565172195435\n",
      "Training NF2:  41%|██████▉          | 4095/10001 [2:04:51<3:29:51,  2.13s/batch]Batch 4100/10001 Done, mean position loss: 20.823139667510986\n",
      "Batch 4100/10001 Done, mean position loss: 20.49108693599701\n",
      "Training NF2:  42%|███████          | 4161/10001 [2:04:52<2:58:51,  1.84s/batch]Batch 4100/10001 Done, mean position loss: 20.606834099292755\n",
      "Training NF2:  41%|██████▉          | 4110/10001 [2:04:55<3:15:15,  1.99s/batch]Batch 4100/10001 Done, mean position loss: 20.67680113554001\n",
      "Training NF2:  41%|██████▉          | 4105/10001 [2:04:58<2:45:20,  1.68s/batch]Batch 4100/10001 Done, mean position loss: 20.722324151992797\n",
      "Training NF2:  41%|██████▉          | 4084/10001 [2:04:58<2:56:49,  1.79s/batch]Batch 4100/10001 Done, mean position loss: 20.565081505775453\n",
      "Training NF2:  41%|██████▉          | 4057/10001 [2:05:00<3:12:18,  1.94s/batch]Batch 4100/10001 Done, mean position loss: 20.91801871538162\n",
      "Training NF2:  41%|██████▉          | 4058/10001 [2:05:02<3:00:56,  1.83s/batch]Batch 4100/10001 Done, mean position loss: 20.558670551776885\n",
      "Training NF2:  41%|██████▉          | 4118/10001 [2:05:23<3:05:47,  1.89s/batch]Batch 4100/10001 Done, mean position loss: 20.856506428718568\n",
      "Training NF2:  41%|███████          | 4124/10001 [2:05:25<3:09:18,  1.93s/batch]Batch 4100/10001 Done, mean position loss: 20.776144371032714\n",
      "Training NF2:  41%|███████          | 4128/10001 [2:05:32<2:48:20,  1.72s/batch]Batch 4100/10001 Done, mean position loss: 20.695613255500795\n",
      "Training NF2:  42%|███████          | 4163/10001 [2:05:33<3:22:58,  2.09s/batch]Batch 4100/10001 Done, mean position loss: 20.331334743499756\n",
      "Training NF2:  41%|██████▉          | 4097/10001 [2:05:35<3:28:01,  2.11s/batch]Batch 4100/10001 Done, mean position loss: 20.985659744739536\n",
      "Training NF2:  41%|███████          | 4149/10001 [2:05:41<3:05:55,  1.91s/batch]Batch 4100/10001 Done, mean position loss: 20.829549894332885\n",
      "Training NF2:  41%|██████▉          | 4114/10001 [2:06:08<3:41:56,  2.26s/batch]Batch 4200/10001 Done, mean position loss: 21.16760148048401\n",
      "Training NF2:  42%|███████          | 4173/10001 [2:06:09<3:14:06,  2.00s/batch]Batch 4100/10001 Done, mean position loss: 20.386641848087308\n",
      "Training NF2:  42%|███████          | 4151/10001 [2:06:11<3:20:24,  2.06s/batch]Batch 4100/10001 Done, mean position loss: 20.616555135250092\n",
      "Training NF2:  41%|███████          | 4148/10001 [2:06:21<3:22:09,  2.07s/batch]Batch 4100/10001 Done, mean position loss: 20.612276718616485\n",
      "Training NF2:  42%|███████          | 4157/10001 [2:06:22<3:01:16,  1.86s/batch]Batch 4200/10001 Done, mean position loss: 20.374054956436158\n",
      "Training NF2:  42%|███████          | 4170/10001 [2:06:23<3:14:44,  2.00s/batch]Batch 4200/10001 Done, mean position loss: 20.533147897720337\n",
      "Training NF2:  41%|██████▉          | 4113/10001 [2:06:34<3:01:31,  1.85s/batch]Batch 4100/10001 Done, mean position loss: 20.669367978572843\n",
      "Training NF2:  42%|███████▏         | 4200/10001 [2:06:36<3:11:27,  1.98s/batch]Batch 4200/10001 Done, mean position loss: 21.015016918182376\n",
      "Training NF2:  41%|██████▉          | 4116/10001 [2:06:38<2:43:13,  1.66s/batch]Batch 4200/10001 Done, mean position loss: 20.654208335876465\n",
      "Training NF2:  42%|███████          | 4161/10001 [2:06:43<2:29:18,  1.53s/batch]Batch 4200/10001 Done, mean position loss: 20.482500588893892\n",
      "Training NF2:  42%|███████          | 4164/10001 [2:06:48<3:11:21,  1.97s/batch]Batch 4200/10001 Done, mean position loss: 20.369184987545015\n",
      "Training NF2:  41%|███████          | 4122/10001 [2:06:48<2:37:02,  1.60s/batch]Batch 4200/10001 Done, mean position loss: 20.659079172611236\n",
      "Training NF2:  42%|███████▏         | 4207/10001 [2:06:53<2:31:27,  1.57s/batch]Batch 4200/10001 Done, mean position loss: 20.59604264974594\n",
      "Training NF2:  42%|███████          | 4171/10001 [2:06:56<3:09:54,  1.95s/batch]Batch 4200/10001 Done, mean position loss: 20.73556341648102\n",
      "Training NF2:  42%|███████          | 4163/10001 [2:06:58<3:21:10,  2.07s/batch]Batch 4200/10001 Done, mean position loss: 20.95260637998581\n",
      "Training NF2:  42%|███████▏         | 4219/10001 [2:07:18<3:07:02,  1.94s/batch]Batch 4200/10001 Done, mean position loss: 20.501686847209932\n",
      "Training NF2:  41%|███████          | 4138/10001 [2:07:18<2:58:35,  1.83s/batch]Batch 4200/10001 Done, mean position loss: 20.711111152172087\n",
      "Training NF2:  42%|███████          | 4157/10001 [2:07:19<3:08:52,  1.94s/batch]Batch 4200/10001 Done, mean position loss: 20.371964726448063\n",
      "Training NF2:  42%|███████▏         | 4196/10001 [2:07:21<3:07:59,  1.94s/batch]Batch 4200/10001 Done, mean position loss: 21.459163987636565\n",
      "Training NF2:  42%|███████▏         | 4217/10001 [2:07:26<3:13:05,  2.00s/batch]Batch 4200/10001 Done, mean position loss: 20.617297101020814\n",
      "Training NF2:  42%|███████          | 4189/10001 [2:07:30<2:18:02,  1.43s/batch]Batch 4200/10001 Done, mean position loss: 21.084653587341307\n",
      "Training NF2:  42%|███████▏         | 4211/10001 [2:07:35<2:33:23,  1.59s/batch]Batch 4200/10001 Done, mean position loss: 20.592360820770264\n",
      "Training NF2:  42%|███████          | 4189/10001 [2:07:38<3:11:08,  1.97s/batch]Batch 4200/10001 Done, mean position loss: 20.54773273229599\n",
      "Training NF2:  41%|███████          | 4140/10001 [2:07:45<3:08:19,  1.93s/batch]Batch 4200/10001 Done, mean position loss: 20.96988570690155\n",
      "Training NF2:  42%|███████▏         | 4239/10001 [2:07:46<2:42:10,  1.69s/batch]Batch 4200/10001 Done, mean position loss: 20.42619760274887\n",
      "Training NF2:  42%|███████▏         | 4234/10001 [2:07:49<3:13:10,  2.01s/batch]Batch 4200/10001 Done, mean position loss: 20.52880973100662\n",
      "Training NF2:  42%|███████▏         | 4235/10001 [2:07:52<2:49:28,  1.76s/batch]Batch 4200/10001 Done, mean position loss: 20.722507359981535\n",
      "Training NF2:  42%|███████▏         | 4244/10001 [2:07:53<2:29:11,  1.55s/batch]Batch 4200/10001 Done, mean position loss: 20.830004251003267\n",
      "Batch 4200/10001 Done, mean position loss: 20.59345943927765\n",
      "Training NF2:  43%|███████▏         | 4253/10001 [2:07:55<2:33:53,  1.61s/batch]Batch 4200/10001 Done, mean position loss: 20.680997841358185\n",
      "Training NF2:  42%|███████▏         | 4234/10001 [2:07:56<3:04:27,  1.92s/batch]Batch 4200/10001 Done, mean position loss: 20.48936198234558\n",
      "Training NF2:  42%|███████          | 4187/10001 [2:08:01<2:45:10,  1.70s/batch]Batch 4200/10001 Done, mean position loss: 20.561120471954347\n",
      "Training NF2:  43%|███████▎         | 4268/10001 [2:08:04<2:46:21,  1.74s/batch]Batch 4200/10001 Done, mean position loss: 20.536334595680238\n",
      "Training NF2:  42%|███████▏         | 4208/10001 [2:08:05<2:46:39,  1.73s/batch]Batch 4200/10001 Done, mean position loss: 20.94606308698654\n",
      "Training NF2:  42%|███████▏         | 4238/10001 [2:08:26<2:39:33,  1.66s/batch]Batch 4200/10001 Done, mean position loss: 20.763689317703246\n",
      "Training NF2:  42%|███████▏         | 4243/10001 [2:08:30<2:27:58,  1.54s/batch]Batch 4200/10001 Done, mean position loss: 20.323106698989868\n",
      "Training NF2:  43%|███████▏         | 4261/10001 [2:08:31<2:42:41,  1.70s/batch]Batch 4200/10001 Done, mean position loss: 20.858534693717957\n",
      "Training NF2:  42%|███████▏         | 4224/10001 [2:08:36<2:37:12,  1.63s/batch]Batch 4200/10001 Done, mean position loss: 20.694575464725496\n",
      "Training NF2:  42%|███████▏         | 4228/10001 [2:08:38<2:44:42,  1.71s/batch]Batch 4200/10001 Done, mean position loss: 20.97125521183014\n",
      "Training NF2:  42%|███████▏         | 4248/10001 [2:08:46<3:04:35,  1.93s/batch]Batch 4200/10001 Done, mean position loss: 20.8353974032402\n",
      "Training NF2:  42%|███████          | 4182/10001 [2:09:00<2:27:35,  1.52s/batch]Batch 4300/10001 Done, mean position loss: 21.138640272617337\n",
      "Training NF2:  42%|███████▏         | 4219/10001 [2:09:08<2:58:45,  1.86s/batch]Batch 4200/10001 Done, mean position loss: 20.391947276592255\n",
      "Training NF2:  42%|███████▏         | 4226/10001 [2:09:15<2:36:15,  1.62s/batch]Batch 4200/10001 Done, mean position loss: 20.60579310178757\n",
      "Batch 4300/10001 Done, mean position loss: 20.378221657276153\n",
      "Training NF2:  43%|███████▎         | 4290/10001 [2:09:17<3:12:35,  2.02s/batch]Batch 4300/10001 Done, mean position loss: 20.541743071079253\n",
      "Training NF2:  43%|███████▎         | 4307/10001 [2:09:25<2:28:58,  1.57s/batch]Batch 4200/10001 Done, mean position loss: 20.620319890975953\n",
      "Training NF2:  43%|███████▎         | 4278/10001 [2:09:34<2:45:30,  1.74s/batch]Batch 4200/10001 Done, mean position loss: 20.674180681705472\n",
      "Training NF2:  43%|███████▎         | 4278/10001 [2:09:35<2:29:47,  1.57s/batch]Batch 4300/10001 Done, mean position loss: 20.66994928121567\n",
      "Training NF2:  43%|███████▎         | 4315/10001 [2:09:37<2:31:01,  1.59s/batch]Batch 4300/10001 Done, mean position loss: 21.02599556684494\n",
      "Training NF2:  43%|███████▏         | 4257/10001 [2:09:42<2:24:23,  1.51s/batch]Batch 4300/10001 Done, mean position loss: 20.47876096963882\n",
      "Training NF2:  43%|███████▎         | 4320/10001 [2:09:47<2:40:51,  1.70s/batch]Batch 4300/10001 Done, mean position loss: 20.59836327552795\n",
      "Training NF2:  42%|███████▏         | 4241/10001 [2:09:48<3:14:39,  2.03s/batch]Batch 4300/10001 Done, mean position loss: 20.381259617805483\n",
      "Training NF2:  43%|███████▎         | 4266/10001 [2:09:51<2:52:34,  1.81s/batch]Batch 4300/10001 Done, mean position loss: 20.72161911010742\n",
      "Training NF2:  43%|███████▎         | 4301/10001 [2:09:51<3:04:15,  1.94s/batch]Batch 4300/10001 Done, mean position loss: 20.66073081254959\n",
      "Training NF2:  42%|███████▏         | 4245/10001 [2:09:55<2:58:25,  1.86s/batch]Batch 4300/10001 Done, mean position loss: 20.930452313423157\n",
      "Training NF2:  43%|███████▍         | 4342/10001 [2:10:16<3:06:54,  1.98s/batch]Batch 4300/10001 Done, mean position loss: 20.730982065200806\n",
      "Training NF2:  43%|███████▎         | 4314/10001 [2:10:16<3:01:43,  1.92s/batch]Batch 4300/10001 Done, mean position loss: 21.472277028560637\n",
      "Training NF2:  43%|███████▎         | 4280/10001 [2:10:17<2:56:01,  1.85s/batch]Batch 4300/10001 Done, mean position loss: 20.36924957275391\n",
      "Training NF2:  43%|███████▏         | 4258/10001 [2:10:18<2:54:05,  1.82s/batch]Batch 4300/10001 Done, mean position loss: 20.49851237297058\n",
      "Training NF2:  43%|███████▎         | 4330/10001 [2:10:26<2:54:22,  1.84s/batch]Batch 4300/10001 Done, mean position loss: 20.620293025970458\n",
      "Training NF2:  43%|███████▎         | 4286/10001 [2:10:29<2:44:20,  1.73s/batch]Batch 4300/10001 Done, mean position loss: 21.095840754508973\n",
      "Training NF2:  43%|███████▎         | 4330/10001 [2:10:33<2:11:02,  1.39s/batch]Batch 4300/10001 Done, mean position loss: 20.59874443054199\n",
      "Training NF2:  43%|███████▎         | 4286/10001 [2:10:37<3:01:38,  1.91s/batch]Batch 4300/10001 Done, mean position loss: 20.540398774147036\n",
      "Training NF2:  43%|███████▎         | 4288/10001 [2:10:40<2:49:51,  1.78s/batch]Batch 4300/10001 Done, mean position loss: 20.984775104522704\n",
      "Training NF2:  44%|███████▍         | 4357/10001 [2:10:41<2:13:46,  1.42s/batch]Batch 4300/10001 Done, mean position loss: 20.407093386650086\n",
      "Training NF2:  43%|███████▎         | 4298/10001 [2:10:48<3:20:27,  2.11s/batch]Batch 4300/10001 Done, mean position loss: 20.590754632949828\n",
      "Training NF2:  43%|███████▎         | 4269/10001 [2:10:49<2:34:15,  1.61s/batch]Batch 4300/10001 Done, mean position loss: 20.51668209552765\n",
      "Training NF2:  43%|███████▎         | 4312/10001 [2:10:54<2:42:40,  1.72s/batch]Batch 4300/10001 Done, mean position loss: 20.810486392974852\n",
      "Training NF2:  43%|███████▎         | 4295/10001 [2:10:53<3:08:45,  1.98s/batch]Batch 4300/10001 Done, mean position loss: 20.726442687511444\n",
      "Training NF2:  43%|███████▎         | 4282/10001 [2:10:55<2:50:26,  1.79s/batch]Batch 4300/10001 Done, mean position loss: 20.479435920715332\n",
      "Training NF2:  43%|███████▎         | 4279/10001 [2:10:58<3:16:59,  2.07s/batch]Batch 4300/10001 Done, mean position loss: 20.672999732494354\n",
      "Training NF2:  44%|███████▍         | 4358/10001 [2:10:59<2:53:56,  1.85s/batch]Batch 4300/10001 Done, mean position loss: 20.93801514148712\n",
      "Training NF2:  44%|███████▍         | 4362/10001 [2:11:04<2:25:54,  1.55s/batch]Batch 4300/10001 Done, mean position loss: 20.58061679601669\n",
      "Training NF2:  43%|███████▍         | 4343/10001 [2:11:07<2:32:40,  1.62s/batch]Batch 4300/10001 Done, mean position loss: 20.496435511112214\n",
      "Training NF2:  44%|███████▍         | 4353/10001 [2:11:27<2:32:04,  1.62s/batch]Batch 4300/10001 Done, mean position loss: 20.856998224258426\n",
      "Training NF2:  43%|███████▎         | 4290/10001 [2:11:28<2:50:10,  1.79s/batch]Batch 4300/10001 Done, mean position loss: 20.77062748670578\n",
      "Training NF2:  44%|███████▍         | 4377/10001 [2:11:33<2:33:10,  1.63s/batch]Batch 4300/10001 Done, mean position loss: 20.327985091209413\n",
      "Training NF2:  43%|███████▍         | 4345/10001 [2:11:37<3:18:56,  2.11s/batch]Batch 4300/10001 Done, mean position loss: 20.68895066976547\n",
      "Training NF2:  43%|███████▎         | 4327/10001 [2:11:39<2:48:21,  1.78s/batch]Batch 4300/10001 Done, mean position loss: 20.98666630744934\n",
      "Training NF2:  44%|███████▍         | 4372/10001 [2:11:48<3:07:07,  1.99s/batch]Batch 4300/10001 Done, mean position loss: 20.840595011711123\n",
      "Training NF2:  44%|███████▍         | 4358/10001 [2:12:00<2:42:28,  1.73s/batch]Batch 4400/10001 Done, mean position loss: 21.114798414707185\n",
      "Training NF2:  43%|███████▍         | 4339/10001 [2:12:07<2:55:47,  1.86s/batch]Batch 4300/10001 Done, mean position loss: 20.385344622135165\n",
      "Training NF2:  43%|███████▍         | 4344/10001 [2:12:13<2:55:20,  1.86s/batch]Batch 4400/10001 Done, mean position loss: 20.36789890527725\n",
      "Training NF2:  44%|███████▍         | 4367/10001 [2:12:16<2:38:54,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 20.546236248016356\n",
      "Training NF2:  43%|███████▍         | 4349/10001 [2:12:22<3:10:22,  2.02s/batch]Batch 4300/10001 Done, mean position loss: 20.596298582553864\n",
      "Training NF2:  44%|███████▍         | 4390/10001 [2:12:23<3:03:39,  1.96s/batch]Batch 4300/10001 Done, mean position loss: 20.613279950618743\n",
      "Training NF2:  44%|███████▍         | 4395/10001 [2:12:32<3:01:16,  1.94s/batch]Batch 4400/10001 Done, mean position loss: 20.662309913635255\n",
      "Training NF2:  44%|███████▍         | 4372/10001 [2:12:35<2:42:55,  1.74s/batch]Batch 4400/10001 Done, mean position loss: 20.977500052452086\n",
      "Training NF2:  44%|███████▌         | 4413/10001 [2:12:37<2:53:00,  1.86s/batch]Batch 4300/10001 Done, mean position loss: 20.684412865638734\n",
      "Training NF2:  44%|███████▌         | 4415/10001 [2:12:39<2:59:58,  1.93s/batch]Batch 4400/10001 Done, mean position loss: 20.380361526012422\n",
      "Training NF2:  44%|███████▍         | 4403/10001 [2:12:42<2:56:23,  1.89s/batch]Batch 4400/10001 Done, mean position loss: 20.475448179244992\n",
      "Training NF2:  44%|███████▍         | 4379/10001 [2:12:56<2:38:34,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 20.59442634344101\n",
      "Training NF2:  44%|███████▌         | 4413/10001 [2:12:56<3:38:33,  2.35s/batch]Batch 4400/10001 Done, mean position loss: 20.717036502361296\n",
      "Training NF2:  43%|███████▍         | 4349/10001 [2:13:00<3:57:48,  2.52s/batch]Batch 4400/10001 Done, mean position loss: 20.63544349908829\n",
      "Training NF2:  44%|███████▍         | 4370/10001 [2:13:00<2:38:07,  1.68s/batch]Batch 4400/10001 Done, mean position loss: 20.92234169483185\n",
      "Training NF2:  44%|███████▍         | 4351/10001 [2:13:14<2:46:37,  1.77s/batch]Batch 4400/10001 Done, mean position loss: 20.727441046237946\n",
      "Training NF2:  44%|███████▌         | 4414/10001 [2:13:20<3:18:09,  2.13s/batch]Batch 4400/10001 Done, mean position loss: 20.499868595600127\n",
      "Training NF2:  44%|███████▌         | 4424/10001 [2:13:20<2:53:32,  1.87s/batch]Batch 4400/10001 Done, mean position loss: 20.370178418159487\n",
      "Training NF2:  44%|███████▍         | 4389/10001 [2:13:23<2:57:07,  1.89s/batch]Batch 4400/10001 Done, mean position loss: 21.448001098632812\n",
      "Training NF2:  44%|███████▌         | 4432/10001 [2:13:31<2:55:25,  1.89s/batch]Batch 4400/10001 Done, mean position loss: 21.09890029191971\n",
      "Training NF2:  44%|███████▍         | 4366/10001 [2:13:36<3:19:55,  2.13s/batch]Batch 4400/10001 Done, mean position loss: 20.595201587677003\n",
      "Training NF2:  44%|███████▍         | 4403/10001 [2:13:38<2:04:22,  1.33s/batch]Batch 4400/10001 Done, mean position loss: 20.586674678325654\n",
      "Training NF2:  44%|███████▍         | 4370/10001 [2:13:44<3:00:04,  1.92s/batch]Batch 4400/10001 Done, mean position loss: 20.545289895534516\n",
      "Training NF2:  44%|███████▍         | 4398/10001 [2:13:48<2:56:41,  1.89s/batch]Batch 4400/10001 Done, mean position loss: 20.952740507125856\n",
      "Training NF2:  44%|███████▍         | 4397/10001 [2:13:53<2:48:15,  1.80s/batch]Batch 4400/10001 Done, mean position loss: 20.793675279617307\n",
      "Training NF2:  44%|███████▌         | 4423/10001 [2:13:53<2:45:37,  1.78s/batch]Batch 4400/10001 Done, mean position loss: 20.52304048061371\n",
      "Training NF2:  45%|███████▌         | 4463/10001 [2:13:55<3:21:40,  2.18s/batch]Batch 4400/10001 Done, mean position loss: 20.403305287361146\n",
      "Training NF2:  44%|███████▍         | 4368/10001 [2:13:56<2:50:08,  1.81s/batch]Batch 4400/10001 Done, mean position loss: 20.70809192895889\n",
      "Training NF2:  44%|███████▍         | 4355/10001 [2:14:01<2:54:59,  1.86s/batch]Batch 4400/10001 Done, mean position loss: 20.587680518627167\n",
      "Training NF2:  44%|███████▌         | 4413/10001 [2:14:02<3:01:52,  1.95s/batch]Batch 4400/10001 Done, mean position loss: 20.475051436424256\n",
      "Training NF2:  44%|███████▌         | 4448/10001 [2:14:08<2:58:54,  1.93s/batch]Batch 4400/10001 Done, mean position loss: 20.906296586990358\n",
      "Training NF2:  44%|███████▍         | 4358/10001 [2:14:09<3:04:52,  1.97s/batch]Batch 4400/10001 Done, mean position loss: 20.68267313480377\n",
      "Training NF2:  45%|███████▌         | 4460/10001 [2:14:10<3:28:56,  2.26s/batch]Batch 4400/10001 Done, mean position loss: 20.50285359621048\n",
      "Training NF2:  45%|███████▌         | 4462/10001 [2:14:13<2:52:00,  1.86s/batch]Batch 4400/10001 Done, mean position loss: 20.55564665555954\n",
      "Training NF2:  44%|███████▍         | 4399/10001 [2:14:36<2:46:32,  1.78s/batch]Batch 4400/10001 Done, mean position loss: 20.83229278087616\n",
      "Training NF2:  44%|███████▍         | 4375/10001 [2:14:39<2:43:17,  1.74s/batch]Batch 4400/10001 Done, mean position loss: 20.69345901489258\n",
      "Training NF2:  45%|███████▌         | 4471/10001 [2:14:39<2:44:45,  1.79s/batch]Batch 4400/10001 Done, mean position loss: 20.33042423009872\n",
      "Training NF2:  44%|███████▍         | 4376/10001 [2:14:42<3:02:56,  1.95s/batch]Batch 4400/10001 Done, mean position loss: 20.74850342750549\n",
      "Training NF2:  44%|███████▌         | 4426/10001 [2:14:49<2:48:30,  1.81s/batch]Batch 4400/10001 Done, mean position loss: 20.992652199268342\n",
      "Training NF2:  44%|███████▌         | 4434/10001 [2:14:53<2:52:12,  1.86s/batch]Batch 4400/10001 Done, mean position loss: 20.82233887910843\n",
      "Training NF2:  44%|███████▍         | 4397/10001 [2:15:05<2:50:05,  1.82s/batch]Batch 4500/10001 Done, mean position loss: 21.117265710830686\n",
      "Training NF2:  45%|███████▌         | 4456/10001 [2:15:13<3:01:23,  1.96s/batch]Batch 4400/10001 Done, mean position loss: 20.386517395973204\n",
      "Training NF2:  45%|███████▋         | 4510/10001 [2:15:22<3:02:05,  1.99s/batch]Batch 4500/10001 Done, mean position loss: 20.530684723854066\n",
      "Training NF2:  44%|███████▌         | 4449/10001 [2:15:24<2:45:59,  1.79s/batch]Batch 4500/10001 Done, mean position loss: 20.39215268135071\n",
      "Training NF2:  44%|███████▍         | 4391/10001 [2:15:28<3:25:22,  2.20s/batch]Batch 4400/10001 Done, mean position loss: 20.592814066410064\n",
      "Training NF2:  45%|███████▋         | 4491/10001 [2:15:28<3:26:46,  2.25s/batch]Batch 4500/10001 Done, mean position loss: 20.672906737327576\n",
      "Training NF2:  45%|███████▌         | 4452/10001 [2:15:29<2:59:23,  1.94s/batch]Batch 4400/10001 Done, mean position loss: 20.61724779367447\n",
      "Training NF2:  45%|███████▋         | 4486/10001 [2:15:32<2:53:29,  1.89s/batch]Batch 4500/10001 Done, mean position loss: 20.96807989835739\n",
      "Training NF2:  45%|███████▋         | 4500/10001 [2:15:45<2:35:09,  1.69s/batch]Batch 4400/10001 Done, mean position loss: 20.688466691970824\n",
      "Batch 4500/10001 Done, mean position loss: 20.471699125766754\n",
      "Training NF2:  45%|███████▌         | 4481/10001 [2:15:47<3:02:06,  1.98s/batch]Batch 4500/10001 Done, mean position loss: 20.39080618143082\n",
      "Training NF2:  45%|███████▋         | 4520/10001 [2:16:00<2:42:51,  1.78s/batch]Batch 4500/10001 Done, mean position loss: 20.607725450992582\n",
      "Training NF2:  45%|███████▋         | 4488/10001 [2:16:00<2:48:47,  1.84s/batch]Batch 4500/10001 Done, mean position loss: 20.723757417201995\n",
      "Training NF2:  45%|███████▋         | 4500/10001 [2:16:02<2:55:47,  1.92s/batch]Batch 4500/10001 Done, mean position loss: 20.905332956314087\n",
      "Training NF2:  45%|███████▌         | 4469/10001 [2:16:04<3:06:03,  2.02s/batch]Batch 4500/10001 Done, mean position loss: 20.628004987239837\n",
      "Training NF2:  45%|███████▌         | 4468/10001 [2:16:17<2:27:40,  1.60s/batch]Batch 4500/10001 Done, mean position loss: 20.706566100120543\n",
      "Training NF2:  45%|███████▋         | 4520/10001 [2:16:21<2:51:44,  1.88s/batch]Batch 4500/10001 Done, mean position loss: 20.37347863435745\n",
      "Training NF2:  45%|███████▌         | 4479/10001 [2:16:24<3:05:08,  2.01s/batch]Batch 4500/10001 Done, mean position loss: 20.486157500743865\n",
      "Training NF2:  44%|███████▌         | 4424/10001 [2:16:29<2:46:15,  1.79s/batch]Batch 4500/10001 Done, mean position loss: 21.448616917133332\n",
      "Training NF2:  45%|███████▋         | 4519/10001 [2:16:33<2:44:51,  1.80s/batch]Batch 4500/10001 Done, mean position loss: 21.093927159309388\n",
      "Training NF2:  45%|███████▋         | 4487/10001 [2:16:38<2:37:27,  1.71s/batch]Batch 4500/10001 Done, mean position loss: 20.57200480699539\n",
      "Training NF2:  45%|███████▋         | 4490/10001 [2:16:43<2:34:56,  1.69s/batch]Batch 4500/10001 Done, mean position loss: 20.6021950173378\n",
      "Training NF2:  45%|███████▋         | 4527/10001 [2:16:52<3:06:48,  2.05s/batch]Batch 4500/10001 Done, mean position loss: 20.53054151058197\n",
      "Training NF2:  45%|███████▋         | 4520/10001 [2:16:52<2:19:23,  1.53s/batch]Batch 4500/10001 Done, mean position loss: 20.787817389965056\n",
      "Training NF2:  45%|███████▋         | 4510/10001 [2:16:55<3:24:22,  2.23s/batch]Batch 4500/10001 Done, mean position loss: 20.384222822189333\n",
      "Training NF2:  45%|███████▋         | 4525/10001 [2:17:01<2:44:00,  1.80s/batch]Batch 4500/10001 Done, mean position loss: 20.52627450466156\n",
      "Training NF2:  45%|███████▋         | 4495/10001 [2:17:03<2:43:04,  1.78s/batch]Batch 4500/10001 Done, mean position loss: 20.587220823764802\n",
      "Training NF2:  46%|███████▋         | 4556/10001 [2:17:05<2:26:37,  1.62s/batch]Batch 4500/10001 Done, mean position loss: 20.717613451480865\n",
      "Training NF2:  45%|███████▋         | 4501/10001 [2:17:05<3:16:36,  2.14s/batch]Batch 4500/10001 Done, mean position loss: 20.988413801193236\n",
      "Training NF2:  45%|███████▋         | 4502/10001 [2:17:07<3:04:15,  2.01s/batch]Batch 4500/10001 Done, mean position loss: 20.900048139095304\n",
      "Training NF2:  45%|███████▋         | 4546/10001 [2:17:10<2:39:27,  1.75s/batch]Batch 4500/10001 Done, mean position loss: 20.47755324602127\n",
      "Training NF2:  45%|███████▋         | 4541/10001 [2:17:14<2:45:10,  1.82s/batch]Batch 4500/10001 Done, mean position loss: 20.563793523311617\n",
      "Training NF2:  46%|███████▊         | 4561/10001 [2:17:14<2:58:55,  1.97s/batch]Batch 4500/10001 Done, mean position loss: 20.66520171403885\n",
      "Training NF2:  45%|███████▋         | 4516/10001 [2:17:19<2:38:17,  1.73s/batch]Batch 4500/10001 Done, mean position loss: 20.498971021175386\n",
      "Training NF2:  45%|███████▋         | 4491/10001 [2:17:40<3:28:38,  2.27s/batch]Batch 4500/10001 Done, mean position loss: 20.332579569816588\n",
      "Training NF2:  45%|███████▋         | 4493/10001 [2:17:43<2:39:59,  1.74s/batch]Batch 4500/10001 Done, mean position loss: 20.858068892955778\n",
      "Training NF2:  45%|███████▋         | 4505/10001 [2:17:46<2:38:53,  1.73s/batch]Batch 4500/10001 Done, mean position loss: 20.738931138515472\n",
      "Training NF2:  45%|███████▋         | 4497/10001 [2:17:51<2:59:16,  1.95s/batch]Batch 4500/10001 Done, mean position loss: 20.679653854370116\n",
      "Training NF2:  45%|███████▋         | 4489/10001 [2:17:57<2:51:24,  1.87s/batch]Batch 4500/10001 Done, mean position loss: 20.822774233818052\n",
      "Training NF2:  45%|███████▋         | 4502/10001 [2:17:58<2:15:52,  1.48s/batch]Batch 4500/10001 Done, mean position loss: 20.9448029756546\n",
      "Training NF2:  46%|███████▊         | 4577/10001 [2:18:09<3:03:21,  2.03s/batch]Batch 4600/10001 Done, mean position loss: 21.112673256397247\n",
      "Training NF2:  45%|███████▋         | 4542/10001 [2:18:20<2:39:02,  1.75s/batch]Batch 4500/10001 Done, mean position loss: 20.38250959157944\n",
      "Training NF2:  45%|███████▋         | 4537/10001 [2:18:23<2:54:51,  1.92s/batch]Batch 4600/10001 Done, mean position loss: 20.375353343486786\n",
      "Training NF2:  45%|███████▋         | 4540/10001 [2:18:29<2:54:58,  1.92s/batch]Batch 4600/10001 Done, mean position loss: 20.53678519010544\n",
      "Training NF2:  45%|███████▋         | 4520/10001 [2:18:30<3:01:51,  1.99s/batch]Batch 4600/10001 Done, mean position loss: 20.675797371864316\n",
      "Training NF2:  45%|███████▋         | 4530/10001 [2:18:33<3:21:25,  2.21s/batch]Batch 4500/10001 Done, mean position loss: 20.602820472717283\n",
      "Training NF2:  45%|███████▋         | 4550/10001 [2:18:36<2:54:56,  1.93s/batch]Batch 4600/10001 Done, mean position loss: 20.965765900611878\n",
      "Training NF2:  46%|███████▋         | 4551/10001 [2:18:37<3:01:40,  2.00s/batch]Batch 4500/10001 Done, mean position loss: 20.605824630260468\n",
      "Training NF2:  46%|███████▊         | 4561/10001 [2:18:46<2:45:11,  1.82s/batch]Batch 4500/10001 Done, mean position loss: 20.695222959518432\n",
      "Training NF2:  45%|███████▋         | 4530/10001 [2:18:51<2:47:46,  1.84s/batch]Batch 4600/10001 Done, mean position loss: 20.37864822387695\n",
      "Training NF2:  46%|███████▋         | 4556/10001 [2:18:53<3:06:55,  2.06s/batch]Batch 4600/10001 Done, mean position loss: 20.467525768280026\n",
      "Training NF2:  46%|███████▊         | 4568/10001 [2:18:57<2:12:25,  1.46s/batch]Batch 4600/10001 Done, mean position loss: 20.597517271041866\n",
      "Training NF2:  46%|███████▊         | 4619/10001 [2:19:03<3:06:07,  2.07s/batch]Batch 4600/10001 Done, mean position loss: 20.720825140476226\n",
      "Training NF2:  45%|███████▋         | 4538/10001 [2:19:08<3:17:05,  2.16s/batch]Batch 4600/10001 Done, mean position loss: 20.91308415174484\n",
      "Training NF2:  45%|███████▋         | 4514/10001 [2:19:10<3:05:35,  2.03s/batch]Batch 4600/10001 Done, mean position loss: 20.620001211166382\n",
      "Training NF2:  46%|███████▋         | 4553/10001 [2:19:19<2:41:46,  1.78s/batch]Batch 4600/10001 Done, mean position loss: 20.354630284309387\n",
      "Training NF2:  46%|███████▊         | 4575/10001 [2:19:24<2:48:14,  1.86s/batch]Batch 4600/10001 Done, mean position loss: 20.712324521541596\n",
      "Training NF2:  46%|███████▋         | 4551/10001 [2:19:28<2:36:20,  1.72s/batch]Batch 4600/10001 Done, mean position loss: 21.46599836587906\n",
      "Training NF2:  46%|███████▊         | 4579/10001 [2:19:30<2:37:32,  1.74s/batch]Batch 4600/10001 Done, mean position loss: 20.499242730140686\n",
      "Training NF2:  46%|███████▊         | 4588/10001 [2:19:40<2:42:10,  1.80s/batch]Batch 4600/10001 Done, mean position loss: 20.57731409549713\n",
      "Training NF2:  46%|███████▋         | 4557/10001 [2:19:40<2:57:42,  1.96s/batch]Batch 4600/10001 Done, mean position loss: 21.111598732471464\n",
      "Training NF2:  46%|███████▊         | 4622/10001 [2:19:47<2:31:39,  1.69s/batch]Batch 4600/10001 Done, mean position loss: 20.58362028837204\n",
      "Training NF2:  46%|███████▊         | 4597/10001 [2:19:57<2:51:25,  1.90s/batch]Batch 4600/10001 Done, mean position loss: 20.77275494813919\n",
      "Training NF2:  46%|███████▊         | 4592/10001 [2:19:57<2:45:04,  1.83s/batch]Batch 4600/10001 Done, mean position loss: 20.411755142211913\n",
      "Training NF2:  46%|███████▉         | 4638/10001 [2:19:58<2:42:30,  1.82s/batch]Batch 4600/10001 Done, mean position loss: 20.499637076854707\n",
      "Training NF2:  46%|███████▊         | 4571/10001 [2:20:02<2:34:40,  1.71s/batch]Batch 4600/10001 Done, mean position loss: 20.52359540462494\n",
      "Training NF2:  46%|███████▊         | 4600/10001 [2:20:06<2:16:36,  1.52s/batch]Batch 4600/10001 Done, mean position loss: 20.587015128135683\n",
      "Training NF2:  46%|███████▊         | 4574/10001 [2:20:06<2:50:32,  1.89s/batch]Batch 4600/10001 Done, mean position loss: 20.948708686828613\n",
      "Training NF2:  46%|███████▊         | 4560/10001 [2:20:08<2:25:17,  1.60s/batch]Batch 4600/10001 Done, mean position loss: 20.7194855594635\n",
      "Training NF2:  46%|███████▊         | 4619/10001 [2:20:11<3:07:43,  2.09s/batch]Batch 4600/10001 Done, mean position loss: 20.913221900463103\n",
      "Training NF2:  46%|███████▊         | 4579/10001 [2:20:15<2:12:24,  1.47s/batch]Batch 4600/10001 Done, mean position loss: 20.567675683498383\n",
      "Training NF2:  47%|███████▉         | 4661/10001 [2:20:18<2:32:10,  1.71s/batch]Batch 4600/10001 Done, mean position loss: 20.477334527969358\n",
      "Training NF2:  46%|███████▋         | 4558/10001 [2:20:21<2:57:11,  1.95s/batch]Batch 4600/10001 Done, mean position loss: 20.67486122608185\n",
      "Training NF2:  46%|███████▊         | 4605/10001 [2:20:25<2:31:03,  1.68s/batch]Batch 4600/10001 Done, mean position loss: 20.496843478679658\n",
      "Training NF2:  46%|███████▊         | 4593/10001 [2:20:48<2:30:45,  1.67s/batch]Batch 4600/10001 Done, mean position loss: 20.82992738008499\n",
      "Training NF2:  46%|███████▊         | 4567/10001 [2:20:51<2:35:35,  1.72s/batch]Batch 4600/10001 Done, mean position loss: 20.306279776096343\n",
      "Training NF2:  47%|███████▉         | 4663/10001 [2:20:55<3:00:03,  2.02s/batch]Batch 4600/10001 Done, mean position loss: 20.74114378929138\n",
      "Training NF2:  46%|███████▊         | 4630/10001 [2:20:59<3:14:08,  2.17s/batch]Batch 4600/10001 Done, mean position loss: 20.692953896522525\n",
      "Training NF2:  47%|███████▉         | 4666/10001 [2:20:59<2:29:54,  1.69s/batch]Batch 4600/10001 Done, mean position loss: 20.838756916522982\n",
      "Training NF2:  46%|███████▊         | 4577/10001 [2:21:04<3:16:49,  2.18s/batch]Batch 4600/10001 Done, mean position loss: 20.96586855649948\n",
      "Training NF2:  46%|███████▊         | 4617/10001 [2:21:23<2:43:40,  1.82s/batch]Batch 4700/10001 Done, mean position loss: 21.133555591106415\n",
      "Training NF2:  46%|███████▊         | 4621/10001 [2:21:24<3:14:24,  2.17s/batch]Batch 4600/10001 Done, mean position loss: 20.394726858139038\n",
      "Training NF2:  47%|███████▉         | 4680/10001 [2:21:31<2:30:58,  1.70s/batch]Batch 4700/10001 Done, mean position loss: 20.38104482650757\n",
      "Training NF2:  47%|███████▉         | 4684/10001 [2:21:33<2:12:56,  1.50s/batch]Batch 4700/10001 Done, mean position loss: 20.660806272029877\n",
      "Training NF2:  46%|███████▉         | 4647/10001 [2:21:36<2:31:17,  1.70s/batch]Batch 4700/10001 Done, mean position loss: 20.545331127643585\n",
      "Training NF2:  47%|███████▉         | 4667/10001 [2:21:40<2:42:06,  1.82s/batch]Batch 4600/10001 Done, mean position loss: 20.620938293933868\n",
      "Training NF2:  47%|███████▉         | 4687/10001 [2:21:44<2:35:36,  1.76s/batch]Batch 4600/10001 Done, mean position loss: 20.623032402992248\n",
      "Training NF2:  47%|███████▉         | 4697/10001 [2:21:47<3:08:15,  2.13s/batch]Batch 4700/10001 Done, mean position loss: 20.977728350162508\n",
      "Training NF2:  47%|███████▉         | 4684/10001 [2:21:53<2:52:51,  1.95s/batch]Batch 4700/10001 Done, mean position loss: 20.376038060188293\n",
      "Training NF2:  47%|███████▉         | 4704/10001 [2:21:54<3:19:31,  2.26s/batch]Batch 4600/10001 Done, mean position loss: 20.702772302627565\n",
      "Training NF2:  47%|███████▉         | 4661/10001 [2:22:01<2:35:08,  1.74s/batch]Batch 4700/10001 Done, mean position loss: 20.59169231891632\n",
      "Batch 4700/10001 Done, mean position loss: 20.44550798416138\n",
      "Training NF2:  46%|███████▉         | 4641/10001 [2:22:07<2:39:56,  1.79s/batch]Batch 4700/10001 Done, mean position loss: 20.619666361808775\n",
      "Training NF2:  47%|████████         | 4718/10001 [2:22:12<2:59:15,  2.04s/batch]Batch 4700/10001 Done, mean position loss: 20.727728583812713\n",
      "Training NF2:  47%|████████         | 4710/10001 [2:22:17<2:32:23,  1.73s/batch]Batch 4700/10001 Done, mean position loss: 20.898428225517275\n",
      "Training NF2:  46%|███████▊         | 4616/10001 [2:22:24<2:42:43,  1.81s/batch]Batch 4700/10001 Done, mean position loss: 20.366351478099823\n",
      "Training NF2:  46%|███████▊         | 4624/10001 [2:22:24<2:33:42,  1.72s/batch]Batch 4700/10001 Done, mean position loss: 20.72216153383255\n",
      "Training NF2:  47%|████████         | 4707/10001 [2:22:28<2:39:06,  1.80s/batch]Batch 4700/10001 Done, mean position loss: 20.484875378608702\n",
      "Training NF2:  47%|███████▉         | 4659/10001 [2:22:42<3:02:25,  2.05s/batch]Batch 4700/10001 Done, mean position loss: 21.07316099882126\n",
      "Training NF2:  47%|████████         | 4738/10001 [2:22:44<3:02:11,  2.08s/batch]Batch 4700/10001 Done, mean position loss: 21.44644219636917\n",
      "Training NF2:  47%|████████         | 4713/10001 [2:22:46<2:30:44,  1.71s/batch]Batch 4700/10001 Done, mean position loss: 20.585958483219148\n",
      "Training NF2:  47%|███████▉         | 4664/10001 [2:22:54<3:32:23,  2.39s/batch]Batch 4700/10001 Done, mean position loss: 20.580644030570987\n",
      "Training NF2:  48%|████████         | 4751/10001 [2:22:55<2:34:13,  1.76s/batch]Batch 4700/10001 Done, mean position loss: 20.39931146621704\n",
      "Training NF2:  47%|███████▉         | 4706/10001 [2:23:03<2:28:43,  1.69s/batch]Batch 4700/10001 Done, mean position loss: 20.50748407125473\n",
      "Training NF2:  47%|███████▉         | 4653/10001 [2:23:03<2:51:12,  1.92s/batch]Batch 4700/10001 Done, mean position loss: 20.52024193763733\n",
      "Training NF2:  47%|███████▉         | 4703/10001 [2:23:05<2:13:50,  1.52s/batch]Batch 4700/10001 Done, mean position loss: 20.760504655838012\n",
      "Training NF2:  47%|████████         | 4728/10001 [2:23:15<3:15:56,  2.23s/batch]Batch 4700/10001 Done, mean position loss: 20.591091122627258\n",
      "Training NF2:  47%|███████▉         | 4694/10001 [2:23:20<2:40:19,  1.81s/batch]Batch 4700/10001 Done, mean position loss: 20.597920644283295\n",
      "Batch 4700/10001 Done, mean position loss: 20.908570897579196\n",
      "Training NF2:  47%|███████▉         | 4653/10001 [2:23:21<2:35:13,  1.74s/batch]Batch 4700/10001 Done, mean position loss: 20.700850150585175\n",
      "Training NF2:  47%|████████         | 4750/10001 [2:23:23<2:27:29,  1.69s/batch]Batch 4700/10001 Done, mean position loss: 20.89977520465851\n",
      "Training NF2:  47%|████████         | 4737/10001 [2:23:26<2:47:12,  1.91s/batch]Batch 4700/10001 Done, mean position loss: 20.4756197810173\n",
      "Training NF2:  47%|████████         | 4709/10001 [2:23:34<2:20:19,  1.59s/batch]Batch 4700/10001 Done, mean position loss: 20.495110862255096\n",
      "Training NF2:  47%|███████▉         | 4683/10001 [2:23:36<3:00:53,  2.04s/batch]Batch 4700/10001 Done, mean position loss: 20.683148567676543\n",
      "Training NF2:  47%|███████▉         | 4675/10001 [2:23:47<2:53:16,  1.95s/batch]Batch 4700/10001 Done, mean position loss: 20.820473022460938\n",
      "Training NF2:  47%|████████         | 4722/10001 [2:23:59<2:38:04,  1.80s/batch]Batch 4700/10001 Done, mean position loss: 20.731905739307404\n",
      "Training NF2:  48%|████████         | 4755/10001 [2:24:02<2:56:04,  2.01s/batch]Batch 4700/10001 Done, mean position loss: 20.321622061729432\n",
      "Training NF2:  47%|████████         | 4735/10001 [2:24:07<2:49:07,  1.93s/batch]Batch 4700/10001 Done, mean position loss: 20.95372200012207\n",
      "Training NF2:  47%|████████         | 4727/10001 [2:24:08<2:35:36,  1.77s/batch]Batch 4700/10001 Done, mean position loss: 20.79956433057785\n",
      "Training NF2:  48%|████████         | 4756/10001 [2:24:09<3:11:17,  2.19s/batch]Batch 4700/10001 Done, mean position loss: 20.692039005756378\n",
      "Training NF2:  48%|████████         | 4766/10001 [2:24:28<2:49:39,  1.94s/batch]Batch 4800/10001 Done, mean position loss: 21.116941735744476\n",
      "Training NF2:  47%|████████         | 4742/10001 [2:24:38<3:03:00,  2.09s/batch]Batch 4800/10001 Done, mean position loss: 20.36552652835846\n",
      "Training NF2:  48%|████████         | 4764/10001 [2:24:38<2:32:47,  1.75s/batch]Batch 4700/10001 Done, mean position loss: 20.379089748859407\n",
      "Training NF2:  48%|████████▏        | 4780/10001 [2:24:41<2:49:46,  1.95s/batch]Batch 4800/10001 Done, mean position loss: 20.655315318107604\n",
      "Training NF2:  48%|████████         | 4758/10001 [2:24:43<2:54:11,  1.99s/batch]Batch 4800/10001 Done, mean position loss: 20.53765261888504\n",
      "Training NF2:  48%|████████▏        | 4792/10001 [2:24:42<2:48:02,  1.94s/batch]Batch 4700/10001 Done, mean position loss: 20.61394785165787\n",
      "Training NF2:  48%|████████         | 4753/10001 [2:24:49<2:44:13,  1.88s/batch]Batch 4700/10001 Done, mean position loss: 20.615189418792724\n",
      "Training NF2:  47%|████████         | 4747/10001 [2:24:59<2:40:32,  1.83s/batch]Batch 4800/10001 Done, mean position loss: 20.36800161361694\n",
      "Training NF2:  48%|████████         | 4759/10001 [2:25:02<2:39:04,  1.82s/batch]Batch 4800/10001 Done, mean position loss: 20.573824756145477\n",
      "Training NF2:  48%|████████         | 4770/10001 [2:25:02<2:16:49,  1.57s/batch]Batch 4800/10001 Done, mean position loss: 20.44524646282196\n",
      "Batch 4700/10001 Done, mean position loss: 20.691482083797453\n",
      "Training NF2:  47%|████████         | 4738/10001 [2:25:04<2:41:15,  1.84s/batch]Batch 4800/10001 Done, mean position loss: 20.977127540111542\n",
      "Training NF2:  48%|████████         | 4775/10001 [2:25:09<2:16:59,  1.57s/batch]Batch 4800/10001 Done, mean position loss: 20.631056473255157\n",
      "Training NF2:  48%|████████▏        | 4823/10001 [2:25:19<2:56:51,  2.05s/batch]Batch 4800/10001 Done, mean position loss: 20.36286818265915\n",
      "Training NF2:  48%|████████▏        | 4796/10001 [2:25:20<2:19:33,  1.61s/batch]Batch 4800/10001 Done, mean position loss: 20.925775299072264\n",
      "Training NF2:  48%|████████         | 4771/10001 [2:25:24<3:06:25,  2.14s/batch]Batch 4800/10001 Done, mean position loss: 20.728177852630616\n",
      "Training NF2:  48%|████████▏        | 4813/10001 [2:25:26<2:44:03,  1.90s/batch]Batch 4800/10001 Done, mean position loss: 20.48552261352539\n",
      "Training NF2:  47%|████████         | 4721/10001 [2:25:29<2:59:04,  2.03s/batch]Batch 4800/10001 Done, mean position loss: 20.708203320503237\n",
      "Training NF2:  48%|████████         | 4776/10001 [2:25:40<3:10:22,  2.19s/batch]Batch 4800/10001 Done, mean position loss: 21.05051841020584\n",
      "Training NF2:  47%|████████         | 4727/10001 [2:25:40<2:44:43,  1.87s/batch]Batch 4800/10001 Done, mean position loss: 21.442244451045987\n",
      "Training NF2:  48%|████████▏        | 4805/10001 [2:25:46<2:23:26,  1.66s/batch]Batch 4800/10001 Done, mean position loss: 20.573983778953554\n",
      "Training NF2:  48%|████████▏        | 4785/10001 [2:25:53<2:30:25,  1.73s/batch]Batch 4800/10001 Done, mean position loss: 20.372645382881164\n",
      "Training NF2:  48%|████████         | 4770/10001 [2:25:55<2:43:45,  1.88s/batch]Batch 4800/10001 Done, mean position loss: 20.618967938423154\n",
      "Training NF2:  48%|████████▏        | 4780/10001 [2:26:00<2:12:36,  1.52s/batch]Batch 4800/10001 Done, mean position loss: 20.509345843791962\n",
      "Training NF2:  48%|████████▏        | 4827/10001 [2:26:07<2:09:52,  1.51s/batch]Batch 4800/10001 Done, mean position loss: 20.503973963260652\n",
      "Training NF2:  48%|████████▏        | 4813/10001 [2:26:13<2:28:14,  1.71s/batch]Batch 4800/10001 Done, mean position loss: 20.564356689453124\n",
      "Training NF2:  48%|████████▏        | 4826/10001 [2:26:14<3:09:28,  2.20s/batch]Batch 4800/10001 Done, mean position loss: 20.94931857585907\n",
      "Training NF2:  48%|████████▏        | 4821/10001 [2:26:15<2:36:35,  1.81s/batch]Batch 4800/10001 Done, mean position loss: 20.74812425851822\n",
      "Training NF2:  48%|████████▏        | 4805/10001 [2:26:22<3:26:12,  2.38s/batch]Batch 4800/10001 Done, mean position loss: 20.714244332313537\n",
      "Training NF2:  47%|████████         | 4750/10001 [2:26:23<2:32:32,  1.74s/batch]Batch 4800/10001 Done, mean position loss: 20.47478661775589\n",
      "Training NF2:  48%|████████▏        | 4823/10001 [2:26:24<2:25:23,  1.68s/batch]Batch 4800/10001 Done, mean position loss: 20.60153568983078\n",
      "Training NF2:  48%|████████▏        | 4848/10001 [2:26:24<2:20:02,  1.63s/batch]Batch 4800/10001 Done, mean position loss: 20.887094461917876\n",
      "Training NF2:  48%|████████▏        | 4850/10001 [2:26:34<2:36:49,  1.83s/batch]Batch 4800/10001 Done, mean position loss: 20.46767314195633\n",
      "Training NF2:  48%|████████▏        | 4839/10001 [2:26:37<2:31:36,  1.76s/batch]Batch 4800/10001 Done, mean position loss: 20.688313713073732\n",
      "Training NF2:  49%|████████▎        | 4855/10001 [2:26:53<2:35:52,  1.82s/batch]Batch 4800/10001 Done, mean position loss: 20.82320684671402\n",
      "Training NF2:  48%|████████         | 4777/10001 [2:27:00<3:07:53,  2.16s/batch]Batch 4800/10001 Done, mean position loss: 20.316992380619048\n",
      "Training NF2:  48%|████████▏        | 4841/10001 [2:27:04<3:03:44,  2.14s/batch]Batch 4800/10001 Done, mean position loss: 20.82295120000839\n",
      "Training NF2:  48%|████████▏        | 4845/10001 [2:27:06<2:41:40,  1.88s/batch]Batch 4800/10001 Done, mean position loss: 20.727715733051298\n",
      "Training NF2:  49%|████████▎        | 4877/10001 [2:27:06<2:27:22,  1.73s/batch]Batch 4800/10001 Done, mean position loss: 20.67814406394959\n",
      "Training NF2:  48%|████████▏        | 4835/10001 [2:27:07<2:53:49,  2.02s/batch]Batch 4800/10001 Done, mean position loss: 20.92005792617798\n",
      "Training NF2:  48%|████████▏        | 4839/10001 [2:27:26<2:21:15,  1.64s/batch]Batch 4900/10001 Done, mean position loss: 20.37778151988983\n",
      "Training NF2:  48%|████████▏        | 4833/10001 [2:27:30<2:32:45,  1.77s/batch]Batch 4900/10001 Done, mean position loss: 21.0995564866066\n",
      "Training NF2:  48%|████████▏        | 4823/10001 [2:27:35<2:46:31,  1.93s/batch]Batch 4900/10001 Done, mean position loss: 20.53263391494751\n",
      "Training NF2:  48%|████████▏        | 4847/10001 [2:27:38<2:17:23,  1.60s/batch]Batch 4900/10001 Done, mean position loss: 20.67864982843399\n",
      "Training NF2:  49%|████████▎        | 4897/10001 [2:27:43<2:41:04,  1.89s/batch]Batch 4800/10001 Done, mean position loss: 20.385431244373322\n",
      "Training NF2:  49%|████████▎        | 4906/10001 [2:27:44<2:41:40,  1.90s/batch]Batch 4800/10001 Done, mean position loss: 20.602451927661896\n",
      "Training NF2:  49%|████████▎        | 4855/10001 [2:27:51<2:27:40,  1.72s/batch]Batch 4900/10001 Done, mean position loss: 20.370894241333005\n",
      "Training NF2:  49%|████████▎        | 4854/10001 [2:27:51<2:25:38,  1.70s/batch]Batch 4900/10001 Done, mean position loss: 20.575680582523347\n",
      "Training NF2:  49%|████████▎        | 4900/10001 [2:27:59<2:38:33,  1.87s/batch]Batch 4900/10001 Done, mean position loss: 20.607823057174684\n",
      "Training NF2:  49%|████████▎        | 4866/10001 [2:28:01<2:53:06,  2.02s/batch]Batch 4900/10001 Done, mean position loss: 20.976238906383514\n",
      "Training NF2:  49%|████████▎        | 4915/10001 [2:28:01<2:49:01,  1.99s/batch]Batch 4800/10001 Done, mean position loss: 20.59953704595566\n",
      "Training NF2:  49%|████████▎        | 4919/10001 [2:28:03<3:02:56,  2.16s/batch]Batch 4800/10001 Done, mean position loss: 20.70285312652588\n",
      "Training NF2:  49%|████████▎        | 4917/10001 [2:28:05<2:24:25,  1.70s/batch]Batch 4900/10001 Done, mean position loss: 20.442085978984835\n",
      "Training NF2:  49%|████████▎        | 4865/10001 [2:28:17<2:17:10,  1.60s/batch]Batch 4900/10001 Done, mean position loss: 20.35716926574707\n",
      "Training NF2:  49%|████████▎        | 4898/10001 [2:28:19<2:48:04,  1.98s/batch]Batch 4900/10001 Done, mean position loss: 20.723596849441527\n",
      "Training NF2:  49%|████████▎        | 4888/10001 [2:28:24<2:12:21,  1.55s/batch]Batch 4900/10001 Done, mean position loss: 20.500190720558166\n",
      "Training NF2:  49%|████████▎        | 4897/10001 [2:28:25<2:46:06,  1.95s/batch]Batch 4900/10001 Done, mean position loss: 20.92625811100006\n",
      "Training NF2:  49%|████████▎        | 4892/10001 [2:28:31<2:22:21,  1.67s/batch]Batch 4900/10001 Done, mean position loss: 20.713871297836306\n",
      "Training NF2:  48%|████████▏        | 4828/10001 [2:28:33<2:52:10,  2.00s/batch]Batch 4900/10001 Done, mean position loss: 21.042229812145237\n",
      "Training NF2:  49%|████████▎        | 4865/10001 [2:28:35<2:48:11,  1.96s/batch]Batch 4900/10001 Done, mean position loss: 21.442826657295228\n",
      "Training NF2:  49%|████████▎        | 4914/10001 [2:28:48<2:56:00,  2.08s/batch]Batch 4900/10001 Done, mean position loss: 20.382977750301364\n",
      "Training NF2:  49%|████████▍        | 4933/10001 [2:28:55<2:19:56,  1.66s/batch]Batch 4900/10001 Done, mean position loss: 20.59287713050842\n",
      "Training NF2:  49%|████████▎        | 4860/10001 [2:28:55<2:17:07,  1.60s/batch]Batch 4900/10001 Done, mean position loss: 20.570036783218384\n",
      "Training NF2:  49%|████████▎        | 4866/10001 [2:29:06<2:40:57,  1.88s/batch]Batch 4900/10001 Done, mean position loss: 20.508673436641693\n",
      "Training NF2:  49%|████████▍        | 4938/10001 [2:29:09<2:46:10,  1.97s/batch]Batch 4900/10001 Done, mean position loss: 20.510880708694458\n",
      "Training NF2:  49%|████████▎        | 4912/10001 [2:29:16<2:40:05,  1.89s/batch]Batch 4900/10001 Done, mean position loss: 20.926156487464908\n",
      "Training NF2:  49%|████████▎        | 4906/10001 [2:29:17<3:01:55,  2.14s/batch]Batch 4900/10001 Done, mean position loss: 20.86767144918442\n",
      "Training NF2:  49%|████████▍        | 4931/10001 [2:29:19<2:33:02,  1.81s/batch]Batch 4900/10001 Done, mean position loss: 20.60606602668762\n",
      "Training NF2:  49%|████████▍        | 4928/10001 [2:29:19<2:46:07,  1.96s/batch]Batch 4900/10001 Done, mean position loss: 20.75233890771866\n",
      "Training NF2:  50%|████████▍        | 4951/10001 [2:29:21<2:43:01,  1.94s/batch]Batch 4900/10001 Done, mean position loss: 20.5567471909523\n",
      "Training NF2:  49%|████████▎        | 4875/10001 [2:29:22<2:29:18,  1.75s/batch]Batch 4900/10001 Done, mean position loss: 20.47013572216034\n",
      "Training NF2:  49%|████████▎        | 4877/10001 [2:29:26<2:19:51,  1.64s/batch]Batch 4900/10001 Done, mean position loss: 20.70207017183304\n",
      "Training NF2:  49%|████████▎        | 4882/10001 [2:29:34<2:21:28,  1.66s/batch]Batch 4900/10001 Done, mean position loss: 20.465558590888975\n",
      "Training NF2:  49%|████████▍        | 4927/10001 [2:29:38<2:38:02,  1.87s/batch]Batch 4900/10001 Done, mean position loss: 20.670559854507445\n",
      "Training NF2:  49%|████████▎        | 4922/10001 [2:29:59<2:47:32,  1.98s/batch]Batch 4900/10001 Done, mean position loss: 20.325959060192105\n",
      "Training NF2:  49%|████████▎        | 4902/10001 [2:30:01<2:54:29,  2.05s/batch]Batch 4900/10001 Done, mean position loss: 20.7989940905571\n",
      "Training NF2:  49%|████████▎        | 4925/10001 [2:30:07<2:44:39,  1.95s/batch]Batch 4900/10001 Done, mean position loss: 20.814359459877014\n",
      "Training NF2:  50%|████████▍        | 4976/10001 [2:30:08<2:17:41,  1.64s/batch]Batch 4900/10001 Done, mean position loss: 20.653976402282716\n",
      "Training NF2:  49%|████████▎        | 4926/10001 [2:30:08<2:33:06,  1.81s/batch]Batch 4900/10001 Done, mean position loss: 20.729219460487364\n",
      "Training NF2:  49%|████████▍        | 4942/10001 [2:30:12<2:43:12,  1.94s/batch]Batch 4900/10001 Done, mean position loss: 20.909794294834136\n",
      "Training NF2:  49%|████████▍        | 4946/10001 [2:30:36<2:46:06,  1.97s/batch]Batch 5000/10001 Done, mean position loss: 20.37679354906082\n",
      "Training NF2:  50%|████████▍        | 4968/10001 [2:30:36<2:39:12,  1.90s/batch]Batch 5000/10001 Done, mean position loss: 20.528932883739472\n",
      "Training NF2:  50%|████████▍        | 4958/10001 [2:30:39<2:33:21,  1.82s/batch]Batch 5000/10001 Done, mean position loss: 21.064194550514223\n",
      "Training NF2:  50%|████████▌        | 5002/10001 [2:30:41<2:32:30,  1.83s/batch]Batch 5000/10001 Done, mean position loss: 20.647610423564913\n",
      "Training NF2:  49%|████████▎        | 4922/10001 [2:30:48<2:33:16,  1.81s/batch]Batch 4900/10001 Done, mean position loss: 20.586468586921693\n",
      "Training NF2:  49%|████████▍        | 4947/10001 [2:30:50<2:34:45,  1.84s/batch]Batch 5000/10001 Done, mean position loss: 20.38283473491669\n",
      "Training NF2:  50%|████████▌        | 5008/10001 [2:30:53<2:56:05,  2.12s/batch]Batch 4900/10001 Done, mean position loss: 20.377134308815002\n",
      "Training NF2:  49%|████████▍        | 4944/10001 [2:30:56<2:35:49,  1.85s/batch]Batch 5000/10001 Done, mean position loss: 20.578236472606662\n",
      "Training NF2:  50%|████████▍        | 4997/10001 [2:31:01<2:29:03,  1.79s/batch]Batch 5000/10001 Done, mean position loss: 20.61112920999527\n",
      "Training NF2:  50%|████████▍        | 4973/10001 [2:31:08<2:48:55,  2.02s/batch]Batch 5000/10001 Done, mean position loss: 20.96442044019699\n",
      "Training NF2:  50%|████████▌        | 5009/10001 [2:31:10<2:51:07,  2.06s/batch]Batch 4900/10001 Done, mean position loss: 20.6914640545845\n",
      "Training NF2:  50%|████████▌        | 5020/10001 [2:31:13<2:21:43,  1.71s/batch]Batch 5000/10001 Done, mean position loss: 20.44497853755951\n",
      "Training NF2:  50%|████████▍        | 4957/10001 [2:31:18<2:25:54,  1.74s/batch]Batch 5000/10001 Done, mean position loss: 20.741625959873197\n",
      "Batch 4900/10001 Done, mean position loss: 20.6055228638649\n",
      "Training NF2:  49%|████████▎        | 4906/10001 [2:31:26<2:32:43,  1.80s/batch]Batch 5000/10001 Done, mean position loss: 20.341306343078614\n",
      "Training NF2:  50%|████████▍        | 4970/10001 [2:31:29<2:25:35,  1.74s/batch]Batch 5000/10001 Done, mean position loss: 20.709520494937898\n",
      "Training NF2:  50%|████████▌        | 5019/10001 [2:31:31<2:15:30,  1.63s/batch]Batch 5000/10001 Done, mean position loss: 20.930356128215788\n",
      "Training NF2:  50%|████████▌        | 5033/10001 [2:31:34<2:20:39,  1.70s/batch]Batch 5000/10001 Done, mean position loss: 20.479779982566832\n",
      "Training NF2:  49%|████████▍        | 4946/10001 [2:31:37<2:04:15,  1.47s/batch]Batch 5000/10001 Done, mean position loss: 21.41021047115326\n",
      "Training NF2:  50%|████████▍        | 4994/10001 [2:31:50<2:32:51,  1.83s/batch]Batch 5000/10001 Done, mean position loss: 21.072240977287294\n",
      "Training NF2:  49%|████████▍        | 4935/10001 [2:31:52<2:26:15,  1.73s/batch]Batch 5000/10001 Done, mean position loss: 20.391184895038606\n",
      "Training NF2:  50%|████████▌        | 5048/10001 [2:32:02<2:29:01,  1.81s/batch]Batch 5000/10001 Done, mean position loss: 20.608069214820862\n",
      "Training NF2:  50%|████████▌        | 5021/10001 [2:32:03<2:34:01,  1.86s/batch]Batch 5000/10001 Done, mean position loss: 20.57147804737091\n",
      "Training NF2:  50%|████████▌        | 5010/10001 [2:32:19<2:20:07,  1.68s/batch]Batch 5000/10001 Done, mean position loss: 20.491187806129457\n",
      "Training NF2:  50%|████████▍        | 4978/10001 [2:32:21<2:36:50,  1.87s/batch]Batch 5000/10001 Done, mean position loss: 20.507270317077634\n",
      "Training NF2:  50%|████████▌        | 5033/10001 [2:32:24<2:15:37,  1.64s/batch]Batch 5000/10001 Done, mean position loss: 20.619844887256622\n",
      "Training NF2:  50%|████████▌        | 5005/10001 [2:32:25<2:12:15,  1.59s/batch]Batch 5000/10001 Done, mean position loss: 20.85935186624527\n",
      "Training NF2:  50%|████████▌        | 5022/10001 [2:32:26<2:34:53,  1.87s/batch]Batch 5000/10001 Done, mean position loss: 20.933852577209475\n",
      "Training NF2:  50%|████████▍        | 4973/10001 [2:32:27<2:45:36,  1.98s/batch]Batch 5000/10001 Done, mean position loss: 20.745165798664093\n",
      "Training NF2:  50%|████████▌        | 5023/10001 [2:32:28<2:39:44,  1.93s/batch]Batch 5000/10001 Done, mean position loss: 20.46215528726578\n",
      "Training NF2:  51%|████████▌        | 5060/10001 [2:32:30<2:23:41,  1.74s/batch]Batch 5000/10001 Done, mean position loss: 20.679811084270476\n",
      "Training NF2:  50%|████████▍        | 4958/10001 [2:32:35<2:25:17,  1.73s/batch]Batch 5000/10001 Done, mean position loss: 20.464002318382263\n",
      "Training NF2:  50%|████████▍        | 4986/10001 [2:32:37<2:36:13,  1.87s/batch]Batch 5000/10001 Done, mean position loss: 20.54977167129517\n",
      "Training NF2:  51%|████████▌        | 5071/10001 [2:32:45<2:27:34,  1.80s/batch]Batch 5000/10001 Done, mean position loss: 20.672946050167084\n",
      "Training NF2:  50%|████████▌        | 5035/10001 [2:33:05<2:36:58,  1.90s/batch]Batch 5000/10001 Done, mean position loss: 20.312927029132844\n",
      "Training NF2:  50%|████████▌        | 5025/10001 [2:33:07<2:33:51,  1.86s/batch]Batch 5000/10001 Done, mean position loss: 20.809993069171902\n",
      "Training NF2:  50%|████████▌        | 5019/10001 [2:33:07<2:29:01,  1.79s/batch]Batch 5000/10001 Done, mean position loss: 20.73658138513565\n",
      "Training NF2:  51%|████████▋        | 5085/10001 [2:33:20<2:30:02,  1.83s/batch]Batch 5000/10001 Done, mean position loss: 20.687520599365236\n",
      "Training NF2:  51%|████████▋        | 5091/10001 [2:33:21<2:48:40,  2.06s/batch]Batch 5000/10001 Done, mean position loss: 20.89424709558487\n",
      "Training NF2:  50%|████████▌        | 5035/10001 [2:33:24<3:07:14,  2.26s/batch]Batch 5000/10001 Done, mean position loss: 20.826529886722565\n",
      "Training NF2:  51%|████████▌        | 5073/10001 [2:33:42<2:29:15,  1.82s/batch]Batch 5100/10001 Done, mean position loss: 20.38525102138519\n",
      "Training NF2:  51%|████████▌        | 5062/10001 [2:33:42<2:34:53,  1.88s/batch]Batch 5100/10001 Done, mean position loss: 20.649698731899264\n",
      "Training NF2:  50%|████████▍        | 4996/10001 [2:33:48<2:47:50,  2.01s/batch]Batch 5100/10001 Done, mean position loss: 20.512461228370668\n",
      "Training NF2:  51%|████████▋        | 5078/10001 [2:33:49<2:30:52,  1.84s/batch]Batch 5100/10001 Done, mean position loss: 21.08956781864166\n",
      "Training NF2:  50%|████████▌        | 5023/10001 [2:33:50<2:13:03,  1.60s/batch]Batch 5100/10001 Done, mean position loss: 20.3971342754364\n",
      "Training NF2:  50%|████████▌        | 5048/10001 [2:33:59<2:28:57,  1.80s/batch]Batch 5000/10001 Done, mean position loss: 20.378866481781007\n",
      "Training NF2:  51%|████████▋        | 5083/10001 [2:33:59<2:39:27,  1.95s/batch]Batch 5000/10001 Done, mean position loss: 20.58729337692261\n",
      "Training NF2:  51%|████████▋        | 5090/10001 [2:34:10<3:21:35,  2.46s/batch]Batch 5100/10001 Done, mean position loss: 20.575946328639986\n",
      "Training NF2:  51%|████████▋        | 5079/10001 [2:34:13<4:06:09,  3.00s/batch]Batch 5100/10001 Done, mean position loss: 20.606504321098328\n",
      "Training NF2:  51%|████████▌        | 5055/10001 [2:34:15<2:27:59,  1.80s/batch]Batch 5100/10001 Done, mean position loss: 20.99030210494995\n",
      "Training NF2:  51%|████████▋        | 5080/10001 [2:34:19<3:08:11,  2.29s/batch]Batch 5000/10001 Done, mean position loss: 20.686578919887545\n",
      "Training NF2:  51%|████████▋        | 5090/10001 [2:34:23<2:26:06,  1.78s/batch]Batch 5100/10001 Done, mean position loss: 20.45572478532791\n",
      "Training NF2:  51%|████████▋        | 5098/10001 [2:34:30<3:01:55,  2.23s/batch]Batch 5100/10001 Done, mean position loss: 20.737708480358123\n",
      "Training NF2:  51%|████████▋        | 5092/10001 [2:34:35<2:43:52,  2.00s/batch]Batch 5000/10001 Done, mean position loss: 20.60544328689575\n",
      "Training NF2:  51%|████████▋        | 5126/10001 [2:34:35<2:35:12,  1.91s/batch]Batch 5100/10001 Done, mean position loss: 20.337691905498506\n",
      "Training NF2:  51%|████████▋        | 5116/10001 [2:34:38<2:08:32,  1.58s/batch]Batch 5100/10001 Done, mean position loss: 20.707841413021086\n",
      "Training NF2:  50%|████████▌        | 5044/10001 [2:34:45<2:44:23,  1.99s/batch]Batch 5100/10001 Done, mean position loss: 21.442403559684756\n",
      "Training NF2:  51%|████████▋        | 5105/10001 [2:34:52<2:35:12,  1.90s/batch]Batch 5100/10001 Done, mean position loss: 20.485564057826995\n",
      "Training NF2:  51%|████████▋        | 5134/10001 [2:34:54<2:36:02,  1.92s/batch]Batch 5100/10001 Done, mean position loss: 20.916579136848448\n",
      "Training NF2:  51%|████████▋        | 5123/10001 [2:34:56<2:28:39,  1.83s/batch]Batch 5100/10001 Done, mean position loss: 21.067084333896638\n",
      "Training NF2:  51%|████████▋        | 5084/10001 [2:35:00<2:29:30,  1.82s/batch]Batch 5100/10001 Done, mean position loss: 20.38713568210602\n",
      "Training NF2:  51%|████████▌        | 5062/10001 [2:35:09<2:52:48,  2.10s/batch]Batch 5100/10001 Done, mean position loss: 20.610712320804595\n",
      "Training NF2:  51%|████████▋        | 5089/10001 [2:35:17<2:30:20,  1.84s/batch]Batch 5100/10001 Done, mean position loss: 20.57085104227066\n",
      "Training NF2:  52%|████████▊        | 5153/10001 [2:35:26<2:20:56,  1.74s/batch]Batch 5100/10001 Done, mean position loss: 20.94264410972595\n",
      "Training NF2:  51%|████████▋        | 5116/10001 [2:35:28<2:21:59,  1.74s/batch]Batch 5100/10001 Done, mean position loss: 20.503988986015322\n",
      "Training NF2:  52%|████████▊        | 5156/10001 [2:35:32<2:31:18,  1.87s/batch]Batch 5100/10001 Done, mean position loss: 20.489132602214813\n",
      "Training NF2:  51%|████████▋        | 5145/10001 [2:35:35<2:34:07,  1.90s/batch]Batch 5100/10001 Done, mean position loss: 20.736063361167908\n",
      "Training NF2:  51%|████████▋        | 5098/10001 [2:35:36<2:23:17,  1.75s/batch]Batch 5100/10001 Done, mean position loss: 20.84858972072601\n",
      "Batch 5100/10001 Done, mean position loss: 20.693373737335204\n",
      "Training NF2:  52%|████████▊        | 5159/10001 [2:35:38<2:24:57,  1.80s/batch]Batch 5100/10001 Done, mean position loss: 20.472442512512206\n",
      "Training NF2:  51%|████████▋        | 5096/10001 [2:35:40<2:26:05,  1.79s/batch]Batch 5100/10001 Done, mean position loss: 20.606859035491944\n",
      "Training NF2:  51%|████████▋        | 5081/10001 [2:35:41<2:19:31,  1.70s/batch]Batch 5100/10001 Done, mean position loss: 20.561347765922548\n",
      "Training NF2:  51%|████████▋        | 5126/10001 [2:35:46<2:26:32,  1.80s/batch]Batch 5100/10001 Done, mean position loss: 20.46896943807602\n",
      "Training NF2:  51%|████████▋        | 5109/10001 [2:35:50<2:10:42,  1.60s/batch]Batch 5100/10001 Done, mean position loss: 20.657031002044675\n",
      "Training NF2:  51%|████████▋        | 5144/10001 [2:36:16<2:50:49,  2.11s/batch]Batch 5100/10001 Done, mean position loss: 20.325489706993103\n",
      "Training NF2:  51%|████████▋        | 5142/10001 [2:36:17<2:57:25,  2.19s/batch]Batch 5100/10001 Done, mean position loss: 20.80770319223404\n",
      "Training NF2:  52%|████████▊        | 5172/10001 [2:36:24<2:23:51,  1.79s/batch]Batch 5100/10001 Done, mean position loss: 20.73764782190323\n",
      "Training NF2:  52%|████████▊        | 5154/10001 [2:36:34<2:16:21,  1.69s/batch]Batch 5100/10001 Done, mean position loss: 20.672622351646424\n",
      "Training NF2:  51%|████████▋        | 5136/10001 [2:36:34<2:30:02,  1.85s/batch]Batch 5100/10001 Done, mean position loss: 20.927728691101073\n",
      "Training NF2:  52%|████████▊        | 5183/10001 [2:36:45<2:40:18,  2.00s/batch]Batch 5100/10001 Done, mean position loss: 20.8196648812294\n",
      "Training NF2:  51%|████████▋        | 5102/10001 [2:36:47<2:36:35,  1.92s/batch]Batch 5200/10001 Done, mean position loss: 21.105972583293912\n",
      "Training NF2:  52%|████████▊        | 5158/10001 [2:36:48<2:43:05,  2.02s/batch]Batch 5200/10001 Done, mean position loss: 20.63852177619934\n",
      "Training NF2:  51%|████████▋        | 5119/10001 [2:36:50<2:44:37,  2.02s/batch]Batch 5200/10001 Done, mean position loss: 20.3672723031044\n",
      "Training NF2:  52%|████████▊        | 5206/10001 [2:36:54<2:08:15,  1.60s/batch]Batch 5200/10001 Done, mean position loss: 20.50753415584564\n",
      "Training NF2:  51%|████████▋        | 5146/10001 [2:36:56<2:23:01,  1.77s/batch]Batch 5200/10001 Done, mean position loss: 20.37603086233139\n",
      "Training NF2:  52%|████████▊        | 5197/10001 [2:37:13<2:16:46,  1.71s/batch]Batch 5100/10001 Done, mean position loss: 20.38576254606247\n",
      "Training NF2:  52%|████████▊        | 5170/10001 [2:37:13<2:55:54,  2.18s/batch]Batch 5100/10001 Done, mean position loss: 20.593084969520568\n",
      "Training NF2:  51%|████████▋        | 5119/10001 [2:37:18<2:18:52,  1.71s/batch]Batch 5200/10001 Done, mean position loss: 20.598402218818663\n",
      "Training NF2:  51%|████████▋        | 5133/10001 [2:37:19<2:50:23,  2.10s/batch]Batch 5200/10001 Done, mean position loss: 20.441052124500274\n",
      "Training NF2:  51%|████████▋        | 5098/10001 [2:37:22<2:41:06,  1.97s/batch]Batch 5200/10001 Done, mean position loss: 20.573418588638305\n",
      "Training NF2:  52%|████████▊        | 5189/10001 [2:37:21<2:41:21,  2.01s/batch]Batch 5200/10001 Done, mean position loss: 20.981802692413332\n",
      "Training NF2:  51%|████████▋        | 5129/10001 [2:37:27<2:26:46,  1.81s/batch]Batch 5100/10001 Done, mean position loss: 20.685113930702208\n",
      "Training NF2:  52%|████████▊        | 5190/10001 [2:37:43<2:38:16,  1.97s/batch]Batch 5200/10001 Done, mean position loss: 20.714759464263913\n",
      "Training NF2:  52%|████████▊        | 5172/10001 [2:37:44<2:24:16,  1.79s/batch]Batch 5200/10001 Done, mean position loss: 20.339583895206452\n",
      "Training NF2:  52%|████████▊        | 5202/10001 [2:37:45<2:31:17,  1.89s/batch]Batch 5100/10001 Done, mean position loss: 20.583954365253447\n",
      "Training NF2:  52%|████████▊        | 5164/10001 [2:37:48<2:32:25,  1.89s/batch]Batch 5200/10001 Done, mean position loss: 21.432831387519837\n",
      "Training NF2:  52%|████████▉        | 5232/10001 [2:37:48<3:01:22,  2.28s/batch]Batch 5200/10001 Done, mean position loss: 20.691801710128786\n",
      "Training NF2:  52%|████████▊        | 5193/10001 [2:37:57<2:26:08,  1.82s/batch]Batch 5200/10001 Done, mean position loss: 20.90280622959137\n",
      "Training NF2:  52%|████████▉        | 5223/10001 [2:38:01<2:43:05,  2.05s/batch]Batch 5200/10001 Done, mean position loss: 20.489346001148224\n",
      "Training NF2:  52%|████████▊        | 5181/10001 [2:38:01<2:49:49,  2.11s/batch]Batch 5200/10001 Done, mean position loss: 21.07560340642929\n",
      "Training NF2:  52%|████████▊        | 5186/10001 [2:38:09<2:21:45,  1.77s/batch]Batch 5200/10001 Done, mean position loss: 20.38525164604187\n",
      "Training NF2:  52%|████████▉        | 5245/10001 [2:38:12<2:47:37,  2.11s/batch]Batch 5200/10001 Done, mean position loss: 20.58241260766983\n",
      "Training NF2:  52%|████████▊        | 5214/10001 [2:38:21<2:34:15,  1.93s/batch]Batch 5200/10001 Done, mean position loss: 20.581794953346254\n",
      "Training NF2:  52%|████████▊        | 5199/10001 [2:38:34<2:21:49,  1.77s/batch]Batch 5200/10001 Done, mean position loss: 20.70228892326355\n",
      "Training NF2:  52%|████████▊        | 5190/10001 [2:38:36<2:45:09,  2.06s/batch]Batch 5200/10001 Done, mean position loss: 20.500245661735534\n",
      "Training NF2:  52%|████████▊        | 5167/10001 [2:38:37<2:10:50,  1.62s/batch]Batch 5200/10001 Done, mean position loss: 20.964254684448242\n",
      "Training NF2:  52%|████████▊        | 5172/10001 [2:38:37<2:15:20,  1.68s/batch]Batch 5200/10001 Done, mean position loss: 20.470523195266722\n",
      "Training NF2:  52%|████████▊        | 5211/10001 [2:38:38<2:30:13,  1.88s/batch]Batch 5200/10001 Done, mean position loss: 20.48540213108063\n",
      "Training NF2:  53%|████████▉        | 5259/10001 [2:38:42<2:34:03,  1.95s/batch]Batch 5200/10001 Done, mean position loss: 20.844837338924407\n",
      "Training NF2:  52%|████████▊        | 5205/10001 [2:38:43<2:10:31,  1.63s/batch]Batch 5200/10001 Done, mean position loss: 20.605080723762512\n",
      "Training NF2:  52%|████████▉        | 5235/10001 [2:38:47<2:24:14,  1.82s/batch]Batch 5200/10001 Done, mean position loss: 20.730893726348874\n",
      "Training NF2:  52%|████████▊        | 5208/10001 [2:38:47<2:26:53,  1.84s/batch]Batch 5200/10001 Done, mean position loss: 20.452859547138214\n",
      "Training NF2:  52%|████████▉        | 5227/10001 [2:38:50<2:17:22,  1.73s/batch]Batch 5200/10001 Done, mean position loss: 20.53151812076569\n",
      "Training NF2:  52%|████████▊        | 5221/10001 [2:38:58<2:44:51,  2.07s/batch]Batch 5200/10001 Done, mean position loss: 20.672188031673432\n",
      "Training NF2:  52%|████████▊        | 5162/10001 [2:39:24<2:37:35,  1.95s/batch]Batch 5200/10001 Done, mean position loss: 20.313401315212253\n",
      "Training NF2:  52%|████████▊        | 5190/10001 [2:39:28<2:33:17,  1.91s/batch]Batch 5200/10001 Done, mean position loss: 20.809594395160673\n",
      "Training NF2:  53%|████████▉        | 5274/10001 [2:39:30<2:40:38,  2.04s/batch]Batch 5200/10001 Done, mean position loss: 20.73290287733078\n",
      "Training NF2:  52%|████████▉        | 5232/10001 [2:39:41<2:13:03,  1.67s/batch]Batch 5200/10001 Done, mean position loss: 20.936736478805543\n",
      "Training NF2:  52%|████████▊        | 5214/10001 [2:39:48<2:43:23,  2.05s/batch]Batch 5200/10001 Done, mean position loss: 20.66973707675934\n",
      "Training NF2:  53%|████████▉        | 5264/10001 [2:39:49<2:14:43,  1.71s/batch]Batch 5200/10001 Done, mean position loss: 20.818624408245086\n",
      "Training NF2:  53%|████████▉        | 5285/10001 [2:39:53<2:53:21,  2.21s/batch]Batch 5300/10001 Done, mean position loss: 20.359417440891264\n",
      "Training NF2:  52%|████████▉        | 5244/10001 [2:39:55<2:24:45,  1.83s/batch]Batch 5300/10001 Done, mean position loss: 20.657592573165893\n",
      "Training NF2:  53%|████████▉        | 5274/10001 [2:40:01<2:42:19,  2.06s/batch]Batch 5300/10001 Done, mean position loss: 21.107202942371366\n",
      "Training NF2:  53%|█████████        | 5301/10001 [2:40:01<2:28:17,  1.89s/batch]Batch 5300/10001 Done, mean position loss: 20.513851416110995\n",
      "Training NF2:  53%|████████▉        | 5251/10001 [2:40:10<2:48:42,  2.13s/batch]Batch 5300/10001 Done, mean position loss: 20.379505860805512\n",
      "Training NF2:  53%|████████▉        | 5252/10001 [2:40:19<2:41:56,  2.05s/batch]Batch 5200/10001 Done, mean position loss: 20.387618446350096\n",
      "Training NF2:  52%|████████▉        | 5230/10001 [2:40:21<2:43:20,  2.05s/batch]Batch 5300/10001 Done, mean position loss: 20.570844049453733\n",
      "Training NF2:  52%|████████▊        | 5185/10001 [2:40:23<2:41:19,  2.01s/batch]Batch 5300/10001 Done, mean position loss: 20.437112526893614\n",
      "Training NF2:  53%|████████▉        | 5268/10001 [2:40:29<2:26:09,  1.85s/batch]Batch 5300/10001 Done, mean position loss: 20.58288111448288\n",
      "Training NF2:  53%|████████▉        | 5263/10001 [2:40:29<2:28:52,  1.89s/batch]Batch 5300/10001 Done, mean position loss: 20.994525954723358\n",
      "Training NF2:  53%|████████▉        | 5264/10001 [2:40:30<2:10:15,  1.65s/batch]Batch 5200/10001 Done, mean position loss: 20.57841475009918\n",
      "Training NF2:  53%|████████▉        | 5294/10001 [2:40:42<2:42:27,  2.07s/batch]Batch 5200/10001 Done, mean position loss: 20.688503296375274\n",
      "Training NF2:  52%|████████▊        | 5218/10001 [2:40:51<2:25:03,  1.82s/batch]Batch 5300/10001 Done, mean position loss: 20.348185429573057\n",
      "Training NF2:  53%|████████▉        | 5285/10001 [2:40:51<2:33:23,  1.95s/batch]Batch 5200/10001 Done, mean position loss: 20.586267700195314\n",
      "Training NF2:  53%|████████▉        | 5272/10001 [2:40:52<2:27:16,  1.87s/batch]Batch 5300/10001 Done, mean position loss: 20.726018640995026\n",
      "Training NF2:  53%|█████████        | 5298/10001 [2:40:54<2:23:29,  1.83s/batch]Batch 5300/10001 Done, mean position loss: 21.437734026908874\n",
      "Training NF2:  53%|████████▉        | 5289/10001 [2:40:58<2:23:07,  1.82s/batch]Batch 5300/10001 Done, mean position loss: 20.702784290313723\n",
      "Training NF2:  52%|████████▊        | 5206/10001 [2:40:58<2:06:34,  1.58s/batch]Batch 5300/10001 Done, mean position loss: 20.92093794822693\n",
      "Training NF2:  53%|█████████        | 5300/10001 [2:41:08<2:14:38,  1.72s/batch]Batch 5300/10001 Done, mean position loss: 20.482084772586823\n",
      "Training NF2:  52%|████████▉        | 5244/10001 [2:41:09<2:40:52,  2.03s/batch]Batch 5300/10001 Done, mean position loss: 21.05794210910797\n",
      "Training NF2:  52%|████████▊        | 5218/10001 [2:41:19<2:31:28,  1.90s/batch]Batch 5300/10001 Done, mean position loss: 20.376923775672914\n",
      "Training NF2:  53%|█████████        | 5342/10001 [2:41:22<2:33:36,  1.98s/batch]Batch 5300/10001 Done, mean position loss: 20.594305124282837\n",
      "Training NF2:  52%|████████▉        | 5240/10001 [2:41:29<2:13:47,  1.69s/batch]Batch 5300/10001 Done, mean position loss: 20.590792906284335\n",
      "Training NF2:  53%|█████████        | 5299/10001 [2:41:36<2:09:14,  1.65s/batch]Batch 5300/10001 Done, mean position loss: 20.71012863874435\n",
      "Training NF2:  53%|█████████        | 5298/10001 [2:41:40<2:20:40,  1.79s/batch]Batch 5300/10001 Done, mean position loss: 20.953140611648557\n",
      "Training NF2:  53%|████████▉        | 5276/10001 [2:41:43<2:14:12,  1.70s/batch]Batch 5300/10001 Done, mean position loss: 20.85206593513489\n",
      "Training NF2:  53%|████████▉        | 5275/10001 [2:41:43<2:10:25,  1.66s/batch]Batch 5300/10001 Done, mean position loss: 20.58495291233063\n",
      "Training NF2:  53%|█████████        | 5321/10001 [2:41:43<2:16:47,  1.75s/batch]Batch 5300/10001 Done, mean position loss: 20.48283155679703\n",
      "Training NF2:  52%|████████▉        | 5244/10001 [2:41:46<2:13:00,  1.68s/batch]Batch 5300/10001 Done, mean position loss: 20.498328516483305\n",
      "Training NF2:  54%|█████████        | 5357/10001 [2:41:46<2:10:26,  1.69s/batch]Batch 5300/10001 Done, mean position loss: 20.485427684783936\n",
      "Training NF2:  54%|█████████        | 5365/10001 [2:41:51<2:10:42,  1.69s/batch]Batch 5300/10001 Done, mean position loss: 20.727058637142182\n",
      "Training NF2:  53%|████████▉        | 5281/10001 [2:41:53<2:04:42,  1.59s/batch]Batch 5300/10001 Done, mean position loss: 20.54693477392197\n",
      "Training NF2:  53%|█████████        | 5338/10001 [2:41:56<2:33:53,  1.98s/batch]Batch 5300/10001 Done, mean position loss: 20.458526582717898\n",
      "Training NF2:  53%|█████████        | 5306/10001 [2:42:01<2:19:17,  1.78s/batch]Batch 5300/10001 Done, mean position loss: 20.66267657995224\n",
      "Training NF2:  53%|█████████        | 5327/10001 [2:42:28<1:50:21,  1.42s/batch]Batch 5300/10001 Done, mean position loss: 20.312713906764984\n",
      "Training NF2:  53%|█████████        | 5302/10001 [2:42:28<2:17:56,  1.76s/batch]Batch 5300/10001 Done, mean position loss: 20.795718548297884\n",
      "Training NF2:  54%|█████████        | 5352/10001 [2:42:32<2:05:00,  1.61s/batch]Batch 5300/10001 Done, mean position loss: 20.731584131717682\n",
      "Training NF2:  53%|████████▉        | 5267/10001 [2:42:39<2:00:58,  1.53s/batch]Batch 5300/10001 Done, mean position loss: 20.940583267211913\n",
      "Training NF2:  54%|█████████▏       | 5394/10001 [2:42:52<2:22:59,  1.86s/batch]Batch 5400/10001 Done, mean position loss: 20.362545607089995\n",
      "Training NF2:  54%|█████████▏       | 5399/10001 [2:42:54<2:20:03,  1.83s/batch]Batch 5300/10001 Done, mean position loss: 20.809058911800385\n",
      "Training NF2:  54%|█████████        | 5365/10001 [2:42:56<2:33:29,  1.99s/batch]Batch 5300/10001 Done, mean position loss: 20.672562632560727\n",
      "Training NF2:  54%|█████████▏       | 5406/10001 [2:42:59<1:43:02,  1.35s/batch]Batch 5400/10001 Done, mean position loss: 20.654666554927825\n",
      "Training NF2:  53%|████████▉        | 5276/10001 [2:43:02<2:19:45,  1.77s/batch]Batch 5400/10001 Done, mean position loss: 20.53268241405487\n",
      "Training NF2:  53%|█████████        | 5343/10001 [2:43:04<2:27:30,  1.90s/batch]Batch 5400/10001 Done, mean position loss: 21.08467751264572\n",
      "Training NF2:  54%|█████████▏       | 5391/10001 [2:43:13<2:32:22,  1.98s/batch]Batch 5400/10001 Done, mean position loss: 20.388440613746646\n",
      "Training NF2:  54%|█████████▏       | 5412/10001 [2:43:17<1:51:21,  1.46s/batch]Batch 5400/10001 Done, mean position loss: 20.564672763347627\n",
      "Training NF2:  54%|█████████        | 5351/10001 [2:43:19<2:11:41,  1.70s/batch]Batch 5300/10001 Done, mean position loss: 20.395807561874392\n",
      "Training NF2:  54%|█████████        | 5361/10001 [2:43:24<2:12:06,  1.71s/batch]Batch 5300/10001 Done, mean position loss: 20.616898627281188\n",
      "Training NF2:  54%|█████████▏       | 5416/10001 [2:43:27<1:59:42,  1.57s/batch]Batch 5400/10001 Done, mean position loss: 20.428985400199892\n",
      "Training NF2:  54%|█████████        | 5362/10001 [2:43:32<2:33:42,  1.99s/batch]Batch 5400/10001 Done, mean position loss: 20.99167636156082\n",
      "Training NF2:  54%|█████████        | 5368/10001 [2:43:36<1:53:08,  1.47s/batch]Batch 5400/10001 Done, mean position loss: 20.585135579109192\n",
      "Training NF2:  54%|█████████▏       | 5377/10001 [2:43:38<2:24:08,  1.87s/batch]Batch 5400/10001 Done, mean position loss: 20.719665174484255\n",
      "Training NF2:  54%|█████████▏       | 5426/10001 [2:43:41<2:27:33,  1.94s/batch]Batch 5300/10001 Done, mean position loss: 20.67853783607483\n",
      "Training NF2:  53%|█████████        | 5346/10001 [2:43:45<2:08:10,  1.65s/batch]Batch 5400/10001 Done, mean position loss: 20.344421298503875\n",
      "Training NF2:  54%|█████████▏       | 5374/10001 [2:43:46<1:59:04,  1.54s/batch]Batch 5300/10001 Done, mean position loss: 20.591143379211424\n",
      "Training NF2:  53%|█████████        | 5349/10001 [2:43:49<1:55:40,  1.49s/batch]Batch 5400/10001 Done, mean position loss: 21.420264794826508\n",
      "Training NF2:  54%|█████████▏       | 5403/10001 [2:43:52<2:06:23,  1.65s/batch]Batch 5400/10001 Done, mean position loss: 20.711187469959256\n",
      "Training NF2:  54%|█████████        | 5355/10001 [2:44:01<2:32:40,  1.97s/batch]Batch 5400/10001 Done, mean position loss: 20.482604486942293\n",
      "Training NF2:  54%|█████████▏       | 5427/10001 [2:44:02<2:13:53,  1.76s/batch]Batch 5400/10001 Done, mean position loss: 20.875495193004607\n",
      "Training NF2:  54%|█████████▏       | 5386/10001 [2:44:07<2:15:41,  1.76s/batch]Batch 5400/10001 Done, mean position loss: 21.09494188785553\n",
      "Training NF2:  54%|█████████        | 5366/10001 [2:44:21<2:40:58,  2.08s/batch]Batch 5400/10001 Done, mean position loss: 20.385178630352023\n",
      "Training NF2:  54%|█████████▏       | 5440/10001 [2:44:24<1:51:45,  1.47s/batch]Batch 5400/10001 Done, mean position loss: 20.605449776649476\n",
      "Training NF2:  54%|█████████▏       | 5389/10001 [2:44:27<2:17:55,  1.79s/batch]Batch 5400/10001 Done, mean position loss: 20.71211674928665\n",
      "Training NF2:  54%|█████████▎       | 5450/10001 [2:44:30<2:08:47,  1.70s/batch]Batch 5400/10001 Done, mean position loss: 20.585059437751774\n",
      "Training NF2:  53%|█████████        | 5330/10001 [2:44:34<1:50:33,  1.42s/batch]Batch 5400/10001 Done, mean position loss: 20.567078006267547\n",
      "Training NF2:  54%|█████████▏       | 5421/10001 [2:44:42<2:09:20,  1.69s/batch]Batch 5400/10001 Done, mean position loss: 20.848350167274475\n",
      "Training NF2:  54%|█████████▏       | 5440/10001 [2:44:42<2:17:49,  1.81s/batch]Batch 5400/10001 Done, mean position loss: 20.47611458539963\n",
      "Training NF2:  53%|█████████        | 5337/10001 [2:44:42<2:17:58,  1.77s/batch]Batch 5400/10001 Done, mean position loss: 20.925880517959595\n",
      "Training NF2:  54%|█████████▏       | 5391/10001 [2:44:44<2:12:26,  1.72s/batch]Batch 5400/10001 Done, mean position loss: 20.497706487178803\n",
      "Training NF2:  54%|█████████▏       | 5441/10001 [2:44:47<2:15:27,  1.78s/batch]Batch 5400/10001 Done, mean position loss: 20.499371092319493\n",
      "Training NF2:  54%|█████████▏       | 5402/10001 [2:44:48<2:05:33,  1.64s/batch]Batch 5400/10001 Done, mean position loss: 20.539907450675965\n",
      "Training NF2:  54%|█████████▏       | 5428/10001 [2:44:49<2:21:03,  1.85s/batch]Batch 5400/10001 Done, mean position loss: 20.740056052207947\n",
      "Training NF2:  54%|█████████▏       | 5408/10001 [2:44:58<1:54:53,  1.50s/batch]Batch 5400/10001 Done, mean position loss: 20.648730185031894\n",
      "Training NF2:  54%|█████████▏       | 5422/10001 [2:45:02<2:03:13,  1.61s/batch]Batch 5400/10001 Done, mean position loss: 20.442643151283264\n",
      "Training NF2:  54%|█████████▏       | 5420/10001 [2:45:24<2:40:44,  2.11s/batch]Batch 5400/10001 Done, mean position loss: 20.325822529792788\n",
      "Training NF2:  54%|█████████▏       | 5399/10001 [2:45:32<2:51:42,  2.24s/batch]Batch 5400/10001 Done, mean position loss: 20.804412789344788\n",
      "Training NF2:  55%|█████████▎       | 5487/10001 [2:45:35<2:08:46,  1.71s/batch]Batch 5400/10001 Done, mean position loss: 20.73967906713486\n",
      "Training NF2:  54%|█████████▏       | 5432/10001 [2:45:40<2:28:18,  1.95s/batch]Batch 5500/10001 Done, mean position loss: 20.349981796741485\n",
      "Training NF2:  55%|█████████▎       | 5463/10001 [2:45:46<2:18:22,  1.83s/batch]Batch 5400/10001 Done, mean position loss: 20.913630349636076\n",
      "Training NF2:  55%|█████████▎       | 5493/10001 [2:45:57<2:14:58,  1.80s/batch]Batch 5400/10001 Done, mean position loss: 20.809272418022154\n",
      "Training NF2:  54%|█████████▏       | 5409/10001 [2:46:04<3:17:26,  2.58s/batch]Batch 5500/10001 Done, mean position loss: 20.518808495998382\n",
      "Training NF2:  54%|█████████▎       | 5442/10001 [2:46:06<2:54:52,  2.30s/batch]Batch 5500/10001 Done, mean position loss: 20.650291829109193\n",
      "Training NF2:  54%|█████████▏       | 5406/10001 [2:46:07<2:25:47,  1.90s/batch]Batch 5500/10001 Done, mean position loss: 21.092070715427397\n",
      "Training NF2:  54%|█████████▏       | 5383/10001 [2:46:09<2:35:01,  2.01s/batch]Batch 5400/10001 Done, mean position loss: 20.67404837846756\n",
      "Training NF2:  54%|█████████▏       | 5427/10001 [2:46:11<2:55:59,  2.31s/batch]Batch 5500/10001 Done, mean position loss: 20.376711418628695\n",
      "Training NF2:  55%|█████████▎       | 5505/10001 [2:46:15<2:37:21,  2.10s/batch]Batch 5500/10001 Done, mean position loss: 20.560941259860996\n",
      "Training NF2:  54%|█████████▏       | 5433/10001 [2:46:23<2:44:28,  2.16s/batch]Batch 5400/10001 Done, mean position loss: 20.383343815803528\n",
      "Training NF2:  54%|█████████▏       | 5426/10001 [2:46:28<2:35:12,  2.04s/batch]Batch 5500/10001 Done, mean position loss: 20.42682636499405\n",
      "Training NF2:  55%|█████████▎       | 5467/10001 [2:46:38<2:26:06,  1.93s/batch]Batch 5400/10001 Done, mean position loss: 20.59040977716446\n",
      "Training NF2:  55%|█████████▎       | 5460/10001 [2:46:41<2:13:48,  1.77s/batch]Batch 5500/10001 Done, mean position loss: 20.948377463817597\n",
      "Training NF2:  55%|█████████▎       | 5499/10001 [2:46:43<2:44:59,  2.20s/batch]Batch 5400/10001 Done, mean position loss: 20.679137570858003\n",
      "Training NF2:  55%|█████████▎       | 5463/10001 [2:46:44<2:17:31,  1.82s/batch]Batch 5500/10001 Done, mean position loss: 20.33092896223068\n",
      "Training NF2:  55%|█████████▎       | 5465/10001 [2:46:46<2:25:37,  1.93s/batch]Batch 5500/10001 Done, mean position loss: 20.708355231285097\n",
      "Training NF2:  55%|█████████▍       | 5520/10001 [2:46:47<2:16:17,  1.82s/batch]Batch 5500/10001 Done, mean position loss: 20.580948219299316\n",
      "Training NF2:  55%|█████████▎       | 5467/10001 [2:46:49<2:31:10,  2.00s/batch]Batch 5400/10001 Done, mean position loss: 20.594380910396577\n",
      "Training NF2:  54%|█████████▏       | 5432/10001 [2:46:58<2:16:59,  1.80s/batch]Batch 5500/10001 Done, mean position loss: 20.705199871063236\n",
      "Training NF2:  54%|█████████▏       | 5412/10001 [2:47:00<2:40:47,  2.10s/batch]Batch 5500/10001 Done, mean position loss: 21.44910274744034\n",
      "Training NF2:  55%|█████████▍       | 5526/10001 [2:47:01<2:59:25,  2.41s/batch]Batch 5500/10001 Done, mean position loss: 20.888823359012605\n",
      "Training NF2:  55%|█████████▎       | 5477/10001 [2:47:10<2:36:06,  2.07s/batch]Batch 5500/10001 Done, mean position loss: 20.46591098308563\n",
      "Training NF2:  55%|█████████▎       | 5489/10001 [2:47:10<2:49:38,  2.26s/batch]Batch 5500/10001 Done, mean position loss: 21.064583659172058\n",
      "Training NF2:  55%|█████████▍       | 5524/10001 [2:47:31<2:24:56,  1.94s/batch]Batch 5500/10001 Done, mean position loss: 20.702132139205933\n",
      "Training NF2:  55%|█████████▎       | 5464/10001 [2:47:33<2:15:36,  1.79s/batch]Batch 5500/10001 Done, mean position loss: 20.375100541114808\n",
      "Training NF2:  55%|█████████▍       | 5546/10001 [2:47:36<2:35:47,  2.10s/batch]Batch 5500/10001 Done, mean position loss: 20.59059657096863\n",
      "Training NF2:  55%|█████████▍       | 5522/10001 [2:47:38<2:05:09,  1.68s/batch]Batch 5500/10001 Done, mean position loss: 20.58463038444519\n",
      "Training NF2:  55%|█████████▎       | 5500/10001 [2:47:44<2:39:09,  2.12s/batch]Batch 5500/10001 Done, mean position loss: 20.826090977191924\n",
      "Training NF2:  55%|█████████▎       | 5506/10001 [2:47:46<2:23:11,  1.91s/batch]Batch 5500/10001 Done, mean position loss: 20.56583078145981\n",
      "Training NF2:  55%|█████████▎       | 5455/10001 [2:47:48<2:30:06,  1.98s/batch]Batch 5500/10001 Done, mean position loss: 20.546875109672545\n",
      "Training NF2:  56%|█████████▍       | 5554/10001 [2:47:52<2:09:48,  1.75s/batch]Batch 5500/10001 Done, mean position loss: 20.4959889960289\n",
      "Training NF2:  55%|█████████▎       | 5513/10001 [2:47:53<1:59:16,  1.59s/batch]Batch 5500/10001 Done, mean position loss: 20.929095618724823\n",
      "Training NF2:  55%|█████████▎       | 5512/10001 [2:47:54<2:24:42,  1.93s/batch]Batch 5500/10001 Done, mean position loss: 20.50103251457214\n",
      "Training NF2:  55%|█████████▍       | 5528/10001 [2:47:56<2:56:33,  2.37s/batch]Batch 5500/10001 Done, mean position loss: 20.73249651193619\n",
      "Training NF2:  54%|█████████▎       | 5447/10001 [2:47:56<2:27:19,  1.94s/batch]Batch 5500/10001 Done, mean position loss: 20.478187866210938\n",
      "Training NF2:  55%|█████████▎       | 5488/10001 [2:48:05<2:38:08,  2.10s/batch]Batch 5500/10001 Done, mean position loss: 20.659834091663363\n",
      "Training NF2:  55%|█████████▍       | 5541/10001 [2:48:14<2:13:01,  1.79s/batch]Batch 5500/10001 Done, mean position loss: 20.442387998104095\n",
      "Training NF2:  55%|█████████▎       | 5476/10001 [2:48:27<2:09:05,  1.71s/batch]Batch 5500/10001 Done, mean position loss: 20.315665330886837\n",
      "Training NF2:  56%|█████████▍       | 5569/10001 [2:48:41<2:03:34,  1.67s/batch]Batch 5500/10001 Done, mean position loss: 20.781697707176207\n",
      "Training NF2:  56%|█████████▍       | 5584/10001 [2:48:50<2:05:25,  1.70s/batch]Batch 5500/10001 Done, mean position loss: 20.730725383758546\n",
      "Training NF2:  55%|█████████▎       | 5471/10001 [2:48:55<2:27:47,  1.96s/batch]Batch 5600/10001 Done, mean position loss: 20.347286767959595\n",
      "Training NF2:  56%|█████████▍       | 5568/10001 [2:49:03<2:03:15,  1.67s/batch]Batch 5500/10001 Done, mean position loss: 20.93953617095947\n",
      "Training NF2:  55%|█████████▍       | 5531/10001 [2:49:11<2:17:39,  1.85s/batch]Batch 5500/10001 Done, mean position loss: 20.81353692293167\n",
      "Training NF2:  56%|█████████▍       | 5587/10001 [2:49:15<2:05:18,  1.70s/batch]Batch 5600/10001 Done, mean position loss: 20.50569906949997\n",
      "Batch 5500/10001 Done, mean position loss: 20.6718266415596\n",
      "Training NF2:  55%|█████████▍       | 5521/10001 [2:49:16<2:15:20,  1.81s/batch]Batch 5600/10001 Done, mean position loss: 20.640891575813292\n",
      "Training NF2:  56%|█████████▌       | 5603/10001 [2:49:19<2:04:36,  1.70s/batch]Batch 5600/10001 Done, mean position loss: 21.09626368522644\n",
      "Training NF2:  56%|█████████▍       | 5555/10001 [2:49:20<2:14:59,  1.82s/batch]Batch 5600/10001 Done, mean position loss: 20.561862363815308\n",
      "Training NF2:  55%|█████████▍       | 5527/10001 [2:49:27<2:12:11,  1.77s/batch]Batch 5600/10001 Done, mean position loss: 20.370712809562683\n",
      "Training NF2:  56%|█████████▍       | 5586/10001 [2:49:38<2:52:45,  2.35s/batch]Batch 5500/10001 Done, mean position loss: 20.37567090749741\n",
      "Training NF2:  56%|█████████▍       | 5560/10001 [2:49:40<2:24:12,  1.95s/batch]Batch 5600/10001 Done, mean position loss: 20.452872154712676\n",
      "Training NF2:  56%|█████████▌       | 5592/10001 [2:49:48<2:13:36,  1.82s/batch]Batch 5500/10001 Done, mean position loss: 20.696592965126037\n",
      "Training NF2:  56%|█████████▍       | 5566/10001 [2:49:50<2:19:27,  1.89s/batch]Batch 5500/10001 Done, mean position loss: 20.581993179321287\n",
      "Training NF2:  56%|█████████▍       | 5574/10001 [2:49:54<2:00:08,  1.63s/batch]Batch 5600/10001 Done, mean position loss: 20.71351964712143\n",
      "Training NF2:  56%|█████████▌       | 5619/10001 [2:49:55<2:28:10,  2.03s/batch]Batch 5600/10001 Done, mean position loss: 20.983502078056333\n",
      "Training NF2:  56%|█████████▍       | 5576/10001 [2:49:56<2:14:24,  1.82s/batch]Batch 5600/10001 Done, mean position loss: 20.33279664039612\n",
      "Training NF2:  55%|█████████▎       | 5506/10001 [2:49:59<2:11:16,  1.75s/batch]Batch 5600/10001 Done, mean position loss: 20.59885127544403\n",
      "Training NF2:  56%|█████████▍       | 5573/10001 [2:50:04<2:18:53,  1.88s/batch]Batch 5500/10001 Done, mean position loss: 20.57940158367157\n",
      "Training NF2:  56%|█████████▍       | 5581/10001 [2:50:04<1:52:40,  1.53s/batch]Batch 5600/10001 Done, mean position loss: 20.713839201927186\n",
      "Training NF2:  56%|█████████▌       | 5601/10001 [2:50:05<2:03:46,  1.69s/batch]Batch 5600/10001 Done, mean position loss: 21.448140697479246\n",
      "Training NF2:  56%|█████████▌       | 5600/10001 [2:50:16<2:26:11,  1.99s/batch]Batch 5600/10001 Done, mean position loss: 20.919365248680116\n",
      "Training NF2:  56%|█████████▌       | 5633/10001 [2:50:16<2:19:15,  1.91s/batch]Batch 5600/10001 Done, mean position loss: 21.07451891899109\n",
      "Training NF2:  56%|█████████▍       | 5562/10001 [2:50:20<2:30:39,  2.04s/batch]Batch 5600/10001 Done, mean position loss: 20.45473227739334\n",
      "Training NF2:  56%|█████████▌       | 5623/10001 [2:50:41<2:31:56,  2.08s/batch]Batch 5600/10001 Done, mean position loss: 20.58968400478363\n",
      "Training NF2:  56%|█████████▌       | 5590/10001 [2:50:44<2:15:55,  1.85s/batch]Batch 5600/10001 Done, mean position loss: 20.691259188652037\n",
      "Training NF2:  56%|█████████▌       | 5627/10001 [2:50:44<2:22:12,  1.95s/batch]Batch 5600/10001 Done, mean position loss: 20.388938331604002\n",
      "Training NF2:  55%|█████████▍       | 5532/10001 [2:50:45<2:06:28,  1.70s/batch]Batch 5600/10001 Done, mean position loss: 20.582903225421905\n",
      "Training NF2:  56%|█████████▍       | 5554/10001 [2:50:52<2:32:51,  2.06s/batch]Batch 5600/10001 Done, mean position loss: 20.57086663007736\n",
      "Training NF2:  56%|█████████▌       | 5620/10001 [2:50:56<2:04:08,  1.70s/batch]Batch 5600/10001 Done, mean position loss: 20.54053137540817\n",
      "Training NF2:  55%|█████████▍       | 5540/10001 [2:50:58<2:10:17,  1.75s/batch]Batch 5600/10001 Done, mean position loss: 20.716411817073823\n",
      "Training NF2:  56%|█████████▌       | 5634/10001 [2:50:58<2:14:57,  1.85s/batch]Batch 5600/10001 Done, mean position loss: 20.858502361774445\n",
      "Training NF2:  56%|█████████▍       | 5575/10001 [2:51:01<2:32:02,  2.06s/batch]Batch 5600/10001 Done, mean position loss: 20.475815792083736\n",
      "Training NF2:  56%|█████████▍       | 5559/10001 [2:51:03<2:40:40,  2.17s/batch]Batch 5600/10001 Done, mean position loss: 20.508574106693267\n",
      "Training NF2:  56%|█████████▍       | 5560/10001 [2:51:04<2:17:53,  1.86s/batch]Batch 5600/10001 Done, mean position loss: 20.92041799068451\n",
      "Training NF2:  56%|█████████▌       | 5606/10001 [2:51:06<2:22:22,  1.94s/batch]Batch 5600/10001 Done, mean position loss: 20.498892703056335\n",
      "Training NF2:  56%|█████████▌       | 5607/10001 [2:51:09<2:11:01,  1.79s/batch]Batch 5600/10001 Done, mean position loss: 20.646778585910795\n",
      "Training NF2:  57%|█████████▋       | 5673/10001 [2:51:27<2:11:25,  1.82s/batch]Batch 5600/10001 Done, mean position loss: 20.455577216148377\n",
      "Training NF2:  56%|█████████▌       | 5641/10001 [2:51:37<2:23:30,  1.97s/batch]Batch 5600/10001 Done, mean position loss: 20.322591943740846\n",
      "Training NF2:  57%|█████████▋       | 5684/10001 [2:51:52<2:11:56,  1.83s/batch]Batch 5600/10001 Done, mean position loss: 20.785146794319154\n",
      "Training NF2:  56%|█████████▌       | 5626/10001 [2:51:55<2:22:08,  1.95s/batch]Batch 5700/10001 Done, mean position loss: 20.354386389255524\n",
      "Training NF2:  56%|█████████▌       | 5637/10001 [2:51:59<2:11:49,  1.81s/batch]Batch 5600/10001 Done, mean position loss: 20.71883061647415\n",
      "Training NF2:  56%|█████████▌       | 5597/10001 [2:52:12<2:21:07,  1.92s/batch]Batch 5600/10001 Done, mean position loss: 20.873294456005098\n",
      "Training NF2:  56%|█████████▌       | 5640/10001 [2:52:17<2:19:52,  1.92s/batch]Batch 5600/10001 Done, mean position loss: 20.8268861413002\n",
      "Batch 5700/10001 Done, mean position loss: 20.492304334640504\n",
      "Training NF2:  56%|█████████▌       | 5644/10001 [2:52:19<2:15:59,  1.87s/batch]Batch 5600/10001 Done, mean position loss: 20.672545554637907\n",
      "Training NF2:  56%|█████████▌       | 5629/10001 [2:52:22<2:21:49,  1.95s/batch]Batch 5700/10001 Done, mean position loss: 21.112283918857578\n",
      "Training NF2:  57%|█████████▌       | 5655/10001 [2:52:26<2:04:15,  1.72s/batch]Batch 5700/10001 Done, mean position loss: 20.62261539697647\n",
      "Training NF2:  56%|█████████▌       | 5633/10001 [2:52:29<2:10:22,  1.79s/batch]Batch 5700/10001 Done, mean position loss: 20.569920382499696\n",
      "Training NF2:  57%|█████████▋       | 5675/10001 [2:52:33<2:12:25,  1.84s/batch]Batch 5700/10001 Done, mean position loss: 20.372408905029296\n",
      "Training NF2:  57%|█████████▋       | 5664/10001 [2:52:44<2:04:42,  1.73s/batch]Batch 5700/10001 Done, mean position loss: 20.446214537620545\n",
      "Training NF2:  57%|█████████▋       | 5695/10001 [2:52:44<2:02:45,  1.71s/batch]Batch 5600/10001 Done, mean position loss: 20.36819831609726\n",
      "Training NF2:  56%|█████████▌       | 5647/10001 [2:52:53<2:06:42,  1.75s/batch]Batch 5600/10001 Done, mean position loss: 20.593071599006652\n",
      "Training NF2:  57%|█████████▋       | 5733/10001 [2:52:56<2:11:50,  1.85s/batch]Batch 5700/10001 Done, mean position loss: 20.724698655605316\n",
      "Training NF2:  57%|█████████▌       | 5659/10001 [2:52:57<2:36:42,  2.17s/batch]Batch 5600/10001 Done, mean position loss: 20.700453133583068\n",
      "Training NF2:  57%|█████████▋       | 5685/10001 [2:52:57<2:16:50,  1.90s/batch]Batch 5700/10001 Done, mean position loss: 20.929890332221987\n",
      "Training NF2:  57%|█████████▋       | 5726/10001 [2:53:01<2:14:18,  1.89s/batch]Batch 5700/10001 Done, mean position loss: 20.696622829437253\n",
      "Training NF2:  57%|█████████▋       | 5690/10001 [2:53:03<2:09:54,  1.81s/batch]Batch 5700/10001 Done, mean position loss: 20.582710044384005\n",
      "Training NF2:  57%|█████████▋       | 5704/10001 [2:53:07<1:58:31,  1.66s/batch]Batch 5700/10001 Done, mean position loss: 20.33082955598831\n",
      "Training NF2:  57%|█████████▋       | 5671/10001 [2:53:13<2:14:26,  1.86s/batch]Batch 5600/10001 Done, mean position loss: 20.582419691085818\n",
      "Training NF2:  57%|█████████▋       | 5680/10001 [2:53:15<2:25:49,  2.02s/batch]Batch 5700/10001 Done, mean position loss: 21.43209832429886\n",
      "Training NF2:  56%|█████████▌       | 5617/10001 [2:53:25<1:56:44,  1.60s/batch]Batch 5700/10001 Done, mean position loss: 21.083214926719666\n",
      "Training NF2:  57%|█████████▋       | 5708/10001 [2:53:28<2:37:36,  2.20s/batch]Batch 5700/10001 Done, mean position loss: 20.869929778575898\n",
      "Training NF2:  57%|█████████▋       | 5718/10001 [2:53:29<2:29:58,  2.10s/batch]Batch 5700/10001 Done, mean position loss: 20.466626832485197\n",
      "Training NF2:  57%|█████████▋       | 5688/10001 [2:53:49<2:11:47,  1.83s/batch]Batch 5700/10001 Done, mean position loss: 20.67691906929016\n",
      "Batch 5700/10001 Done, mean position loss: 20.57253743171692\n",
      "Training NF2:  57%|█████████▌       | 5651/10001 [2:53:51<2:23:41,  1.98s/batch]Batch 5700/10001 Done, mean position loss: 20.588117399215697\n",
      "Training NF2:  57%|█████████▋       | 5679/10001 [2:53:55<2:19:01,  1.93s/batch]Batch 5700/10001 Done, mean position loss: 20.533630073070526\n",
      "Training NF2:  56%|█████████▌       | 5635/10001 [2:53:56<2:29:16,  2.05s/batch]Batch 5700/10001 Done, mean position loss: 20.378498377799986\n",
      "Training NF2:  57%|█████████▊       | 5746/10001 [2:53:56<2:10:59,  1.85s/batch]Batch 5700/10001 Done, mean position loss: 20.587230365276337\n",
      "Training NF2:  56%|█████████▌       | 5638/10001 [2:54:02<2:22:36,  1.96s/batch]Batch 5700/10001 Done, mean position loss: 20.858785808086395\n",
      "Training NF2:  56%|█████████▌       | 5639/10001 [2:54:04<2:18:08,  1.90s/batch]Batch 5700/10001 Done, mean position loss: 20.721206250190733\n",
      "Training NF2:  57%|█████████▋       | 5735/10001 [2:54:09<2:13:07,  1.87s/batch]Batch 5700/10001 Done, mean position loss: 20.48414213657379\n",
      "Training NF2:  57%|█████████▋       | 5704/10001 [2:54:10<2:22:02,  1.98s/batch]Batch 5700/10001 Done, mean position loss: 20.473444321155547\n",
      "Training NF2:  57%|█████████▋       | 5679/10001 [2:54:15<2:16:11,  1.89s/batch]Batch 5700/10001 Done, mean position loss: 20.48733023166657\n",
      "Training NF2:  57%|█████████▋       | 5701/10001 [2:54:16<2:23:31,  2.00s/batch]Batch 5700/10001 Done, mean position loss: 20.63531005382538\n",
      "Training NF2:  57%|█████████▋       | 5684/10001 [2:54:16<2:44:04,  2.28s/batch]Batch 5700/10001 Done, mean position loss: 20.9174831700325\n",
      "Training NF2:  57%|█████████▋       | 5675/10001 [2:54:37<2:04:19,  1.72s/batch]Batch 5700/10001 Done, mean position loss: 20.439566235542298\n",
      "Training NF2:  57%|█████████▋       | 5665/10001 [2:54:48<2:01:35,  1.68s/batch]Batch 5700/10001 Done, mean position loss: 20.32901877403259\n",
      "Training NF2:  57%|█████████▋       | 5663/10001 [2:54:56<2:24:29,  2.00s/batch]Batch 5700/10001 Done, mean position loss: 20.828657641410828\n",
      "Training NF2:  58%|█████████▊       | 5800/10001 [2:55:01<2:10:41,  1.87s/batch]Batch 5700/10001 Done, mean position loss: 20.717888121604922\n",
      "Training NF2:  57%|█████████▋       | 5714/10001 [2:55:03<2:11:41,  1.84s/batch]Batch 5800/10001 Done, mean position loss: 20.34945168018341\n",
      "Training NF2:  57%|█████████▊       | 5741/10001 [2:55:21<2:03:45,  1.74s/batch]Batch 5800/10001 Done, mean position loss: 20.522453496456144\n",
      "Training NF2:  58%|█████████▊       | 5751/10001 [2:55:22<1:48:08,  1.53s/batch]Batch 5800/10001 Done, mean position loss: 21.078732202053068\n",
      "Training NF2:  58%|█████████▊       | 5753/10001 [2:55:30<2:17:50,  1.95s/batch]Batch 5700/10001 Done, mean position loss: 20.90653933286667\n",
      "Training NF2:  57%|█████████▊       | 5741/10001 [2:55:31<2:44:10,  2.31s/batch]Batch 5800/10001 Done, mean position loss: 20.628748710155485\n",
      "Training NF2:  58%|█████████▊       | 5806/10001 [2:55:30<2:14:18,  1.92s/batch]Batch 5700/10001 Done, mean position loss: 20.816462318897248\n",
      "Training NF2:  57%|█████████▋       | 5685/10001 [2:55:34<1:48:55,  1.51s/batch]Batch 5800/10001 Done, mean position loss: 20.571866533756257\n",
      "Training NF2:  58%|█████████▊       | 5758/10001 [2:55:38<1:56:38,  1.65s/batch]Batch 5700/10001 Done, mean position loss: 20.683089270591736\n",
      "Training NF2:  58%|█████████▉       | 5824/10001 [2:55:44<2:06:07,  1.81s/batch]Batch 5800/10001 Done, mean position loss: 20.365835275650024\n",
      "Training NF2:  58%|█████████▊       | 5766/10001 [2:55:52<2:17:01,  1.94s/batch]Batch 5700/10001 Done, mean position loss: 20.368839433193205\n",
      "Training NF2:  58%|█████████▊       | 5780/10001 [2:55:52<2:00:16,  1.71s/batch]Batch 5800/10001 Done, mean position loss: 20.451672134399413\n",
      "Training NF2:  58%|█████████▊       | 5772/10001 [2:55:58<1:57:53,  1.67s/batch]Batch 5800/10001 Done, mean position loss: 20.714019973278045\n",
      "Training NF2:  58%|█████████▊       | 5786/10001 [2:56:01<1:59:18,  1.70s/batch]Batch 5700/10001 Done, mean position loss: 20.68754442691803\n",
      "Training NF2:  58%|█████████▊       | 5769/10001 [2:56:02<2:07:48,  1.81s/batch]Batch 5800/10001 Done, mean position loss: 20.72811252117157\n",
      "Training NF2:  58%|█████████▉       | 5813/10001 [2:56:03<1:59:32,  1.71s/batch]Batch 5700/10001 Done, mean position loss: 20.571394052505493\n",
      "Training NF2:  57%|█████████▊       | 5748/10001 [2:56:07<2:07:35,  1.80s/batch]Batch 5800/10001 Done, mean position loss: 20.93995371103287\n",
      "Training NF2:  58%|█████████▊       | 5776/10001 [2:56:08<2:18:15,  1.96s/batch]Batch 5800/10001 Done, mean position loss: 20.58969364643097\n",
      "Training NF2:  58%|█████████▊       | 5762/10001 [2:56:11<2:15:57,  1.92s/batch]Batch 5800/10001 Done, mean position loss: 20.317764246463774\n",
      "Training NF2:  58%|█████████▊       | 5808/10001 [2:56:11<2:05:49,  1.80s/batch]Batch 5800/10001 Done, mean position loss: 21.446824460029603\n",
      "Training NF2:  58%|█████████▊       | 5777/10001 [2:56:19<1:56:02,  1.65s/batch]Batch 5700/10001 Done, mean position loss: 20.58474739074707\n",
      "Training NF2:  58%|█████████▉       | 5814/10001 [2:56:31<2:47:25,  2.40s/batch]Batch 5800/10001 Done, mean position loss: 20.463795680999755\n",
      "Training NF2:  58%|█████████▊       | 5772/10001 [2:56:32<2:38:59,  2.26s/batch]Batch 5800/10001 Done, mean position loss: 21.027626905441284\n",
      "Training NF2:  58%|█████████▉       | 5813/10001 [2:56:32<2:26:18,  2.10s/batch]Batch 5800/10001 Done, mean position loss: 20.906713163852693\n",
      "Training NF2:  58%|█████████▊       | 5759/10001 [2:56:54<2:01:01,  1.71s/batch]Batch 5800/10001 Done, mean position loss: 20.587260673046114\n",
      "Training NF2:  58%|█████████▉       | 5827/10001 [2:56:58<2:14:33,  1.93s/batch]Batch 5800/10001 Done, mean position loss: 20.68555244922638\n",
      "Training NF2:  58%|█████████▊       | 5794/10001 [2:57:01<2:17:53,  1.97s/batch]Batch 5800/10001 Done, mean position loss: 20.377423198223113\n",
      "Training NF2:  58%|█████████▉       | 5830/10001 [2:57:01<2:16:55,  1.97s/batch]Batch 5800/10001 Done, mean position loss: 20.551407351493836\n",
      "Training NF2:  58%|█████████▊       | 5802/10001 [2:57:02<1:48:33,  1.55s/batch]Batch 5800/10001 Done, mean position loss: 20.577489688396454\n",
      "Training NF2:  58%|█████████▊       | 5773/10001 [2:57:03<2:05:43,  1.78s/batch]Batch 5800/10001 Done, mean position loss: 20.554941792488098\n",
      "Training NF2:  57%|█████████▋       | 5724/10001 [2:57:06<2:16:49,  1.92s/batch]Batch 5800/10001 Done, mean position loss: 20.708181488513947\n",
      "Training NF2:  58%|█████████▊       | 5806/10001 [2:57:13<2:20:28,  2.01s/batch]Batch 5800/10001 Done, mean position loss: 20.8402184009552\n",
      "Training NF2:  58%|█████████▊       | 5808/10001 [2:57:16<2:04:31,  1.78s/batch]Batch 5800/10001 Done, mean position loss: 20.47179207801819\n",
      "Training NF2:  58%|█████████▉       | 5838/10001 [2:57:18<2:01:39,  1.75s/batch]Batch 5800/10001 Done, mean position loss: 20.496271078586577\n",
      "Training NF2:  58%|█████████▊       | 5805/10001 [2:57:26<2:04:48,  1.78s/batch]Batch 5800/10001 Done, mean position loss: 20.63485890865326\n",
      "Training NF2:  59%|█████████▉       | 5880/10001 [2:57:27<1:54:07,  1.66s/batch]Batch 5800/10001 Done, mean position loss: 20.483298246860503\n",
      "Training NF2:  57%|█████████▊       | 5745/10001 [2:57:27<2:04:22,  1.75s/batch]Batch 5800/10001 Done, mean position loss: 20.951071016788482\n",
      "Training NF2:  58%|█████████▊       | 5756/10001 [2:57:46<2:01:57,  1.72s/batch]Batch 5800/10001 Done, mean position loss: 20.433465096950528\n",
      "Training NF2:  59%|█████████▉       | 5858/10001 [2:57:55<1:59:12,  1.73s/batch]Batch 5800/10001 Done, mean position loss: 20.32281025648117\n",
      "Training NF2:  58%|█████████▊       | 5770/10001 [2:57:59<1:59:07,  1.69s/batch]Batch 5800/10001 Done, mean position loss: 20.71260100841522\n",
      "Training NF2:  58%|█████████▉       | 5836/10001 [2:58:03<1:53:50,  1.64s/batch]Batch 5900/10001 Done, mean position loss: 20.348485643863675\n",
      "Training NF2:  59%|██████████       | 5904/10001 [2:58:09<1:55:45,  1.70s/batch]Batch 5800/10001 Done, mean position loss: 20.785267660617826\n",
      "Training NF2:  59%|█████████▉       | 5875/10001 [2:58:22<1:55:09,  1.67s/batch]Batch 5900/10001 Done, mean position loss: 20.51975210189819\n",
      "Training NF2:  58%|█████████▉       | 5812/10001 [2:58:27<1:48:47,  1.56s/batch]Batch 5900/10001 Done, mean position loss: 21.085605726242065\n",
      "Training NF2:  59%|█████████▉       | 5860/10001 [2:58:29<2:33:13,  2.22s/batch]Batch 5900/10001 Done, mean position loss: 20.55434767007828\n",
      "Training NF2:  58%|█████████▉       | 5847/10001 [2:58:31<1:42:13,  1.48s/batch]Batch 5900/10001 Done, mean position loss: 20.639542984962464\n",
      "Training NF2:  59%|█████████▉       | 5858/10001 [2:58:32<2:10:42,  1.89s/batch]Batch 5800/10001 Done, mean position loss: 20.833593566417694\n",
      "Training NF2:  59%|██████████       | 5886/10001 [2:58:40<1:51:20,  1.62s/batch]Batch 5800/10001 Done, mean position loss: 20.67905065059662\n",
      "Training NF2:  58%|█████████▉       | 5832/10001 [2:58:41<1:47:24,  1.55s/batch]Batch 5800/10001 Done, mean position loss: 20.893041410446166\n",
      "Training NF2:  58%|█████████▉       | 5834/10001 [2:58:44<1:44:09,  1.50s/batch]Batch 5900/10001 Done, mean position loss: 20.376890785694123\n",
      "Training NF2:  58%|█████████▊       | 5793/10001 [2:58:54<1:43:10,  1.47s/batch]Batch 5800/10001 Done, mean position loss: 20.385974369049073\n",
      "Training NF2:  59%|██████████       | 5892/10001 [2:58:57<2:24:53,  2.12s/batch]Batch 5900/10001 Done, mean position loss: 20.697319571971896\n",
      "Training NF2:  59%|██████████       | 5911/10001 [2:58:58<1:43:38,  1.52s/batch]Batch 5900/10001 Done, mean position loss: 20.441111495494845\n",
      "Training NF2:  58%|█████████▊       | 5805/10001 [2:59:01<2:01:20,  1.74s/batch]Batch 5900/10001 Done, mean position loss: 20.71054314851761\n",
      "Training NF2:  59%|█████████▉       | 5880/10001 [2:59:07<2:14:34,  1.96s/batch]Batch 5800/10001 Done, mean position loss: 20.679016981124878\n",
      "Training NF2:  59%|█████████▉       | 5872/10001 [2:59:06<2:18:28,  2.01s/batch]Batch 5900/10001 Done, mean position loss: 20.586144890785214\n",
      "Training NF2:  59%|██████████       | 5908/10001 [2:59:10<1:59:07,  1.75s/batch]Batch 5800/10001 Done, mean position loss: 20.553835361003873\n",
      "Training NF2:  58%|█████████▊       | 5789/10001 [2:59:10<2:33:10,  2.18s/batch]Batch 5900/10001 Done, mean position loss: 20.318742368221283\n",
      "Training NF2:  59%|█████████▉       | 5878/10001 [2:59:11<2:04:08,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 21.444655096530916\n",
      "Training NF2:  58%|█████████▉       | 5813/10001 [2:59:17<2:13:17,  1.91s/batch]Batch 5900/10001 Done, mean position loss: 20.958484699726107\n",
      "Training NF2:  59%|██████████       | 5914/10001 [2:59:31<1:52:38,  1.65s/batch]Batch 5800/10001 Done, mean position loss: 20.56369604587555\n",
      "Training NF2:  59%|█████████▉       | 5875/10001 [2:59:34<2:14:16,  1.95s/batch]Batch 5900/10001 Done, mean position loss: 20.46053919315338\n",
      "Training NF2:  59%|█████████▉       | 5865/10001 [2:59:36<1:50:21,  1.60s/batch]Batch 5900/10001 Done, mean position loss: 20.952077090740204\n",
      "Training NF2:  59%|██████████       | 5925/10001 [2:59:43<1:45:55,  1.56s/batch]Batch 5900/10001 Done, mean position loss: 21.01203652381897\n",
      "Training NF2:  60%|██████████       | 5951/10001 [2:59:51<2:00:42,  1.79s/batch]Batch 5900/10001 Done, mean position loss: 20.58121932029724\n",
      "Training NF2:  60%|██████████       | 5954/10001 [2:59:56<1:52:15,  1.66s/batch]Batch 5900/10001 Done, mean position loss: 20.690171470642092\n",
      "Training NF2:  59%|██████████       | 5900/10001 [2:59:57<2:20:03,  2.05s/batch]Batch 5900/10001 Done, mean position loss: 20.57540183067322\n",
      "Training NF2:  58%|█████████▉       | 5844/10001 [2:59:57<2:29:09,  2.15s/batch]Batch 5900/10001 Done, mean position loss: 20.52487066030502\n",
      "Training NF2:  58%|█████████▉       | 5848/10001 [2:59:59<2:05:23,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 20.379803974628448\n",
      "Training NF2:  59%|██████████       | 5886/10001 [2:59:59<1:56:36,  1.70s/batch]Batch 5900/10001 Done, mean position loss: 20.560834290981294\n",
      "Training NF2:  59%|██████████       | 5887/10001 [3:00:02<1:50:18,  1.61s/batch]Batch 5900/10001 Done, mean position loss: 20.709427902698515\n",
      "Training NF2:  59%|█████████▉       | 5877/10001 [3:00:11<2:12:26,  1.93s/batch]Batch 5900/10001 Done, mean position loss: 20.81156648397446\n",
      "Training NF2:  59%|█████████▉       | 5853/10001 [3:00:12<1:38:27,  1.42s/batch]Batch 5900/10001 Done, mean position loss: 20.476648616790772\n",
      "Training NF2:  59%|█████████▉       | 5877/10001 [3:00:21<2:03:37,  1.80s/batch]Batch 5900/10001 Done, mean position loss: 20.46888337850571\n",
      "Training NF2:  59%|██████████       | 5945/10001 [3:00:26<2:11:51,  1.95s/batch]Batch 5900/10001 Done, mean position loss: 20.497330334186554\n",
      "Training NF2:  58%|█████████▉       | 5831/10001 [3:00:27<2:10:32,  1.88s/batch]Batch 5900/10001 Done, mean position loss: 20.925262706279753\n",
      "Training NF2:  59%|██████████       | 5943/10001 [3:00:28<2:11:46,  1.95s/batch]Batch 5900/10001 Done, mean position loss: 20.630176100730896\n",
      "Training NF2:  59%|██████████       | 5938/10001 [3:00:42<1:40:54,  1.49s/batch]Batch 5900/10001 Done, mean position loss: 20.42496372938156\n",
      "Training NF2:  60%|██████████▏      | 5984/10001 [3:00:55<2:30:40,  2.25s/batch]Batch 5900/10001 Done, mean position loss: 20.31969731807709\n",
      "Training NF2:  59%|██████████       | 5950/10001 [3:01:02<2:05:17,  1.86s/batch]Batch 6000/10001 Done, mean position loss: 20.35855551481247\n",
      "Training NF2:  59%|██████████       | 5938/10001 [3:01:04<2:10:10,  1.92s/batch]Batch 5900/10001 Done, mean position loss: 20.712709164619447\n",
      "Training NF2:  60%|██████████▏      | 5991/10001 [3:01:07<1:53:34,  1.70s/batch]Batch 5900/10001 Done, mean position loss: 20.79571501970291\n",
      "Training NF2:  60%|██████████▏      | 5972/10001 [3:01:19<2:07:39,  1.90s/batch]Batch 6000/10001 Done, mean position loss: 20.521356678009035\n",
      "Training NF2:  60%|██████████       | 5952/10001 [3:01:24<2:34:37,  2.29s/batch]Batch 6000/10001 Done, mean position loss: 20.55554782152176\n",
      "Training NF2:  59%|██████████       | 5918/10001 [3:01:24<1:53:31,  1.67s/batch]Batch 6000/10001 Done, mean position loss: 21.089471487998964\n",
      "Training NF2:  59%|██████████       | 5900/10001 [3:01:33<1:48:34,  1.59s/batch]Batch 6000/10001 Done, mean position loss: 20.637095239162445\n",
      "Training NF2:  59%|██████████       | 5894/10001 [3:01:35<2:21:05,  2.06s/batch]Batch 5900/10001 Done, mean position loss: 20.890624537467957\n",
      "Training NF2:  60%|██████████▏      | 5986/10001 [3:01:37<2:00:27,  1.80s/batch]Batch 5900/10001 Done, mean position loss: 20.815480594635012\n",
      "Training NF2:  60%|██████████▏      | 5988/10001 [3:01:40<1:53:40,  1.70s/batch]Batch 6000/10001 Done, mean position loss: 20.38216204881668\n",
      "Training NF2:  60%|██████████▏      | 6010/10001 [3:01:49<2:00:39,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 20.670630815029146\n",
      "Training NF2:  59%|██████████       | 5937/10001 [3:01:50<1:51:13,  1.64s/batch]Batch 6000/10001 Done, mean position loss: 20.68999027967453\n",
      "Training NF2:  59%|██████████       | 5906/10001 [3:01:57<2:01:02,  1.77s/batch]Batch 5900/10001 Done, mean position loss: 20.371292741298674\n",
      "Training NF2:  60%|██████████▏      | 5958/10001 [3:02:03<1:59:57,  1.78s/batch]Batch 6000/10001 Done, mean position loss: 20.435871090888977\n",
      "Training NF2:  59%|██████████       | 5944/10001 [3:02:05<2:12:10,  1.95s/batch]Batch 6000/10001 Done, mean position loss: 20.701231713294984\n",
      "Training NF2:  60%|██████████▏      | 5960/10001 [3:02:08<2:10:05,  1.93s/batch]Batch 5900/10001 Done, mean position loss: 20.59152011871338\n",
      "Training NF2:  59%|██████████       | 5899/10001 [3:02:12<2:19:06,  2.03s/batch]Batch 6000/10001 Done, mean position loss: 20.608101935386657\n",
      "Training NF2:  59%|██████████       | 5886/10001 [3:02:12<2:05:37,  1.83s/batch]Batch 6000/10001 Done, mean position loss: 20.3175506401062\n",
      "Training NF2:  60%|██████████▏      | 6014/10001 [3:02:13<1:56:16,  1.75s/batch]Batch 6000/10001 Done, mean position loss: 21.432086379528045\n",
      "Training NF2:  60%|██████████▏      | 5987/10001 [3:02:15<2:01:20,  1.81s/batch]Batch 5900/10001 Done, mean position loss: 20.666750409603118\n",
      "Training NF2:  60%|██████████▏      | 6004/10001 [3:02:19<2:24:29,  2.17s/batch]Batch 6000/10001 Done, mean position loss: 20.96970915555954\n",
      "Training NF2:  60%|██████████▏      | 5985/10001 [3:02:37<1:58:14,  1.77s/batch]Batch 6000/10001 Done, mean position loss: 20.921076319217683\n",
      "Training NF2:  60%|██████████       | 5951/10001 [3:02:38<1:57:45,  1.74s/batch]Batch 6000/10001 Done, mean position loss: 20.451392505168915\n",
      "Training NF2:  60%|██████████▏      | 5992/10001 [3:02:40<2:01:21,  1.82s/batch]Batch 5900/10001 Done, mean position loss: 20.583969843387603\n",
      "Training NF2:  59%|██████████       | 5936/10001 [3:02:46<1:57:02,  1.73s/batch]Batch 6000/10001 Done, mean position loss: 21.04321380138397\n",
      "Training NF2:  60%|██████████▏      | 5964/10001 [3:02:58<2:14:16,  2.00s/batch]Batch 6000/10001 Done, mean position loss: 20.5722394990921\n",
      "Training NF2:  60%|██████████▏      | 5998/10001 [3:03:00<1:56:14,  1.74s/batch]Batch 6000/10001 Done, mean position loss: 20.57598862171173\n",
      "Training NF2:  60%|██████████▏      | 6022/10001 [3:03:00<2:14:28,  2.03s/batch]Batch 6000/10001 Done, mean position loss: 20.68909113883972\n",
      "Training NF2:  60%|██████████▎      | 6039/10001 [3:03:01<2:15:34,  2.05s/batch]Batch 6000/10001 Done, mean position loss: 20.530138499736786\n",
      "Training NF2:  61%|██████████▎      | 6057/10001 [3:03:05<1:43:46,  1.58s/batch]Batch 6000/10001 Done, mean position loss: 20.70873397350311\n",
      "Training NF2:  60%|██████████▎      | 6033/10001 [3:03:06<2:14:09,  2.03s/batch]Batch 6000/10001 Done, mean position loss: 20.384137718677522\n",
      "Training NF2:  61%|██████████▎      | 6057/10001 [3:03:08<2:01:38,  1.85s/batch]Batch 6000/10001 Done, mean position loss: 20.558217689990997\n",
      "Training NF2:  60%|██████████▏      | 5985/10001 [3:03:22<2:11:32,  1.97s/batch]Batch 6000/10001 Done, mean position loss: 20.825951676368714\n",
      "Training NF2:  60%|██████████▎      | 6049/10001 [3:03:22<2:04:45,  1.89s/batch]Batch 6000/10001 Done, mean position loss: 20.47440547466278\n",
      "Training NF2:  60%|██████████▎      | 6045/10001 [3:03:28<2:08:40,  1.95s/batch]Batch 6000/10001 Done, mean position loss: 20.61307683944702\n",
      "Training NF2:  59%|██████████       | 5927/10001 [3:03:30<2:17:33,  2.03s/batch]Batch 6000/10001 Done, mean position loss: 20.478004622459412\n",
      "Training NF2:  60%|██████████▏      | 6019/10001 [3:03:33<2:13:28,  2.01s/batch]Batch 6000/10001 Done, mean position loss: 20.92122602701187\n",
      "Training NF2:  60%|██████████▏      | 6008/10001 [3:03:35<1:57:04,  1.76s/batch]Batch 6000/10001 Done, mean position loss: 20.490206589698793\n",
      "Training NF2:  61%|██████████▎      | 6054/10001 [3:03:53<2:04:49,  1.90s/batch]Batch 6000/10001 Done, mean position loss: 20.434063427448272\n",
      "Training NF2:  60%|██████████▏      | 5982/10001 [3:04:08<1:55:06,  1.72s/batch]Batch 6000/10001 Done, mean position loss: 20.308561239242554\n",
      "Training NF2:  61%|██████████▎      | 6063/10001 [3:04:14<2:21:36,  2.16s/batch]Batch 6000/10001 Done, mean position loss: 20.80140733718872\n",
      "Training NF2:  60%|██████████▎      | 6032/10001 [3:04:17<2:00:11,  1.82s/batch]Batch 6000/10001 Done, mean position loss: 20.73993217945099\n",
      "Training NF2:  60%|██████████▎      | 6038/10001 [3:04:17<1:44:34,  1.58s/batch]Batch 6100/10001 Done, mean position loss: 20.345626332759856\n",
      "Training NF2:  60%|██████████▏      | 5979/10001 [3:04:27<1:57:11,  1.75s/batch]Batch 6100/10001 Done, mean position loss: 21.091712832450867\n",
      "Training NF2:  60%|██████████▏      | 5971/10001 [3:04:28<2:07:56,  1.90s/batch]Batch 6100/10001 Done, mean position loss: 20.506606874465945\n",
      "Training NF2:  60%|██████████▎      | 6034/10001 [3:04:34<2:02:43,  1.86s/batch]Batch 6100/10001 Done, mean position loss: 20.54380427598953\n",
      "Training NF2:  60%|██████████▎      | 6050/10001 [3:04:34<2:18:55,  2.11s/batch]Batch 6100/10001 Done, mean position loss: 20.643006544113156\n",
      "Training NF2:  60%|██████████▏      | 6017/10001 [3:04:43<1:56:39,  1.76s/batch]Batch 6000/10001 Done, mean position loss: 20.806898050308227\n",
      "Training NF2:  61%|██████████▍      | 6107/10001 [3:04:42<1:39:46,  1.54s/batch]Batch 6100/10001 Done, mean position loss: 20.3633642911911\n",
      "Training NF2:  60%|██████████▏      | 6006/10001 [3:04:51<1:56:34,  1.75s/batch]Batch 6000/10001 Done, mean position loss: 20.86326069831848\n",
      "Training NF2:  61%|██████████▍      | 6111/10001 [3:04:52<1:50:37,  1.71s/batch]Batch 6100/10001 Done, mean position loss: 20.691109008789063\n",
      "Training NF2:  61%|██████████▎      | 6085/10001 [3:05:01<2:35:37,  2.38s/batch]Batch 6000/10001 Done, mean position loss: 20.669376435279847\n",
      "Training NF2:  60%|██████████▏      | 5979/10001 [3:05:08<2:11:44,  1.97s/batch]Batch 6000/10001 Done, mean position loss: 20.356260974407196\n",
      "Training NF2:  60%|██████████▎      | 6044/10001 [3:05:10<2:02:50,  1.86s/batch]Batch 6100/10001 Done, mean position loss: 20.441824793815613\n",
      "Training NF2:  61%|██████████▍      | 6131/10001 [3:05:16<2:06:34,  1.96s/batch]Batch 6000/10001 Done, mean position loss: 20.57719887256622\n",
      "Training NF2:  61%|██████████▎      | 6085/10001 [3:05:16<2:10:18,  2.00s/batch]Batch 6100/10001 Done, mean position loss: 20.572043364048003\n",
      "Training NF2:  61%|██████████▎      | 6089/10001 [3:05:20<2:00:14,  1.84s/batch]Batch 6100/10001 Done, mean position loss: 20.696426169872282\n",
      "Training NF2:  61%|██████████▎      | 6102/10001 [3:05:24<2:34:44,  2.38s/batch]Batch 6100/10001 Done, mean position loss: 20.321870772838594\n",
      "Training NF2:  61%|██████████▍      | 6135/10001 [3:05:27<1:50:50,  1.72s/batch]Batch 6000/10001 Done, mean position loss: 20.660378699302672\n",
      "Training NF2:  61%|██████████▎      | 6084/10001 [3:05:30<2:17:51,  2.11s/batch]Batch 6100/10001 Done, mean position loss: 21.412260360717774\n",
      "Training NF2:  61%|██████████▍      | 6140/10001 [3:05:34<2:00:24,  1.87s/batch]Batch 6100/10001 Done, mean position loss: 20.983051860332488\n",
      "Training NF2:  60%|██████████▏      | 6014/10001 [3:05:41<2:11:48,  1.98s/batch]Batch 6100/10001 Done, mean position loss: 20.940464599132536\n",
      "Training NF2:  61%|██████████▎      | 6079/10001 [3:05:48<1:46:18,  1.63s/batch]Batch 6100/10001 Done, mean position loss: 20.44680978059769\n",
      "Training NF2:  61%|██████████▍      | 6131/10001 [3:05:49<2:07:09,  1.97s/batch]Batch 6000/10001 Done, mean position loss: 20.57689234972\n",
      "Training NF2:  60%|██████████▏      | 6021/10001 [3:06:02<1:58:30,  1.79s/batch]Batch 6100/10001 Done, mean position loss: 20.576161086559296\n",
      "Training NF2:  62%|██████████▍      | 6157/10001 [3:06:04<1:56:04,  1.81s/batch]Batch 6100/10001 Done, mean position loss: 20.663548271656037\n",
      "Training NF2:  61%|██████████▍      | 6140/10001 [3:06:04<1:57:30,  1.83s/batch]Batch 6100/10001 Done, mean position loss: 21.068916795253756\n",
      "Training NF2:  61%|██████████▍      | 6112/10001 [3:06:06<1:31:12,  1.41s/batch]Batch 6100/10001 Done, mean position loss: 20.367388706207276\n",
      "Training NF2:  61%|██████████▎      | 6089/10001 [3:06:08<2:06:09,  1.93s/batch]Batch 6100/10001 Done, mean position loss: 20.547695884704588\n",
      "Training NF2:  61%|██████████▍      | 6131/10001 [3:06:11<1:54:52,  1.78s/batch]Batch 6100/10001 Done, mean position loss: 20.536708805561066\n",
      "Training NF2:  60%|██████████▏      | 6013/10001 [3:06:11<1:59:06,  1.79s/batch]Batch 6100/10001 Done, mean position loss: 20.575747911930083\n",
      "Training NF2:  61%|██████████▍      | 6106/10001 [3:06:18<2:18:58,  2.14s/batch]Batch 6100/10001 Done, mean position loss: 20.684359471797944\n",
      "Training NF2:  60%|██████████▎      | 6045/10001 [3:06:29<2:14:59,  2.05s/batch]Batch 6100/10001 Done, mean position loss: 20.469434587955476\n",
      "Training NF2:  60%|██████████▏      | 6021/10001 [3:06:29<2:32:22,  2.30s/batch]Batch 6100/10001 Done, mean position loss: 20.82900687456131\n",
      "Training NF2:  62%|██████████▍      | 6161/10001 [3:06:29<2:04:28,  1.94s/batch]Batch 6100/10001 Done, mean position loss: 20.484907736778258\n",
      "Training NF2:  62%|██████████▍      | 6158/10001 [3:06:38<1:56:49,  1.82s/batch]Batch 6100/10001 Done, mean position loss: 20.48223036766052\n",
      "Training NF2:  60%|██████████▎      | 6039/10001 [3:06:38<2:04:10,  1.88s/batch]Batch 6100/10001 Done, mean position loss: 20.639550671577453\n",
      "Training NF2:  61%|██████████▍      | 6107/10001 [3:06:40<1:51:56,  1.72s/batch]Batch 6100/10001 Done, mean position loss: 20.92409518480301\n",
      "Training NF2:  62%|██████████▍      | 6156/10001 [3:06:57<2:05:43,  1.96s/batch]Batch 6100/10001 Done, mean position loss: 20.441523952484133\n",
      "Training NF2:  61%|██████████▍      | 6121/10001 [3:07:14<2:04:53,  1.93s/batch]Batch 6100/10001 Done, mean position loss: 20.326986038684844\n",
      "Training NF2:  62%|██████████▍      | 6157/10001 [3:07:22<2:01:00,  1.89s/batch]Batch 6100/10001 Done, mean position loss: 20.727315318584445\n",
      "Training NF2:  61%|██████████▍      | 6134/10001 [3:07:22<1:48:19,  1.68s/batch]Batch 6100/10001 Done, mean position loss: 20.799953763484957\n",
      "Training NF2:  62%|██████████▌      | 6199/10001 [3:07:29<2:01:48,  1.92s/batch]Batch 6200/10001 Done, mean position loss: 21.06638909816742\n",
      "Training NF2:  61%|██████████▍      | 6107/10001 [3:07:33<1:56:01,  1.79s/batch]Batch 6200/10001 Done, mean position loss: 20.33751185655594\n",
      "Training NF2:  62%|██████████▍      | 6175/10001 [3:07:35<2:03:17,  1.93s/batch]Batch 6200/10001 Done, mean position loss: 20.52300062894821\n",
      "Training NF2:  62%|██████████▌      | 6204/10001 [3:07:41<1:58:40,  1.88s/batch]Batch 6200/10001 Done, mean position loss: 20.53212914466858\n",
      "Training NF2:  61%|██████████▍      | 6148/10001 [3:07:47<2:08:14,  2.00s/batch]Batch 6200/10001 Done, mean position loss: 20.651405315399167\n",
      "Training NF2:  62%|██████████▌      | 6188/10001 [3:07:52<1:54:39,  1.80s/batch]Batch 6200/10001 Done, mean position loss: 20.373972136974334\n",
      "Training NF2:  61%|██████████▍      | 6148/10001 [3:07:53<1:59:01,  1.85s/batch]Batch 6100/10001 Done, mean position loss: 20.8211670088768\n",
      "Training NF2:  62%|██████████▍      | 6162/10001 [3:07:57<1:49:27,  1.71s/batch]Batch 6100/10001 Done, mean position loss: 20.85563251256943\n",
      "Training NF2:  61%|██████████▍      | 6126/10001 [3:07:59<1:53:02,  1.75s/batch]Batch 6200/10001 Done, mean position loss: 20.688087279796598\n",
      "Training NF2:  62%|██████████▌      | 6225/10001 [3:08:16<1:40:11,  1.59s/batch]Batch 6200/10001 Done, mean position loss: 20.55576828956604\n",
      "Training NF2:  61%|██████████▎      | 6093/10001 [3:08:17<2:00:49,  1.85s/batch]Batch 6100/10001 Done, mean position loss: 20.678598520755767\n",
      "Training NF2:  62%|██████████▍      | 6163/10001 [3:08:20<1:49:45,  1.72s/batch]Batch 6200/10001 Done, mean position loss: 20.431015262603758\n",
      "Training NF2:  61%|██████████▍      | 6115/10001 [3:08:20<2:27:28,  2.28s/batch]Batch 6100/10001 Done, mean position loss: 20.370657596588135\n",
      "Training NF2:  62%|██████████▌      | 6204/10001 [3:08:21<1:58:21,  1.87s/batch]Batch 6200/10001 Done, mean position loss: 20.71274002313614\n",
      "Training NF2:  62%|██████████▌      | 6216/10001 [3:08:25<1:59:13,  1.89s/batch]Batch 6200/10001 Done, mean position loss: 20.31513286828995\n",
      "Training NF2:  62%|██████████▌      | 6198/10001 [3:08:28<1:42:33,  1.62s/batch]Batch 6100/10001 Done, mean position loss: 20.56809725999832\n",
      "Training NF2:  62%|██████████▌      | 6181/10001 [3:08:31<1:58:05,  1.85s/batch]Batch 6100/10001 Done, mean position loss: 20.666462087631224\n",
      "Training NF2:  62%|██████████▌      | 6195/10001 [3:08:33<2:04:40,  1.97s/batch]Batch 6200/10001 Done, mean position loss: 21.43937764406204\n",
      "Training NF2:  62%|██████████▌      | 6221/10001 [3:08:34<1:45:56,  1.68s/batch]Batch 6200/10001 Done, mean position loss: 21.03069785833359\n",
      "Training NF2:  61%|██████████▍      | 6125/10001 [3:08:44<2:11:11,  2.03s/batch]Batch 6200/10001 Done, mean position loss: 20.898786647319795\n",
      "Training NF2:  61%|██████████▍      | 6112/10001 [3:08:48<2:00:47,  1.86s/batch]Batch 6200/10001 Done, mean position loss: 20.467816405296325\n",
      "Training NF2:  61%|██████████▍      | 6117/10001 [3:09:01<2:06:13,  1.95s/batch]Batch 6100/10001 Done, mean position loss: 20.58804121017456\n",
      "Training NF2:  62%|██████████▌      | 6213/10001 [3:09:09<1:57:13,  1.86s/batch]Batch 6200/10001 Done, mean position loss: 20.351663835048676\n",
      "Training NF2:  62%|██████████▌      | 6212/10001 [3:09:09<1:54:13,  1.81s/batch]Batch 6200/10001 Done, mean position loss: 20.534495615959166\n",
      "Training NF2:  62%|██████████▌      | 6189/10001 [3:09:12<2:14:09,  2.11s/batch]Batch 6200/10001 Done, mean position loss: 20.670318806171416\n",
      "Training NF2:  62%|██████████▌      | 6184/10001 [3:09:12<2:29:25,  2.35s/batch]Batch 6200/10001 Done, mean position loss: 20.59906368017197\n",
      "Training NF2:  62%|██████████▌      | 6240/10001 [3:09:13<2:05:12,  2.00s/batch]Batch 6200/10001 Done, mean position loss: 21.027440106868745\n",
      "Training NF2:  62%|██████████▌      | 6203/10001 [3:09:15<2:14:07,  2.12s/batch]Batch 6200/10001 Done, mean position loss: 20.570251302719115\n",
      "Training NF2:  63%|██████████▋      | 6253/10001 [3:09:19<2:16:43,  2.19s/batch]Batch 6200/10001 Done, mean position loss: 20.563014500141144\n",
      "Training NF2:  62%|██████████▌      | 6210/10001 [3:09:27<1:52:49,  1.79s/batch]Batch 6200/10001 Done, mean position loss: 20.69077643632889\n",
      "Training NF2:  62%|██████████▌      | 6180/10001 [3:09:28<2:03:00,  1.93s/batch]Batch 6200/10001 Done, mean position loss: 20.476776344776155\n",
      "Training NF2:  62%|██████████▌      | 6215/10001 [3:09:33<1:47:25,  1.70s/batch]Batch 6200/10001 Done, mean position loss: 20.472631888389586\n",
      "Training NF2:  62%|██████████▌      | 6214/10001 [3:09:34<1:45:41,  1.67s/batch]Batch 6200/10001 Done, mean position loss: 20.817621200084687\n",
      "Training NF2:  62%|██████████▌      | 6231/10001 [3:09:43<2:07:43,  2.03s/batch]Batch 6200/10001 Done, mean position loss: 20.625999166965485\n",
      "Training NF2:  62%|██████████▌      | 6216/10001 [3:09:44<2:15:24,  2.15s/batch]Batch 6200/10001 Done, mean position loss: 20.94661013841629\n",
      "Training NF2:  62%|██████████▌      | 6222/10001 [3:09:44<1:36:05,  1.53s/batch]Batch 6200/10001 Done, mean position loss: 20.493936250209806\n",
      "Training NF2:  63%|██████████▋      | 6277/10001 [3:10:05<2:03:21,  1.99s/batch]Batch 6200/10001 Done, mean position loss: 20.41348907470703\n",
      "Training NF2:  62%|██████████▌      | 6219/10001 [3:10:18<2:15:41,  2.15s/batch]Batch 6200/10001 Done, mean position loss: 20.319237880706787\n",
      "Training NF2:  63%|██████████▋      | 6297/10001 [3:10:29<2:01:08,  1.96s/batch]Batch 6200/10001 Done, mean position loss: 20.80526683807373\n",
      "Training NF2:  63%|██████████▋      | 6299/10001 [3:10:33<1:45:07,  1.70s/batch]Batch 6200/10001 Done, mean position loss: 20.72465085029602\n",
      "Training NF2:  63%|██████████▋      | 6284/10001 [3:10:35<1:58:29,  1.91s/batch]Batch 6300/10001 Done, mean position loss: 20.505425252914428\n",
      "Training NF2:  62%|██████████▌      | 6213/10001 [3:10:37<1:51:56,  1.77s/batch]Batch 6300/10001 Done, mean position loss: 21.050839393138887\n",
      "Batch 6300/10001 Done, mean position loss: 20.348889930248262\n",
      "Training NF2:  63%|██████████▋      | 6295/10001 [3:10:44<1:43:03,  1.67s/batch]Batch 6300/10001 Done, mean position loss: 20.650031716823577\n",
      "Training NF2:  62%|██████████▌      | 6193/10001 [3:10:49<1:53:38,  1.79s/batch]Batch 6300/10001 Done, mean position loss: 20.531368257999418\n",
      "Training NF2:  63%|██████████▋      | 6305/10001 [3:10:55<1:31:54,  1.49s/batch]Batch 6300/10001 Done, mean position loss: 20.373765983581542\n",
      "Training NF2:  62%|██████████▌      | 6235/10001 [3:11:04<1:39:43,  1.59s/batch]Batch 6200/10001 Done, mean position loss: 20.801734519004825\n",
      "Training NF2:  63%|██████████▋      | 6262/10001 [3:11:06<1:39:20,  1.59s/batch]Batch 6300/10001 Done, mean position loss: 20.705681157112124\n",
      "Training NF2:  63%|██████████▋      | 6261/10001 [3:11:11<1:59:16,  1.91s/batch]Batch 6200/10001 Done, mean position loss: 20.83367570400238\n",
      "Training NF2:  63%|██████████▋      | 6298/10001 [3:11:21<2:01:27,  1.97s/batch]Batch 6300/10001 Done, mean position loss: 20.436235563755034\n",
      "Training NF2:  63%|██████████▊      | 6328/10001 [3:11:24<2:00:06,  1.96s/batch]Batch 6200/10001 Done, mean position loss: 20.65700676202774\n",
      "Training NF2:  63%|██████████▋      | 6268/10001 [3:11:25<2:01:36,  1.95s/batch]Batch 6300/10001 Done, mean position loss: 20.568368155956268\n",
      "Training NF2:  62%|██████████▌      | 6202/10001 [3:11:25<1:45:47,  1.67s/batch]Batch 6200/10001 Done, mean position loss: 20.365708224773407\n",
      "Training NF2:  63%|██████████▋      | 6256/10001 [3:11:28<2:09:39,  2.08s/batch]Batch 6300/10001 Done, mean position loss: 20.725247054100038\n",
      "Training NF2:  62%|██████████▌      | 6210/10001 [3:11:29<1:57:52,  1.87s/batch]Batch 6300/10001 Done, mean position loss: 20.31208074569702\n",
      "Training NF2:  63%|██████████▋      | 6266/10001 [3:11:36<1:53:32,  1.82s/batch]Batch 6200/10001 Done, mean position loss: 20.571189153194428\n",
      "Training NF2:  63%|██████████▊      | 6335/10001 [3:11:36<1:56:24,  1.91s/batch]Batch 6200/10001 Done, mean position loss: 20.691096608638762\n",
      "Training NF2:  63%|██████████▋      | 6265/10001 [3:11:46<2:09:59,  2.09s/batch]Batch 6300/10001 Done, mean position loss: 21.464690659046173\n",
      "Training NF2:  62%|██████████▌      | 6208/10001 [3:11:48<1:50:45,  1.75s/batch]Batch 6300/10001 Done, mean position loss: 20.97188833236694\n",
      "Training NF2:  63%|██████████▋      | 6283/10001 [3:11:50<1:51:58,  1.81s/batch]Batch 6300/10001 Done, mean position loss: 20.920648300647734\n",
      "Training NF2:  63%|██████████▊      | 6341/10001 [3:11:59<1:56:32,  1.91s/batch]Batch 6300/10001 Done, mean position loss: 20.461192164421085\n",
      "Training NF2:  63%|██████████▋      | 6265/10001 [3:12:09<1:45:18,  1.69s/batch]Batch 6200/10001 Done, mean position loss: 20.57232813119888\n",
      "Training NF2:  63%|██████████▋      | 6282/10001 [3:12:13<1:55:34,  1.86s/batch]Batch 6300/10001 Done, mean position loss: 20.355964314937594\n",
      "Training NF2:  63%|██████████▋      | 6285/10001 [3:12:13<2:00:35,  1.95s/batch]Batch 6300/10001 Done, mean position loss: 20.521309373378756\n",
      "Training NF2:  62%|██████████▌      | 6230/10001 [3:12:17<1:59:03,  1.89s/batch]Batch 6300/10001 Done, mean position loss: 20.601682422161105\n",
      "Training NF2:  63%|██████████▋      | 6286/10001 [3:12:22<2:04:48,  2.02s/batch]Batch 6300/10001 Done, mean position loss: 20.67683927536011\n",
      "Training NF2:  63%|██████████▋      | 6276/10001 [3:12:22<2:19:15,  2.24s/batch]Batch 6300/10001 Done, mean position loss: 21.058732249736785\n",
      "Batch 6300/10001 Done, mean position loss: 20.557324469089508\n",
      "Training NF2:  64%|██████████▊      | 6356/10001 [3:12:27<2:03:36,  2.03s/batch]Batch 6300/10001 Done, mean position loss: 20.548333892822267\n",
      "Training NF2:  63%|██████████▋      | 6306/10001 [3:12:31<1:49:00,  1.77s/batch]Batch 6300/10001 Done, mean position loss: 20.465857455730436\n",
      "Training NF2:  63%|██████████▋      | 6309/10001 [3:12:37<2:00:20,  1.96s/batch]Batch 6300/10001 Done, mean position loss: 20.68884451150894\n",
      "Training NF2:  63%|██████████▊      | 6330/10001 [3:12:41<1:52:22,  1.84s/batch]Batch 6300/10001 Done, mean position loss: 20.798417942523955\n",
      "Training NF2:  63%|██████████▊      | 6344/10001 [3:12:44<1:41:48,  1.67s/batch]Batch 6300/10001 Done, mean position loss: 20.459562139511107\n",
      "Training NF2:  63%|██████████▊      | 6342/10001 [3:12:45<1:58:13,  1.94s/batch]Batch 6300/10001 Done, mean position loss: 20.49828618049622\n",
      "Training NF2:  63%|██████████▋      | 6311/10001 [3:12:49<2:12:54,  2.16s/batch]Batch 6300/10001 Done, mean position loss: 20.925293154716492\n",
      "Training NF2:  63%|██████████▋      | 6288/10001 [3:12:53<2:04:54,  2.02s/batch]Batch 6300/10001 Done, mean position loss: 20.644695930480957\n",
      "Training NF2:  63%|██████████▊      | 6331/10001 [3:13:14<1:45:55,  1.73s/batch]Batch 6300/10001 Done, mean position loss: 20.409649164676665\n",
      "Training NF2:  63%|██████████▋      | 6264/10001 [3:13:16<1:43:54,  1.67s/batch]Batch 6300/10001 Done, mean position loss: 20.320168364048\n",
      "Training NF2:  64%|██████████▊      | 6358/10001 [3:13:36<1:41:26,  1.67s/batch]Batch 6300/10001 Done, mean position loss: 20.802400906085968\n",
      "Training NF2:  63%|██████████▋      | 6324/10001 [3:13:37<1:53:46,  1.86s/batch]Batch 6400/10001 Done, mean position loss: 20.500888376235963\n",
      "Training NF2:  63%|██████████▊      | 6329/10001 [3:13:37<1:43:46,  1.70s/batch]Batch 6400/10001 Done, mean position loss: 21.067068140506745\n",
      "Training NF2:  63%|██████████▊      | 6348/10001 [3:13:45<1:54:20,  1.88s/batch]Batch 6300/10001 Done, mean position loss: 20.73806557893753\n",
      "Training NF2:  63%|██████████▊      | 6337/10001 [3:13:51<1:49:15,  1.79s/batch]Batch 6400/10001 Done, mean position loss: 20.343982238769534\n",
      "Training NF2:  63%|██████████▊      | 6339/10001 [3:13:54<2:06:30,  2.07s/batch]Batch 6400/10001 Done, mean position loss: 20.631899619102477\n",
      "Training NF2:  63%|██████████▋      | 6280/10001 [3:13:55<1:58:32,  1.91s/batch]Batch 6400/10001 Done, mean position loss: 20.56074279785156\n",
      "Training NF2:  63%|██████████▊      | 6326/10001 [3:14:00<2:07:00,  2.07s/batch]Batch 6400/10001 Done, mean position loss: 20.382639317512513\n",
      "Training NF2:  63%|██████████▋      | 6300/10001 [3:14:06<1:50:14,  1.79s/batch]Batch 6400/10001 Done, mean position loss: 20.705202968120574\n",
      "Training NF2:  64%|██████████▊      | 6390/10001 [3:14:06<1:37:50,  1.63s/batch]Batch 6300/10001 Done, mean position loss: 20.80936434984207\n",
      "Training NF2:  64%|██████████▊      | 6368/10001 [3:14:21<1:56:38,  1.93s/batch]Batch 6400/10001 Done, mean position loss: 20.44120658636093\n",
      "Training NF2:  64%|██████████▉      | 6427/10001 [3:14:20<1:41:23,  1.70s/batch]Batch 6300/10001 Done, mean position loss: 20.86847268342972\n",
      "Training NF2:  64%|██████████▉      | 6413/10001 [3:14:25<1:39:05,  1.66s/batch]Batch 6400/10001 Done, mean position loss: 20.568101732730867\n",
      "Training NF2:  64%|██████████▊      | 6389/10001 [3:14:30<1:47:40,  1.79s/batch]Batch 6300/10001 Done, mean position loss: 20.578390691280365\n",
      "Training NF2:  64%|██████████▊      | 6379/10001 [3:14:33<1:34:17,  1.56s/batch]Batch 6300/10001 Done, mean position loss: 20.663389203548434\n",
      "Training NF2:  63%|██████████▋      | 6303/10001 [3:14:33<1:58:35,  1.92s/batch]Batch 6300/10001 Done, mean position loss: 20.355847818851473\n",
      "Training NF2:  64%|██████████▊      | 6391/10001 [3:14:34<1:46:57,  1.78s/batch]Batch 6400/10001 Done, mean position loss: 20.717873380184173\n",
      "Training NF2:  64%|██████████▊      | 6384/10001 [3:14:38<1:48:28,  1.80s/batch]Batch 6400/10001 Done, mean position loss: 20.30056218624115\n",
      "Training NF2:  63%|██████████▋      | 6308/10001 [3:14:44<1:54:19,  1.86s/batch]Batch 6300/10001 Done, mean position loss: 20.70063461780548\n",
      "Training NF2:  64%|██████████▊      | 6369/10001 [3:14:49<1:39:54,  1.65s/batch]Batch 6400/10001 Done, mean position loss: 21.45051257133484\n",
      "Training NF2:  64%|██████████▊      | 6384/10001 [3:14:51<1:46:04,  1.76s/batch]Batch 6400/10001 Done, mean position loss: 20.91151768445969\n",
      "Batch 6400/10001 Done, mean position loss: 20.964149532318114\n",
      "Training NF2:  64%|██████████▊      | 6397/10001 [3:15:07<1:33:20,  1.55s/batch]Batch 6400/10001 Done, mean position loss: 20.465626862049103\n",
      "Training NF2:  64%|██████████▊      | 6366/10001 [3:15:12<1:57:03,  1.93s/batch]Batch 6400/10001 Done, mean position loss: 20.367710311412814\n",
      "Training NF2:  63%|██████████▊      | 6329/10001 [3:15:14<1:43:47,  1.70s/batch]Batch 6400/10001 Done, mean position loss: 20.538732080459596\n",
      "Training NF2:  64%|██████████▉      | 6401/10001 [3:15:13<1:42:04,  1.70s/batch]Batch 6400/10001 Done, mean position loss: 21.046048099994657\n",
      "Training NF2:  63%|██████████▊      | 6325/10001 [3:15:18<2:24:38,  2.36s/batch]Batch 6300/10001 Done, mean position loss: 20.578747367858888\n",
      "Training NF2:  64%|██████████▉      | 6417/10001 [3:15:19<1:54:55,  1.92s/batch]Batch 6400/10001 Done, mean position loss: 20.66123473882675\n",
      "Training NF2:  64%|██████████▉      | 6443/10001 [3:15:22<1:43:26,  1.74s/batch]Batch 6400/10001 Done, mean position loss: 20.590962970256804\n",
      "Batch 6400/10001 Done, mean position loss: 20.54900504589081\n",
      "Training NF2:  63%|██████████▊      | 6332/10001 [3:15:28<1:53:27,  1.86s/batch]Batch 6400/10001 Done, mean position loss: 20.576582744121552\n",
      "Training NF2:  63%|██████████▊      | 6331/10001 [3:15:33<2:30:32,  2.46s/batch]Batch 6400/10001 Done, mean position loss: 20.47557079076767\n",
      "Training NF2:  64%|██████████▉      | 6407/10001 [3:15:42<1:35:16,  1.59s/batch]Batch 6400/10001 Done, mean position loss: 20.48567847251892\n",
      "Training NF2:  64%|██████████▉      | 6429/10001 [3:15:43<1:44:14,  1.75s/batch]Batch 6400/10001 Done, mean position loss: 20.691304309368135\n",
      "Training NF2:  65%|██████████▉      | 6460/10001 [3:15:45<1:57:50,  2.00s/batch]Batch 6400/10001 Done, mean position loss: 20.46691026210785\n",
      "Training NF2:  64%|██████████▉      | 6417/10001 [3:15:49<1:47:44,  1.80s/batch]Batch 6400/10001 Done, mean position loss: 20.8107057929039\n",
      "Training NF2:  63%|██████████▋      | 6316/10001 [3:15:51<1:58:39,  1.93s/batch]Batch 6400/10001 Done, mean position loss: 20.950274174213412\n",
      "Training NF2:  64%|██████████▉      | 6440/10001 [3:16:01<1:54:17,  1.93s/batch]Batch 6400/10001 Done, mean position loss: 20.62917986869812\n",
      "Training NF2:  65%|██████████▉      | 6458/10001 [3:16:09<1:46:18,  1.80s/batch]Batch 6400/10001 Done, mean position loss: 20.395499019622804\n",
      "Training NF2:  65%|██████████▉      | 6455/10001 [3:16:15<1:45:56,  1.79s/batch]Batch 6400/10001 Done, mean position loss: 20.31340317964554\n",
      "Training NF2:  64%|██████████▊      | 6391/10001 [3:16:29<2:03:27,  2.05s/batch]Batch 6500/10001 Done, mean position loss: 20.503440988063808\n",
      "Training NF2:  65%|██████████▉      | 6465/10001 [3:16:29<1:38:09,  1.67s/batch]Batch 6500/10001 Done, mean position loss: 21.076309843063356\n",
      "Training NF2:  64%|██████████▊      | 6363/10001 [3:16:39<1:33:58,  1.55s/batch]Batch 6400/10001 Done, mean position loss: 20.78009130716324\n",
      "Training NF2:  65%|██████████▉      | 6465/10001 [3:16:46<1:47:55,  1.83s/batch]Batch 6500/10001 Done, mean position loss: 20.356964371204377\n",
      "Training NF2:  64%|██████████▉      | 6435/10001 [3:16:47<1:39:34,  1.68s/batch]Batch 6400/10001 Done, mean position loss: 20.735591924190523\n",
      "Training NF2:  64%|██████████▉      | 6420/10001 [3:16:49<2:04:15,  2.08s/batch]Batch 6500/10001 Done, mean position loss: 20.64859278678894\n",
      "Training NF2:  65%|██████████▉      | 6452/10001 [3:16:52<1:59:29,  2.02s/batch]Batch 6500/10001 Done, mean position loss: 20.38361614227295\n",
      "Training NF2:  64%|██████████▉      | 6441/10001 [3:16:55<1:49:15,  1.84s/batch]Batch 6500/10001 Done, mean position loss: 20.55345370292664\n",
      "Training NF2:  64%|██████████▊      | 6390/10001 [3:17:07<1:34:10,  1.56s/batch]Batch 6500/10001 Done, mean position loss: 20.695423171520233\n",
      "Training NF2:  64%|██████████▊      | 6391/10001 [3:17:09<1:41:59,  1.70s/batch]Batch 6400/10001 Done, mean position loss: 20.82029758453369\n",
      "Training NF2:  65%|███████████      | 6492/10001 [3:17:17<1:49:56,  1.88s/batch]Batch 6500/10001 Done, mean position loss: 20.438858511447904\n",
      "Training NF2:  65%|██████████▉      | 6471/10001 [3:17:25<1:49:14,  1.86s/batch]Batch 6400/10001 Done, mean position loss: 20.868859868049622\n",
      "Training NF2:  64%|██████████▉      | 6428/10001 [3:17:28<1:48:29,  1.82s/batch]Batch 6500/10001 Done, mean position loss: 20.58836925506592\n",
      "Training NF2:  65%|███████████      | 6485/10001 [3:17:29<1:34:48,  1.62s/batch]Batch 6400/10001 Done, mean position loss: 20.556155054569246\n",
      "Training NF2:  65%|███████████      | 6472/10001 [3:17:31<2:01:24,  2.06s/batch]Batch 6400/10001 Done, mean position loss: 20.37233754634857\n",
      "Training NF2:  65%|███████████      | 6473/10001 [3:17:35<1:43:38,  1.76s/batch]Batch 6500/10001 Done, mean position loss: 20.30642858505249\n",
      "Training NF2:  64%|██████████▊      | 6376/10001 [3:17:37<2:05:58,  2.09s/batch]Batch 6500/10001 Done, mean position loss: 20.712559547424316\n",
      "Training NF2:  65%|███████████      | 6508/10001 [3:17:41<1:59:00,  2.04s/batch]Batch 6400/10001 Done, mean position loss: 20.67173943042755\n",
      "Training NF2:  64%|██████████▉      | 6400/10001 [3:17:51<2:04:54,  2.08s/batch]Batch 6500/10001 Done, mean position loss: 21.4323770403862\n",
      "Training NF2:  65%|███████████      | 6526/10001 [3:17:54<1:32:20,  1.59s/batch]Batch 6500/10001 Done, mean position loss: 20.87141873598099\n",
      "Training NF2:  65%|███████████      | 6486/10001 [3:17:54<1:47:06,  1.83s/batch]Batch 6400/10001 Done, mean position loss: 20.667475249767303\n",
      "Training NF2:  65%|███████████      | 6496/10001 [3:17:59<1:37:02,  1.66s/batch]Batch 6500/10001 Done, mean position loss: 20.950918390750886\n",
      "Training NF2:  66%|███████████▏     | 6555/10001 [3:18:08<2:08:05,  2.23s/batch]Batch 6500/10001 Done, mean position loss: 20.463668086528777\n",
      "Training NF2:  64%|██████████▉      | 6418/10001 [3:18:10<1:38:51,  1.66s/batch]Batch 6500/10001 Done, mean position loss: 20.525689702033997\n",
      "Training NF2:  64%|██████████▉      | 6427/10001 [3:18:17<1:42:57,  1.73s/batch]Batch 6500/10001 Done, mean position loss: 21.034426546096803\n",
      "Training NF2:  65%|███████████      | 6485/10001 [3:18:19<1:47:53,  1.84s/batch]Batch 6500/10001 Done, mean position loss: 20.372560560703278\n",
      "Training NF2:  65%|██████████▉      | 6451/10001 [3:18:21<1:41:46,  1.72s/batch]Batch 6500/10001 Done, mean position loss: 20.54889876127243\n",
      "Training NF2:  65%|███████████      | 6487/10001 [3:18:23<1:48:15,  1.85s/batch]Batch 6500/10001 Done, mean position loss: 20.58830122232437\n",
      "Training NF2:  65%|███████████      | 6509/10001 [3:18:23<1:46:01,  1.82s/batch]Batch 6400/10001 Done, mean position loss: 20.57999963283539\n",
      "Training NF2:  65%|███████████      | 6515/10001 [3:18:24<1:54:44,  1.97s/batch]Batch 6500/10001 Done, mean position loss: 20.659765491485594\n",
      "Training NF2:  65%|███████████      | 6491/10001 [3:18:28<1:25:42,  1.46s/batch]Batch 6500/10001 Done, mean position loss: 20.590319399833678\n",
      "Training NF2:  65%|███████████      | 6495/10001 [3:18:35<1:47:59,  1.85s/batch]Batch 6500/10001 Done, mean position loss: 20.452765595912936\n",
      "Training NF2:  66%|███████████▏     | 6565/10001 [3:18:43<1:40:25,  1.75s/batch]Batch 6500/10001 Done, mean position loss: 20.815181696414946\n",
      "Training NF2:  65%|██████████▉      | 6455/10001 [3:18:45<1:52:27,  1.90s/batch]Batch 6500/10001 Done, mean position loss: 20.478466989994047\n",
      "Training NF2:  64%|██████████▉      | 6414/10001 [3:18:47<1:53:45,  1.90s/batch]Batch 6500/10001 Done, mean position loss: 20.67516238451004\n",
      "Training NF2:  65%|███████████      | 6530/10001 [3:18:53<1:44:52,  1.81s/batch]Batch 6500/10001 Done, mean position loss: 20.457566883564\n",
      "Training NF2:  65%|███████████      | 6500/10001 [3:19:01<1:40:59,  1.73s/batch]Batch 6500/10001 Done, mean position loss: 20.40466160058975\n",
      "Training NF2:  65%|███████████      | 6502/10001 [3:19:02<1:30:43,  1.56s/batch]Batch 6500/10001 Done, mean position loss: 20.936822967529295\n",
      "Training NF2:  64%|██████████▉      | 6447/10001 [3:19:04<2:00:31,  2.03s/batch]Batch 6500/10001 Done, mean position loss: 20.61790751695633\n",
      "Training NF2:  65%|███████████      | 6524/10001 [3:19:30<1:49:17,  1.89s/batch]Batch 6600/10001 Done, mean position loss: 21.080319573879244\n",
      "Training NF2:  66%|███████████▏     | 6570/10001 [3:19:32<1:51:18,  1.95s/batch]Batch 6500/10001 Done, mean position loss: 20.316062934398648\n",
      "Training NF2:  65%|███████████      | 6533/10001 [3:19:37<1:58:42,  2.05s/batch]Batch 6600/10001 Done, mean position loss: 20.502437636852264\n",
      "Training NF2:  65%|███████████▏     | 6546/10001 [3:19:45<1:35:07,  1.65s/batch]Batch 6500/10001 Done, mean position loss: 20.75907302856445\n",
      "Training NF2:  65%|███████████▏     | 6547/10001 [3:19:54<1:41:55,  1.77s/batch]Batch 6600/10001 Done, mean position loss: 20.35519669055939\n",
      "Training NF2:  65%|███████████      | 6527/10001 [3:19:54<1:51:04,  1.92s/batch]Batch 6600/10001 Done, mean position loss: 20.63630991220474\n",
      "Training NF2:  65%|███████████      | 6529/10001 [3:19:56<1:52:10,  1.94s/batch]Batch 6500/10001 Done, mean position loss: 20.730625486373903\n",
      "Training NF2:  65%|███████████      | 6530/10001 [3:20:00<1:55:37,  2.00s/batch]Batch 6600/10001 Done, mean position loss: 20.387363376617433\n",
      "Training NF2:  66%|███████████▏     | 6600/10001 [3:20:08<1:25:58,  1.52s/batch]Batch 6500/10001 Done, mean position loss: 20.80182336091995\n",
      "Training NF2:  65%|███████████      | 6486/10001 [3:20:09<1:50:13,  1.88s/batch]Batch 6600/10001 Done, mean position loss: 20.560182707309725\n",
      "Training NF2:  66%|███████████▏     | 6572/10001 [3:20:10<1:37:48,  1.71s/batch]Batch 6600/10001 Done, mean position loss: 20.688320727348326\n",
      "Training NF2:  65%|███████████▏     | 6548/10001 [3:20:31<1:56:12,  2.02s/batch]Batch 6600/10001 Done, mean position loss: 20.443448383808136\n",
      "Training NF2:  66%|███████████▏     | 6618/10001 [3:20:36<2:11:50,  2.34s/batch]Batch 6500/10001 Done, mean position loss: 20.86370872974396\n",
      "Training NF2:  66%|███████████▏     | 6576/10001 [3:20:37<1:49:21,  1.92s/batch]Batch 6500/10001 Done, mean position loss: 20.363543429374694\n",
      "Training NF2:  66%|███████████▏     | 6560/10001 [3:20:38<1:40:22,  1.75s/batch]Batch 6500/10001 Done, mean position loss: 20.539484684467315\n",
      "Training NF2:  66%|███████████▏     | 6618/10001 [3:20:40<1:56:32,  2.07s/batch]Batch 6600/10001 Done, mean position loss: 20.57740653038025\n",
      "Training NF2:  66%|███████████▏     | 6606/10001 [3:20:42<1:58:31,  2.09s/batch]Batch 6600/10001 Done, mean position loss: 20.739657483100892\n",
      "Training NF2:  66%|███████████▎     | 6638/10001 [3:20:42<1:47:12,  1.91s/batch]Batch 6600/10001 Done, mean position loss: 20.31161149740219\n",
      "Training NF2:  65%|███████████      | 6522/10001 [3:20:49<1:38:50,  1.70s/batch]Batch 6500/10001 Done, mean position loss: 20.679727244377137\n",
      "Training NF2:  65%|███████████      | 6481/10001 [3:20:57<1:58:30,  2.02s/batch]Batch 6600/10001 Done, mean position loss: 21.44175931215286\n",
      "Training NF2:  66%|███████████▏     | 6577/10001 [3:21:00<1:47:33,  1.88s/batch]Batch 6500/10001 Done, mean position loss: 20.672528686523435\n",
      "Training NF2:  65%|███████████▏     | 6550/10001 [3:21:06<1:50:14,  1.92s/batch]Batch 6600/10001 Done, mean position loss: 20.880092101097105\n",
      "Training NF2:  67%|███████████▎     | 6652/10001 [3:21:10<1:27:54,  1.58s/batch]Batch 6600/10001 Done, mean position loss: 20.951844487190247\n",
      "Training NF2:  65%|███████████      | 6511/10001 [3:21:17<1:34:34,  1.63s/batch]Batch 6600/10001 Done, mean position loss: 20.528891611099244\n",
      "Training NF2:  66%|███████████▏     | 6587/10001 [3:21:19<1:41:03,  1.78s/batch]Batch 6600/10001 Done, mean position loss: 20.46537091255188\n",
      "Training NF2:  66%|███████████▏     | 6586/10001 [3:21:26<1:43:55,  1.83s/batch]Batch 6600/10001 Done, mean position loss: 21.044184606075284\n",
      "Training NF2:  66%|███████████▏     | 6584/10001 [3:21:28<1:39:17,  1.74s/batch]Batch 6600/10001 Done, mean position loss: 20.562847120761873\n",
      "Training NF2:  65%|███████████      | 6518/10001 [3:21:29<1:31:01,  1.57s/batch]Batch 6600/10001 Done, mean position loss: 20.56494122982025\n",
      "Training NF2:  66%|███████████▏     | 6563/10001 [3:21:32<1:58:34,  2.07s/batch]Batch 6600/10001 Done, mean position loss: 20.564690930843355\n",
      "Training NF2:  66%|███████████▏     | 6580/10001 [3:21:33<2:02:26,  2.15s/batch]Batch 6600/10001 Done, mean position loss: 20.352073438167572\n",
      "Training NF2:  66%|███████████▏     | 6606/10001 [3:21:39<1:47:57,  1.91s/batch]Batch 6500/10001 Done, mean position loss: 20.579071009159087\n",
      "Training NF2:  66%|███████████▎     | 6620/10001 [3:21:40<1:31:16,  1.62s/batch]Batch 6600/10001 Done, mean position loss: 20.667470526695254\n",
      "Training NF2:  67%|███████████▎     | 6651/10001 [3:21:44<1:40:09,  1.79s/batch]Batch 6600/10001 Done, mean position loss: 20.66798162460327\n",
      "Training NF2:  66%|███████████▏     | 6570/10001 [3:21:45<1:49:08,  1.91s/batch]Batch 6600/10001 Done, mean position loss: 20.461211714744564\n",
      "Training NF2:  66%|███████████▎     | 6640/10001 [3:21:50<1:44:48,  1.87s/batch]Batch 6600/10001 Done, mean position loss: 20.825828413963315\n",
      "Training NF2:  66%|███████████▏     | 6562/10001 [3:21:55<1:55:28,  2.01s/batch]Batch 6600/10001 Done, mean position loss: 20.474763135910035\n",
      "Training NF2:  67%|███████████▎     | 6678/10001 [3:21:58<1:34:01,  1.70s/batch]Batch 6600/10001 Done, mean position loss: 20.46999859571457\n",
      "Training NF2:  66%|███████████▎     | 6649/10001 [3:22:11<1:46:45,  1.91s/batch]Batch 6600/10001 Done, mean position loss: 20.38951058626175\n",
      "Training NF2:  66%|███████████▎     | 6622/10001 [3:22:15<1:49:03,  1.94s/batch]Batch 6600/10001 Done, mean position loss: 20.916801280975342\n",
      "Training NF2:  66%|███████████▏     | 6612/10001 [3:22:18<1:46:03,  1.88s/batch]Batch 6600/10001 Done, mean position loss: 20.632021863460544\n",
      "Training NF2:  66%|███████████▏     | 6617/10001 [3:22:43<1:40:43,  1.79s/batch]Batch 6700/10001 Done, mean position loss: 21.10622587919235\n",
      "Training NF2:  66%|███████████▎     | 6644/10001 [3:22:45<1:42:21,  1.83s/batch]Batch 6700/10001 Done, mean position loss: 20.494899628162383\n",
      "Training NF2:  66%|███████████▏     | 6560/10001 [3:22:45<1:55:34,  2.02s/batch]Batch 6600/10001 Done, mean position loss: 20.321601390838623\n",
      "Training NF2:  66%|███████████▎     | 6642/10001 [3:22:45<1:52:29,  2.01s/batch]Batch 6600/10001 Done, mean position loss: 20.78681955099106\n",
      "Training NF2:  67%|███████████▎     | 6656/10001 [3:22:59<1:38:49,  1.77s/batch]Batch 6700/10001 Done, mean position loss: 20.359795579910276\n",
      "Training NF2:  66%|███████████▎     | 6638/10001 [3:23:03<1:41:48,  1.82s/batch]Batch 6700/10001 Done, mean position loss: 20.38845227479935\n",
      "Batch 6700/10001 Done, mean position loss: 20.617881538867948\n",
      "Training NF2:  67%|███████████▍     | 6714/10001 [3:23:05<1:30:40,  1.66s/batch]Batch 6600/10001 Done, mean position loss: 20.748127498626708\n",
      "Training NF2:  67%|███████████▎     | 6682/10001 [3:23:12<1:28:07,  1.59s/batch]Batch 6700/10001 Done, mean position loss: 20.687075641155243\n",
      "Training NF2:  67%|███████████▎     | 6654/10001 [3:23:15<1:42:09,  1.83s/batch]Batch 6700/10001 Done, mean position loss: 20.533282420635224\n",
      "Batch 6600/10001 Done, mean position loss: 20.792903990745543\n",
      "Training NF2:  67%|███████████▎     | 6666/10001 [3:23:39<1:59:19,  2.15s/batch]Batch 6700/10001 Done, mean position loss: 20.439779863357543\n",
      "Training NF2:  67%|███████████▎     | 6673/10001 [3:23:43<1:42:12,  1.84s/batch]Batch 6600/10001 Done, mean position loss: 20.864985594749452\n",
      "Batch 6600/10001 Done, mean position loss: 20.363674204349515\n",
      "Training NF2:  67%|███████████▎     | 6666/10001 [3:23:45<1:45:50,  1.90s/batch]Batch 6700/10001 Done, mean position loss: 20.5860403585434\n",
      "Training NF2:  67%|███████████▎     | 6659/10001 [3:23:46<1:34:25,  1.70s/batch]Batch 6700/10001 Done, mean position loss: 20.72182435274124\n",
      "Training NF2:  66%|███████████▎     | 6635/10001 [3:23:47<1:42:14,  1.82s/batch]Batch 6600/10001 Done, mean position loss: 20.555538012981415\n",
      "Training NF2:  67%|███████████▍     | 6727/10001 [3:23:49<1:47:51,  1.98s/batch]Batch 6700/10001 Done, mean position loss: 20.297693111896514\n",
      "Training NF2:  67%|███████████▎     | 6662/10001 [3:24:01<1:44:18,  1.87s/batch]Batch 6600/10001 Done, mean position loss: 20.68956995010376\n",
      "Training NF2:  67%|███████████▎     | 6657/10001 [3:24:03<1:45:15,  1.89s/batch]Batch 6600/10001 Done, mean position loss: 20.682986502647402\n",
      "Training NF2:  67%|███████████▍     | 6715/10001 [3:24:07<1:34:28,  1.73s/batch]Batch 6700/10001 Done, mean position loss: 21.418060805797577\n",
      "Training NF2:  67%|███████████▍     | 6701/10001 [3:24:08<1:38:52,  1.80s/batch]Batch 6700/10001 Done, mean position loss: 20.860697808265684\n",
      "Training NF2:  67%|███████████▍     | 6706/10001 [3:24:17<1:45:28,  1.92s/batch]Batch 6700/10001 Done, mean position loss: 20.949639456272127\n",
      "Training NF2:  67%|███████████▎     | 6681/10001 [3:24:20<1:25:13,  1.54s/batch]Batch 6700/10001 Done, mean position loss: 20.52662945508957\n",
      "Training NF2:  67%|███████████▎     | 6687/10001 [3:24:21<1:32:53,  1.68s/batch]Batch 6700/10001 Done, mean position loss: 20.4509659409523\n",
      "Training NF2:  67%|███████████▎     | 6673/10001 [3:24:34<1:40:55,  1.82s/batch]Batch 6700/10001 Done, mean position loss: 20.575209596157073\n",
      "Training NF2:  68%|███████████▍     | 6761/10001 [3:24:35<1:35:54,  1.78s/batch]Batch 6700/10001 Done, mean position loss: 21.01788304328918\n",
      "Training NF2:  68%|███████████▍     | 6764/10001 [3:24:40<1:34:44,  1.76s/batch]Batch 6700/10001 Done, mean position loss: 20.529388496875765\n",
      "Training NF2:  67%|███████████▍     | 6699/10001 [3:24:45<1:56:51,  2.12s/batch]Batch 6600/10001 Done, mean position loss: 20.562063138484955\n",
      "Training NF2:  67%|███████████▎     | 6654/10001 [3:24:47<1:46:31,  1.91s/batch]Batch 6700/10001 Done, mean position loss: 20.567941374778748\n",
      "Training NF2:  68%|███████████▍     | 6756/10001 [3:24:47<1:45:47,  1.96s/batch]Batch 6700/10001 Done, mean position loss: 20.366610987186434\n",
      "Training NF2:  67%|███████████▍     | 6702/10001 [3:24:49<1:40:33,  1.83s/batch]Batch 6700/10001 Done, mean position loss: 20.4686816239357\n",
      "Training NF2:  67%|███████████▍     | 6698/10001 [3:24:51<1:58:22,  2.15s/batch]Batch 6700/10001 Done, mean position loss: 20.65419435977936\n",
      "Training NF2:  68%|███████████▍     | 6761/10001 [3:24:55<1:28:59,  1.65s/batch]Batch 6700/10001 Done, mean position loss: 20.680948970317843\n",
      "Training NF2:  67%|███████████▍     | 6707/10001 [3:24:59<1:40:39,  1.83s/batch]Batch 6700/10001 Done, mean position loss: 20.481381916999815\n",
      "Training NF2:  66%|███████████▎     | 6638/10001 [3:24:59<1:44:55,  1.87s/batch]Batch 6700/10001 Done, mean position loss: 20.834447934627534\n",
      "Training NF2:  66%|███████████▎     | 6636/10001 [3:25:05<1:39:16,  1.77s/batch]Batch 6700/10001 Done, mean position loss: 20.451222643852233\n",
      "Training NF2:  67%|███████████▍     | 6737/10001 [3:25:14<1:35:41,  1.76s/batch]Batch 6700/10001 Done, mean position loss: 20.40951354503632\n",
      "Training NF2:  67%|███████████▍     | 6718/10001 [3:25:22<1:59:01,  2.18s/batch]Batch 6700/10001 Done, mean position loss: 20.959568111896516\n",
      "Training NF2:  68%|███████████▍     | 6757/10001 [3:25:25<1:30:53,  1.68s/batch]Batch 6700/10001 Done, mean position loss: 20.6564564037323\n",
      "Training NF2:  67%|███████████▍     | 6729/10001 [3:25:44<1:56:04,  2.13s/batch]Batch 6800/10001 Done, mean position loss: 20.492404842376708\n",
      "Training NF2:  67%|███████████▍     | 6716/10001 [3:25:49<1:48:25,  1.98s/batch]Batch 6700/10001 Done, mean position loss: 20.315972697734836\n",
      "Training NF2:  67%|███████████▍     | 6698/10001 [3:25:54<1:52:26,  2.04s/batch]Batch 6800/10001 Done, mean position loss: 21.081812887191774\n",
      "Training NF2:  68%|███████████▍     | 6762/10001 [3:26:00<1:42:56,  1.91s/batch]Batch 6800/10001 Done, mean position loss: 20.37477770090103\n",
      "Training NF2:  67%|███████████▍     | 6727/10001 [3:26:01<1:40:37,  1.84s/batch]Batch 6700/10001 Done, mean position loss: 20.799697127342228\n",
      "Training NF2:  67%|███████████▎     | 6671/10001 [3:26:07<1:42:32,  1.85s/batch]Batch 6800/10001 Done, mean position loss: 20.619686753749846\n",
      "Training NF2:  68%|███████████▌     | 6794/10001 [3:26:13<1:49:33,  2.05s/batch]Batch 6800/10001 Done, mean position loss: 20.380570709705353\n",
      "Training NF2:  67%|███████████▍     | 6697/10001 [3:26:14<1:33:04,  1.69s/batch]Batch 6700/10001 Done, mean position loss: 20.71510354757309\n",
      "Training NF2:  67%|███████████▍     | 6748/10001 [3:26:17<1:31:31,  1.69s/batch]Batch 6800/10001 Done, mean position loss: 20.66472002983093\n",
      "Training NF2:  68%|███████████▍     | 6751/10001 [3:26:23<1:39:06,  1.83s/batch]Batch 6700/10001 Done, mean position loss: 20.790392429828643\n",
      "Training NF2:  68%|███████████▍     | 6752/10001 [3:26:26<1:33:41,  1.73s/batch]Batch 6800/10001 Done, mean position loss: 20.53629161119461\n",
      "Training NF2:  68%|███████████▌     | 6828/10001 [3:26:43<1:37:25,  1.84s/batch]Batch 6800/10001 Done, mean position loss: 20.57108201265335\n",
      "Training NF2:  68%|███████████▌     | 6787/10001 [3:26:46<1:24:57,  1.59s/batch]Batch 6800/10001 Done, mean position loss: 20.700842373371124\n",
      "Training NF2:  67%|███████████▍     | 6716/10001 [3:26:47<1:22:40,  1.51s/batch]Batch 6800/10001 Done, mean position loss: 20.442023077011108\n",
      "Training NF2:  67%|███████████▍     | 6719/10001 [3:26:48<1:44:47,  1.92s/batch]Batch 6700/10001 Done, mean position loss: 20.3625093960762\n",
      "Training NF2:  67%|███████████▍     | 6705/10001 [3:26:54<1:28:10,  1.61s/batch]Batch 6700/10001 Done, mean position loss: 20.87175503730774\n",
      "Training NF2:  67%|███████████▍     | 6737/10001 [3:27:00<1:58:58,  2.19s/batch]Batch 6800/10001 Done, mean position loss: 20.303289136886598\n",
      "Training NF2:  67%|███████████▍     | 6698/10001 [3:27:01<1:40:19,  1.82s/batch]Batch 6700/10001 Done, mean position loss: 20.57039976835251\n",
      "Training NF2:  68%|███████████▌     | 6812/10001 [3:27:06<1:33:01,  1.75s/batch]Batch 6700/10001 Done, mean position loss: 20.696938655376435\n",
      "Training NF2:  68%|███████████▌     | 6768/10001 [3:27:09<1:32:33,  1.72s/batch]Batch 6700/10001 Done, mean position loss: 20.68792932510376\n",
      "Training NF2:  67%|███████████▍     | 6727/10001 [3:27:11<1:53:29,  2.08s/batch]Batch 6800/10001 Done, mean position loss: 21.418750119209292\n",
      "Training NF2:  68%|███████████▌     | 6830/10001 [3:27:11<1:49:53,  2.08s/batch]Batch 6800/10001 Done, mean position loss: 20.885556373596188\n",
      "Training NF2:  69%|███████████▋     | 6851/10001 [3:27:24<1:36:51,  1.84s/batch]Batch 6800/10001 Done, mean position loss: 20.448191499710084\n",
      "Training NF2:  67%|███████████▎     | 6683/10001 [3:27:25<1:46:47,  1.93s/batch]Batch 6800/10001 Done, mean position loss: 20.526435923576358\n",
      "Training NF2:  68%|███████████▌     | 6809/10001 [3:27:26<1:34:16,  1.77s/batch]Batch 6800/10001 Done, mean position loss: 20.984535887241364\n",
      "Training NF2:  67%|███████████▍     | 6722/10001 [3:27:34<1:54:06,  2.09s/batch]Batch 6800/10001 Done, mean position loss: 20.60395557165146\n",
      "Training NF2:  68%|███████████▌     | 6794/10001 [3:27:42<1:48:03,  2.02s/batch]Batch 6800/10001 Done, mean position loss: 21.02606783628464\n",
      "Training NF2:  68%|███████████▌     | 6821/10001 [3:27:47<1:32:30,  1.75s/batch]Batch 6800/10001 Done, mean position loss: 20.554874930381775\n",
      "Training NF2:  68%|███████████▌     | 6817/10001 [3:27:50<1:25:11,  1.61s/batch]Batch 6800/10001 Done, mean position loss: 20.56590934753418\n",
      "Training NF2:  68%|███████████▌     | 6796/10001 [3:27:53<1:44:07,  1.95s/batch]Batch 6800/10001 Done, mean position loss: 20.455197319984435\n",
      "Training NF2:  68%|███████████▌     | 6782/10001 [3:27:56<1:45:44,  1.97s/batch]Batch 6800/10001 Done, mean position loss: 20.34908580303192\n",
      "Training NF2:  67%|███████████▍     | 6737/10001 [3:28:00<1:35:04,  1.75s/batch]Batch 6800/10001 Done, mean position loss: 20.62580599308014\n",
      "Batch 6700/10001 Done, mean position loss: 20.559001462459563\n",
      "Training NF2:  67%|███████████▍     | 6738/10001 [3:28:02<1:40:38,  1.85s/batch]Batch 6800/10001 Done, mean position loss: 20.836952304840086\n",
      "Training NF2:  68%|███████████▌     | 6799/10001 [3:28:04<1:15:14,  1.41s/batch]Batch 6800/10001 Done, mean position loss: 20.667195734977724\n",
      "Training NF2:  68%|███████████▌     | 6832/10001 [3:28:07<1:32:31,  1.75s/batch]Batch 6800/10001 Done, mean position loss: 20.457766067981723\n",
      "Training NF2:  67%|███████████▍     | 6741/10001 [3:28:08<1:49:21,  2.01s/batch]Batch 6800/10001 Done, mean position loss: 20.463079433441163\n",
      "Training NF2:  69%|███████████▋     | 6876/10001 [3:28:20<1:30:26,  1.74s/batch]Batch 6800/10001 Done, mean position loss: 20.405730452537536\n",
      "Training NF2:  69%|███████████▋     | 6871/10001 [3:28:22<1:36:11,  1.84s/batch]Batch 6800/10001 Done, mean position loss: 20.961381607055664\n",
      "Training NF2:  68%|███████████▌     | 6827/10001 [3:28:32<1:42:10,  1.93s/batch]Batch 6800/10001 Done, mean position loss: 20.663409376144408\n",
      "Training NF2:  68%|███████████▌     | 6766/10001 [3:28:52<1:20:00,  1.48s/batch]Batch 6900/10001 Done, mean position loss: 20.497117795944213\n",
      "Training NF2:  68%|███████████▌     | 6837/10001 [3:28:59<1:55:13,  2.19s/batch]Batch 6800/10001 Done, mean position loss: 20.314487872123717\n",
      "Training NF2:  69%|███████████▋     | 6855/10001 [3:29:01<1:29:15,  1.70s/batch]Batch 6900/10001 Done, mean position loss: 21.076582894325256\n",
      "Training NF2:  69%|███████████▋     | 6886/10001 [3:29:03<1:29:29,  1.72s/batch]Batch 6900/10001 Done, mean position loss: 20.362166912555693\n",
      "Training NF2:  68%|███████████▌     | 6780/10001 [3:29:09<1:35:12,  1.77s/batch]Batch 6800/10001 Done, mean position loss: 20.765316734313963\n",
      "Training NF2:  68%|███████████▌     | 6834/10001 [3:29:17<1:15:23,  1.43s/batch]Batch 6900/10001 Done, mean position loss: 20.605472428798677\n",
      "Training NF2:  68%|███████████▌     | 6814/10001 [3:29:20<1:21:52,  1.54s/batch]Batch 6800/10001 Done, mean position loss: 20.720610060691833\n",
      "Training NF2:  69%|███████████▋     | 6885/10001 [3:29:23<1:23:46,  1.61s/batch]Batch 6900/10001 Done, mean position loss: 20.66190942287445\n",
      "Training NF2:  69%|███████████▋     | 6868/10001 [3:29:24<1:34:26,  1.81s/batch]Batch 6900/10001 Done, mean position loss: 20.375394003391264\n",
      "Training NF2:  68%|███████████▌     | 6807/10001 [3:29:33<1:44:15,  1.96s/batch]Batch 6800/10001 Done, mean position loss: 20.80838217973709\n",
      "Training NF2:  69%|███████████▋     | 6855/10001 [3:29:33<1:34:09,  1.80s/batch]Batch 6900/10001 Done, mean position loss: 20.5252352643013\n",
      "Training NF2:  68%|███████████▌     | 6786/10001 [3:29:49<1:40:16,  1.87s/batch]Batch 6900/10001 Done, mean position loss: 20.704750270843505\n",
      "Training NF2:  69%|███████████▋     | 6885/10001 [3:29:51<1:31:39,  1.76s/batch]Batch 6900/10001 Done, mean position loss: 20.580045013427736\n",
      "Training NF2:  68%|███████████▍     | 6759/10001 [3:29:52<1:44:52,  1.94s/batch]Batch 6800/10001 Done, mean position loss: 20.366826202869415\n",
      "Training NF2:  69%|███████████▋     | 6867/10001 [3:29:54<1:47:46,  2.06s/batch]Batch 6900/10001 Done, mean position loss: 20.442637276649478\n",
      "Training NF2:  69%|███████████▋     | 6862/10001 [3:29:57<1:38:37,  1.89s/batch]Batch 6800/10001 Done, mean position loss: 20.828282260894774\n",
      "Training NF2:  69%|███████████▋     | 6873/10001 [3:30:11<1:46:36,  2.04s/batch]Batch 6900/10001 Done, mean position loss: 20.294756989479065\n",
      "Training NF2:  69%|███████████▋     | 6863/10001 [3:30:15<1:43:31,  1.98s/batch]Batch 6900/10001 Done, mean position loss: 21.44016469717026\n",
      "Training NF2:  69%|███████████▋     | 6882/10001 [3:30:20<1:44:24,  2.01s/batch]Batch 6800/10001 Done, mean position loss: 20.691381430625917\n",
      "Training NF2:  69%|███████████▋     | 6866/10001 [3:30:20<1:36:15,  1.84s/batch]Batch 6800/10001 Done, mean position loss: 20.585845885276797\n",
      "Training NF2:  69%|███████████▊     | 6931/10001 [3:30:21<1:26:02,  1.68s/batch]Batch 6800/10001 Done, mean position loss: 20.671816325187685\n",
      "Training NF2:  69%|███████████▋     | 6897/10001 [3:30:21<1:39:49,  1.93s/batch]Batch 6900/10001 Done, mean position loss: 20.87890551328659\n",
      "Training NF2:  69%|███████████▋     | 6876/10001 [3:30:30<1:55:32,  2.22s/batch]Batch 6900/10001 Done, mean position loss: 20.519655742645263\n",
      "Training NF2:  69%|███████████▊     | 6922/10001 [3:30:31<1:40:55,  1.97s/batch]Batch 6900/10001 Done, mean position loss: 20.96271116256714\n",
      "Training NF2:  68%|███████████▌     | 6834/10001 [3:30:39<1:46:05,  2.01s/batch]Batch 6900/10001 Done, mean position loss: 20.44595440864563\n",
      "Training NF2:  69%|███████████▋     | 6910/10001 [3:30:46<1:35:31,  1.85s/batch]Batch 6900/10001 Done, mean position loss: 20.592478675842283\n",
      "Training NF2:  68%|███████████▋     | 6848/10001 [3:30:55<1:42:42,  1.95s/batch]Batch 6900/10001 Done, mean position loss: 21.02561320066452\n",
      "Training NF2:  68%|███████████▌     | 6820/10001 [3:30:55<1:37:08,  1.83s/batch]Batch 6900/10001 Done, mean position loss: 20.57704778432846\n",
      "Training NF2:  68%|███████████▋     | 6849/10001 [3:30:57<1:44:44,  1.99s/batch]Batch 6900/10001 Done, mean position loss: 20.44829815149307\n",
      "Training NF2:  69%|███████████▋     | 6904/10001 [3:31:01<1:40:05,  1.94s/batch]Batch 6900/10001 Done, mean position loss: 20.575154185295105\n",
      "Training NF2:  68%|███████████▌     | 6823/10001 [3:31:01<1:40:16,  1.89s/batch]Batch 6900/10001 Done, mean position loss: 20.359245345592498\n",
      "Training NF2:  69%|███████████▋     | 6895/10001 [3:31:08<1:36:14,  1.86s/batch]Batch 6900/10001 Done, mean position loss: 20.822790708541874\n",
      "Training NF2:  69%|███████████▋     | 6897/10001 [3:31:10<1:22:06,  1.59s/batch]Batch 6800/10001 Done, mean position loss: 20.566795840263367\n",
      "Training NF2:  69%|███████████▋     | 6891/10001 [3:31:13<1:16:16,  1.47s/batch]Batch 6900/10001 Done, mean position loss: 20.667288784980773\n",
      "Training NF2:  69%|███████████▋     | 6911/10001 [3:31:17<1:30:00,  1.75s/batch]Batch 6900/10001 Done, mean position loss: 20.462633025646213\n",
      "Training NF2:  69%|███████████▋     | 6893/10001 [3:31:17<1:29:24,  1.73s/batch]Batch 6900/10001 Done, mean position loss: 20.435973649024966\n",
      "Training NF2:  69%|███████████▊     | 6932/10001 [3:31:17<1:30:37,  1.77s/batch]Batch 6900/10001 Done, mean position loss: 20.628632421493528\n",
      "Training NF2:  69%|███████████▊     | 6931/10001 [3:31:22<1:34:59,  1.86s/batch]Batch 6900/10001 Done, mean position loss: 20.40208704471588\n",
      "Training NF2:  69%|███████████▋     | 6885/10001 [3:31:32<1:25:19,  1.64s/batch]Batch 6900/10001 Done, mean position loss: 20.947617185115813\n",
      "Training NF2:  69%|███████████▋     | 6858/10001 [3:31:40<1:41:46,  1.94s/batch]Batch 6900/10001 Done, mean position loss: 20.64994195461273\n",
      "Training NF2:  69%|███████████▊     | 6925/10001 [3:31:58<1:32:21,  1.80s/batch]Batch 7000/10001 Done, mean position loss: 20.481658494472505\n",
      "Training NF2:  70%|███████████▊     | 6970/10001 [3:31:58<1:26:47,  1.72s/batch]Batch 6900/10001 Done, mean position loss: 20.30967797756195\n",
      "Training NF2:  69%|███████████▊     | 6936/10001 [3:32:01<1:24:06,  1.65s/batch]Batch 7000/10001 Done, mean position loss: 20.34556360721588\n",
      "Training NF2:  70%|███████████▊     | 6958/10001 [3:32:06<1:28:14,  1.74s/batch]Batch 7000/10001 Done, mean position loss: 21.04926253080368\n",
      "Training NF2:  69%|███████████▊     | 6937/10001 [3:32:12<1:37:24,  1.91s/batch]Batch 6900/10001 Done, mean position loss: 20.76805103302002\n",
      "Training NF2:  69%|███████████▋     | 6905/10001 [3:32:19<1:32:08,  1.79s/batch]Batch 7000/10001 Done, mean position loss: 20.633901872634887\n",
      "Training NF2:  70%|███████████▊     | 6953/10001 [3:32:27<1:21:13,  1.60s/batch]Batch 7000/10001 Done, mean position loss: 20.377775766849517\n",
      "Training NF2:  70%|███████████▉     | 7017/10001 [3:32:28<1:22:06,  1.65s/batch]Batch 7000/10001 Done, mean position loss: 20.681469254493713\n",
      "Training NF2:  69%|███████████▊     | 6942/10001 [3:32:33<1:16:18,  1.50s/batch]Batch 6900/10001 Done, mean position loss: 20.72347204685211\n",
      "Training NF2:  69%|███████████▊     | 6950/10001 [3:32:36<1:38:44,  1.94s/batch]Batch 7000/10001 Done, mean position loss: 20.518279032707213\n",
      "Training NF2:  69%|███████████▊     | 6944/10001 [3:32:44<1:24:32,  1.66s/batch]Batch 6900/10001 Done, mean position loss: 20.783810994625092\n",
      "Training NF2:  70%|███████████▉     | 6996/10001 [3:32:52<1:25:59,  1.72s/batch]Batch 7000/10001 Done, mean position loss: 20.428825647830962\n",
      "Training NF2:  70%|███████████▊     | 6965/10001 [3:32:54<1:39:49,  1.97s/batch]Batch 7000/10001 Done, mean position loss: 20.57973855257034\n",
      "Training NF2:  70%|███████████▉     | 6998/10001 [3:32:56<1:36:44,  1.93s/batch]Batch 7000/10001 Done, mean position loss: 20.723426990509033\n",
      "Training NF2:  69%|███████████▊     | 6925/10001 [3:32:56<1:29:33,  1.75s/batch]Batch 6900/10001 Done, mean position loss: 20.872034850120546\n",
      "Training NF2:  69%|███████████▊     | 6927/10001 [3:33:00<1:28:22,  1.73s/batch]Batch 7000/10001 Done, mean position loss: 20.300772161483764\n",
      "Training NF2:  69%|███████████▊     | 6934/10001 [3:33:01<1:23:47,  1.64s/batch]Batch 6900/10001 Done, mean position loss: 20.37953406572342\n",
      "Training NF2:  70%|███████████▉     | 7010/10001 [3:33:08<1:29:27,  1.79s/batch]Batch 7000/10001 Done, mean position loss: 21.430905697345736\n",
      "Training NF2:  70%|███████████▉     | 7028/10001 [3:33:16<1:32:36,  1.87s/batch]Batch 6900/10001 Done, mean position loss: 20.562896599769594\n",
      "Training NF2:  69%|███████████▊     | 6925/10001 [3:33:21<1:48:56,  2.12s/batch]Batch 7000/10001 Done, mean position loss: 20.894190762043\n",
      "Training NF2:  69%|███████████▋     | 6900/10001 [3:33:21<1:30:49,  1.76s/batch]Batch 6900/10001 Done, mean position loss: 20.667591924667356\n",
      "Training NF2:  70%|███████████▊     | 6972/10001 [3:33:23<1:31:42,  1.82s/batch]Batch 6900/10001 Done, mean position loss: 20.682276699543003\n",
      "Training NF2:  70%|███████████▊     | 6951/10001 [3:33:31<1:26:13,  1.70s/batch]Batch 7000/10001 Done, mean position loss: 20.45111128091812\n",
      "Training NF2:  70%|███████████▉     | 6993/10001 [3:33:33<1:28:46,  1.77s/batch]Batch 7000/10001 Done, mean position loss: 20.540178806781768\n",
      "Training NF2:  70%|███████████▉     | 6993/10001 [3:33:39<1:34:31,  1.89s/batch]Batch 7000/10001 Done, mean position loss: 20.962801611423494\n",
      "Training NF2:  70%|███████████▉     | 6988/10001 [3:33:49<1:29:13,  1.78s/batch]Batch 7000/10001 Done, mean position loss: 20.630468225479127\n",
      "Training NF2:  70%|███████████▉     | 7000/10001 [3:33:53<1:28:52,  1.78s/batch]Batch 7000/10001 Done, mean position loss: 20.56023826122284\n",
      "Training NF2:  70%|███████████▉     | 6987/10001 [3:33:55<1:35:17,  1.90s/batch]Batch 7000/10001 Done, mean position loss: 21.027641100883486\n",
      "Training NF2:  70%|███████████▉     | 7044/10001 [3:33:54<1:30:26,  1.84s/batch]Batch 7000/10001 Done, mean position loss: 20.447262115478516\n",
      "Training NF2:  70%|███████████▉     | 6997/10001 [3:33:59<1:43:51,  2.07s/batch]Batch 7000/10001 Done, mean position loss: 20.555703432559966\n",
      "Training NF2:  70%|███████████▉     | 6996/10001 [3:34:03<1:25:37,  1.71s/batch]Batch 7000/10001 Done, mean position loss: 20.356046392917634\n",
      "Training NF2:  70%|███████████▉     | 6993/10001 [3:34:08<1:48:31,  2.16s/batch]Batch 7000/10001 Done, mean position loss: 20.817766971588135\n",
      "Training NF2:  70%|███████████▉     | 7040/10001 [3:34:12<1:36:33,  1.96s/batch]Batch 6900/10001 Done, mean position loss: 20.53931305408478\n",
      "Training NF2:  69%|███████████▊     | 6941/10001 [3:34:13<1:49:00,  2.14s/batch]Batch 7000/10001 Done, mean position loss: 20.63555589914322\n",
      "Training NF2:  70%|███████████▊     | 6973/10001 [3:34:13<1:30:40,  1.80s/batch]Batch 7000/10001 Done, mean position loss: 20.677633767127993\n",
      "Training NF2:  70%|███████████▉     | 6999/10001 [3:34:14<1:35:57,  1.92s/batch]Batch 7000/10001 Done, mean position loss: 20.456093680858615\n",
      "Training NF2:  70%|███████████▉     | 7015/10001 [3:34:17<1:22:24,  1.66s/batch]Batch 7000/10001 Done, mean position loss: 20.454487283229827\n",
      "Training NF2:  70%|███████████▉     | 7045/10001 [3:34:22<1:43:49,  2.11s/batch]Batch 7000/10001 Done, mean position loss: 20.41458503007889\n",
      "Training NF2:  70%|███████████▉     | 7023/10001 [3:34:33<1:26:41,  1.75s/batch]Batch 7000/10001 Done, mean position loss: 20.946907663345335\n",
      "Training NF2:  70%|███████████▊     | 6982/10001 [3:34:39<1:21:59,  1.63s/batch]Batch 7000/10001 Done, mean position loss: 20.648981251716613\n",
      "Training NF2:  71%|████████████     | 7073/10001 [3:34:59<1:26:06,  1.76s/batch]Batch 7100/10001 Done, mean position loss: 20.47873749732971\n",
      "Training NF2:  71%|████████████     | 7089/10001 [3:35:01<1:49:00,  2.25s/batch]Batch 7100/10001 Done, mean position loss: 20.373312637805938\n",
      "Training NF2:  71%|████████████     | 7066/10001 [3:35:04<1:44:50,  2.14s/batch]Batch 7100/10001 Done, mean position loss: 21.075975515842437\n",
      "Training NF2:  70%|███████████▉     | 7042/10001 [3:35:06<1:36:07,  1.95s/batch]Batch 7000/10001 Done, mean position loss: 20.314325845241548\n",
      "Training NF2:  71%|████████████     | 7095/10001 [3:35:20<1:34:30,  1.95s/batch]Batch 7000/10001 Done, mean position loss: 20.756595838069916\n",
      "Training NF2:  71%|███████████▉     | 7058/10001 [3:35:23<1:36:09,  1.96s/batch]Batch 7100/10001 Done, mean position loss: 20.636225175857547\n",
      "Training NF2:  71%|███████████▉     | 7052/10001 [3:35:26<1:39:40,  2.03s/batch]Batch 7100/10001 Done, mean position loss: 20.381966457366943\n",
      "Training NF2:  70%|███████████▉     | 6994/10001 [3:35:31<1:24:28,  1.69s/batch]Batch 7100/10001 Done, mean position loss: 20.672090039253234\n",
      "Training NF2:  70%|███████████▉     | 7047/10001 [3:35:42<1:47:22,  2.18s/batch]Batch 7000/10001 Done, mean position loss: 20.71122302055359\n",
      "Training NF2:  71%|████████████     | 7070/10001 [3:35:45<1:26:23,  1.77s/batch]Batch 7100/10001 Done, mean position loss: 20.532446258068084\n",
      "Training NF2:  71%|████████████     | 7113/10001 [3:35:45<1:28:10,  1.83s/batch]Batch 7000/10001 Done, mean position loss: 20.779764239788058\n",
      "Training NF2:  71%|████████████     | 7113/10001 [3:35:52<1:32:23,  1.92s/batch]Batch 7100/10001 Done, mean position loss: 20.441011900901795\n",
      "Training NF2:  71%|████████████     | 7130/10001 [3:35:58<1:30:14,  1.89s/batch]Batch 7100/10001 Done, mean position loss: 20.586834626197813\n",
      "Training NF2:  71%|████████████     | 7062/10001 [3:36:00<1:48:15,  2.21s/batch]Batch 7100/10001 Done, mean position loss: 20.72466220140457\n",
      "Training NF2:  71%|████████████     | 7071/10001 [3:36:03<1:39:43,  2.04s/batch]Batch 7000/10001 Done, mean position loss: 20.8453275680542\n",
      "Training NF2:  71%|████████████     | 7120/10001 [3:36:05<1:29:56,  1.87s/batch]Batch 7000/10001 Done, mean position loss: 20.367333984375\n",
      "Training NF2:  71%|████████████     | 7104/10001 [3:36:06<1:19:37,  1.65s/batch]Batch 7100/10001 Done, mean position loss: 20.30190849065781\n",
      "Training NF2:  70%|███████████▉     | 7017/10001 [3:36:13<1:40:08,  2.01s/batch]Batch 7100/10001 Done, mean position loss: 21.439139876365665\n",
      "Training NF2:  71%|████████████▏    | 7150/10001 [3:36:29<1:24:50,  1.79s/batch]Batch 7000/10001 Done, mean position loss: 20.68647102355957\n",
      "Training NF2:  71%|████████████▏    | 7136/10001 [3:36:29<1:42:40,  2.15s/batch]Batch 7000/10001 Done, mean position loss: 20.55961356163025\n",
      "Training NF2:  71%|████████████     | 7111/10001 [3:36:32<1:48:21,  2.25s/batch]Batch 7100/10001 Done, mean position loss: 20.88303610086441\n",
      "Training NF2:  71%|████████████     | 7097/10001 [3:36:35<1:10:17,  1.45s/batch]Batch 7000/10001 Done, mean position loss: 20.670428116321567\n",
      "Training NF2:  72%|████████████▏    | 7157/10001 [3:36:43<1:31:10,  1.92s/batch]Batch 7100/10001 Done, mean position loss: 20.528286504745484\n",
      "Training NF2:  70%|███████████▉     | 7009/10001 [3:36:43<1:31:04,  1.83s/batch]Batch 7100/10001 Done, mean position loss: 20.935470283031464\n",
      "Training NF2:  70%|███████████▉     | 7031/10001 [3:36:45<1:33:42,  1.89s/batch]Batch 7100/10001 Done, mean position loss: 20.449934077262878\n",
      "Training NF2:  71%|████████████     | 7129/10001 [3:36:54<1:31:16,  1.91s/batch]Batch 7100/10001 Done, mean position loss: 21.031186707019806\n",
      "Training NF2:  70%|███████████▉     | 7014/10001 [3:36:58<1:29:38,  1.80s/batch]Batch 7100/10001 Done, mean position loss: 20.61692742586136\n",
      "Training NF2:  71%|████████████     | 7111/10001 [3:36:59<1:24:53,  1.76s/batch]Batch 7100/10001 Done, mean position loss: 20.555442960262297\n",
      "Training NF2:  70%|███████████▉     | 7021/10001 [3:37:05<1:28:47,  1.79s/batch]Batch 7100/10001 Done, mean position loss: 20.467024672031403\n",
      "Training NF2:  71%|████████████     | 7107/10001 [3:37:05<1:28:29,  1.83s/batch]Batch 7100/10001 Done, mean position loss: 20.543574199676513\n",
      "Training NF2:  71%|████████████▏    | 7143/10001 [3:37:15<1:32:13,  1.94s/batch]Batch 7100/10001 Done, mean position loss: 20.366653189659118\n",
      "Training NF2:  71%|████████████     | 7118/10001 [3:37:19<1:38:41,  2.05s/batch]Batch 7100/10001 Done, mean position loss: 20.82994474887848\n",
      "Training NF2:  71%|████████████▏    | 7137/10001 [3:37:21<1:27:15,  1.83s/batch]Batch 7100/10001 Done, mean position loss: 20.457074794769287\n",
      "Training NF2:  71%|████████████     | 7129/10001 [3:37:26<1:35:24,  1.99s/batch]Batch 7100/10001 Done, mean position loss: 20.459574706554413\n",
      "Training NF2:  71%|████████████     | 7101/10001 [3:37:26<1:34:01,  1.95s/batch]Batch 7100/10001 Done, mean position loss: 20.637375745773312\n",
      "Training NF2:  70%|███████████▉     | 7033/10001 [3:37:27<1:39:25,  2.01s/batch]Batch 7100/10001 Done, mean position loss: 20.406117260456085\n",
      "Training NF2:  72%|████████████▏    | 7153/10001 [3:37:28<1:34:10,  1.98s/batch]Batch 7000/10001 Done, mean position loss: 20.557087886333466\n",
      "Training NF2:  71%|████████████     | 7067/10001 [3:37:28<1:25:39,  1.75s/batch]Batch 7100/10001 Done, mean position loss: 20.677433247566224\n",
      "Training NF2:  71%|████████████     | 7063/10001 [3:37:41<1:35:33,  1.95s/batch]Batch 7100/10001 Done, mean position loss: 20.95457046985626\n",
      "Training NF2:  71%|████████████     | 7120/10001 [3:37:54<1:27:34,  1.82s/batch]Batch 7100/10001 Done, mean position loss: 20.62836976766586\n",
      "Training NF2:  71%|████████████     | 7122/10001 [3:38:08<1:31:28,  1.91s/batch]Batch 7200/10001 Done, mean position loss: 20.464536049365996\n",
      "Training NF2:  71%|████████████▏    | 7138/10001 [3:38:12<1:37:54,  2.05s/batch]Batch 7100/10001 Done, mean position loss: 20.315659742355344\n",
      "Training NF2:  71%|███████████▉     | 7056/10001 [3:38:13<1:28:45,  1.81s/batch]Batch 7200/10001 Done, mean position loss: 20.35765076875687\n",
      "Training NF2:  71%|████████████     | 7083/10001 [3:38:16<1:39:46,  2.05s/batch]Batch 7200/10001 Done, mean position loss: 21.106549031734467\n",
      "Training NF2:  71%|████████████     | 7087/10001 [3:38:32<1:33:37,  1.93s/batch]Batch 7200/10001 Done, mean position loss: 20.387652559280394\n",
      "Training NF2:  70%|███████████▉     | 7035/10001 [3:38:32<1:47:37,  2.18s/batch]Batch 7100/10001 Done, mean position loss: 20.767722377777098\n",
      "Training NF2:  72%|████████████▏    | 7185/10001 [3:38:35<1:48:09,  2.30s/batch]Batch 7200/10001 Done, mean position loss: 20.61973385810852\n",
      "Training NF2:  71%|████████████▏    | 7140/10001 [3:38:39<1:39:18,  2.08s/batch]Batch 7200/10001 Done, mean position loss: 20.661994388103487\n",
      "Training NF2:  72%|████████████▏    | 7184/10001 [3:38:49<1:29:28,  1.91s/batch]Batch 7100/10001 Done, mean position loss: 20.72299403190613\n",
      "Training NF2:  72%|████████████▏    | 7172/10001 [3:38:54<1:18:42,  1.67s/batch]Batch 7200/10001 Done, mean position loss: 20.511593434810635\n",
      "Training NF2:  72%|████████████▏    | 7164/10001 [3:38:58<1:24:06,  1.78s/batch]Batch 7100/10001 Done, mean position loss: 20.79611590862274\n",
      "Training NF2:  71%|████████████     | 7118/10001 [3:39:04<1:26:30,  1.80s/batch]Batch 7200/10001 Done, mean position loss: 20.427635412216187\n",
      "Training NF2:  72%|████████████▎    | 7215/10001 [3:39:08<1:38:08,  2.11s/batch]Batch 7200/10001 Done, mean position loss: 20.563779921531676\n",
      "Training NF2:  71%|████████████     | 7131/10001 [3:39:11<1:31:09,  1.91s/batch]Batch 7200/10001 Done, mean position loss: 20.29603131055832\n",
      "Training NF2:  72%|████████████▏    | 7156/10001 [3:39:12<1:30:36,  1.91s/batch]Batch 7200/10001 Done, mean position loss: 20.721051902770995\n",
      "Training NF2:  72%|████████████▏    | 7158/10001 [3:39:14<1:20:20,  1.70s/batch]Batch 7100/10001 Done, mean position loss: 20.87052710533142\n",
      "Training NF2:  72%|████████████▏    | 7169/10001 [3:39:17<1:44:38,  2.22s/batch]Batch 7100/10001 Done, mean position loss: 20.37010366201401\n",
      "Training NF2:  72%|████████████▏    | 7165/10001 [3:39:22<1:40:54,  2.13s/batch]Batch 7200/10001 Done, mean position loss: 21.408214547634124\n",
      "Training NF2:  72%|████████████▏    | 7187/10001 [3:39:35<1:12:05,  1.54s/batch]Batch 7100/10001 Done, mean position loss: 20.669442813396454\n",
      "Training NF2:  72%|████████████▏    | 7198/10001 [3:39:37<1:26:18,  1.85s/batch]Batch 7100/10001 Done, mean position loss: 20.552773926258087\n",
      "Training NF2:  72%|████████████▏    | 7198/10001 [3:39:42<1:26:27,  1.85s/batch]Batch 7200/10001 Done, mean position loss: 20.51874255180359\n",
      "Training NF2:  71%|████████████     | 7104/10001 [3:39:43<1:30:51,  1.88s/batch]Batch 7100/10001 Done, mean position loss: 20.672982664108275\n",
      "Training NF2:  72%|████████████▏    | 7204/10001 [3:39:48<1:35:46,  2.05s/batch]Batch 7200/10001 Done, mean position loss: 20.878091697692874\n",
      "Training NF2:  72%|████████████▎    | 7242/10001 [3:39:49<1:32:21,  2.01s/batch]Batch 7200/10001 Done, mean position loss: 20.942318737506866\n",
      "Training NF2:  72%|████████████▏    | 7195/10001 [3:39:55<1:23:52,  1.79s/batch]Batch 7200/10001 Done, mean position loss: 20.442607266902925\n",
      "Training NF2:  72%|████████████▎    | 7231/10001 [3:39:58<1:44:46,  2.27s/batch]Batch 7200/10001 Done, mean position loss: 21.02460925579071\n",
      "Training NF2:  72%|████████████▏    | 7182/10001 [3:40:07<1:26:22,  1.84s/batch]Batch 7200/10001 Done, mean position loss: 20.45386775970459\n",
      "Training NF2:  71%|████████████     | 7118/10001 [3:40:07<1:24:34,  1.76s/batch]Batch 7200/10001 Done, mean position loss: 20.55814136505127\n",
      "Training NF2:  71%|████████████▏    | 7137/10001 [3:40:11<1:45:27,  2.21s/batch]Batch 7200/10001 Done, mean position loss: 20.600837972164154\n",
      "Training NF2:  72%|████████████▎    | 7207/10001 [3:40:17<1:24:27,  1.81s/batch]Batch 7200/10001 Done, mean position loss: 20.56234325170517\n",
      "Training NF2:  73%|████████████▎    | 7274/10001 [3:40:28<1:22:05,  1.81s/batch]Batch 7200/10001 Done, mean position loss: 20.3555490899086\n",
      "Training NF2:  72%|████████████▎    | 7250/10001 [3:40:33<1:25:12,  1.86s/batch]Batch 7200/10001 Done, mean position loss: 20.7868040895462\n",
      "Training NF2:  72%|████████████▏    | 7201/10001 [3:40:33<1:23:07,  1.78s/batch]Batch 7200/10001 Done, mean position loss: 20.467998113632202\n",
      "Training NF2:  72%|████████████▎    | 7246/10001 [3:40:33<1:23:02,  1.81s/batch]Batch 7200/10001 Done, mean position loss: 20.64506072759628\n",
      "Training NF2:  72%|████████████▏    | 7197/10001 [3:40:34<1:35:22,  2.04s/batch]Batch 7200/10001 Done, mean position loss: 20.62550632953644\n",
      "Training NF2:  72%|████████████▏    | 7187/10001 [3:40:39<1:17:09,  1.65s/batch]Batch 7200/10001 Done, mean position loss: 20.4600274848938\n",
      "Training NF2:  71%|████████████     | 7131/10001 [3:40:42<1:40:41,  2.11s/batch]Batch 7200/10001 Done, mean position loss: 20.41455047607422\n",
      "Training NF2:  72%|████████████▏    | 7171/10001 [3:40:43<1:43:04,  2.19s/batch]Batch 7200/10001 Done, mean position loss: 20.938440001010896\n",
      "Training NF2:  71%|████████████▏    | 7136/10001 [3:40:44<1:49:11,  2.29s/batch]Batch 7100/10001 Done, mean position loss: 20.56020042181015\n",
      "Training NF2:  72%|████████████▏    | 7161/10001 [3:41:07<1:27:39,  1.85s/batch]Batch 7200/10001 Done, mean position loss: 20.64396554708481\n",
      "Training NF2:  73%|████████████▍    | 7297/10001 [3:41:19<1:27:52,  1.95s/batch]Batch 7200/10001 Done, mean position loss: 20.31733324050903\n",
      "Training NF2:  73%|████████████▎    | 7269/10001 [3:41:20<1:36:26,  2.12s/batch]Batch 7300/10001 Done, mean position loss: 20.35728426218033\n",
      "Training NF2:  72%|████████████▏    | 7156/10001 [3:41:22<1:28:05,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 20.471622593402863\n",
      "Training NF2:  72%|████████████▎    | 7225/10001 [3:41:28<1:26:11,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 21.068614904880523\n",
      "Training NF2:  72%|████████████▎    | 7231/10001 [3:41:36<1:35:03,  2.06s/batch]Batch 7200/10001 Done, mean position loss: 20.76681051015854\n",
      "Training NF2:  72%|████████████▎    | 7237/10001 [3:41:40<1:27:34,  1.90s/batch]Batch 7300/10001 Done, mean position loss: 20.37271309137344\n",
      "Training NF2:  72%|████████████▎    | 7238/10001 [3:41:42<1:31:55,  2.00s/batch]Batch 7300/10001 Done, mean position loss: 20.624546587467194\n",
      "Training NF2:  72%|████████████▏    | 7182/10001 [3:41:57<1:30:10,  1.92s/batch]Batch 7300/10001 Done, mean position loss: 20.675884535312655\n",
      "Training NF2:  73%|████████████▍    | 7302/10001 [3:41:59<1:17:34,  1.72s/batch]Batch 7200/10001 Done, mean position loss: 20.70548337936401\n",
      "Training NF2:  73%|████████████▍    | 7287/10001 [3:42:09<1:19:56,  1.77s/batch]Batch 7200/10001 Done, mean position loss: 20.794495232105255\n",
      "Training NF2:  73%|████████████▎    | 7255/10001 [3:42:13<1:28:21,  1.93s/batch]Batch 7300/10001 Done, mean position loss: 20.50723788738251\n",
      "Training NF2:  72%|████████████▏    | 7182/10001 [3:42:19<1:20:39,  1.72s/batch]Batch 7300/10001 Done, mean position loss: 20.429610981941224\n",
      "Training NF2:  73%|████████████▍    | 7294/10001 [3:42:20<1:15:04,  1.66s/batch]Batch 7300/10001 Done, mean position loss: 20.72362853527069\n",
      "Training NF2:  73%|████████████▍    | 7335/10001 [3:42:21<1:18:11,  1.76s/batch]Batch 7300/10001 Done, mean position loss: 20.56491178035736\n",
      "Training NF2:  73%|████████████▍    | 7308/10001 [3:42:24<1:17:44,  1.73s/batch]Batch 7300/10001 Done, mean position loss: 20.304461297988894\n",
      "Training NF2:  73%|████████████▍    | 7337/10001 [3:42:25<1:21:22,  1.83s/batch]Batch 7200/10001 Done, mean position loss: 20.849461164474487\n",
      "Training NF2:  73%|████████████▍    | 7329/10001 [3:42:29<1:12:23,  1.63s/batch]Batch 7200/10001 Done, mean position loss: 20.366722309589385\n",
      "Training NF2:  73%|████████████▎    | 7264/10001 [3:42:31<1:12:49,  1.60s/batch]Batch 7300/10001 Done, mean position loss: 21.453980391025542\n",
      "Training NF2:  73%|████████████▎    | 7268/10001 [3:42:45<1:23:29,  1.83s/batch]Batch 7200/10001 Done, mean position loss: 20.572260580062867\n",
      "Training NF2:  72%|████████████▎    | 7225/10001 [3:42:52<1:17:22,  1.67s/batch]Batch 7200/10001 Done, mean position loss: 20.675444664955137\n",
      "Training NF2:  73%|████████████▍    | 7311/10001 [3:42:52<1:40:52,  2.25s/batch]Batch 7200/10001 Done, mean position loss: 20.656751980781554\n",
      "Training NF2:  73%|████████████▍    | 7318/10001 [3:42:53<1:31:48,  2.05s/batch]Batch 7300/10001 Done, mean position loss: 20.52513624191284\n",
      "Training NF2:  73%|████████████▎    | 7275/10001 [3:42:59<1:10:00,  1.54s/batch]Batch 7300/10001 Done, mean position loss: 20.920024597644804\n",
      "Training NF2:  72%|████████████▎    | 7235/10001 [3:43:02<1:24:37,  1.84s/batch]Batch 7300/10001 Done, mean position loss: 20.929313344955446\n",
      "Training NF2:  73%|████████████▍    | 7349/10001 [3:43:07<1:17:10,  1.75s/batch]Batch 7300/10001 Done, mean position loss: 20.450664274692535\n",
      "Training NF2:  72%|████████████▎    | 7223/10001 [3:43:09<1:21:18,  1.76s/batch]Batch 7300/10001 Done, mean position loss: 21.028656260967253\n",
      "Training NF2:  72%|████████████▎    | 7215/10001 [3:43:12<1:28:33,  1.91s/batch]Batch 7300/10001 Done, mean position loss: 20.450264253616332\n",
      "Training NF2:  73%|████████████▍    | 7296/10001 [3:43:16<1:20:48,  1.79s/batch]Batch 7300/10001 Done, mean position loss: 20.559480135440825\n",
      "Training NF2:  74%|████████████▌    | 7369/10001 [3:43:26<1:21:27,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 20.53678184270859\n",
      "Batch 7300/10001 Done, mean position loss: 20.631584599018097\n",
      "Training NF2:  73%|████████████▍    | 7295/10001 [3:43:39<1:26:45,  1.92s/batch]Batch 7300/10001 Done, mean position loss: 20.81604032278061\n",
      "Training NF2:  73%|████████████▍    | 7297/10001 [3:43:43<1:30:05,  2.00s/batch]Batch 7300/10001 Done, mean position loss: 20.465029060840607\n",
      "Training NF2:  73%|████████████▍    | 7344/10001 [3:43:43<1:32:57,  2.10s/batch]Batch 7300/10001 Done, mean position loss: 20.355324935913085\n",
      "Training NF2:  73%|████████████▍    | 7284/10001 [3:43:46<1:20:03,  1.77s/batch]Batch 7300/10001 Done, mean position loss: 20.456560809612274\n",
      "Training NF2:  72%|████████████▎    | 7244/10001 [3:43:48<1:38:03,  2.13s/batch]Batch 7300/10001 Done, mean position loss: 20.64368680715561\n",
      "Training NF2:  72%|████████████▎    | 7231/10001 [3:43:48<1:25:04,  1.84s/batch]Batch 7300/10001 Done, mean position loss: 20.92559638738632\n",
      "Training NF2:  72%|████████████▏    | 7195/10001 [3:43:48<1:28:36,  1.89s/batch]Batch 7300/10001 Done, mean position loss: 20.423082845211027\n",
      "Training NF2:  72%|████████████▎    | 7235/10001 [3:43:50<1:24:57,  1.84s/batch]Batch 7300/10001 Done, mean position loss: 20.659123961925506\n",
      "Training NF2:  72%|████████████▎    | 7241/10001 [3:44:00<1:22:06,  1.79s/batch]Batch 7200/10001 Done, mean position loss: 20.56953891992569\n",
      "Training NF2:  73%|████████████▍    | 7321/10001 [3:44:21<1:23:05,  1.86s/batch]Batch 7300/10001 Done, mean position loss: 20.641348419189452\n",
      "Training NF2:  73%|████████████▍    | 7336/10001 [3:44:22<1:24:45,  1.91s/batch]Batch 7400/10001 Done, mean position loss: 20.350690925121306\n",
      "Training NF2:  73%|████████████▍    | 7292/10001 [3:44:22<1:09:27,  1.54s/batch]Batch 7300/10001 Done, mean position loss: 20.318508772850038\n",
      "Training NF2:  74%|████████████▌    | 7357/10001 [3:44:29<1:25:37,  1.94s/batch]Batch 7400/10001 Done, mean position loss: 20.487348895072937\n",
      "Training NF2:  73%|████████████▎    | 7258/10001 [3:44:38<1:14:50,  1.64s/batch]Batch 7400/10001 Done, mean position loss: 20.620249371528626\n",
      "Training NF2:  74%|████████████▌    | 7395/10001 [3:44:39<1:19:58,  1.84s/batch]Batch 7300/10001 Done, mean position loss: 20.75893784761429\n",
      "Training NF2:  73%|████████████▍    | 7343/10001 [3:44:41<1:21:20,  1.84s/batch]Batch 7400/10001 Done, mean position loss: 21.069735510349275\n",
      "Training NF2:  73%|████████████▎    | 7271/10001 [3:44:48<1:43:58,  2.29s/batch]Batch 7400/10001 Done, mean position loss: 20.374254944324495\n",
      "Training NF2:  73%|████████████▍    | 7344/10001 [3:45:08<1:26:07,  1.94s/batch]Batch 7400/10001 Done, mean position loss: 20.683450272083284\n",
      "Training NF2:  74%|████████████▌    | 7422/10001 [3:45:09<1:25:54,  2.00s/batch]Batch 7300/10001 Done, mean position loss: 20.69785236120224\n",
      "Training NF2:  73%|████████████▍    | 7321/10001 [3:45:13<1:02:22,  1.40s/batch]Batch 7300/10001 Done, mean position loss: 20.797515532970426\n",
      "Training NF2:  73%|████████████▍    | 7331/10001 [3:45:18<1:38:53,  2.22s/batch]Batch 7400/10001 Done, mean position loss: 20.51904305934906\n",
      "Training NF2:  74%|████████████▌    | 7370/10001 [3:45:20<1:39:55,  2.28s/batch]Batch 7400/10001 Done, mean position loss: 20.420782759189606\n",
      "Training NF2:  74%|████████████▌    | 7379/10001 [3:45:24<1:12:42,  1.66s/batch]Batch 7400/10001 Done, mean position loss: 20.570669760704043\n",
      "Training NF2:  74%|████████████▌    | 7373/10001 [3:45:26<1:24:00,  1.92s/batch]Batch 7400/10001 Done, mean position loss: 20.719277317523954\n",
      "Training NF2:  74%|████████████▌    | 7375/10001 [3:45:27<1:16:15,  1.74s/batch]Batch 7300/10001 Done, mean position loss: 20.371909205913546\n",
      "Training NF2:  73%|████████████▍    | 7330/10001 [3:45:30<1:19:23,  1.78s/batch]Batch 7400/10001 Done, mean position loss: 20.300322539806366\n",
      "Training NF2:  74%|████████████▌    | 7364/10001 [3:45:40<1:17:35,  1.77s/batch]Batch 7400/10001 Done, mean position loss: 21.43384369850159\n",
      "Training NF2:  74%|████████████▋    | 7441/10001 [3:45:48<1:19:44,  1.87s/batch]Batch 7300/10001 Done, mean position loss: 20.856202683448792\n",
      "Training NF2:  74%|████████████▋    | 7439/10001 [3:45:52<1:18:10,  1.83s/batch]Batch 7400/10001 Done, mean position loss: 20.516914114952087\n",
      "Training NF2:  74%|████████████▌    | 7419/10001 [3:45:54<1:06:41,  1.55s/batch]Batch 7300/10001 Done, mean position loss: 20.567806606292724\n",
      "Training NF2:  74%|████████████▌    | 7377/10001 [3:46:03<1:26:23,  1.98s/batch]Batch 7300/10001 Done, mean position loss: 20.694690687656404\n",
      "Training NF2:  75%|████████████▋    | 7457/10001 [3:46:04<1:16:14,  1.80s/batch]Batch 7300/10001 Done, mean position loss: 20.65507726430893\n",
      "Training NF2:  74%|████████████▌    | 7422/10001 [3:46:05<1:10:55,  1.65s/batch]Batch 7400/10001 Done, mean position loss: 20.87372567892075\n",
      "Training NF2:  73%|████████████▍    | 7323/10001 [3:46:10<1:17:09,  1.73s/batch]Batch 7400/10001 Done, mean position loss: 20.919218618869785\n",
      "Training NF2:  75%|████████████▋    | 7453/10001 [3:46:11<1:19:30,  1.87s/batch]Batch 7400/10001 Done, mean position loss: 20.449895191192628\n",
      "Training NF2:  74%|████████████▌    | 7405/10001 [3:46:12<1:10:12,  1.62s/batch]Batch 7400/10001 Done, mean position loss: 21.01817660570145\n",
      "Training NF2:  74%|████████████▌    | 7355/10001 [3:46:18<1:30:40,  2.06s/batch]Batch 7400/10001 Done, mean position loss: 20.43684445858002\n",
      "Training NF2:  74%|████████████▋    | 7431/10001 [3:46:20<1:22:10,  1.92s/batch]Batch 7400/10001 Done, mean position loss: 20.545125024318693\n",
      "Training NF2:  74%|████████████▌    | 7411/10001 [3:46:31<1:26:05,  1.99s/batch]Batch 7400/10001 Done, mean position loss: 20.53452850818634\n",
      "Training NF2:  74%|████████████▋    | 7448/10001 [3:46:36<1:29:15,  2.10s/batch]Batch 7400/10001 Done, mean position loss: 20.61440054893494\n",
      "Training NF2:  74%|████████████▌    | 7402/10001 [3:46:37<1:07:10,  1.55s/batch]Batch 7400/10001 Done, mean position loss: 20.831719980239868\n",
      "Training NF2:  73%|████████████▍    | 7327/10001 [3:46:42<1:24:10,  1.89s/batch]Batch 7400/10001 Done, mean position loss: 20.456473579406737\n",
      "Training NF2:  74%|████████████▌    | 7421/10001 [3:46:48<1:08:41,  1.60s/batch]Batch 7400/10001 Done, mean position loss: 20.362141087055207\n",
      "Training NF2:  75%|████████████▋    | 7472/10001 [3:46:49<1:23:18,  1.98s/batch]Batch 7400/10001 Done, mean position loss: 20.453842766284943\n",
      "Training NF2:  74%|████████████▌    | 7357/10001 [3:46:54<1:04:55,  1.47s/batch]Batch 7400/10001 Done, mean position loss: 20.408142364025114\n",
      "Training NF2:  74%|████████████▌    | 7426/10001 [3:46:58<1:36:42,  2.25s/batch]Batch 7400/10001 Done, mean position loss: 20.648371896743775\n",
      "Training NF2:  74%|████████████▌    | 7407/10001 [3:46:59<1:11:04,  1.64s/batch]Batch 7400/10001 Done, mean position loss: 20.942462391853333\n",
      "Training NF2:  74%|████████████▌    | 7389/10001 [3:47:02<1:16:04,  1.75s/batch]Batch 7400/10001 Done, mean position loss: 20.642877161502838\n",
      "Training NF2:  74%|████████████▌    | 7362/10001 [3:47:05<1:30:38,  2.06s/batch]Batch 7300/10001 Done, mean position loss: 20.56504818201065\n",
      "Training NF2:  73%|████████████▍    | 7350/10001 [3:47:25<1:15:49,  1.72s/batch]Batch 7500/10001 Done, mean position loss: 20.359111893177033\n",
      "Training NF2:  74%|████████████▌    | 7400/10001 [3:47:26<1:08:20,  1.58s/batch]Batch 7400/10001 Done, mean position loss: 20.63946521282196\n",
      "Training NF2:  73%|████████████▍    | 7347/10001 [3:47:29<1:12:07,  1.63s/batch]Batch 7400/10001 Done, mean position loss: 20.30374139547348\n",
      "Training NF2:  74%|████████████▋    | 7436/10001 [3:47:40<1:25:45,  2.01s/batch]Batch 7500/10001 Done, mean position loss: 20.48155606508255\n",
      "Training NF2:  74%|████████████▌    | 7410/10001 [3:47:43<1:19:10,  1.83s/batch]Batch 7500/10001 Done, mean position loss: 21.03990446805954\n",
      "Training NF2:  75%|████████████▊    | 7501/10001 [3:47:43<1:19:05,  1.90s/batch]Batch 7400/10001 Done, mean position loss: 20.785361294746398\n",
      "Training NF2:  74%|████████████▌    | 7410/10001 [3:47:44<1:14:06,  1.72s/batch]Batch 7500/10001 Done, mean position loss: 20.62195822715759\n",
      "Training NF2:  74%|████████████▌    | 7425/10001 [3:47:49<1:30:09,  2.10s/batch]Batch 7500/10001 Done, mean position loss: 20.381533749103546\n",
      "Training NF2:  75%|████████████▋    | 7456/10001 [3:48:12<1:31:55,  2.17s/batch]Batch 7400/10001 Done, mean position loss: 20.693777680397034\n",
      "Training NF2:  75%|████████████▋    | 7471/10001 [3:48:18<1:11:32,  1.70s/batch]Batch 7400/10001 Done, mean position loss: 20.798620657920836\n",
      "Training NF2:  75%|████████████▋    | 7482/10001 [3:48:20<1:28:41,  2.11s/batch]Batch 7500/10001 Done, mean position loss: 20.529659802913667\n",
      "Training NF2:  75%|████████████▋    | 7472/10001 [3:48:22<1:18:25,  1.86s/batch]Batch 7500/10001 Done, mean position loss: 20.69420556306839\n",
      "Training NF2:  75%|████████████▊    | 7520/10001 [3:48:25<1:15:40,  1.83s/batch]Batch 7500/10001 Done, mean position loss: 20.429411203861235\n",
      "Training NF2:  73%|████████████▍    | 7344/10001 [3:48:28<1:24:55,  1.92s/batch]Batch 7500/10001 Done, mean position loss: 20.568602683544157\n",
      "Training NF2:  74%|████████████▋    | 7436/10001 [3:48:30<1:09:48,  1.63s/batch]Batch 7500/10001 Done, mean position loss: 20.73345544099808\n",
      "Training NF2:  74%|████████████▋    | 7437/10001 [3:48:34<1:10:06,  1.64s/batch]Batch 7400/10001 Done, mean position loss: 20.36250410795212\n",
      "Training NF2:  75%|████████████▊    | 7532/10001 [3:48:41<1:19:20,  1.93s/batch]Batch 7500/10001 Done, mean position loss: 20.30853857278824\n",
      "Training NF2:  75%|████████████▋    | 7461/10001 [3:48:54<1:17:26,  1.83s/batch]Batch 7500/10001 Done, mean position loss: 20.532678565979005\n",
      "Training NF2:  75%|████████████▊    | 7501/10001 [3:48:54<1:16:27,  1.84s/batch]Batch 7500/10001 Done, mean position loss: 21.44021866321564\n",
      "Training NF2:  74%|████████████▌    | 7426/10001 [3:48:59<1:24:17,  1.96s/batch]Batch 7400/10001 Done, mean position loss: 20.84669652223587\n",
      "Training NF2:  75%|████████████▋    | 7482/10001 [3:49:02<1:25:40,  2.04s/batch]Batch 7400/10001 Done, mean position loss: 20.568221933841706\n",
      "Training NF2:  75%|████████████▋    | 7500/10001 [3:49:07<1:16:33,  1.84s/batch]Batch 7400/10001 Done, mean position loss: 20.69141443490982\n",
      "Training NF2:  75%|████████████▋    | 7494/10001 [3:49:09<1:20:30,  1.93s/batch]Batch 7500/10001 Done, mean position loss: 20.814053444862367\n",
      "Training NF2:  75%|████████████▊    | 7501/10001 [3:49:09<1:10:39,  1.70s/batch]Batch 7400/10001 Done, mean position loss: 20.65985404253006\n",
      "Training NF2:  74%|████████████▌    | 7403/10001 [3:49:12<1:09:41,  1.61s/batch]Batch 7500/10001 Done, mean position loss: 20.99460304737091\n",
      "Training NF2:  74%|████████████▌    | 7408/10001 [3:49:14<1:10:06,  1.62s/batch]Batch 7500/10001 Done, mean position loss: 20.435843877792358\n",
      "Training NF2:  75%|████████████▊    | 7533/10001 [3:49:21<1:10:25,  1.71s/batch]Batch 7500/10001 Done, mean position loss: 20.443554513454437\n",
      "Training NF2:  76%|████████████▊    | 7552/10001 [3:49:20<1:10:08,  1.72s/batch]Batch 7500/10001 Done, mean position loss: 21.016977627277374\n",
      "Training NF2:  76%|████████████▊    | 7573/10001 [3:49:33<1:11:36,  1.77s/batch]Batch 7500/10001 Done, mean position loss: 20.56149130344391\n",
      "Training NF2:  74%|████████████▌    | 7421/10001 [3:49:38<1:22:37,  1.92s/batch]Batch 7500/10001 Done, mean position loss: 20.545315198898315\n",
      "Training NF2:  74%|████████████▌    | 7418/10001 [3:49:38<1:15:48,  1.76s/batch]Batch 7500/10001 Done, mean position loss: 20.600882625579832\n",
      "Training NF2:  75%|████████████▋    | 7492/10001 [3:49:42<1:15:02,  1.79s/batch]Batch 7500/10001 Done, mean position loss: 20.831938679218293\n",
      "Training NF2:  75%|████████████▊    | 7519/10001 [3:49:43<1:00:00,  1.45s/batch]Batch 7500/10001 Done, mean position loss: 20.460507066249846\n",
      "Training NF2:  74%|████████████▋    | 7430/10001 [3:49:56<1:33:18,  2.18s/batch]Batch 7500/10001 Done, mean position loss: 20.349490633010866\n",
      "Training NF2:  75%|████████████▋    | 7500/10001 [3:49:59<1:17:40,  1.86s/batch]Batch 7500/10001 Done, mean position loss: 20.46196616411209\n",
      "Training NF2:  75%|████████████▊    | 7523/10001 [3:49:59<1:13:36,  1.78s/batch]Batch 7500/10001 Done, mean position loss: 20.622227058410644\n",
      "Training NF2:  74%|████████████▌    | 7393/10001 [3:50:00<1:17:23,  1.78s/batch]Batch 7500/10001 Done, mean position loss: 20.39983332633972\n",
      "Training NF2:  76%|████████████▊    | 7555/10001 [3:50:03<1:06:46,  1.64s/batch]Batch 7500/10001 Done, mean position loss: 20.668910863399507\n",
      "Training NF2:  75%|████████████▋    | 7491/10001 [3:50:09<1:15:08,  1.80s/batch]Batch 7500/10001 Done, mean position loss: 20.94637366771698\n",
      "Training NF2:  74%|████████████▋    | 7437/10001 [3:50:15<1:27:47,  2.05s/batch]Batch 7400/10001 Done, mean position loss: 20.562792613506318\n",
      "Training NF2:  75%|████████████▊    | 7541/10001 [3:50:23<1:17:01,  1.88s/batch]Batch 7600/10001 Done, mean position loss: 20.366116511821748\n",
      "Training NF2:  76%|██████████████▍    | 7603/10001 [3:50:26<58:52,  1.47s/batch]Batch 7500/10001 Done, mean position loss: 20.63016046524048\n",
      "Training NF2:  75%|████████████▊    | 7528/10001 [3:50:31<1:12:58,  1.77s/batch]Batch 7500/10001 Done, mean position loss: 20.306461327075958\n",
      "Training NF2:  75%|████████████▊    | 7550/10001 [3:50:38<1:03:51,  1.56s/batch]Batch 7600/10001 Done, mean position loss: 20.487875390052793\n",
      "Training NF2:  76%|██████████████▍    | 7605/10001 [3:50:43<57:39,  1.44s/batch]Batch 7600/10001 Done, mean position loss: 20.623602826595306\n",
      "Training NF2:  75%|████████████▊    | 7538/10001 [3:50:44<1:01:20,  1.49s/batch]Batch 7600/10001 Done, mean position loss: 21.068795907497407\n",
      "Training NF2:  75%|████████████▊    | 7511/10001 [3:50:45<1:14:46,  1.80s/batch]Batch 7500/10001 Done, mean position loss: 20.78301989555359\n",
      "Training NF2:  75%|████████████▊    | 7531/10001 [3:50:52<1:17:51,  1.89s/batch]Batch 7600/10001 Done, mean position loss: 20.374693284034727\n",
      "Training NF2:  76%|████████████▊    | 7566/10001 [3:51:14<1:10:06,  1.73s/batch]Batch 7500/10001 Done, mean position loss: 20.786906616687773\n",
      "Training NF2:  76%|████████████▊    | 7555/10001 [3:51:16<1:19:07,  1.94s/batch]Batch 7500/10001 Done, mean position loss: 20.695746753215786\n",
      "Training NF2:  75%|████████████▊    | 7501/10001 [3:51:16<1:12:03,  1.73s/batch]Batch 7600/10001 Done, mean position loss: 20.510619904994964\n",
      "Training NF2:  76%|████████████▉    | 7588/10001 [3:51:25<1:10:25,  1.75s/batch]Batch 7600/10001 Done, mean position loss: 20.576550505161286\n",
      "Batch 7600/10001 Done, mean position loss: 20.423838448524474\n",
      "Training NF2:  75%|████████████▊    | 7549/10001 [3:51:30<1:25:34,  2.09s/batch]Batch 7600/10001 Done, mean position loss: 20.713637635707855\n",
      "Training NF2:  75%|████████████▊    | 7545/10001 [3:51:30<1:21:29,  1.99s/batch]Batch 7600/10001 Done, mean position loss: 20.6863009762764\n",
      "Training NF2:  76%|████████████▉    | 7629/10001 [3:51:36<1:11:31,  1.81s/batch]Batch 7600/10001 Done, mean position loss: 20.291833238601683\n",
      "Training NF2:  76%|████████████▉    | 7577/10001 [3:51:40<1:15:54,  1.88s/batch]Batch 7500/10001 Done, mean position loss: 20.369931082725525\n",
      "Training NF2:  76%|████████████▉    | 7608/10001 [3:51:48<1:13:42,  1.85s/batch]Batch 7600/10001 Done, mean position loss: 21.421267366409303\n",
      "Training NF2:  76%|████████████▉    | 7613/10001 [3:51:56<1:06:33,  1.67s/batch]Batch 7600/10001 Done, mean position loss: 20.53216283082962\n",
      "Training NF2:  76%|████████████▊    | 7552/10001 [3:52:03<1:06:28,  1.63s/batch]Batch 7500/10001 Done, mean position loss: 20.555949602127075\n",
      "Training NF2:  76%|████████████▊    | 7573/10001 [3:52:07<1:27:48,  2.17s/batch]Batch 7500/10001 Done, mean position loss: 20.823803873062133\n",
      "Training NF2:  76%|████████████▉    | 7583/10001 [3:52:07<1:07:16,  1.67s/batch]Batch 7600/10001 Done, mean position loss: 20.427274453639985\n",
      "Training NF2:  76%|████████████▉    | 7588/10001 [3:52:09<1:09:48,  1.74s/batch]Batch 7600/10001 Done, mean position loss: 20.968239097595216\n",
      "Training NF2:  76%|████████████▉    | 7615/10001 [3:52:13<1:02:37,  1.57s/batch]Batch 7500/10001 Done, mean position loss: 20.642159991264343\n",
      "Training NF2:  75%|████████████▊    | 7501/10001 [3:52:13<1:36:12,  2.31s/batch]Batch 7500/10001 Done, mean position loss: 20.678796167373655\n",
      "Training NF2:  75%|████████████▊    | 7519/10001 [3:52:16<1:34:10,  2.28s/batch]Batch 7600/10001 Done, mean position loss: 20.845117034912107\n",
      "Training NF2:  76%|████████████▉    | 7586/10001 [3:52:16<1:15:22,  1.87s/batch]Batch 7600/10001 Done, mean position loss: 21.028094921112064\n",
      "Training NF2:  75%|████████████▊    | 7539/10001 [3:52:21<1:20:36,  1.96s/batch]Batch 7600/10001 Done, mean position loss: 20.453957016468046\n",
      "Training NF2:  76%|████████████▊    | 7568/10001 [3:52:32<1:11:56,  1.77s/batch]Batch 7600/10001 Done, mean position loss: 20.515551917552948\n",
      "Training NF2:  76%|████████████▉    | 7602/10001 [3:52:34<1:07:56,  1.70s/batch]Batch 7600/10001 Done, mean position loss: 20.551144070625305\n",
      "Training NF2:  76%|████████████▉    | 7606/10001 [3:52:41<1:06:19,  1.66s/batch]Batch 7600/10001 Done, mean position loss: 20.447030324935913\n",
      "Training NF2:  75%|████████████▊    | 7550/10001 [3:52:41<1:13:23,  1.80s/batch]Batch 7600/10001 Done, mean position loss: 20.815362737178802\n",
      "Training NF2:  76%|████████████▉    | 7577/10001 [3:52:46<1:06:05,  1.64s/batch]Batch 7600/10001 Done, mean position loss: 20.602845544815064\n",
      "Training NF2:  76%|████████████▉    | 7607/10001 [3:52:52<1:13:51,  1.85s/batch]Batch 7600/10001 Done, mean position loss: 20.6507865858078\n",
      "Training NF2:  76%|████████████▉    | 7599/10001 [3:53:01<1:17:12,  1.93s/batch]Batch 7600/10001 Done, mean position loss: 20.457966494560242\n",
      "Training NF2:  76%|████████████▉    | 7588/10001 [3:53:04<1:10:04,  1.74s/batch]Batch 7600/10001 Done, mean position loss: 20.360765330791473\n",
      "Training NF2:  76%|████████████▉    | 7638/10001 [3:53:03<1:13:26,  1.86s/batch]Batch 7600/10001 Done, mean position loss: 20.411774663925172\n",
      "Training NF2:  75%|████████████▋    | 7497/10001 [3:53:10<1:02:48,  1.51s/batch]Batch 7600/10001 Done, mean position loss: 20.632613046169283\n",
      "Batch 7600/10001 Done, mean position loss: 20.96587641477585\n",
      "Training NF2:  77%|█████████████    | 7659/10001 [3:53:19<1:16:22,  1.96s/batch]Batch 7500/10001 Done, mean position loss: 20.571157906055454\n",
      "Training NF2:  76%|████████████▉    | 7643/10001 [3:53:29<1:14:59,  1.91s/batch]Batch 7600/10001 Done, mean position loss: 20.63706280231476\n",
      "Training NF2:  76%|████████████▉    | 7576/10001 [3:53:29<1:14:41,  1.85s/batch]Batch 7700/10001 Done, mean position loss: 20.35783727169037\n",
      "Training NF2:  77%|█████████████    | 7655/10001 [3:53:32<1:30:07,  2.31s/batch]Batch 7600/10001 Done, mean position loss: 20.306310131549836\n",
      "Training NF2:  75%|████████████▊    | 7510/10001 [3:53:36<1:22:34,  1.99s/batch]Batch 7700/10001 Done, mean position loss: 20.459501767158507\n",
      "Training NF2:  77%|█████████████    | 7652/10001 [3:53:45<1:07:41,  1.73s/batch]Batch 7600/10001 Done, mean position loss: 20.78004509449005\n",
      "Training NF2:  75%|████████████▊    | 7550/10001 [3:53:47<1:05:22,  1.60s/batch]Batch 7700/10001 Done, mean position loss: 21.05470422029495\n",
      "Training NF2:  77%|█████████████    | 7674/10001 [3:53:49<1:22:36,  2.13s/batch]Batch 7700/10001 Done, mean position loss: 20.652015194892883\n",
      "Training NF2:  77%|█████████████    | 7679/10001 [3:53:56<1:33:04,  2.40s/batch]Batch 7700/10001 Done, mean position loss: 20.371137993335722\n",
      "Training NF2:  77%|█████████████    | 7694/10001 [3:54:17<1:13:44,  1.92s/batch]Batch 7600/10001 Done, mean position loss: 20.79557915687561\n",
      "Training NF2:  76%|█████████████    | 7650/10001 [3:54:17<1:21:07,  2.07s/batch]Batch 7600/10001 Done, mean position loss: 20.6946884059906\n",
      "Training NF2:  76%|████████████▊    | 7567/10001 [3:54:23<1:31:14,  2.25s/batch]Batch 7700/10001 Done, mean position loss: 20.512399158477784\n",
      "Training NF2:  76%|████████████▉    | 7632/10001 [3:54:30<1:13:25,  1.86s/batch]Batch 7700/10001 Done, mean position loss: 20.416973609924316\n",
      "Training NF2:  77%|█████████████    | 7652/10001 [3:54:34<1:12:01,  1.84s/batch]Batch 7700/10001 Done, mean position loss: 20.578219425678252\n",
      "Training NF2:  76%|████████████▉    | 7636/10001 [3:54:38<1:13:52,  1.87s/batch]Batch 7700/10001 Done, mean position loss: 20.72307166814804\n",
      "Training NF2:  77%|█████████████    | 7678/10001 [3:54:40<1:11:06,  1.84s/batch]Batch 7700/10001 Done, mean position loss: 20.68067174196243\n",
      "Training NF2:  77%|█████████████▏   | 7742/10001 [3:54:45<1:16:50,  2.04s/batch]Batch 7700/10001 Done, mean position loss: 20.30290091753006\n",
      "Training NF2:  77%|█████████████▏   | 7733/10001 [3:54:53<1:07:52,  1.80s/batch]Batch 7600/10001 Done, mean position loss: 20.36214002132416\n",
      "Training NF2:  77%|█████████████    | 7689/10001 [3:55:00<1:16:53,  2.00s/batch]Batch 7700/10001 Done, mean position loss: 20.50243605136871\n",
      "Training NF2:  77%|█████████████    | 7674/10001 [3:55:00<1:13:10,  1.89s/batch]Batch 7700/10001 Done, mean position loss: 21.406534392833713\n",
      "Training NF2:  77%|█████████████    | 7719/10001 [3:55:07<1:15:58,  2.00s/batch]Batch 7600/10001 Done, mean position loss: 20.54756479024887\n",
      "Training NF2:  77%|█████████████    | 7689/10001 [3:55:19<1:13:09,  1.90s/batch]Batch 7700/10001 Done, mean position loss: 20.41811644077301\n",
      "Training NF2:  76%|██████████████▍    | 7599/10001 [3:55:19<59:44,  1.49s/batch]Batch 7700/10001 Done, mean position loss: 20.86794394731522\n",
      "Training NF2:  76%|██████████████▍    | 7598/10001 [3:55:20<58:56,  1.47s/batch]Batch 7700/10001 Done, mean position loss: 21.00785675048828\n",
      "Training NF2:  76%|████████████▉    | 7616/10001 [3:55:21<1:15:27,  1.90s/batch]Batch 7700/10001 Done, mean position loss: 20.94705151796341\n",
      "Training NF2:  76%|████████████▉    | 7609/10001 [3:55:23<1:34:11,  2.36s/batch]Batch 7600/10001 Done, mean position loss: 20.82005898237228\n",
      "Training NF2:  77%|█████████████    | 7672/10001 [3:55:25<1:09:03,  1.78s/batch]Batch 7600/10001 Done, mean position loss: 20.66211037874222\n",
      "Training NF2:  77%|██████████████▌    | 7680/10001 [3:55:27<55:37,  1.44s/batch]Batch 7600/10001 Done, mean position loss: 20.66230745077133\n",
      "Training NF2:  77%|█████████████    | 7707/10001 [3:55:29<1:07:10,  1.76s/batch]Batch 7700/10001 Done, mean position loss: 20.540163218975067\n",
      "Training NF2:  77%|█████████████    | 7690/10001 [3:55:32<1:29:14,  2.32s/batch]Batch 7700/10001 Done, mean position loss: 20.430486369132996\n",
      "Training NF2:  76%|████████████▉    | 7619/10001 [3:55:42<1:14:37,  1.88s/batch]�▉    | 7641/10001 [3:55:36<1:22:17,  2.09s/batch]Batch 7700/10001 Done, mean position loss: 20.531080074310303\n",
      "Training NF2:  76%|████████████▉    | 7626/10001 [3:55:56<1:20:19,  2.03s/batch]Batch 7700/10001 Done, mean position loss: 20.466044023036957\n",
      "Training NF2:  76%|████████████▉    | 7616/10001 [3:55:57<1:24:03,  2.11s/batch]Batch 7700/10001 Done, mean position loss: 20.791967582702636\n",
      "Training NF2:  77%|█████████████    | 7701/10001 [3:55:57<1:40:23,  2.62s/batch]Batch 7700/10001 Done, mean position loss: 20.59605716705322\n",
      "Training NF2:  77%|█████████████    | 7695/10001 [3:56:09<1:18:56,  2.05s/batch]Batch 7700/10001 Done, mean position loss: 20.45428147315979\n",
      "Training NF2:  77%|█████████████    | 7675/10001 [3:56:09<1:21:22,  2.10s/batch]Batch 7700/10001 Done, mean position loss: 20.6400088095665\n",
      "Training NF2:  77%|█████████████    | 7700/10001 [3:56:13<1:16:40,  2.00s/batch]Batch 7700/10001 Done, mean position loss: 20.399668736457826\n",
      "Training NF2:  77%|██████████████▋    | 7725/10001 [3:56:15<55:49,  1.47s/batch]Batch 7700/10001 Done, mean position loss: 20.361859755516054\n",
      "Training NF2:  76%|████████████▉    | 7595/10001 [3:56:19<1:20:21,  2.00s/batch]Batch 7700/10001 Done, mean position loss: 20.95534703731537\n",
      "Training NF2:  77%|█████████████    | 7703/10001 [3:56:20<1:17:48,  2.03s/batch]Batch 7700/10001 Done, mean position loss: 20.631210961341857\n",
      "Training NF2:  77%|█████████████    | 7720/10001 [3:56:32<1:20:32,  2.12s/batch]Batch 7600/10001 Done, mean position loss: 20.542160432338715\n",
      "Training NF2:  76%|████████████▉    | 7605/10001 [3:56:37<1:06:53,  1.68s/batch]Batch 7800/10001 Done, mean position loss: 20.357856225967407\n",
      "Training NF2:  77%|█████████████▏   | 7739/10001 [3:56:40<1:24:59,  2.25s/batch]Batch 7800/10001 Done, mean position loss: 20.498497719764707\n",
      "Batch 7700/10001 Done, mean position loss: 20.308781599998476\n",
      "Training NF2:  78%|█████████████▎   | 7797/10001 [3:56:40<1:20:06,  2.18s/batch]Batch 7700/10001 Done, mean position loss: 20.6347603058815\n",
      "Training NF2:  77%|█████████████    | 7719/10001 [3:56:47<1:14:53,  1.97s/batch]Batch 7800/10001 Done, mean position loss: 21.056846315860746\n",
      "Training NF2:  77%|█████████████    | 7698/10001 [3:56:51<1:06:49,  1.74s/batch]Batch 7800/10001 Done, mean position loss: 20.635832865238193\n",
      "Training NF2:  78%|█████████████▏   | 7769/10001 [3:56:56<1:09:28,  1.87s/batch]Batch 7700/10001 Done, mean position loss: 20.768500769138335\n",
      "Training NF2:  78%|█████████████▏   | 7782/10001 [3:57:05<1:04:47,  1.75s/batch]Batch 7800/10001 Done, mean position loss: 20.361225531101226\n",
      "Training NF2:  77%|█████████████    | 7664/10001 [3:57:29<1:14:20,  1.91s/batch]Batch 7700/10001 Done, mean position loss: 20.782131226062774\n",
      "Training NF2:  78%|█████████████▏   | 7767/10001 [3:57:29<1:16:40,  2.06s/batch]Batch 7800/10001 Done, mean position loss: 20.505358769893647\n",
      "Training NF2:  78%|█████████████▏   | 7768/10001 [3:57:31<1:16:36,  2.06s/batch]Batch 7700/10001 Done, mean position loss: 20.70330250740051\n",
      "Training NF2:  77%|█████████████▏   | 7742/10001 [3:57:37<1:01:45,  1.64s/batch]Batch 7800/10001 Done, mean position loss: 20.71120672225952\n",
      "Training NF2:  77%|█████████████    | 7675/10001 [3:57:40<1:09:31,  1.79s/batch]Batch 7800/10001 Done, mean position loss: 20.430231511592865\n",
      "Training NF2:  78%|█████████████▎   | 7798/10001 [3:57:40<1:14:17,  2.02s/batch]Batch 7800/10001 Done, mean position loss: 20.563766531944275\n",
      "Training NF2:  78%|█████████████▏   | 7778/10001 [3:57:47<1:11:14,  1.92s/batch]Batch 7800/10001 Done, mean position loss: 20.68375879049301\n",
      "Training NF2:  78%|█████████████▏   | 7754/10001 [3:57:56<1:09:22,  1.85s/batch]Batch 7800/10001 Done, mean position loss: 20.309480986595155\n",
      "Training NF2:  77%|█████████████    | 7692/10001 [3:58:03<1:23:17,  2.16s/batch]Batch 7800/10001 Done, mean position loss: 20.516746597290037\n",
      "Training NF2:  78%|█████████████▎   | 7801/10001 [3:58:02<1:21:08,  2.21s/batch]Batch 7700/10001 Done, mean position loss: 20.349090509414673\n",
      "Training NF2:  78%|█████████████▎   | 7822/10001 [3:58:16<1:02:31,  1.72s/batch]Batch 7800/10001 Done, mean position loss: 21.40143728494644\n",
      "Training NF2:  78%|█████████████▏   | 7767/10001 [3:58:19<1:12:09,  1.94s/batch]Batch 7700/10001 Done, mean position loss: 20.524723060131073\n",
      "Training NF2:  78%|█████████████▏   | 7775/10001 [3:58:27<1:05:10,  1.76s/batch]Batch 7800/10001 Done, mean position loss: 20.951893756389616\n",
      "Training NF2:  77%|█████████████    | 7706/10001 [3:58:27<1:08:02,  1.78s/batch]Batch 7800/10001 Done, mean position loss: 21.013399019241334\n",
      "Training NF2:  77%|█████████████    | 7664/10001 [3:58:29<1:14:54,  1.92s/batch]Batch 7700/10001 Done, mean position loss: 20.818423449993134\n",
      "Training NF2:  78%|██████████████▊    | 7804/10001 [3:58:32<58:33,  1.60s/batch]Batch 7800/10001 Done, mean position loss: 20.888012926578522\n",
      "Training NF2:  78%|█████████████▎   | 7806/10001 [3:58:35<1:02:03,  1.70s/batch]Batch 7800/10001 Done, mean position loss: 20.41584212064743\n",
      "Training NF2:  79%|██████████████▉    | 7851/10001 [3:58:36<59:30,  1.66s/batch]Batch 7800/10001 Done, mean position loss: 20.52304371118545\n",
      "Training NF2:  78%|█████████████▎   | 7808/10001 [3:58:40<1:14:03,  2.03s/batch]Batch 7700/10001 Done, mean position loss: 20.671344485282898\n",
      "Training NF2:  78%|█████████████▏   | 7778/10001 [3:58:41<1:18:15,  2.11s/batch]Batch 7800/10001 Done, mean position loss: 20.428456330299376\n",
      "Training NF2:  77%|██████████████▋    | 7702/10001 [3:58:41<58:25,  1.53s/batch]Batch 7700/10001 Done, mean position loss: 20.661447627544405\n",
      "Training NF2:  77%|█████████████    | 7719/10001 [3:58:51<1:06:53,  1.76s/batch]Batch 7800/10001 Done, mean position loss: 20.530335600376127\n",
      "Training NF2:  77%|█████████████    | 7711/10001 [3:59:01<1:15:17,  1.97s/batch]Batch 7800/10001 Done, mean position loss: 20.80359852552414\n",
      "Training NF2:  78%|█████████████▎   | 7816/10001 [3:59:03<1:16:48,  2.11s/batch]Batch 7800/10001 Done, mean position loss: 20.592665462493898\n",
      "Training NF2:  77%|█████████████▏   | 7727/10001 [3:59:05<1:08:22,  1.80s/batch]Batch 7800/10001 Done, mean position loss: 20.45637280225754\n",
      "Training NF2:  77%|█████████████    | 7720/10001 [3:59:16<1:01:37,  1.62s/batch]Batch 7800/10001 Done, mean position loss: 20.466684544086455\n",
      "Training NF2:  78%|█████████████▎   | 7818/10001 [3:59:23<1:05:07,  1.79s/batch]Batch 7800/10001 Done, mean position loss: 20.629914433956145\n",
      "Training NF2:  78%|█████████████▎   | 7828/10001 [3:59:23<1:10:44,  1.95s/batch]Batch 7800/10001 Done, mean position loss: 20.619131584167484\n",
      "Training NF2:  77%|█████████████▏   | 7722/10001 [3:59:23<1:16:56,  2.03s/batch]Batch 7800/10001 Done, mean position loss: 20.360446748733523\n",
      "Training NF2:  78%|█████████████▎   | 7832/10001 [3:59:32<1:06:11,  1.83s/batch]Batch 7800/10001 Done, mean position loss: 20.402700307369233\n",
      "Training NF2:  78%|█████████████▎   | 7806/10001 [3:59:33<1:17:37,  2.12s/batch]Batch 7800/10001 Done, mean position loss: 20.940212881565092\n",
      "Training NF2:  78%|█████████████▏   | 7787/10001 [3:59:37<1:11:35,  1.94s/batch]Batch 7700/10001 Done, mean position loss: 20.552885944843293\n",
      "Training NF2:  78%|██████████████▊    | 7791/10001 [3:59:43<58:51,  1.60s/batch]Batch 7800/10001 Done, mean position loss: 20.64455682992935\n",
      "Training NF2:  78%|█████████████▎   | 7827/10001 [3:59:48<1:18:49,  2.18s/batch]Batch 7900/10001 Done, mean position loss: 20.345597953796386\n",
      "Training NF2:  78%|█████████████▏   | 7773/10001 [3:59:48<1:19:27,  2.14s/batch]Batch 7800/10001 Done, mean position loss: 20.32167106389999\n",
      "Training NF2:  79%|██████████████▉    | 7893/10001 [3:59:51<58:43,  1.67s/batch]Batch 7900/10001 Done, mean position loss: 20.478646116256716\n",
      "Training NF2:  77%|█████████████▏   | 7746/10001 [3:59:53<1:09:06,  1.84s/batch]Batch 7900/10001 Done, mean position loss: 21.057816784381867\n",
      "Training NF2:  78%|█████████████▎   | 7847/10001 [4:00:00<1:01:09,  1.70s/batch]Batch 7800/10001 Done, mean position loss: 20.765686864852906\n",
      "Training NF2:  79%|█████████████▍   | 7878/10001 [4:00:01<1:06:59,  1.89s/batch]Batch 7900/10001 Done, mean position loss: 20.638311746120454\n",
      "Training NF2:  79%|███████████████    | 7911/10001 [4:00:06<54:00,  1.55s/batch]Batch 7900/10001 Done, mean position loss: 20.367484948635102\n",
      "Training NF2:  78%|█████████████▏   | 7769/10001 [4:00:35<1:20:43,  2.17s/batch]Batch 7800/10001 Done, mean position loss: 20.774256365299223\n",
      "Training NF2:  78%|█████████████▏   | 7772/10001 [4:00:40<1:04:21,  1.73s/batch]Batch 7800/10001 Done, mean position loss: 20.70626468896866\n",
      "Training NF2:  79%|█████████████▎   | 7867/10001 [4:00:42<1:04:13,  1.81s/batch]Batch 7900/10001 Done, mean position loss: 20.705063955783842\n",
      "Training NF2:  78%|█████████████▏   | 7788/10001 [4:00:44<1:01:28,  1.67s/batch]Batch 7900/10001 Done, mean position loss: 20.436695981025697\n",
      "Training NF2:  78%|█████████████▎   | 7805/10001 [4:00:46<1:27:39,  2.40s/batch]Batch 7900/10001 Done, mean position loss: 20.498819851875304\n",
      "Training NF2:  78%|█████████████▎   | 7807/10001 [4:00:49<1:09:39,  1.90s/batch]Batch 7900/10001 Done, mean position loss: 20.561330435276034\n",
      "Training NF2:  78%|█████████████▎   | 7807/10001 [4:00:50<1:04:19,  1.76s/batch]Batch 7900/10001 Done, mean position loss: 20.680473377704622\n",
      "Training NF2:  78%|█████████████▏   | 7786/10001 [4:00:56<1:09:24,  1.88s/batch]Batch 7900/10001 Done, mean position loss: 20.29133833169937\n",
      "Training NF2:  78%|█████████████▏   | 7751/10001 [4:01:07<1:06:49,  1.78s/batch]Batch 7800/10001 Done, mean position loss: 20.356797323226928\n",
      "Training NF2:  78%|█████████████▎   | 7802/10001 [4:01:09<1:06:06,  1.80s/batch]Batch 7900/10001 Done, mean position loss: 20.517900226116183\n",
      "Training NF2:  79%|█████████████▍   | 7874/10001 [4:01:26<1:04:12,  1.81s/batch]Batch 7800/10001 Done, mean position loss: 20.544377403259276\n",
      "Training NF2:  79%|█████████████▌   | 7948/10001 [4:01:28<1:09:36,  2.03s/batch]Batch 7900/10001 Done, mean position loss: 21.02323294878006\n",
      "Training NF2:  80%|█████████████▌   | 7957/10001 [4:01:30<1:03:45,  1.87s/batch]Batch 7900/10001 Done, mean position loss: 21.418045229911804\n",
      "Training NF2:  78%|█████████████▎   | 7805/10001 [4:01:33<1:09:27,  1.90s/batch]Batch 7900/10001 Done, mean position loss: 20.941403059959413\n",
      "Training NF2:  80%|█████████████▌   | 7959/10001 [4:01:34<1:04:12,  1.89s/batch]Batch 7800/10001 Done, mean position loss: 20.828318192958832\n",
      "Training NF2:  79%|█████████████▍   | 7896/10001 [4:01:40<1:26:06,  2.45s/batch]Batch 7900/10001 Done, mean position loss: 20.52517157793045\n",
      "Training NF2:  79%|█████████████▍   | 7901/10001 [4:01:40<1:17:18,  2.21s/batch]Batch 7900/10001 Done, mean position loss: 20.859188494682314\n",
      "Training NF2:  79%|█████████████▍   | 7879/10001 [4:01:47<1:11:56,  2.03s/batch]Batch 7900/10001 Done, mean position loss: 20.42790542125702\n",
      "Training NF2:  79%|█████████████▍   | 7930/10001 [4:01:47<1:07:27,  1.95s/batch]Batch 7900/10001 Done, mean position loss: 20.417196118831633\n",
      "Training NF2:  79%|█████████████▍   | 7936/10001 [4:01:49<1:08:28,  1.99s/batch]Batch 7800/10001 Done, mean position loss: 20.657457420825956\n",
      "Training NF2:  79%|█████████████▎   | 7861/10001 [4:01:56<1:06:29,  1.86s/batch]Batch 7800/10001 Done, mean position loss: 20.65882056951523\n",
      "Training NF2:  79%|██████████████▉    | 7894/10001 [4:02:03<59:29,  1.69s/batch]Batch 7900/10001 Done, mean position loss: 20.531687016487123\n",
      "Training NF2:  80%|███████████████    | 7951/10001 [4:02:10<49:57,  1.46s/batch]Batch 7900/10001 Done, mean position loss: 20.818852336406707\n",
      "Training NF2:  80%|█████████████▌   | 7954/10001 [4:02:17<1:04:27,  1.89s/batch]Batch 7900/10001 Done, mean position loss: 20.467924559116362\n",
      "Training NF2:  80%|█████████████▌   | 7968/10001 [4:02:18<1:09:32,  2.05s/batch]Batch 7900/10001 Done, mean position loss: 20.59442937374115\n",
      "Training NF2:  80%|█████████████▌   | 7986/10001 [4:02:29<1:08:01,  2.03s/batch]Batch 7900/10001 Done, mean position loss: 20.63616005182266\n",
      "Training NF2:  78%|█████████████▏   | 7794/10001 [4:02:29<1:10:21,  1.91s/batch]Batch 7900/10001 Done, mean position loss: 20.457041487693786\n",
      "Training NF2:  80%|███████████████▏   | 7962/10001 [4:02:36<58:27,  1.72s/batch]Batch 7900/10001 Done, mean position loss: 20.39430359363556\n",
      "Training NF2:  78%|█████████████▎   | 7849/10001 [4:02:36<1:01:41,  1.72s/batch]Batch 7900/10001 Done, mean position loss: 20.367350430488585\n",
      "Training NF2:  80%|█████████████▌   | 7963/10001 [4:02:41<1:05:50,  1.94s/batch]Batch 7900/10001 Done, mean position loss: 20.620179185867308\n",
      "Training NF2:  80%|███████████████▏   | 7967/10001 [4:02:43<48:34,  1.43s/batch]Batch 7800/10001 Done, mean position loss: 20.545138974189758\n",
      "Training NF2:  79%|█████████████▍   | 7917/10001 [4:02:46<1:02:25,  1.80s/batch]Batch 7900/10001 Done, mean position loss: 20.989148867130282\n",
      "Training NF2:  80%|█████████████▌   | 7984/10001 [4:02:51<1:16:30,  2.28s/batch]Batch 8000/10001 Done, mean position loss: 20.34048672199249\n",
      "Training NF2:  80%|███████████████▏   | 7970/10001 [4:02:52<58:54,  1.74s/batch]Batch 7900/10001 Done, mean position loss: 20.648684117794037\n",
      "Training NF2:  80%|███████████████▏   | 7974/10001 [4:02:58<58:56,  1.74s/batch]Batch 8000/10001 Done, mean position loss: 21.094685041904448\n",
      "Training NF2:  79%|█████████████▍   | 7912/10001 [4:02:59<1:01:01,  1.75s/batch]Batch 7900/10001 Done, mean position loss: 20.31216073513031\n",
      "Training NF2:  79%|███████████████    | 7946/10001 [4:03:01<55:15,  1.61s/batch]Batch 8000/10001 Done, mean position loss: 20.48445420026779\n",
      "Training NF2:  79%|█████████████▍   | 7913/10001 [4:03:07<1:06:43,  1.92s/batch]Batch 8000/10001 Done, mean position loss: 20.61018772125244\n",
      "Training NF2:  79%|█████████████▍   | 7912/10001 [4:03:13<1:11:28,  2.05s/batch]Batch 7900/10001 Done, mean position loss: 20.759641246795653\n",
      "Training NF2:  80%|█████████████▋   | 8018/10001 [4:03:22<1:05:29,  1.98s/batch]Batch 8000/10001 Done, mean position loss: 20.377698786258698\n",
      "Training NF2:  79%|█████████████▎   | 7867/10001 [4:03:40<1:07:47,  1.91s/batch]Batch 8000/10001 Done, mean position loss: 20.71521426677704\n",
      "Training NF2:  80%|███████████████▏   | 8024/10001 [4:03:43<54:34,  1.66s/batch]Batch 7900/10001 Done, mean position loss: 20.698407492637635\n",
      "Training NF2:  80%|█████████████▌   | 7970/10001 [4:03:47<1:03:07,  1.86s/batch]Batch 8000/10001 Done, mean position loss: 20.42724123477936\n",
      "Training NF2:  79%|█████████████▍   | 7941/10001 [4:03:48<1:05:34,  1.91s/batch]Batch 7900/10001 Done, mean position loss: 20.78883308172226\n",
      "Training NF2:  80%|█████████████▌   | 7974/10001 [4:03:49<1:04:58,  1.92s/batch]Batch 8000/10001 Done, mean position loss: 20.482937920093537\n",
      "Training NF2:  79%|█████████████▍   | 7888/10001 [4:03:53<1:07:38,  1.92s/batch]Batch 8000/10001 Done, mean position loss: 20.67718159914017\n",
      "Training NF2:  79%|█████████████▍   | 7908/10001 [4:03:55<1:01:43,  1.77s/batch]Batch 8000/10001 Done, mean position loss: 20.30328931331635\n",
      "Training NF2:  80%|███████████████▏   | 8006/10001 [4:03:56<58:06,  1.75s/batch]Batch 8000/10001 Done, mean position loss: 20.578222935199737\n",
      "Training NF2:  80%|███████████████▏   | 8018/10001 [4:04:16<52:47,  1.60s/batch]Batch 7900/10001 Done, mean position loss: 20.36416428089142\n",
      "Training NF2:  79%|█████████████▌   | 7949/10001 [4:04:17<1:06:15,  1.94s/batch]Batch 8000/10001 Done, mean position loss: 20.530771534442902\n",
      "Training NF2:  80%|███████████████▏   | 7992/10001 [4:04:33<57:22,  1.71s/batch]Batch 8000/10001 Done, mean position loss: 21.037881426811218\n",
      "Training NF2:  80%|███████████████▏   | 7988/10001 [4:04:36<59:26,  1.77s/batch]Batch 8000/10001 Done, mean position loss: 21.416010813713072\n",
      "Training NF2:  81%|███████████████▎   | 8055/10001 [4:04:35<51:52,  1.60s/batch]Batch 7900/10001 Done, mean position loss: 20.56936249256134\n",
      "Training NF2:  79%|███████████████    | 7903/10001 [4:04:38<58:15,  1.67s/batch]Batch 8000/10001 Done, mean position loss: 20.947287273406985\n",
      "Training NF2:  80%|█████████████▌   | 7970/10001 [4:04:42<1:14:50,  2.21s/batch]Batch 8000/10001 Done, mean position loss: 20.526618633270264\n",
      "Training NF2:  80%|█████████████▌   | 7972/10001 [4:04:47<1:15:31,  2.23s/batch]Batch 8000/10001 Done, mean position loss: 20.904463300704954\n",
      "Training NF2:  80%|███████████████▏   | 8008/10001 [4:04:47<53:14,  1.60s/batch]Batch 7900/10001 Done, mean position loss: 20.814734418392185\n",
      "Training NF2:  79%|█████████████▍   | 7900/10001 [4:04:52<1:14:55,  2.14s/batch]Batch 8000/10001 Done, mean position loss: 20.41943651676178\n",
      "Training NF2:  80%|█████████████▌   | 8009/10001 [4:04:54<1:15:28,  2.27s/batch]Batch 7900/10001 Done, mean position loss: 20.669304339885713\n",
      "Training NF2:  80%|███████████████▏   | 7967/10001 [4:05:00<59:57,  1.77s/batch]Batch 7900/10001 Done, mean position loss: 20.654576144218446\n",
      "Training NF2:  81%|███████████████▎   | 8061/10001 [4:05:02<55:05,  1.70s/batch]Batch 8000/10001 Done, mean position loss: 20.41946212053299\n",
      "Training NF2:  81%|███████████████▎   | 8052/10001 [4:05:13<57:26,  1.77s/batch]Batch 8000/10001 Done, mean position loss: 20.536840209960936\n",
      "Training NF2:  80%|█████████████▋   | 8022/10001 [4:05:15<1:02:19,  1.89s/batch]Batch 8000/10001 Done, mean position loss: 20.800824048519132\n",
      "Training NF2:  81%|█████████████▋   | 8070/10001 [4:05:22<1:09:20,  2.15s/batch]Batch 8000/10001 Done, mean position loss: 20.609023966789245\n",
      "Training NF2:  80%|█████████████▋   | 8023/10001 [4:05:28<1:01:34,  1.87s/batch]Batch 8000/10001 Done, mean position loss: 20.449925298690797\n",
      "Training NF2:  80%|███████████████▎   | 8044/10001 [4:05:38<55:21,  1.70s/batch]Batch 8000/10001 Done, mean position loss: 20.639493820667266\n",
      "Training NF2:  79%|███████████████    | 7896/10001 [4:05:41<58:29,  1.67s/batch]Batch 8000/10001 Done, mean position loss: 20.36756163358688\n",
      "Training NF2:  81%|█████████████▊   | 8095/10001 [4:05:44<1:06:21,  2.09s/batch]Batch 8000/10001 Done, mean position loss: 20.45671782493591\n",
      "Training NF2:  80%|█████████████▋   | 8032/10001 [4:05:45<1:00:29,  1.84s/batch]Batch 8000/10001 Done, mean position loss: 20.37132661819458\n",
      "Training NF2:  80%|███████████████▏   | 8015/10001 [4:05:47<59:00,  1.78s/batch]Batch 8000/10001 Done, mean position loss: 20.63187526702881\n",
      "Training NF2:  81%|███████████████▎   | 8063/10001 [4:05:51<54:25,  1.69s/batch]Batch 7900/10001 Done, mean position loss: 20.54874527215958\n",
      "Training NF2:  80%|█████████████▋   | 8037/10001 [4:05:55<1:03:05,  1.93s/batch]Batch 8100/10001 Done, mean position loss: 20.35781732082367\n",
      "Training NF2:  81%|███████████████▎   | 8065/10001 [4:05:55<56:50,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.95486192941666\n",
      "Training NF2:  80%|█████████████▌   | 7974/10001 [4:06:02<1:01:41,  1.83s/batch]Batch 8000/10001 Done, mean position loss: 20.64289296388626\n",
      "Training NF2:  80%|█████████████▋   | 8042/10001 [4:06:04<1:01:02,  1.87s/batch]Batch 8100/10001 Done, mean position loss: 20.473311235904696\n",
      "Training NF2:  80%|███████████████▏   | 7976/10001 [4:06:05<55:20,  1.64s/batch]Batch 8000/10001 Done, mean position loss: 20.3132817029953\n",
      "Training NF2:  80%|█████████████▋   | 8044/10001 [4:06:06<1:01:15,  1.88s/batch]Batch 8100/10001 Done, mean position loss: 21.105948433876037\n",
      "Training NF2:  80%|███████████████    | 7956/10001 [4:06:19<54:01,  1.59s/batch]Batch 8000/10001 Done, mean position loss: 20.762799129486083\n",
      "Training NF2:  80%|███████████████▏   | 8003/10001 [4:06:22<47:05,  1.41s/batch]Batch 8100/10001 Done, mean position loss: 20.609997472763062\n",
      "Training NF2:  81%|███████████████▎   | 8083/10001 [4:06:25<49:34,  1.55s/batch]Batch 8100/10001 Done, mean position loss: 20.355292074680328\n",
      "Training NF2:  81%|█████████████▊   | 8092/10001 [4:06:41<1:05:06,  2.05s/batch]Batch 8100/10001 Done, mean position loss: 20.713041751384733\n",
      "Training NF2:  81%|███████████████▍   | 8093/10001 [4:06:45<55:51,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.690446751117705\n",
      "Training NF2:  81%|███████████████▍   | 8106/10001 [4:06:48<48:39,  1.54s/batch]Batch 8000/10001 Done, mean position loss: 20.771290583610536\n",
      "Training NF2:  81%|███████████████▍   | 8095/10001 [4:06:48<53:07,  1.67s/batch]Batch 8100/10001 Done, mean position loss: 20.433356981277466\n",
      "Training NF2:  81%|█████████████▋   | 8074/10001 [4:06:55<1:03:06,  1.97s/batch]Batch 8100/10001 Done, mean position loss: 20.318312015533447\n",
      "Training NF2:  80%|█████████████▋   | 8049/10001 [4:06:57<1:00:54,  1.87s/batch]Batch 8100/10001 Done, mean position loss: 20.478017008304597\n",
      "Training NF2:  81%|███████████████▍   | 8131/10001 [4:06:59<53:35,  1.72s/batch]Batch 8100/10001 Done, mean position loss: 20.68876519441605\n",
      "Training NF2:  80%|█████████████▋   | 8040/10001 [4:07:00<1:00:00,  1.84s/batch]Batch 8100/10001 Done, mean position loss: 20.553425898551943\n",
      "Training NF2:  81%|███████████████▍   | 8127/10001 [4:07:24<49:20,  1.58s/batch]Batch 8000/10001 Done, mean position loss: 20.37760123968124\n",
      "Training NF2:  81%|███████████████▍   | 8121/10001 [4:07:27<54:34,  1.74s/batch]Batch 8100/10001 Done, mean position loss: 20.541297721862794\n",
      "Training NF2:  81%|█████████████▊   | 8125/10001 [4:07:34<1:05:46,  2.10s/batch]Batch 8100/10001 Done, mean position loss: 21.429270737171173\n",
      "Training NF2:  81%|███████████████▍   | 8099/10001 [4:07:39<52:19,  1.65s/batch]Batch 8100/10001 Done, mean position loss: 21.017359228134154\n",
      "Training NF2:  80%|███████████████▏   | 8012/10001 [4:07:41<50:41,  1.53s/batch]Batch 8000/10001 Done, mean position loss: 20.550599417686463\n",
      "Training NF2:  82%|███████████████▍   | 8156/10001 [4:07:42<50:07,  1.63s/batch]Batch 8100/10001 Done, mean position loss: 20.937791974544524\n",
      "Training NF2:  81%|███████████████▍   | 8100/10001 [4:07:44<51:02,  1.61s/batch]Batch 8100/10001 Done, mean position loss: 20.524583175182343\n",
      "Training NF2:  81%|█████████████▋   | 8082/10001 [4:07:45<1:09:19,  2.17s/batch]Batch 8100/10001 Done, mean position loss: 20.881056134700778\n",
      "Training NF2:  80%|███████████████▎   | 8039/10001 [4:07:55<57:35,  1.76s/batch]Batch 8000/10001 Done, mean position loss: 20.830885982513426\n",
      "Training NF2:  81%|███████████████▍   | 8112/10001 [4:07:57<47:07,  1.50s/batch]Batch 8100/10001 Done, mean position loss: 20.405595030784607\n",
      "Training NF2:  81%|███████████████▍   | 8141/10001 [4:08:02<53:23,  1.72s/batch]Batch 8000/10001 Done, mean position loss: 20.665826342105866\n",
      "Training NF2:  81%|███████████████▍   | 8148/10001 [4:08:02<57:37,  1.87s/batch]Batch 8100/10001 Done, mean position loss: 20.43523026227951\n",
      "Training NF2:  81%|█████████████▋   | 8060/10001 [4:08:03<1:02:02,  1.92s/batch]Batch 8000/10001 Done, mean position loss: 20.65419120311737\n",
      "Training NF2:  81%|███████████████▍   | 8144/10001 [4:08:11<57:29,  1.86s/batch]Batch 8100/10001 Done, mean position loss: 20.82604316711426\n",
      "Training NF2:  82%|███████████████▌   | 8165/10001 [4:08:11<45:48,  1.50s/batch]Batch 8100/10001 Done, mean position loss: 20.532633681297302\n",
      "Training NF2:  81%|███████████████▎   | 8087/10001 [4:08:20<50:39,  1.59s/batch]Batch 8100/10001 Done, mean position loss: 20.61138144016266\n",
      "Training NF2:  81%|█████████████▊   | 8093/10001 [4:08:31<1:03:41,  2.00s/batch]Batch 8100/10001 Done, mean position loss: 20.447500157356263\n",
      "Training NF2:  81%|███████████████▎   | 8091/10001 [4:08:33<53:44,  1.69s/batch]Batch 8100/10001 Done, mean position loss: 20.636841197013855\n",
      "Training NF2:  81%|███████████████▍   | 8143/10001 [4:08:42<53:51,  1.74s/batch]Batch 8100/10001 Done, mean position loss: 20.36386937856674\n",
      "Training NF2:  81%|███████████████▍   | 8127/10001 [4:08:47<55:55,  1.79s/batch]Batch 8100/10001 Done, mean position loss: 20.455381355285645\n",
      "Training NF2:  81%|█████████████▊   | 8120/10001 [4:08:49<1:08:35,  2.19s/batch]Batch 8100/10001 Done, mean position loss: 20.631570997238157\n",
      "Training NF2:  82%|███████████████▌   | 8170/10001 [4:08:50<47:21,  1.55s/batch]Batch 8200/10001 Done, mean position loss: 20.33989063501358\n",
      "Training NF2:  80%|█████████████▋   | 8030/10001 [4:08:55<1:02:47,  1.91s/batch]Batch 8000/10001 Done, mean position loss: 20.552252259254455\n",
      "Training NF2:  82%|███████████████▌   | 8195/10001 [4:08:54<50:37,  1.68s/batch]Batch 8100/10001 Done, mean position loss: 20.352077422142028\n",
      "Training NF2:  80%|███████████████▎   | 8037/10001 [4:09:00<55:52,  1.71s/batch]Batch 8200/10001 Done, mean position loss: 20.477492699623106\n",
      "Training NF2:  80%|███████████████▎   | 8039/10001 [4:09:03<59:32,  1.82s/batch]Batch 8100/10001 Done, mean position loss: 20.946825294494626\n",
      "Training NF2:  82%|███████████████▌   | 8183/10001 [4:09:06<48:41,  1.61s/batch]Batch 8100/10001 Done, mean position loss: 20.643589749336243\n",
      "Training NF2:  82%|█████████████▊   | 8152/10001 [4:09:07<1:04:53,  2.11s/batch]Batch 8200/10001 Done, mean position loss: 21.056588196754454\n",
      "Training NF2:  81%|█████████████▊   | 8093/10001 [4:09:07<1:11:25,  2.25s/batch]Batch 8100/10001 Done, mean position loss: 20.31249388933182\n",
      "Training NF2:  81%|███████████████▍   | 8120/10001 [4:09:15<50:35,  1.61s/batch]Batch 8200/10001 Done, mean position loss: 20.61306609869003\n",
      "Training NF2:  81%|███████████████▍   | 8120/10001 [4:09:22<50:31,  1.61s/batch]Batch 8100/10001 Done, mean position loss: 20.755405259132388\n",
      "Training NF2:  82%|███████████████▌   | 8214/10001 [4:09:24<57:35,  1.93s/batch]Batch 8200/10001 Done, mean position loss: 20.3802685213089\n",
      "Training NF2:  82%|███████████████▋   | 8227/10001 [4:09:36<50:40,  1.71s/batch]Batch 8200/10001 Done, mean position loss: 20.718976266384125\n",
      "Training NF2:  82%|███████████████▌   | 8193/10001 [4:09:44<59:46,  1.98s/batch]Batch 8200/10001 Done, mean position loss: 20.429478845596314\n",
      "Training NF2:  82%|███████████████▌   | 8199/10001 [4:09:45<51:15,  1.71s/batch]Batch 8100/10001 Done, mean position loss: 20.684139380455015\n",
      "Training NF2:  81%|█████████████▊   | 8101/10001 [4:09:45<1:01:49,  1.95s/batch]Batch 8100/10001 Done, mean position loss: 20.76215185880661\n",
      "Training NF2:  81%|███████████████▍   | 8116/10001 [4:09:49<56:52,  1.81s/batch]Batch 8200/10001 Done, mean position loss: 20.295590865612027\n",
      "Training NF2:  81%|█████████████▊   | 8137/10001 [4:09:54<1:17:40,  2.50s/batch]Batch 8200/10001 Done, mean position loss: 20.490469372272493\n",
      "Training NF2:  82%|███████████████▌   | 8197/10001 [4:09:59<54:29,  1.81s/batch]Batch 8200/10001 Done, mean position loss: 20.549261696338654\n",
      "Training NF2:  82%|███████████████▍   | 8152/10001 [4:10:06<55:58,  1.82s/batch]Batch 8200/10001 Done, mean position loss: 20.683936355113985\n",
      "Training NF2:  82%|███████████████▍   | 8155/10001 [4:10:24<44:18,  1.44s/batch]Batch 8100/10001 Done, mean position loss: 20.35908092260361\n",
      "Training NF2:  82%|███████████████▌   | 8199/10001 [4:10:31<53:13,  1.77s/batch]Batch 8200/10001 Done, mean position loss: 21.005890152454377\n",
      "Training NF2:  82%|███████████████▍   | 8151/10001 [4:10:33<56:30,  1.83s/batch]Batch 8200/10001 Done, mean position loss: 20.525751404762268\n",
      "Training NF2:  82%|███████████████▋   | 8233/10001 [4:10:36<51:21,  1.74s/batch]Batch 8200/10001 Done, mean position loss: 20.903574533462525\n",
      "Training NF2:  82%|███████████████▋   | 8245/10001 [4:10:43<50:57,  1.74s/batch]Batch 8200/10001 Done, mean position loss: 21.42515271663666\n",
      "Training NF2:  82%|███████████████▌   | 8181/10001 [4:10:47<56:54,  1.88s/batch]Batch 8200/10001 Done, mean position loss: 20.53353373527527\n",
      "Training NF2:  82%|███████████████▌   | 8199/10001 [4:10:48<51:08,  1.70s/batch]Batch 8100/10001 Done, mean position loss: 20.54916386127472\n",
      "Training NF2:  82%|███████████████▌   | 8198/10001 [4:10:51<53:37,  1.78s/batch]Batch 8200/10001 Done, mean position loss: 20.860611348152162\n",
      "Training NF2:  83%|███████████████▋   | 8258/10001 [4:10:53<52:13,  1.80s/batch]Batch 8100/10001 Done, mean position loss: 20.82527958393097\n",
      "Training NF2:  82%|███████████████▌   | 8165/10001 [4:10:58<58:40,  1.92s/batch]Batch 8200/10001 Done, mean position loss: 20.425501625537873\n",
      "Training NF2:  82%|█████████████▉   | 8164/10001 [4:11:04<1:00:00,  1.96s/batch]Batch 8100/10001 Done, mean position loss: 20.649467961788176\n",
      "Training NF2:  82%|███████████████▋   | 8242/10001 [4:11:03<56:49,  1.94s/batch]Batch 8200/10001 Done, mean position loss: 20.4359996175766\n",
      "Training NF2:  82%|█████████████▉   | 8167/10001 [4:11:10<1:03:49,  2.09s/batch]Batch 8100/10001 Done, mean position loss: 20.649374878406526\n",
      "Training NF2:  83%|███████████████▋   | 8265/10001 [4:11:21<54:07,  1.87s/batch]Batch 8200/10001 Done, mean position loss: 20.525912024974822\n",
      "Training NF2:  83%|███████████████▋   | 8269/10001 [4:11:26<49:09,  1.70s/batch]Batch 8200/10001 Done, mean position loss: 20.808236360549927\n",
      "Training NF2:  82%|█████████████▉   | 8191/10001 [4:11:28<1:07:00,  2.22s/batch]Batch 8200/10001 Done, mean position loss: 20.61123013973236\n",
      "Training NF2:  81%|███████████████▍   | 8116/10001 [4:11:40<59:46,  1.90s/batch]Batch 8200/10001 Done, mean position loss: 20.457605097293854\n",
      "Training NF2:  82%|███████████████▌   | 8192/10001 [4:11:46<51:24,  1.70s/batch]Batch 8200/10001 Done, mean position loss: 20.64665325164795\n",
      "Training NF2:  83%|███████████████▋   | 8255/10001 [4:11:46<54:49,  1.88s/batch]Batch 8200/10001 Done, mean position loss: 20.37615609884262\n",
      "Training NF2:  82%|█████████████▉   | 8180/10001 [4:11:50<1:03:46,  2.10s/batch]Batch 8200/10001 Done, mean position loss: 20.449718086719514\n",
      "Training NF2:  82%|███████████████▌   | 8175/10001 [4:11:57<49:00,  1.61s/batch]Batch 8300/10001 Done, mean position loss: 20.338806178569794\n",
      "Training NF2:  82%|███████████████▌   | 8193/10001 [4:12:02<59:13,  1.97s/batch]Batch 8200/10001 Done, mean position loss: 20.61920590162277\n",
      "Training NF2:  81%|█████████████▊   | 8138/10001 [4:12:02<1:03:15,  2.04s/batch]Batch 8100/10001 Done, mean position loss: 20.556643640995027\n",
      "Training NF2:  81%|███████████████▍   | 8128/10001 [4:12:02<55:53,  1.79s/batch]Batch 8200/10001 Done, mean position loss: 20.369679346084595\n",
      "Training NF2:  82%|███████████████▌   | 8213/10001 [4:12:03<54:03,  1.81s/batch]Batch 8200/10001 Done, mean position loss: 20.952037460803986\n",
      "Training NF2:  82%|███████████████▋   | 8228/10001 [4:12:13<53:39,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 20.483072183132172\n",
      "Training NF2:  82%|███████████████▋   | 8250/10001 [4:12:17<55:11,  1.89s/batch]Batch 8300/10001 Done, mean position loss: 21.066195316314698\n",
      "Training NF2:  82%|█████████████▉   | 8208/10001 [4:12:17<1:03:12,  2.11s/batch]Batch 8200/10001 Done, mean position loss: 20.632771730422974\n",
      "Training NF2:  83%|███████████████▊   | 8307/10001 [4:12:22<48:33,  1.72s/batch]Batch 8200/10001 Done, mean position loss: 20.308720705509188\n",
      "Training NF2:  83%|███████████████▊   | 8318/10001 [4:12:29<51:06,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 20.363923056125643\n",
      "Training NF2:  83%|███████████████▋   | 8289/10001 [4:12:30<50:30,  1.77s/batch]Batch 8300/10001 Done, mean position loss: 20.608540546894073\n",
      "Training NF2:  83%|███████████████▋   | 8263/10001 [4:12:32<56:22,  1.95s/batch]Batch 8200/10001 Done, mean position loss: 20.79201330661774\n",
      "Training NF2:  83%|███████████████▋   | 8256/10001 [4:12:43<53:55,  1.85s/batch]Batch 8300/10001 Done, mean position loss: 20.698372173309327\n",
      "Training NF2:  82%|█████████████▉   | 8196/10001 [4:12:47<1:02:18,  2.07s/batch]Batch 8200/10001 Done, mean position loss: 20.693255338668823\n",
      "Training NF2:  82%|█████████████▉   | 8163/10001 [4:12:52<1:02:58,  2.06s/batch]Batch 8300/10001 Done, mean position loss: 20.436677553653716\n",
      "Training NF2:  82%|█████████████▉   | 8179/10001 [4:12:55<1:02:02,  2.04s/batch]Batch 8300/10001 Done, mean position loss: 20.313874769210816\n",
      "Training NF2:  83%|██████████████▏  | 8324/10001 [4:12:57<1:04:41,  2.31s/batch]Batch 8200/10001 Done, mean position loss: 20.77166047334671\n",
      "Training NF2:  83%|███████████████▊   | 8326/10001 [4:13:00<52:55,  1.90s/batch]Batch 8300/10001 Done, mean position loss: 20.476123030185697\n",
      "Training NF2:  83%|███████████████▋   | 8267/10001 [4:13:05<52:26,  1.81s/batch]Batch 8300/10001 Done, mean position loss: 20.56202973127365\n",
      "Training NF2:  83%|███████████████▋   | 8281/10001 [4:13:17<45:07,  1.57s/batch]Batch 8300/10001 Done, mean position loss: 20.66363798618317\n",
      "Training NF2:  83%|███████████████▋   | 8261/10001 [4:13:35<42:39,  1.47s/batch]Batch 8200/10001 Done, mean position loss: 20.363082485198973\n",
      "Training NF2:  82%|██████████████   | 8242/10001 [4:13:38<1:01:38,  2.10s/batch]Batch 8300/10001 Done, mean position loss: 20.9900279545784\n",
      "Training NF2:  82%|███████████████▌   | 8187/10001 [4:13:42<50:12,  1.66s/batch]Batch 8300/10001 Done, mean position loss: 20.918502025604248\n",
      "Training NF2:  83%|███████████████▊   | 8349/10001 [4:13:43<48:57,  1.78s/batch]Batch 8300/10001 Done, mean position loss: 20.536733496189118\n",
      "Training NF2:  83%|███████████████▊   | 8348/10001 [4:13:56<52:25,  1.90s/batch]Batch 8300/10001 Done, mean position loss: 21.410359468460083\n",
      "Training NF2:  82%|███████████████▌   | 8164/10001 [4:13:59<55:53,  1.83s/batch]Batch 8300/10001 Done, mean position loss: 20.52395129919052\n",
      "Training NF2:  83%|███████████████▋   | 8269/10001 [4:14:01<50:58,  1.77s/batch]Batch 8300/10001 Done, mean position loss: 20.857993817329408\n",
      "Training NF2:  83%|███████████████▋   | 8280/10001 [4:14:03<57:44,  2.01s/batch]Batch 8200/10001 Done, mean position loss: 20.53337998390198\n",
      "Training NF2:  82%|███████████████▌   | 8203/10001 [4:14:07<52:43,  1.76s/batch]Batch 8200/10001 Done, mean position loss: 20.83313466787338\n",
      "Training NF2:  82%|███████████████▋   | 8240/10001 [4:14:09<53:33,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 20.419014098644258\n",
      "Training NF2:  83%|███████████████▊   | 8329/10001 [4:14:12<54:39,  1.96s/batch]Batch 8200/10001 Done, mean position loss: 20.64247442007065\n",
      "Training NF2:  83%|███████████████▊   | 8318/10001 [4:14:15<50:01,  1.78s/batch]Batch 8300/10001 Done, mean position loss: 20.43713460445404\n",
      "Training NF2:  82%|███████████████▌   | 8206/10001 [4:14:21<52:18,  1.75s/batch]Batch 8200/10001 Done, mean position loss: 20.661848833560942\n",
      "Training NF2:  83%|███████████████▋   | 8289/10001 [4:14:30<53:08,  1.86s/batch]Batch 8300/10001 Done, mean position loss: 20.81892191410065\n",
      "Training NF2:  84%|███████████████▉   | 8359/10001 [4:14:31<45:56,  1.68s/batch]Batch 8300/10001 Done, mean position loss: 20.54411451339722\n",
      "Training NF2:  83%|███████████████▋   | 8277/10001 [4:14:44<51:06,  1.78s/batch]Batch 8300/10001 Done, mean position loss: 20.60890758275986\n",
      "Training NF2:  83%|███████████████▋   | 8289/10001 [4:14:49<52:46,  1.85s/batch]Batch 8300/10001 Done, mean position loss: 20.364941711425782\n",
      "Training NF2:  83%|███████████████▊   | 8350/10001 [4:14:49<49:47,  1.81s/batch]Batch 8300/10001 Done, mean position loss: 20.44751934289932\n",
      "Training NF2:  83%|██████████████   | 8289/10001 [4:14:53<1:06:54,  2.34s/batch]Batch 8300/10001 Done, mean position loss: 20.631982626914976\n",
      "Training NF2:  82%|███████████████▋   | 8231/10001 [4:15:02<57:48,  1.96s/batch]Batch 8300/10001 Done, mean position loss: 20.455135929584507\n",
      "Training NF2:  83%|███████████████▋   | 8281/10001 [4:15:08<59:12,  2.07s/batch]Batch 8400/10001 Done, mean position loss: 20.345015172958373\n",
      "Training NF2:  82%|█████████████▉   | 8226/10001 [4:15:11<1:05:41,  2.22s/batch]Batch 8200/10001 Done, mean position loss: 20.56145735025406\n",
      "Training NF2:  84%|███████████████▉   | 8376/10001 [4:15:13<58:43,  2.17s/batch]Batch 8300/10001 Done, mean position loss: 20.63389168024063\n",
      "Training NF2:  83%|███████████████▊   | 8340/10001 [4:15:14<47:46,  1.73s/batch]Batch 8300/10001 Done, mean position loss: 20.36857033967972\n",
      "Training NF2:  83%|██████████████   | 8303/10001 [4:15:17<1:00:59,  2.16s/batch]Batch 8300/10001 Done, mean position loss: 20.976412971019744\n",
      "Training NF2:  83%|███████████████▊   | 8341/10001 [4:15:23<48:01,  1.74s/batch]Batch 8400/10001 Done, mean position loss: 20.485039083957673\n",
      "Training NF2:  84%|███████████████▊   | 8353/10001 [4:15:24<55:35,  2.02s/batch]Batch 8400/10001 Done, mean position loss: 21.052403371334076\n",
      "Training NF2:  84%|███████████████▉   | 8404/10001 [4:15:28<52:02,  1.96s/batch]Batch 8300/10001 Done, mean position loss: 20.631049909591674\n",
      "Training NF2:  83%|███████████████▊   | 8308/10001 [4:15:31<58:11,  2.06s/batch]Batch 8300/10001 Done, mean position loss: 20.317937858104706\n",
      "Training NF2:  83%|██████████████   | 8295/10001 [4:15:37<1:00:24,  2.12s/batch]Batch 8400/10001 Done, mean position loss: 20.39508641958237\n",
      "Training NF2:  83%|███████████████▊   | 8313/10001 [4:15:41<54:15,  1.93s/batch]Batch 8400/10001 Done, mean position loss: 20.605064823627472\n",
      "Training NF2:  82%|███████████████▌   | 8221/10001 [4:15:47<42:38,  1.44s/batch]Batch 8300/10001 Done, mean position loss: 20.801230678558348\n",
      "Training NF2:  83%|███████████████▊   | 8297/10001 [4:15:54<51:25,  1.81s/batch]Batch 8400/10001 Done, mean position loss: 20.71277518749237\n",
      "Training NF2:  84%|███████████████▉   | 8413/10001 [4:15:57<45:08,  1.71s/batch]Batch 8400/10001 Done, mean position loss: 20.413475954532622\n",
      "Training NF2:  83%|███████████████▊   | 8326/10001 [4:16:01<57:06,  2.05s/batch]Batch 8300/10001 Done, mean position loss: 20.683255791664124\n",
      "Training NF2:  84%|████████████████   | 8424/10001 [4:16:06<48:42,  1.85s/batch]Batch 8300/10001 Done, mean position loss: 20.753077535629274\n",
      "Training NF2:  84%|████████████████   | 8432/10001 [4:16:07<48:53,  1.87s/batch]Batch 8400/10001 Done, mean position loss: 20.486559433937074\n",
      "Training NF2:  84%|███████████████▉   | 8409/10001 [4:16:09<48:52,  1.84s/batch]Batch 8400/10001 Done, mean position loss: 20.30932549953461\n",
      "Training NF2:  84%|███████████████▉   | 8383/10001 [4:16:15<51:12,  1.90s/batch]Batch 8400/10001 Done, mean position loss: 20.560712974071503\n",
      "Training NF2:  83%|██████████████   | 8271/10001 [4:16:24<1:02:27,  2.17s/batch]Batch 8400/10001 Done, mean position loss: 20.669894309043883\n",
      "Training NF2:  84%|███████████████▉   | 8414/10001 [4:16:48<46:55,  1.77s/batch]Batch 8300/10001 Done, mean position loss: 20.367834756374357\n",
      "Training NF2:  84%|███████████████▉   | 8420/10001 [4:16:50<51:04,  1.94s/batch]Batch 8400/10001 Done, mean position loss: 20.930012030601503\n",
      "Training NF2:  84%|████████████████   | 8442/10001 [4:16:50<50:22,  1.94s/batch]Batch 8400/10001 Done, mean position loss: 21.000905282497406\n",
      "Training NF2:  84%|████████████████   | 8436/10001 [4:17:02<48:57,  1.88s/batch]Batch 8400/10001 Done, mean position loss: 20.54348747253418\n",
      "Training NF2:  85%|████████████████   | 8455/10001 [4:17:03<45:16,  1.76s/batch]Batch 8400/10001 Done, mean position loss: 21.424560964107513\n",
      "Training NF2:  84%|███████████████▉   | 8412/10001 [4:17:10<53:54,  2.04s/batch]Batch 8400/10001 Done, mean position loss: 20.87028804063797\n",
      "Training NF2:  84%|████████████████   | 8441/10001 [4:17:11<45:38,  1.76s/batch]Batch 8400/10001 Done, mean position loss: 20.419623575210572\n",
      "Training NF2:  84%|██████████████▎  | 8413/10001 [4:17:15<1:01:30,  2.32s/batch]Batch 8400/10001 Done, mean position loss: 20.540758678913118\n",
      "Training NF2:  84%|██████████████▏  | 8366/10001 [4:17:20<1:02:06,  2.28s/batch]Batch 8300/10001 Done, mean position loss: 20.650198011398317\n",
      "Training NF2:  84%|████████████████   | 8432/10001 [4:17:24<53:22,  2.04s/batch]Batch 8400/10001 Done, mean position loss: 20.432135746479034\n",
      "Training NF2:  84%|███████████████▉   | 8401/10001 [4:17:24<54:14,  2.03s/batch]Batch 8300/10001 Done, mean position loss: 20.54498743057251\n",
      "Training NF2:  84%|███████████████▉   | 8382/10001 [4:17:30<50:35,  1.87s/batch]Batch 8300/10001 Done, mean position loss: 20.810128335952758\n",
      "Training NF2:  84%|███████████████▉   | 8409/10001 [4:17:32<54:48,  2.07s/batch]Batch 8400/10001 Done, mean position loss: 20.81019718647003\n",
      "Training NF2:  84%|███████████████▉   | 8383/10001 [4:17:34<49:06,  1.82s/batch]Batch 8300/10001 Done, mean position loss: 20.65168260574341\n",
      "Training NF2:  84%|████████████████   | 8449/10001 [4:17:42<45:08,  1.75s/batch]Batch 8400/10001 Done, mean position loss: 20.536798055171968\n",
      "Training NF2:  84%|████████████████   | 8427/10001 [4:17:53<52:43,  2.01s/batch]Batch 8400/10001 Done, mean position loss: 20.606370518207548\n",
      "Training NF2:  83%|███████████████▊   | 8341/10001 [4:18:05<55:17,  2.00s/batch]Batch 8400/10001 Done, mean position loss: 20.43353945970535\n",
      "Training NF2:  85%|████████████████   | 8461/10001 [4:18:05<53:31,  2.09s/batch]Batch 8400/10001 Done, mean position loss: 20.64038744211197\n",
      "Training NF2:  85%|████████████████▏  | 8489/10001 [4:18:08<47:36,  1.89s/batch]Batch 8400/10001 Done, mean position loss: 20.434830482006074\n",
      "Training NF2:  84%|███████████████▉   | 8403/10001 [4:18:09<54:08,  2.03s/batch]Batch 8400/10001 Done, mean position loss: 20.371630523204804\n",
      "Training NF2:  84%|███████████████▉   | 8420/10001 [4:18:18<49:19,  1.87s/batch]Batch 8500/10001 Done, mean position loss: 20.331847257614136\n",
      "Training NF2:  84%|███████████████▉   | 8410/10001 [4:18:21<45:05,  1.70s/batch]Batch 8400/10001 Done, mean position loss: 20.38055871963501\n",
      "Training NF2:  84%|███████████████▉   | 8420/10001 [4:18:26<50:30,  1.92s/batch]Batch 8300/10001 Done, mean position loss: 20.53186131954193\n",
      "Training NF2:  84%|████████████████   | 8437/10001 [4:18:29<51:10,  1.96s/batch]Batch 8400/10001 Done, mean position loss: 20.61662576198578\n",
      "Training NF2:  84%|███████████████▉   | 8413/10001 [4:18:31<42:39,  1.61s/batch]Batch 8500/10001 Done, mean position loss: 20.4613990855217\n",
      "Training NF2:  84%|████████████████   | 8439/10001 [4:18:34<58:58,  2.27s/batch]Batch 8500/10001 Done, mean position loss: 21.048416242599487\n",
      "Training NF2:  84%|███████████████▉   | 8397/10001 [4:18:36<50:30,  1.89s/batch]Batch 8400/10001 Done, mean position loss: 20.948472464084627\n",
      "Training NF2:  84%|███████████████▉   | 8380/10001 [4:18:38<50:05,  1.85s/batch]Batch 8400/10001 Done, mean position loss: 20.31223999261856\n",
      "Training NF2:  84%|████████████████   | 8427/10001 [4:18:39<45:21,  1.73s/batch]Batch 8500/10001 Done, mean position loss: 20.366706545352933\n",
      "Training NF2:  85%|████████████████   | 8481/10001 [4:18:43<50:46,  2.00s/batch]Batch 8400/10001 Done, mean position loss: 20.62994482278824\n",
      "Training NF2:  85%|████████████████   | 8479/10001 [4:18:51<49:34,  1.95s/batch]Batch 8500/10001 Done, mean position loss: 20.604190549850465\n",
      "Training NF2:  85%|████████████████   | 8482/10001 [4:18:57<49:10,  1.94s/batch]Batch 8400/10001 Done, mean position loss: 20.812488322257995\n",
      "Training NF2:  84%|████████████████   | 8429/10001 [4:19:01<53:14,  2.03s/batch]Batch 8500/10001 Done, mean position loss: 20.713901863098144\n",
      "Training NF2:  83%|███████████████▊   | 8348/10001 [4:19:05<46:46,  1.70s/batch]Batch 8500/10001 Done, mean position loss: 20.42499912023544\n",
      "Training NF2:  84%|████████████████   | 8429/10001 [4:19:11<49:55,  1.91s/batch]Batch 8400/10001 Done, mean position loss: 20.69471539735794\n",
      "Training NF2:  84%|███████████████▉   | 8366/10001 [4:19:20<51:29,  1.89s/batch]Batch 8400/10001 Done, mean position loss: 20.759884893894196\n",
      "Training NF2:  84%|███████████████▉   | 8413/10001 [4:19:21<53:25,  2.02s/batch]Batch 8500/10001 Done, mean position loss: 20.308437502384187\n",
      "Training NF2:  85%|████████████████▏  | 8512/10001 [4:19:22<47:36,  1.92s/batch]Batch 8500/10001 Done, mean position loss: 20.48581827402115\n",
      "Training NF2:  85%|████████████████▏  | 8497/10001 [4:19:25<43:01,  1.72s/batch]Batch 8500/10001 Done, mean position loss: 20.562908167839048\n",
      "Training NF2:  85%|████████████████   | 8469/10001 [4:19:33<52:48,  2.07s/batch]Batch 8500/10001 Done, mean position loss: 20.678245334625245\n",
      "Training NF2:  85%|████████████████▏  | 8491/10001 [4:20:00<44:05,  1.75s/batch]Batch 8400/10001 Done, mean position loss: 20.355991919040683\n",
      "Training NF2:  85%|████████████████▏  | 8525/10001 [4:20:03<43:58,  1.79s/batch]Batch 8500/10001 Done, mean position loss: 20.976629621982575\n",
      "Training NF2:  85%|████████████████▏  | 8521/10001 [4:20:10<40:09,  1.63s/batch]Batch 8500/10001 Done, mean position loss: 21.00503807067871\n",
      "Training NF2:  85%|████████████████▏  | 8536/10001 [4:20:12<38:21,  1.57s/batch]Batch 8500/10001 Done, mean position loss: 21.442682235240937\n",
      "Training NF2:  85%|████████████████▏  | 8500/10001 [4:20:17<48:04,  1.92s/batch]Batch 8500/10001 Done, mean position loss: 20.519878554344178\n",
      "Training NF2:  85%|████████████████▏  | 8490/10001 [4:20:18<47:02,  1.87s/batch]Batch 8500/10001 Done, mean position loss: 20.435620024204255\n",
      "Training NF2:  85%|████████████████▏  | 8534/10001 [4:20:18<42:54,  1.76s/batch]Batch 8500/10001 Done, mean position loss: 20.860694882869723\n",
      "Training NF2:  84%|████████████████   | 8447/10001 [4:20:23<52:10,  2.01s/batch]Batch 8400/10001 Done, mean position loss: 20.65772507429123\n",
      "Training NF2:  85%|████████████████   | 8475/10001 [4:20:32<44:45,  1.76s/batch]Batch 8500/10001 Done, mean position loss: 20.449737429618835\n",
      "Training NF2:  84%|███████████████▉   | 8399/10001 [4:20:32<41:30,  1.55s/batch]Batch 8500/10001 Done, mean position loss: 20.542717978954315\n",
      "Training NF2:  85%|████████████████▏  | 8503/10001 [4:20:36<44:37,  1.79s/batch]Batch 8400/10001 Done, mean position loss: 20.797254986763\n",
      "Training NF2:  85%|████████████████   | 8470/10001 [4:20:41<47:12,  1.85s/batch]Batch 8500/10001 Done, mean position loss: 20.81845730781555\n",
      "Training NF2:  84%|██████████████▎  | 8404/10001 [4:20:43<1:02:42,  2.36s/batch]Batch 8400/10001 Done, mean position loss: 20.548384292125704\n",
      "Training NF2:  85%|████████████████   | 8485/10001 [4:20:50<46:44,  1.85s/batch]Batch 8400/10001 Done, mean position loss: 20.63883814096451\n",
      "Training NF2:  86%|████████████████▎  | 8554/10001 [4:20:58<49:59,  2.07s/batch]Batch 8500/10001 Done, mean position loss: 20.543145673274992\n",
      "Training NF2:  84%|███████████████▉   | 8407/10001 [4:21:02<53:22,  2.01s/batch]Batch 8500/10001 Done, mean position loss: 20.591674654483796\n",
      "Training NF2:  85%|████████████████▏  | 8509/10001 [4:21:18<55:17,  2.22s/batch]Batch 8500/10001 Done, mean position loss: 20.44303203582764\n",
      "Training NF2:  85%|████████████████   | 8461/10001 [4:21:20<41:23,  1.61s/batch]Batch 8500/10001 Done, mean position loss: 20.34765878200531\n",
      "Training NF2:  85%|████████████████▏  | 8523/10001 [4:21:20<49:51,  2.02s/batch]Batch 8500/10001 Done, mean position loss: 20.44040904045105\n",
      "Training NF2:  84%|███████████████▉   | 8395/10001 [4:21:25<51:16,  1.92s/batch]Batch 8500/10001 Done, mean position loss: 20.62860812187195\n",
      "Training NF2:  84%|████████████████   | 8436/10001 [4:21:27<42:33,  1.63s/batch]Batch 8500/10001 Done, mean position loss: 20.377761566638945\n",
      "Training NF2:  85%|████████████████   | 8485/10001 [4:21:32<45:14,  1.79s/batch]Batch 8500/10001 Done, mean position loss: 20.618176093101503\n",
      "Training NF2:  85%|████████████████▏  | 8502/10001 [4:21:34<42:54,  1.72s/batch]Batch 8600/10001 Done, mean position loss: 20.33687844276428\n",
      "Training NF2:  85%|████████████████   | 8486/10001 [4:21:34<46:44,  1.85s/batch]Batch 8600/10001 Done, mean position loss: 21.061151239871982\n",
      "Training NF2:  85%|████████████████▏  | 8507/10001 [4:21:37<50:03,  2.01s/batch]Batch 8400/10001 Done, mean position loss: 20.553857233524322\n",
      "Training NF2:  86%|████████████████▎  | 8592/10001 [4:21:38<36:24,  1.55s/batch]Batch 8600/10001 Done, mean position loss: 20.478743793964387\n",
      "Training NF2:  86%|████████████████▎  | 8606/10001 [4:21:47<43:37,  1.88s/batch]Batch 8500/10001 Done, mean position loss: 20.64433815956116\n",
      "Training NF2:  85%|████████████████   | 8459/10001 [4:21:50<36:37,  1.43s/batch]Batch 8500/10001 Done, mean position loss: 20.302832195758818\n",
      "Training NF2:  85%|████████████████▏  | 8491/10001 [4:21:51<42:23,  1.68s/batch]Batch 8600/10001 Done, mean position loss: 20.376600518226624\n",
      "Training NF2:  86%|████████████████▎  | 8559/10001 [4:21:54<37:53,  1.58s/batch]Batch 8600/10001 Done, mean position loss: 20.599419145584108\n",
      "Training NF2:  85%|████████████████   | 8453/10001 [4:21:56<40:47,  1.58s/batch]Batch 8500/10001 Done, mean position loss: 20.95257610797882\n",
      "Training NF2:  85%|████████████████▏  | 8545/10001 [4:22:04<49:32,  2.04s/batch]Batch 8500/10001 Done, mean position loss: 20.776852028369902\n",
      "Training NF2:  85%|████████████████▏  | 8489/10001 [4:22:12<46:47,  1.86s/batch]Batch 8500/10001 Done, mean position loss: 20.701215226650238\n",
      "Training NF2:  86%|████████████████▎  | 8566/10001 [4:22:14<41:52,  1.75s/batch]Batch 8600/10001 Done, mean position loss: 20.71749940872192\n",
      "Training NF2:  86%|████████████████▎  | 8603/10001 [4:22:16<38:15,  1.64s/batch]Batch 8600/10001 Done, mean position loss: 20.43522341012955\n",
      "Training NF2:  86%|████████████████▍  | 8635/10001 [4:22:33<37:22,  1.64s/batch]Batch 8600/10001 Done, mean position loss: 20.30842994451523\n",
      "Training NF2:  85%|████████████████   | 8473/10001 [4:22:34<46:52,  1.84s/batch]Batch 8500/10001 Done, mean position loss: 20.771036331653598\n",
      "Training NF2:  86%|████████████████▎  | 8570/10001 [4:22:35<47:34,  2.00s/batch]Batch 8600/10001 Done, mean position loss: 20.561793770790104\n",
      "Training NF2:  86%|████████████████▎  | 8604/10001 [4:22:37<38:42,  1.66s/batch]Batch 8600/10001 Done, mean position loss: 20.659445445537568\n",
      "Training NF2:  85%|████████████████▏  | 8543/10001 [4:22:39<44:24,  1.83s/batch]Batch 8600/10001 Done, mean position loss: 20.491139962673188\n",
      "Training NF2:  85%|████████████████   | 8476/10001 [4:23:10<49:26,  1.94s/batch]Batch 8500/10001 Done, mean position loss: 20.355401620864868\n",
      "Training NF2:  85%|████████████████▏  | 8537/10001 [4:23:10<49:25,  2.03s/batch]Batch 8600/10001 Done, mean position loss: 20.951476645469665\n",
      "Training NF2:  86%|████████████████▍  | 8629/10001 [4:23:15<56:34,  2.47s/batch]Batch 8600/10001 Done, mean position loss: 21.435504534244537\n",
      "Training NF2:  86%|████████████████▎  | 8595/10001 [4:23:20<41:03,  1.75s/batch]Batch 8600/10001 Done, mean position loss: 20.99541519165039\n",
      "Training NF2:  86%|████████████████▎  | 8597/10001 [4:23:26<48:48,  2.09s/batch]Batch 8600/10001 Done, mean position loss: 20.549882876873014\n",
      "Training NF2:  86%|████████████████▎  | 8577/10001 [4:23:27<59:02,  2.49s/batch]Batch 8500/10001 Done, mean position loss: 20.643154485225676\n",
      "Training NF2:  85%|████████████████▏  | 8504/10001 [4:23:34<58:23,  2.34s/batch]Batch 8600/10001 Done, mean position loss: 20.434916770458223\n",
      "Training NF2:  86%|████████████████▍  | 8639/10001 [4:23:36<52:06,  2.30s/batch]Batch 8600/10001 Done, mean position loss: 20.854276077747347\n",
      "Training NF2:  85%|████████████████▏  | 8488/10001 [4:23:35<49:14,  1.95s/batch]Batch 8600/10001 Done, mean position loss: 20.41587801218033\n",
      "Training NF2:  86%|████████████████▎  | 8611/10001 [4:23:42<46:33,  2.01s/batch]Batch 8600/10001 Done, mean position loss: 20.541666650772093\n",
      "Training NF2:  86%|████████████████▍  | 8640/10001 [4:23:42<42:31,  1.87s/batch]Batch 8600/10001 Done, mean position loss: 20.80327769756317\n",
      "Training NF2:  85%|████████████████▏  | 8515/10001 [4:23:52<38:42,  1.56s/batch]Batch 8500/10001 Done, mean position loss: 20.829862117767334\n",
      "Training NF2:  86%|████████████████▍  | 8643/10001 [4:23:58<43:56,  1.94s/batch]Batch 8500/10001 Done, mean position loss: 20.531749749183653\n",
      "Training NF2:  86%|████████████████▍  | 8621/10001 [4:23:59<37:47,  1.64s/batch]Batch 8500/10001 Done, mean position loss: 20.6279829120636\n",
      "Training NF2:  85%|████████████████▏  | 8512/10001 [4:24:11<45:00,  1.81s/batch]Batch 8600/10001 Done, mean position loss: 20.54583094358444\n",
      "Training NF2:  85%|████████████████▏  | 8509/10001 [4:24:13<43:16,  1.74s/batch]Batch 8600/10001 Done, mean position loss: 20.60561952829361\n",
      "Training NF2:  86%|████████████████▎  | 8577/10001 [4:24:18<40:29,  1.71s/batch]Batch 8600/10001 Done, mean position loss: 20.35322245836258\n",
      "Training NF2:  87%|████████████████▍  | 8668/10001 [4:24:25<37:59,  1.71s/batch]Batch 8600/10001 Done, mean position loss: 20.445008509159088\n",
      "Training NF2:  86%|████████████████▍  | 8630/10001 [4:24:30<39:53,  1.75s/batch]Batch 8600/10001 Done, mean position loss: 20.437085766792297\n",
      "Training NF2:  86%|████████████████▍  | 8646/10001 [4:24:34<42:21,  1.88s/batch]Batch 8600/10001 Done, mean position loss: 20.382192347049713\n",
      "Training NF2:  87%|████████████████▌  | 8694/10001 [4:24:37<43:31,  2.00s/batch]Batch 8700/10001 Done, mean position loss: 21.059431643486022\n",
      "Training NF2:  86%|████████████████▎  | 8612/10001 [4:24:44<38:26,  1.66s/batch]Batch 8600/10001 Done, mean position loss: 20.628101682662965\n",
      "Training NF2:  87%|████████████████▍  | 8681/10001 [4:24:48<37:27,  1.70s/batch]Batch 8700/10001 Done, mean position loss: 20.48382642030716\n",
      "Batch 8600/10001 Done, mean position loss: 20.623859529495242\n",
      "Training NF2:  87%|████████████████▌  | 8702/10001 [4:24:48<38:10,  1.76s/batch]Batch 8700/10001 Done, mean position loss: 20.342977309226992\n",
      "Training NF2:  87%|████████████████▌  | 8704/10001 [4:24:52<40:52,  1.89s/batch]Batch 8500/10001 Done, mean position loss: 20.5417934846878\n",
      "Training NF2:  87%|████████████████▍  | 8652/10001 [4:24:59<39:07,  1.74s/batch]Batch 8700/10001 Done, mean position loss: 20.395610601902007\n",
      "Batch 8600/10001 Done, mean position loss: 20.30848252296448\n",
      "Training NF2:  87%|████████████████▌  | 8717/10001 [4:25:01<36:22,  1.70s/batch]Batch 8600/10001 Done, mean position loss: 20.954392511844635\n",
      "Training NF2:  87%|████████████████▍  | 8662/10001 [4:25:03<36:44,  1.65s/batch]Batch 8600/10001 Done, mean position loss: 20.66928811311722\n",
      "Training NF2:  85%|████████████████▏  | 8538/10001 [4:25:03<39:43,  1.63s/batch]Batch 8700/10001 Done, mean position loss: 20.60454196929932\n",
      "Training NF2:  87%|████████████████▍  | 8655/10001 [4:25:11<35:50,  1.60s/batch]Batch 8600/10001 Done, mean position loss: 20.790606980323794\n",
      "Training NF2:  85%|████████████████▏  | 8547/10001 [4:25:19<40:37,  1.68s/batch]Batch 8600/10001 Done, mean position loss: 20.694006371498105\n",
      "Training NF2:  85%|████████████████▏  | 8546/10001 [4:25:20<37:06,  1.53s/batch]Batch 8700/10001 Done, mean position loss: 20.42215348005295\n",
      "Training NF2:  87%|████████████████▌  | 8707/10001 [4:25:29<31:56,  1.48s/batch]Batch 8700/10001 Done, mean position loss: 20.71724870920181\n",
      "Training NF2:  87%|████████████████▌  | 8705/10001 [4:25:36<41:33,  1.92s/batch]Batch 8700/10001 Done, mean position loss: 20.315655603408814\n",
      "Training NF2:  86%|████████████████▎  | 8600/10001 [4:25:43<51:47,  2.22s/batch]Batch 8700/10001 Done, mean position loss: 20.550787754058838\n",
      "Training NF2:  87%|████████████████▍  | 8651/10001 [4:25:43<35:31,  1.58s/batch]Batch 8700/10001 Done, mean position loss: 20.48959350347519\n",
      "Training NF2:  87%|████████████████▌  | 8706/10001 [4:25:44<41:43,  1.93s/batch]Batch 8700/10001 Done, mean position loss: 20.64897001504898\n",
      "Training NF2:  87%|████████████████▌  | 8725/10001 [4:25:44<32:03,  1.51s/batch]Batch 8600/10001 Done, mean position loss: 20.76190210342407\n",
      "Training NF2:  86%|████████████████▍  | 8644/10001 [4:26:17<41:28,  1.83s/batch]Batch 8700/10001 Done, mean position loss: 21.423313088417054\n",
      "Batch 8700/10001 Done, mean position loss: 20.9214329123497\n",
      "Training NF2:  87%|████████████████▍  | 8660/10001 [4:26:19<41:22,  1.85s/batch]Batch 8600/10001 Done, mean position loss: 20.360296411514284\n",
      "Training NF2:  86%|████████████████▍  | 8647/10001 [4:26:23<45:10,  2.00s/batch]Batch 8700/10001 Done, mean position loss: 20.98824415922165\n",
      "Training NF2:  86%|████████████████▍  | 8640/10001 [4:26:30<41:59,  1.85s/batch]Batch 8700/10001 Done, mean position loss: 20.53049143075943\n",
      "Training NF2:  87%|████████████████▌  | 8729/10001 [4:26:30<37:43,  1.78s/batch]Batch 8700/10001 Done, mean position loss: 20.41548731803894\n",
      "Training NF2:  87%|████████████████▌  | 8701/10001 [4:26:30<33:08,  1.53s/batch]Batch 8600/10001 Done, mean position loss: 20.64837197303772\n",
      "Training NF2:  87%|████████████████▌  | 8703/10001 [4:26:33<32:40,  1.51s/batch]Batch 8700/10001 Done, mean position loss: 20.442479870319367\n",
      "Training NF2:  87%|████████████████▌  | 8701/10001 [4:26:34<41:16,  1.91s/batch]Batch 8700/10001 Done, mean position loss: 20.866566350460054\n",
      "Training NF2:  87%|████████████████▌  | 8740/10001 [4:26:37<31:46,  1.51s/batch]Batch 8700/10001 Done, mean position loss: 20.553156673908234\n",
      "Training NF2:  88%|████████████████▋  | 8758/10001 [4:26:38<40:53,  1.97s/batch]Batch 8700/10001 Done, mean position loss: 20.81863876581192\n",
      "Training NF2:  87%|████████████████▌  | 8747/10001 [4:26:55<33:21,  1.60s/batch]Batch 8600/10001 Done, mean position loss: 20.540580241680146\n",
      "Batch 8600/10001 Done, mean position loss: 20.798115332126617\n",
      "Training NF2:  87%|████████████████▌  | 8725/10001 [4:26:58<38:50,  1.83s/batch]Batch 8600/10001 Done, mean position loss: 20.63532457113266\n",
      "Training NF2:  87%|████████████████▌  | 8721/10001 [4:27:10<37:12,  1.74s/batch]Batch 8700/10001 Done, mean position loss: 20.53616079568863\n",
      "Training NF2:  87%|████████████████▌  | 8732/10001 [4:27:20<40:38,  1.92s/batch]Batch 8700/10001 Done, mean position loss: 20.359023485183712\n",
      "Training NF2:  87%|████████████████▌  | 8689/10001 [4:27:20<38:18,  1.75s/batch]Batch 8700/10001 Done, mean position loss: 20.443499348163606\n",
      "Training NF2:  88%|████████████████▋  | 8755/10001 [4:27:20<43:14,  2.08s/batch]Batch 8700/10001 Done, mean position loss: 20.610522108078\n",
      "Training NF2:  86%|████████████████▍  | 8622/10001 [4:27:34<42:26,  1.85s/batch]Batch 8700/10001 Done, mean position loss: 20.380619249343873\n",
      "Batch 8800/10001 Done, mean position loss: 21.08212165594101\n",
      "Training NF2:  88%|████████████████▋  | 8796/10001 [4:27:40<36:40,  1.83s/batch]Batch 8700/10001 Done, mean position loss: 20.445491354465485\n",
      "Training NF2:  87%|████████████████▍  | 8680/10001 [4:27:44<38:24,  1.74s/batch]Batch 8800/10001 Done, mean position loss: 20.473663277626038\n",
      "Training NF2:  88%|████████████████▋  | 8801/10001 [4:27:44<31:34,  1.58s/batch]Batch 8700/10001 Done, mean position loss: 20.636673076152803\n",
      "Training NF2:  86%|████████████████▎  | 8592/10001 [4:27:46<40:21,  1.72s/batch]Batch 8800/10001 Done, mean position loss: 20.33474778175354\n",
      "Training NF2:  88%|████████████████▋  | 8774/10001 [4:27:49<35:59,  1.76s/batch]Batch 8700/10001 Done, mean position loss: 20.622158370018006\n",
      "Training NF2:  88%|████████████████▋  | 8758/10001 [4:27:56<38:54,  1.88s/batch]Batch 8800/10001 Done, mean position loss: 20.397888610363005\n",
      "Training NF2:  87%|████████████████▌  | 8724/10001 [4:28:01<42:25,  1.99s/batch]Batch 8700/10001 Done, mean position loss: 20.305007493495943\n",
      "Training NF2:  87%|████████████████▍  | 8678/10001 [4:28:02<40:45,  1.85s/batch]Batch 8600/10001 Done, mean position loss: 20.540422432422638\n",
      "Training NF2:  87%|████████████████▍  | 8679/10001 [4:28:05<44:06,  2.00s/batch]Batch 8800/10001 Done, mean position loss: 20.590691883564\n",
      "Training NF2:  86%|████████████████▎  | 8603/10001 [4:28:05<38:23,  1.65s/batch]Batch 8700/10001 Done, mean position loss: 20.65589600086212\n",
      "Training NF2:  86%|████████████████▍  | 8638/10001 [4:28:06<43:58,  1.94s/batch]Batch 8700/10001 Done, mean position loss: 20.962239513397215\n",
      "Training NF2:  88%|████████████████▊  | 8822/10001 [4:28:13<36:08,  1.84s/batch]Batch 8700/10001 Done, mean position loss: 20.789358632564543\n",
      "Training NF2:  88%|████████████████▊  | 8825/10001 [4:28:18<36:06,  1.84s/batch]Batch 8800/10001 Done, mean position loss: 20.427204771041872\n",
      "Training NF2:  87%|████████████████▌  | 8737/10001 [4:28:24<34:31,  1.64s/batch]Batch 8700/10001 Done, mean position loss: 20.695973711013792\n",
      "Training NF2:  88%|████████████████▋  | 8774/10001 [4:28:28<42:27,  2.08s/batch]Batch 8800/10001 Done, mean position loss: 20.72501174211502\n",
      "Training NF2:  87%|████████████████▌  | 8725/10001 [4:28:34<36:42,  1.73s/batch]Batch 8800/10001 Done, mean position loss: 20.29394902229309\n",
      "Training NF2:  88%|████████████████▊  | 8832/10001 [4:28:42<38:01,  1.95s/batch]Batch 8800/10001 Done, mean position loss: 20.556638641357424\n",
      "Training NF2:  88%|████████████████▊  | 8841/10001 [4:28:47<41:08,  2.13s/batch]Batch 8700/10001 Done, mean position loss: 20.75308758497238\n",
      "Training NF2:  88%|████████████████▋  | 8751/10001 [4:28:50<45:58,  2.21s/batch]Batch 8800/10001 Done, mean position loss: 20.49570873260498\n",
      "Training NF2:  87%|████████████████▍  | 8662/10001 [4:28:52<45:24,  2.03s/batch]Batch 8800/10001 Done, mean position loss: 20.659585461616516\n",
      "Training NF2:  87%|████████████████▌  | 8739/10001 [4:29:16<39:16,  1.87s/batch]Batch 8800/10001 Done, mean position loss: 21.433265335559845\n",
      "Training NF2:  88%|████████████████▋  | 8800/10001 [4:29:18<39:29,  1.97s/batch]Batch 8700/10001 Done, mean position loss: 20.347863099575044\n",
      "Training NF2:  89%|████████████████▊  | 8856/10001 [4:29:21<47:04,  2.47s/batch]Batch 8800/10001 Done, mean position loss: 20.936916456222534\n",
      "Training NF2:  88%|████████████████▋  | 8771/10001 [4:29:30<38:51,  1.90s/batch]Batch 8800/10001 Done, mean position loss: 20.518805322647093\n",
      "Training NF2:  87%|████████████████▍  | 8683/10001 [4:29:32<39:52,  1.82s/batch]Batch 8800/10001 Done, mean position loss: 21.01036672830582\n",
      "Training NF2:  88%|████████████████▊  | 8835/10001 [4:29:37<39:54,  2.05s/batch]Batch 8800/10001 Done, mean position loss: 20.869625239372255\n",
      "Training NF2:  87%|████████████████▌  | 8698/10001 [4:29:38<42:14,  1.94s/batch]Batch 8800/10001 Done, mean position loss: 20.432268176078793\n",
      "Training NF2:  89%|████████████████▊  | 8862/10001 [4:29:41<34:07,  1.80s/batch]Batch 8800/10001 Done, mean position loss: 20.420385601520536\n",
      "Training NF2:  88%|████████████████▋  | 8754/10001 [4:29:44<40:44,  1.96s/batch]Batch 8800/10001 Done, mean position loss: 20.56007203578949\n",
      "Training NF2:  88%|████████████████▋  | 8755/10001 [4:29:44<31:36,  1.52s/batch]Batch 8700/10001 Done, mean position loss: 20.61480237007141\n",
      "Training NF2:  88%|████████████████▊  | 8841/10001 [4:29:48<35:04,  1.81s/batch]Batch 8800/10001 Done, mean position loss: 20.815601704120635\n",
      "Training NF2:  89%|████████████████▊  | 8866/10001 [4:30:01<33:04,  1.75s/batch]Batch 8700/10001 Done, mean position loss: 20.625129141807555\n",
      "Training NF2:  87%|████████████████▌  | 8725/10001 [4:30:05<39:02,  1.84s/batch]Batch 8700/10001 Done, mean position loss: 20.7990456533432\n",
      "Training NF2:  89%|████████████████▊  | 8856/10001 [4:30:15<36:02,  1.89s/batch]Batch 8700/10001 Done, mean position loss: 20.53372368812561\n",
      "Training NF2:  88%|████████████████▊  | 8843/10001 [4:30:21<36:23,  1.89s/batch]Batch 8800/10001 Done, mean position loss: 20.54480218887329\n",
      "Training NF2:  88%|████████████████▋  | 8781/10001 [4:30:29<33:57,  1.67s/batch]Batch 8800/10001 Done, mean position loss: 20.328486385345457\n",
      "Training NF2:  88%|████████████████▊  | 8821/10001 [4:30:30<35:37,  1.81s/batch]Batch 8800/10001 Done, mean position loss: 20.60117069244385\n",
      "Training NF2:  88%|████████████████▋  | 8803/10001 [4:30:34<40:49,  2.05s/batch]Batch 8800/10001 Done, mean position loss: 20.443168938159943\n",
      "Training NF2:  87%|████████████████▌  | 8718/10001 [4:30:46<34:33,  1.62s/batch]Batch 8800/10001 Done, mean position loss: 20.431889998912812\n",
      "Training NF2:  88%|████████████████▊  | 8846/10001 [4:30:47<33:47,  1.76s/batch]Batch 8800/10001 Done, mean position loss: 20.384885177612304\n",
      "Training NF2:  88%|████████████████▋  | 8797/10001 [4:30:48<34:24,  1.71s/batch]Batch 8900/10001 Done, mean position loss: 21.058967723846436\n",
      "Training NF2:  87%|████████████████▌  | 8735/10001 [4:30:51<50:56,  2.41s/batch]Batch 8800/10001 Done, mean position loss: 20.626823239326477\n",
      "Training NF2:  88%|████████████████▊  | 8835/10001 [4:30:52<38:51,  2.00s/batch]Batch 8900/10001 Done, mean position loss: 20.34744838476181\n",
      "Training NF2:  87%|████████████████▌  | 8723/10001 [4:30:55<39:55,  1.87s/batch]Batch 8900/10001 Done, mean position loss: 20.480855255126954\n",
      "Training NF2:  87%|████████████████▌  | 8689/10001 [4:30:55<43:24,  1.98s/batch]Batch 8800/10001 Done, mean position loss: 20.629747457504273\n",
      "Training NF2:  89%|████████████████▉  | 8905/10001 [4:30:59<31:42,  1.74s/batch]Batch 8900/10001 Done, mean position loss: 20.387831752300265\n",
      "Training NF2:  87%|████████████████▌  | 8694/10001 [4:31:04<42:50,  1.97s/batch]Batch 8800/10001 Done, mean position loss: 20.306010987758636\n",
      "Training NF2:  87%|████████████████▌  | 8729/10001 [4:31:08<42:31,  2.01s/batch]Batch 8900/10001 Done, mean position loss: 20.601664571762086\n",
      "Training NF2:  89%|████████████████▉  | 8890/10001 [4:31:11<38:31,  2.08s/batch]Batch 8800/10001 Done, mean position loss: 20.668416297435762\n",
      "Training NF2:  87%|████████████████▌  | 8748/10001 [4:31:16<37:45,  1.81s/batch]Batch 8800/10001 Done, mean position loss: 20.942660624980924\n",
      "Training NF2:  89%|████████████████▉  | 8891/10001 [4:31:20<34:57,  1.89s/batch]Batch 8700/10001 Done, mean position loss: 20.540388791561128\n",
      "Training NF2:  89%|████████████████▉  | 8888/10001 [4:31:25<39:27,  2.13s/batch]Batch 8800/10001 Done, mean position loss: 20.76889387369156\n",
      "Training NF2:  89%|████████████████▉  | 8900/10001 [4:31:29<31:07,  1.70s/batch]Batch 8800/10001 Done, mean position loss: 20.709990904331207\n",
      "Training NF2:  89%|████████████████▉  | 8896/10001 [4:31:30<31:48,  1.73s/batch]Batch 8900/10001 Done, mean position loss: 20.4147479391098\n",
      "Training NF2:  89%|████████████████▊  | 8862/10001 [4:31:38<37:42,  1.99s/batch]Batch 8900/10001 Done, mean position loss: 20.720287072658536\n",
      "Training NF2:  89%|████████████████▉  | 8907/10001 [4:31:42<32:46,  1.80s/batch]Batch 8900/10001 Done, mean position loss: 20.30295380592346\n",
      "Training NF2:  89%|████████████████▉  | 8931/10001 [4:31:50<34:34,  1.94s/batch]Batch 8900/10001 Done, mean position loss: 20.54518180608749\n",
      "Training NF2:  88%|████████████████▊  | 8827/10001 [4:32:01<34:54,  1.78s/batch]Batch 8800/10001 Done, mean position loss: 20.761656258106232\n",
      "Training NF2:  89%|████████████████▊  | 8857/10001 [4:32:02<38:23,  2.01s/batch]Batch 8900/10001 Done, mean position loss: 20.667232294082645\n",
      "Training NF2:  89%|████████████████▉  | 8920/10001 [4:32:12<28:02,  1.56s/batch]Batch 8900/10001 Done, mean position loss: 20.47563822031021\n",
      "Training NF2:  89%|████████████████▉  | 8894/10001 [4:32:26<30:42,  1.66s/batch]Batch 8900/10001 Done, mean position loss: 21.418914964199068\n",
      "Training NF2:  89%|████████████████▊  | 8855/10001 [4:32:28<39:51,  2.09s/batch]Batch 8800/10001 Done, mean position loss: 20.357690732479096\n",
      "Training NF2:  90%|█████████████████  | 8956/10001 [4:32:29<25:35,  1.47s/batch]Batch 8900/10001 Done, mean position loss: 20.9269358921051\n",
      "Training NF2:  89%|████████████████▉  | 8925/10001 [4:32:39<43:22,  2.42s/batch]Batch 8900/10001 Done, mean position loss: 20.52399862766266\n",
      "Training NF2:  88%|████████████████▊  | 8841/10001 [4:32:42<34:34,  1.79s/batch]Batch 8900/10001 Done, mean position loss: 21.02803920984268\n",
      "Training NF2:  89%|████████████████▉  | 8913/10001 [4:32:49<29:10,  1.61s/batch]Batch 8900/10001 Done, mean position loss: 20.562175211906432\n",
      "Training NF2:  89%|████████████████▉  | 8944/10001 [4:32:52<32:38,  1.85s/batch]Batch 8900/10001 Done, mean position loss: 20.85520571708679\n",
      "Training NF2:  88%|████████████████▋  | 8799/10001 [4:32:53<41:53,  2.09s/batch]Batch 8900/10001 Done, mean position loss: 20.445657563209537\n",
      "Training NF2:  90%|█████████████████  | 8966/10001 [4:32:57<39:49,  2.31s/batch]Batch 8800/10001 Done, mean position loss: 20.621551129817963\n",
      "Training NF2:  89%|████████████████▉  | 8906/10001 [4:33:00<38:15,  2.10s/batch]Batch 8900/10001 Done, mean position loss: 20.418765971660612\n",
      "Training NF2:  89%|████████████████▉  | 8933/10001 [4:33:04<33:49,  1.90s/batch]Batch 8900/10001 Done, mean position loss: 20.807621304988864\n",
      "Training NF2:  89%|████████████████▉  | 8934/10001 [4:33:11<30:35,  1.72s/batch]Batch 8800/10001 Done, mean position loss: 20.631902151107788\n",
      "Training NF2:  89%|████████████████▊  | 8873/10001 [4:33:16<35:26,  1.88s/batch]Batch 8800/10001 Done, mean position loss: 20.788552765846255\n",
      "Training NF2:  88%|████████████████▋  | 8765/10001 [4:33:27<47:11,  2.29s/batch]Batch 8800/10001 Done, mean position loss: 20.552544531822203\n",
      "Training NF2:  88%|████████████████▋  | 8802/10001 [4:33:29<35:03,  1.75s/batch]Batch 8900/10001 Done, mean position loss: 20.546136367321015\n",
      "Training NF2:  90%|█████████████████  | 8993/10001 [4:33:37<29:36,  1.76s/batch]Batch 8900/10001 Done, mean position loss: 20.351621947288514\n",
      "Batch 8900/10001 Done, mean position loss: 20.606348185539247\n",
      "Training NF2:  88%|████████████████▊  | 8818/10001 [4:33:43<33:36,  1.70s/batch]Batch 8900/10001 Done, mean position loss: 20.439864687919616\n",
      "Training NF2:  90%|█████████████████  | 8989/10001 [4:33:50<26:55,  1.60s/batch]Batch 8900/10001 Done, mean position loss: 20.63912143945694\n",
      "Training NF2:  90%|█████████████████  | 8998/10001 [4:33:53<37:27,  2.24s/batch]Batch 9000/10001 Done, mean position loss: 21.082642986774445\n",
      "Training NF2:  89%|████████████████▊  | 8863/10001 [4:33:55<38:05,  2.01s/batch]Batch 8900/10001 Done, mean position loss: 20.447002921104435\n",
      "Training NF2:  89%|████████████████▉  | 8934/10001 [4:33:58<25:16,  1.42s/batch]Batch 9000/10001 Done, mean position loss: 20.48681933403015\n",
      "Training NF2:  89%|████████████████▉  | 8936/10001 [4:34:01<35:28,  2.00s/batch]Batch 8900/10001 Done, mean position loss: 20.375282783508297\n",
      "Training NF2:  89%|████████████████▉  | 8912/10001 [4:34:04<36:09,  1.99s/batch]Batch 9000/10001 Done, mean position loss: 20.351343827247618\n",
      "Training NF2:  90%|█████████████████  | 8997/10001 [4:34:06<29:13,  1.75s/batch]Batch 8900/10001 Done, mean position loss: 20.624731333255767\n",
      "Training NF2:  89%|████████████████▉  | 8894/10001 [4:34:07<33:02,  1.79s/batch]Batch 8900/10001 Done, mean position loss: 20.30884377002716\n",
      "Training NF2:  90%|█████████████████  | 8990/10001 [4:34:14<27:11,  1.61s/batch]Batch 9000/10001 Done, mean position loss: 20.624690058231355\n",
      "Training NF2:  89%|████████████████▉  | 8890/10001 [4:34:16<39:55,  2.16s/batch]Batch 9000/10001 Done, mean position loss: 20.384695127010346\n",
      "Training NF2:  89%|████████████████▉  | 8911/10001 [4:34:20<31:33,  1.74s/batch]Batch 8900/10001 Done, mean position loss: 20.660570850372316\n",
      "Training NF2:  88%|████████████████▊  | 8838/10001 [4:34:21<36:37,  1.89s/batch]Batch 8900/10001 Done, mean position loss: 20.963006737232206\n",
      "Training NF2:  89%|████████████████▉  | 8921/10001 [4:34:34<33:06,  1.84s/batch]Batch 9000/10001 Done, mean position loss: 20.42523587703705\n",
      "Training NF2:  89%|████████████████▉  | 8930/10001 [4:34:35<34:42,  1.94s/batch]Batch 8900/10001 Done, mean position loss: 20.78758641242981\n",
      "Training NF2:  88%|████████████████▊  | 8836/10001 [4:34:37<37:52,  1.95s/batch]Batch 8800/10001 Done, mean position loss: 20.5416051530838\n",
      "Training NF2:  89%|████████████████▉  | 8913/10001 [4:34:42<30:21,  1.67s/batch]Batch 8900/10001 Done, mean position loss: 20.695009002685545\n",
      "Training NF2:  88%|████████████████▊  | 8840/10001 [4:34:43<31:37,  1.63s/batch]Batch 9000/10001 Done, mean position loss: 20.71451199531555\n",
      "Training NF2:  90%|█████████████████  | 8992/10001 [4:34:53<33:43,  2.01s/batch]Batch 9000/10001 Done, mean position loss: 20.301181979179383\n",
      "Training NF2:  90%|█████████████████  | 8980/10001 [4:34:58<34:22,  2.02s/batch]Batch 9000/10001 Done, mean position loss: 20.56027509689331\n",
      "Training NF2:  89%|████████████████▉  | 8925/10001 [4:35:05<29:22,  1.64s/batch]Batch 8900/10001 Done, mean position loss: 20.748815932273864\n",
      "Training NF2:  88%|████████████████▊  | 8819/10001 [4:35:11<41:03,  2.08s/batch]Batch 9000/10001 Done, mean position loss: 20.66295770406723\n",
      "Training NF2:  89%|████████████████▉  | 8940/10001 [4:35:20<35:03,  1.98s/batch]Batch 9000/10001 Done, mean position loss: 20.50533050775528\n",
      "Training NF2:  90%|█████████████████  | 8962/10001 [4:35:34<30:37,  1.77s/batch]Batch 9000/10001 Done, mean position loss: 21.411565442085266\n",
      "Training NF2:  90%|█████████████████▏ | 9029/10001 [4:35:35<28:26,  1.76s/batch]Batch 8900/10001 Done, mean position loss: 20.35264387130737\n",
      "Training NF2:  90%|█████████████████  | 8990/10001 [4:35:42<32:22,  1.92s/batch]Batch 9000/10001 Done, mean position loss: 20.908175711631774\n",
      "Training NF2:  90%|█████████████████  | 8995/10001 [4:35:55<31:43,  1.89s/batch]Batch 9000/10001 Done, mean position loss: 20.995040643215177\n",
      "Training NF2:  89%|████████████████▉  | 8942/10001 [4:35:59<30:54,  1.75s/batch]Batch 9000/10001 Done, mean position loss: 20.50643129348755\n",
      "Training NF2:  90%|█████████████████  | 8997/10001 [4:36:02<39:25,  2.36s/batch]Batch 9000/10001 Done, mean position loss: 20.527943120002746\n",
      "Training NF2:  89%|████████████████▊  | 8880/10001 [4:36:03<39:52,  2.13s/batch]Batch 9000/10001 Done, mean position loss: 20.419388163089753\n",
      "Training NF2:  90%|█████████████████  | 8973/10001 [4:36:08<34:21,  2.01s/batch]Batch 8900/10001 Done, mean position loss: 20.629854588508607\n",
      "Training NF2:  88%|████████████████▊  | 8848/10001 [4:36:08<41:06,  2.14s/batch]Batch 9000/10001 Done, mean position loss: 20.432654519081115\n",
      "Training NF2:  88%|████████████████▊  | 8849/10001 [4:36:09<38:54,  2.03s/batch]Batch 9000/10001 Done, mean position loss: 20.824152421951293\n",
      "Training NF2:  91%|█████████████████▏ | 9070/10001 [4:36:09<31:54,  2.06s/batch]Batch 9000/10001 Done, mean position loss: 20.831282663345338\n",
      "Training NF2:  90%|█████████████████▏ | 9031/10001 [4:36:15<26:18,  1.63s/batch]Batch 8900/10001 Done, mean position loss: 20.637010900974275\n",
      "Training NF2:  90%|█████████████████  | 8992/10001 [4:36:25<32:14,  1.92s/batch]Batch 8900/10001 Done, mean position loss: 20.786459195613862\n",
      "Training NF2:  89%|████████████████▉  | 8908/10001 [4:36:39<35:40,  1.96s/batch]Batch 9000/10001 Done, mean position loss: 20.537453377246855\n",
      "Training NF2:  90%|█████████████████▏ | 9018/10001 [4:36:40<24:57,  1.52s/batch]Batch 8900/10001 Done, mean position loss: 20.542130217552184\n",
      "Training NF2:  90%|█████████████████  | 8990/10001 [4:36:51<37:10,  2.21s/batch]Batch 9000/10001 Done, mean position loss: 20.35960817337036\n",
      "Training NF2:  91%|█████████████████▏ | 9063/10001 [4:36:52<28:43,  1.84s/batch]Batch 9000/10001 Done, mean position loss: 20.59840510606766\n",
      "Training NF2:  90%|█████████████████▏ | 9030/10001 [4:36:58<34:34,  2.14s/batch]Batch 9000/10001 Done, mean position loss: 20.441690764427186\n",
      "Training NF2:  90%|█████████████████  | 8975/10001 [4:37:00<30:59,  1.81s/batch]Batch 9100/10001 Done, mean position loss: 21.067941074371337\n",
      "Training NF2:  91%|█████████████████▏ | 9056/10001 [4:37:01<26:21,  1.67s/batch]Batch 9000/10001 Done, mean position loss: 20.623958919048306\n",
      "Training NF2:  90%|█████████████████▏ | 9032/10001 [4:37:11<29:16,  1.81s/batch]Batch 9100/10001 Done, mean position loss: 20.486224708557128\n",
      "Training NF2:  91%|█████████████████▎ | 9101/10001 [4:37:11<28:46,  1.92s/batch]Batch 9000/10001 Done, mean position loss: 20.375467205047606\n",
      "Training NF2:  90%|█████████████████▏ | 9038/10001 [4:37:11<24:10,  1.51s/batch]Batch 9000/10001 Done, mean position loss: 20.442009685039523\n",
      "Training NF2:  91%|█████████████████▏ | 9076/10001 [4:37:19<29:31,  1.92s/batch]Batch 9100/10001 Done, mean position loss: 20.337446382045748\n",
      "Training NF2:  90%|█████████████████  | 9012/10001 [4:37:19<33:19,  2.02s/batch]Batch 9000/10001 Done, mean position loss: 20.308977971076963\n",
      "Training NF2:  90%|█████████████████  | 9013/10001 [4:37:21<31:30,  1.91s/batch]Batch 9100/10001 Done, mean position loss: 20.6042592048645\n",
      "Training NF2:  90%|█████████████████▏ | 9044/10001 [4:37:24<36:01,  2.26s/batch]Batch 9000/10001 Done, mean position loss: 20.610797455310824\n",
      "Training NF2:  90%|█████████████████  | 9001/10001 [4:37:25<41:05,  2.47s/batch]Batch 9100/10001 Done, mean position loss: 20.3886060833931\n",
      "Training NF2:  91%|█████████████████▎ | 9083/10001 [4:37:33<28:36,  1.87s/batch]Batch 9000/10001 Done, mean position loss: 20.646357190608978\n",
      "Training NF2:  90%|█████████████████▏ | 9035/10001 [4:37:38<26:59,  1.68s/batch]Batch 9000/10001 Done, mean position loss: 20.938733592033387\n",
      "Training NF2:  90%|█████████████████▏ | 9037/10001 [4:37:42<28:09,  1.75s/batch]Batch 9100/10001 Done, mean position loss: 20.418616449832918\n",
      "Training NF2:  91%|█████████████████▎ | 9092/10001 [4:37:49<24:27,  1.61s/batch]Batch 8900/10001 Done, mean position loss: 20.52373783826828\n",
      "Batch 9000/10001 Done, mean position loss: 20.761360776424407\n",
      "Training NF2:  91%|█████████████████▎ | 9095/10001 [4:37:52<29:46,  1.97s/batch]Batch 9100/10001 Done, mean position loss: 20.70220512866974\n",
      "Training NF2:  90%|█████████████████▏ | 9022/10001 [4:37:54<28:39,  1.76s/batch]Batch 9000/10001 Done, mean position loss: 20.691631128787996\n",
      "Training NF2:  90%|█████████████████  | 9006/10001 [4:38:03<32:43,  1.97s/batch]Batch 9100/10001 Done, mean position loss: 20.30317889690399\n",
      "Training NF2:  90%|█████████████████  | 9010/10001 [4:38:06<27:51,  1.69s/batch]Batch 9100/10001 Done, mean position loss: 20.551036770343778\n",
      "Training NF2:  90%|█████████████████▏ | 9039/10001 [4:38:15<35:08,  2.19s/batch]Batch 9000/10001 Done, mean position loss: 20.762621831893924\n",
      "Training NF2:  90%|█████████████████  | 8967/10001 [4:38:20<35:20,  2.05s/batch]Batch 9100/10001 Done, mean position loss: 20.681268265247347\n",
      "Training NF2:  91%|█████████████████▏ | 9075/10001 [4:38:30<26:24,  1.71s/batch]Batch 9100/10001 Done, mean position loss: 20.493976266384124\n",
      "Training NF2:  90%|█████████████████▏ | 9025/10001 [4:38:38<31:57,  1.96s/batch]Batch 9100/10001 Done, mean position loss: 21.420349099636077\n",
      "Training NF2:  90%|█████████████████▏ | 9036/10001 [4:38:46<32:21,  2.01s/batch]Batch 9000/10001 Done, mean position loss: 20.348444199562074\n",
      "Training NF2:  91%|█████████████████▎ | 9090/10001 [4:38:54<28:46,  1.89s/batch]Batch 9100/10001 Done, mean position loss: 20.967260158061983\n",
      "Training NF2:  90%|█████████████████  | 8988/10001 [4:39:00<33:22,  1.98s/batch]Batch 9100/10001 Done, mean position loss: 21.049559564590453\n",
      "Training NF2:  91%|█████████████████▎ | 9096/10001 [4:39:05<25:49,  1.71s/batch]Batch 9100/10001 Done, mean position loss: 20.5263384103775\n",
      "Training NF2:  91%|█████████████████▏ | 9058/10001 [4:39:13<29:54,  1.90s/batch]Batch 9100/10001 Done, mean position loss: 20.509534857273103\n",
      "Training NF2:  92%|█████████████████▍ | 9161/10001 [4:39:14<26:40,  1.91s/batch]Batch 9100/10001 Done, mean position loss: 20.42808848142624\n",
      "Training NF2:  90%|█████████████████▏ | 9045/10001 [4:39:16<27:59,  1.76s/batch]Batch 9100/10001 Done, mean position loss: 20.404666180610658\n",
      "Training NF2:  91%|█████████████████▏ | 9069/10001 [4:39:19<36:25,  2.34s/batch]Batch 9100/10001 Done, mean position loss: 20.821867647171022\n",
      "Training NF2:  91%|█████████████████▎ | 9103/10001 [4:39:21<20:08,  1.35s/batch]Batch 9100/10001 Done, mean position loss: 20.839711985588075\n",
      "Training NF2:  91%|█████████████████▎ | 9111/10001 [4:39:25<31:57,  2.15s/batch]Batch 9000/10001 Done, mean position loss: 20.647216942310333\n",
      "Training NF2:  91%|█████████████████▏ | 9059/10001 [4:39:26<27:47,  1.77s/batch]Batch 9000/10001 Done, mean position loss: 20.611398966312407\n",
      "Training NF2:  91%|█████████████████▎ | 9092/10001 [4:39:43<26:46,  1.77s/batch]Batch 9000/10001 Done, mean position loss: 20.78894947528839\n",
      "Training NF2:  91%|█████████████████▏ | 9075/10001 [4:39:46<28:34,  1.85s/batch]Batch 9100/10001 Done, mean position loss: 20.542725336551666\n",
      "Training NF2:  92%|█████████████████▍ | 9180/10001 [4:39:50<28:58,  2.12s/batch]Batch 9000/10001 Done, mean position loss: 20.54233684539795\n",
      "Training NF2:  91%|█████████████████▎ | 9131/10001 [4:39:56<25:47,  1.78s/batch]Batch 9100/10001 Done, mean position loss: 20.594526851177214\n",
      "Training NF2:  92%|█████████████████▍ | 9181/10001 [4:40:02<29:28,  2.16s/batch]Batch 9100/10001 Done, mean position loss: 20.356792008876802\n",
      "Training NF2:  91%|█████████████████▏ | 9073/10001 [4:40:08<29:55,  1.93s/batch]Batch 9200/10001 Done, mean position loss: 21.065978114604953\n",
      "Training NF2:  92%|█████████████████▍ | 9194/10001 [4:40:18<20:57,  1.56s/batch]Batch 9200/10001 Done, mean position loss: 20.48730606794357\n",
      "Training NF2:  90%|█████████████████  | 8981/10001 [4:40:18<25:27,  1.50s/batch]Batch 9100/10001 Done, mean position loss: 20.454325520992278\n",
      "Training NF2:  91%|█████████████████▎ | 9110/10001 [4:40:21<32:54,  2.22s/batch]Batch 9100/10001 Done, mean position loss: 20.369933044910432\n",
      "Training NF2:  91%|█████████████████▎ | 9137/10001 [4:40:24<24:10,  1.68s/batch]Batch 9100/10001 Done, mean position loss: 20.638616223335266\n",
      "Training NF2:  92%|█████████████████▍ | 9170/10001 [4:40:26<21:59,  1.59s/batch]Batch 9100/10001 Done, mean position loss: 20.43198225736618\n",
      "Training NF2:  91%|█████████████████▎ | 9113/10001 [4:40:28<34:05,  2.30s/batch]Batch 9100/10001 Done, mean position loss: 20.30531965494156\n",
      "Training NF2:  92%|█████████████████▍ | 9198/10001 [4:40:27<20:06,  1.50s/batch]Batch 9200/10001 Done, mean position loss: 20.607449905872343\n",
      "Training NF2:  91%|█████████████████▎ | 9137/10001 [4:40:32<28:59,  2.01s/batch]Batch 9200/10001 Done, mean position loss: 20.34257008552551\n",
      "Training NF2:  91%|█████████████████▏ | 9078/10001 [4:40:34<26:49,  1.74s/batch]Batch 9200/10001 Done, mean position loss: 20.36772619009018\n",
      "Training NF2:  92%|█████████████████▍ | 9210/10001 [4:40:34<26:01,  1.97s/batch]Batch 9100/10001 Done, mean position loss: 20.613204934597015\n",
      "Training NF2:  92%|█████████████████▌ | 9220/10001 [4:40:38<22:20,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 20.425866150856017\n",
      "Training NF2:  91%|█████████████████▏ | 9064/10001 [4:40:44<28:00,  1.79s/batch]Batch 9100/10001 Done, mean position loss: 20.639923300743106\n",
      "Training NF2:  92%|█████████████████▍ | 9211/10001 [4:40:45<22:39,  1.72s/batch]Batch 9100/10001 Done, mean position loss: 20.989448771476745\n",
      "Training NF2:  92%|█████████████████▍ | 9159/10001 [4:40:55<24:04,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 20.71766834497452\n",
      "Training NF2:  91%|█████████████████▎ | 9109/10001 [4:40:58<26:34,  1.79s/batch]Batch 9000/10001 Done, mean position loss: 20.516460871696474\n",
      "Training NF2:  91%|█████████████████▏ | 9072/10001 [4:41:00<33:21,  2.15s/batch]Batch 9100/10001 Done, mean position loss: 20.71438018321991\n",
      "Training NF2:  91%|█████████████████▎ | 9122/10001 [4:41:03<31:45,  2.17s/batch]Batch 9100/10001 Done, mean position loss: 20.781805934906004\n",
      "Training NF2:  91%|█████████████████▎ | 9124/10001 [4:41:05<27:20,  1.87s/batch]Batch 9200/10001 Done, mean position loss: 20.29270697116852\n",
      "Training NF2:  91%|█████████████████▎ | 9131/10001 [4:41:14<26:04,  1.80s/batch]Batch 9200/10001 Done, mean position loss: 20.547556281089783\n",
      "Training NF2:  92%|█████████████████▍ | 9171/10001 [4:41:15<28:31,  2.06s/batch]Batch 9100/10001 Done, mean position loss: 20.768330280780795\n",
      "Training NF2:  91%|█████████████████▎ | 9132/10001 [4:41:28<26:16,  1.81s/batch]Batch 9200/10001 Done, mean position loss: 20.673935391902923\n",
      "Training NF2:  92%|█████████████████▍ | 9189/10001 [4:41:39<21:49,  1.61s/batch]Batch 9200/10001 Done, mean position loss: 20.492129092216494\n",
      "Training NF2:  92%|█████████████████▍ | 9204/10001 [4:41:43<22:49,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 21.43752141237259\n",
      "Training NF2:  92%|█████████████████▍ | 9169/10001 [4:41:56<26:24,  1.90s/batch]Batch 9100/10001 Done, mean position loss: 20.357726602554322\n",
      "Training NF2:  92%|█████████████████▍ | 9162/10001 [4:42:00<25:59,  1.86s/batch]Batch 9200/10001 Done, mean position loss: 20.52254534006119\n",
      "Training NF2:  93%|█████████████████▌ | 9271/10001 [4:42:14<22:03,  1.81s/batch]Batch 9200/10001 Done, mean position loss: 21.016564667224884\n",
      "Training NF2:  92%|█████████████████▍ | 9169/10001 [4:42:14<25:36,  1.85s/batch]Batch 9200/10001 Done, mean position loss: 20.92264115571976\n",
      "Training NF2:  93%|█████████████████▌ | 9262/10001 [4:42:17<21:45,  1.77s/batch]Batch 9200/10001 Done, mean position loss: 20.519921822547914\n",
      "Training NF2:  91%|█████████████████▎ | 9136/10001 [4:42:21<27:09,  1.88s/batch]Batch 9200/10001 Done, mean position loss: 20.432557785511015\n",
      "Training NF2:  90%|█████████████████▏ | 9045/10001 [4:42:22<30:16,  1.90s/batch]Batch 9200/10001 Done, mean position loss: 20.81535202503204\n",
      "Training NF2:  91%|█████████████████▎ | 9137/10001 [4:42:24<30:18,  2.10s/batch]Batch 9200/10001 Done, mean position loss: 20.42298394680023\n",
      "Training NF2:  92%|█████████████████▍ | 9211/10001 [4:42:31<22:58,  1.75s/batch]Batch 9100/10001 Done, mean position loss: 20.62324444532394\n",
      "Training NF2:  92%|█████████████████▍ | 9207/10001 [4:42:32<26:09,  1.98s/batch]Batch 9200/10001 Done, mean position loss: 20.845791339874268\n",
      "Training NF2:  92%|█████████████████▍ | 9190/10001 [4:42:35<27:17,  2.02s/batch]Batch 9100/10001 Done, mean position loss: 20.636479253768922\n",
      "Training NF2:  91%|█████████████████▎ | 9110/10001 [4:42:47<24:57,  1.68s/batch]Batch 9100/10001 Done, mean position loss: 20.806188097000124\n",
      "Training NF2:  93%|█████████████████▌ | 9258/10001 [4:42:54<21:40,  1.75s/batch]Batch 9200/10001 Done, mean position loss: 20.520915317535398\n",
      "Training NF2:  92%|█████████████████▍ | 9184/10001 [4:42:57<23:06,  1.70s/batch]Batch 9100/10001 Done, mean position loss: 20.53507686853409\n",
      "Training NF2:  92%|█████████████████▌ | 9246/10001 [4:43:02<24:33,  1.95s/batch]Batch 9200/10001 Done, mean position loss: 20.594112195968627\n",
      "Training NF2:  92%|█████████████████▍ | 9189/10001 [4:43:04<27:58,  2.07s/batch]Batch 9300/10001 Done, mean position loss: 21.065177626609803\n",
      "Training NF2:  92%|█████████████████▌ | 9228/10001 [4:43:12<21:04,  1.64s/batch]Batch 9200/10001 Done, mean position loss: 20.37103021144867\n",
      "Training NF2:  93%|█████████████████▋ | 9296/10001 [4:43:19<21:49,  1.86s/batch]Batch 9200/10001 Done, mean position loss: 20.453629121780395\n",
      "Training NF2:  91%|█████████████████▎ | 9121/10001 [4:43:22<28:01,  1.91s/batch]Batch 9300/10001 Done, mean position loss: 20.484251704216003\n",
      "Training NF2:  92%|█████████████████▍ | 9188/10001 [4:43:26<28:33,  2.11s/batch]Batch 9200/10001 Done, mean position loss: 20.373833758831026\n",
      "Training NF2:  93%|█████████████████▋ | 9295/10001 [4:43:27<18:08,  1.54s/batch]Batch 9300/10001 Done, mean position loss: 20.616256411075593\n",
      "Training NF2:  93%|█████████████████▋ | 9315/10001 [4:43:27<16:38,  1.46s/batch]Batch 9200/10001 Done, mean position loss: 20.627961053848267\n",
      "Training NF2:  93%|█████████████████▌ | 9272/10001 [4:43:35<22:04,  1.82s/batch]Batch 9300/10001 Done, mean position loss: 20.34578671455383\n",
      "Training NF2:  93%|█████████████████▋ | 9282/10001 [4:43:37<19:29,  1.63s/batch]Batch 9200/10001 Done, mean position loss: 20.608872489929198\n",
      "Training NF2:  92%|█████████████████▍ | 9201/10001 [4:43:38<22:48,  1.71s/batch]Batch 9200/10001 Done, mean position loss: 20.31095324277878\n",
      "Training NF2:  92%|█████████████████▌ | 9235/10001 [4:43:38<24:04,  1.89s/batch]Batch 9300/10001 Done, mean position loss: 20.37742718935013\n",
      "Training NF2:  93%|█████████████████▋ | 9301/10001 [4:43:38<23:36,  2.02s/batch]Batch 9200/10001 Done, mean position loss: 20.430429577827454\n",
      "Training NF2:  92%|█████████████████▌ | 9249/10001 [4:43:39<22:00,  1.76s/batch]Batch 9300/10001 Done, mean position loss: 20.418958170413973\n",
      "Training NF2:  92%|█████████████████▌ | 9250/10001 [4:43:47<21:08,  1.69s/batch]Batch 9200/10001 Done, mean position loss: 20.638802301883697\n",
      "Training NF2:  92%|█████████████████▍ | 9208/10001 [4:43:51<25:22,  1.92s/batch]Batch 9200/10001 Done, mean position loss: 20.94076813697815\n",
      "Training NF2:  93%|█████████████████▋ | 9314/10001 [4:44:00<15:41,  1.37s/batch]Batch 9300/10001 Done, mean position loss: 20.706697800159453\n",
      "Training NF2:  92%|█████████████████▌ | 9214/10001 [4:44:01<23:23,  1.78s/batch]Batch 9100/10001 Done, mean position loss: 20.516366388797763\n",
      "Training NF2:  93%|█████████████████▋ | 9287/10001 [4:44:03<21:42,  1.82s/batch]Batch 9200/10001 Done, mean position loss: 20.684071378707884\n",
      "Training NF2:  93%|█████████████████▌ | 9262/10001 [4:44:03<23:36,  1.92s/batch]Batch 9200/10001 Done, mean position loss: 20.769833698272706\n",
      "Training NF2:  93%|█████████████████▌ | 9264/10001 [4:44:12<21:00,  1.71s/batch]Batch 9300/10001 Done, mean position loss: 20.292343208789823\n",
      "Training NF2:  92%|█████████████████▍ | 9158/10001 [4:44:12<20:50,  1.48s/batch]Batch 9300/10001 Done, mean position loss: 20.52378189086914\n",
      "Training NF2:  92%|█████████████████▍ | 9163/10001 [4:44:21<24:01,  1.72s/batch]Batch 9200/10001 Done, mean position loss: 20.7718702173233\n",
      "Training NF2:  93%|█████████████████▌ | 9263/10001 [4:44:29<20:04,  1.63s/batch]Batch 9300/10001 Done, mean position loss: 20.670518848896027\n",
      "Training NF2:  93%|█████████████████▋ | 9340/10001 [4:44:38<21:55,  1.99s/batch]Batch 9300/10001 Done, mean position loss: 21.45263907432556\n",
      "Training NF2:  92%|█████████████████▌ | 9241/10001 [4:44:45<26:22,  2.08s/batch]Batch 9300/10001 Done, mean position loss: 20.495306351184844\n",
      "Training NF2:  92%|█████████████████▌ | 9247/10001 [4:44:57<20:36,  1.64s/batch]Batch 9300/10001 Done, mean position loss: 20.51750734090805\n",
      "Training NF2:  92%|█████████████████▌ | 9249/10001 [4:45:01<23:13,  1.85s/batch]Batch 9200/10001 Done, mean position loss: 20.36036946773529\n",
      "Training NF2:  93%|█████████████████▌ | 9274/10001 [4:45:12<24:14,  2.00s/batch]Batch 9300/10001 Done, mean position loss: 20.42176817893982\n",
      "Training NF2:  94%|█████████████████▊ | 9354/10001 [4:45:14<21:49,  2.02s/batch]Batch 9300/10001 Done, mean position loss: 20.521886563301084\n",
      "Training NF2:  93%|█████████████████▌ | 9257/10001 [4:45:15<26:24,  2.13s/batch]Batch 9300/10001 Done, mean position loss: 21.01358477115631\n",
      "Training NF2:  92%|█████████████████▍ | 9184/10001 [4:45:18<27:01,  1.99s/batch]Batch 9300/10001 Done, mean position loss: 20.90912497520447\n",
      "Training NF2:  93%|█████████████████▋ | 9334/10001 [4:45:24<20:16,  1.82s/batch]Batch 9300/10001 Done, mean position loss: 20.444432847499847\n",
      "Training NF2:  93%|█████████████████▌ | 9262/10001 [4:45:24<24:33,  1.99s/batch]Batch 9300/10001 Done, mean position loss: 20.81204748630524\n",
      "Training NF2:  93%|█████████████████▌ | 9274/10001 [4:45:32<20:22,  1.68s/batch]Batch 9200/10001 Done, mean position loss: 20.6132500743866\n",
      "Training NF2:  93%|█████████████████▌ | 9277/10001 [4:45:40<26:21,  2.18s/batch]Batch 9300/10001 Done, mean position loss: 20.82332739591599\n",
      "Training NF2:  94%|█████████████████▊ | 9367/10001 [4:45:44<20:27,  1.94s/batch]Batch 9200/10001 Done, mean position loss: 20.641427204608917\n",
      "Training NF2:  93%|█████████████████▌ | 9272/10001 [4:45:49<28:14,  2.32s/batch]Batch 9200/10001 Done, mean position loss: 20.79323916196823\n",
      "Training NF2:  94%|█████████████████▊ | 9377/10001 [4:45:56<20:00,  1.92s/batch]Batch 9300/10001 Done, mean position loss: 20.532836911678316\n",
      "Training NF2:  94%|█████████████████▊ | 9366/10001 [4:46:03<19:50,  1.88s/batch]Batch 9300/10001 Done, mean position loss: 20.596440982818603\n",
      "Training NF2:  93%|█████████████████▋ | 9321/10001 [4:46:05<23:33,  2.08s/batch]Batch 9200/10001 Done, mean position loss: 20.55487886428833\n",
      "Training NF2:  93%|█████████████████▋ | 9292/10001 [4:46:11<27:17,  2.31s/batch]Batch 9400/10001 Done, mean position loss: 21.095890212059018\n",
      "Training NF2:  93%|█████████████████▋ | 9311/10001 [4:46:20<18:58,  1.65s/batch]Batch 9300/10001 Done, mean position loss: 20.36152018547058\n",
      "Training NF2:  94%|█████████████████▊ | 9378/10001 [4:46:27<18:06,  1.74s/batch]Batch 9300/10001 Done, mean position loss: 20.448495352268218\n",
      "Training NF2:  93%|█████████████████▌ | 9270/10001 [4:46:28<21:57,  1.80s/batch]Batch 9400/10001 Done, mean position loss: 20.588844327926637\n",
      "Training NF2:  94%|█████████████████▊ | 9374/10001 [4:46:28<20:10,  1.93s/batch]Batch 9400/10001 Done, mean position loss: 20.490908555984497\n",
      "Training NF2:  92%|█████████████████▌ | 9221/10001 [4:46:38<22:17,  1.72s/batch]Batch 9300/10001 Done, mean position loss: 20.618876049518583\n",
      "Training NF2:  93%|█████████████████▋ | 9332/10001 [4:46:39<20:44,  1.86s/batch]Batch 9300/10001 Done, mean position loss: 20.384583220481872\n",
      "Training NF2:  93%|█████████████████▋ | 9339/10001 [4:46:40<24:39,  2.24s/batch]Batch 9300/10001 Done, mean position loss: 20.609072349071504\n",
      "Training NF2:  93%|█████████████████▋ | 9301/10001 [4:46:40<25:33,  2.19s/batch]Batch 9400/10001 Done, mean position loss: 20.427298190593717\n",
      "Training NF2:  93%|█████████████████▋ | 9289/10001 [4:46:41<19:35,  1.65s/batch]Batch 9400/10001 Done, mean position loss: 20.370152988433837\n",
      "Training NF2:  94%|█████████████████▊ | 9368/10001 [4:46:43<15:02,  1.43s/batch]Batch 9300/10001 Done, mean position loss: 20.422402815818785\n",
      "Training NF2:  94%|█████████████████▉ | 9420/10001 [4:46:45<18:06,  1.87s/batch]Batch 9300/10001 Done, mean position loss: 20.30730756759644\n",
      "Training NF2:  93%|█████████████████▋ | 9312/10001 [4:46:49<24:08,  2.10s/batch]Batch 9400/10001 Done, mean position loss: 20.33403425693512\n",
      "Training NF2:  94%|█████████████████▊ | 9352/10001 [4:46:50<20:56,  1.94s/batch]Batch 9300/10001 Done, mean position loss: 20.643670108318332\n",
      "Training NF2:  93%|█████████████████▋ | 9298/10001 [4:46:58<21:09,  1.81s/batch]Batch 9300/10001 Done, mean position loss: 20.967153017520907\n",
      "Training NF2:  94%|█████████████████▊ | 9357/10001 [4:47:02<17:12,  1.60s/batch]Batch 9300/10001 Done, mean position loss: 20.697616896629334\n",
      "Training NF2:  94%|█████████████████▊ | 9361/10001 [4:47:07<20:20,  1.91s/batch]Batch 9200/10001 Done, mean position loss: 20.514733049869537\n",
      "Training NF2:  94%|█████████████████▊ | 9397/10001 [4:47:10<22:04,  2.19s/batch]Batch 9400/10001 Done, mean position loss: 20.70370355844498\n",
      "Training NF2:  94%|█████████████████▊ | 9398/10001 [4:47:11<18:47,  1.87s/batch]Batch 9300/10001 Done, mean position loss: 20.7863685131073\n",
      "Training NF2:  94%|█████████████████▊ | 9379/10001 [4:47:18<21:16,  2.05s/batch]Batch 9400/10001 Done, mean position loss: 20.544391117095948\n",
      "Training NF2:  93%|█████████████████▋ | 9320/10001 [4:47:21<20:01,  1.76s/batch]Batch 9400/10001 Done, mean position loss: 20.299402306079863\n",
      "Training NF2:  94%|█████████████████▉ | 9419/10001 [4:47:24<18:18,  1.89s/batch]Batch 9400/10001 Done, mean position loss: 20.669655900001523\n",
      "Training NF2:  94%|█████████████████▊ | 9359/10001 [4:47:30<17:48,  1.66s/batch]Batch 9300/10001 Done, mean position loss: 20.769242408275602\n",
      "Training NF2:  93%|█████████████████▌ | 9266/10001 [4:47:46<23:43,  1.94s/batch]Batch 9400/10001 Done, mean position loss: 21.44076800107956\n",
      "Training NF2:  93%|█████████████████▋ | 9325/10001 [4:47:47<23:20,  2.07s/batch]Batch 9400/10001 Done, mean position loss: 20.476227159500123\n",
      "Training NF2:  93%|█████████████████▋ | 9342/10001 [4:48:02<25:42,  2.34s/batch]Batch 9400/10001 Done, mean position loss: 20.512260744571684\n",
      "Training NF2:  94%|█████████████████▊ | 9405/10001 [4:48:10<21:13,  2.14s/batch]Batch 9300/10001 Done, mean position loss: 20.362733643054963\n",
      "Training NF2:  94%|█████████████████▉ | 9435/10001 [4:48:21<16:37,  1.76s/batch]Batch 9400/10001 Done, mean position loss: 20.423793632984165\n",
      "Training NF2:  93%|█████████████████▌ | 9277/10001 [4:48:25<24:51,  2.06s/batch]Batch 9400/10001 Done, mean position loss: 21.023418221473694\n",
      "Training NF2:  94%|█████████████████▊ | 9392/10001 [4:48:25<20:41,  2.04s/batch]Batch 9400/10001 Done, mean position loss: 20.51238504886627\n",
      "Training NF2:  94%|█████████████████▊ | 9398/10001 [4:48:30<20:17,  2.02s/batch]Batch 9400/10001 Done, mean position loss: 20.80715617656708\n",
      "Training NF2:  95%|█████████████████▉ | 9456/10001 [4:48:37<16:10,  1.78s/batch]Batch 9400/10001 Done, mean position loss: 20.964690930843354\n",
      "Training NF2:  94%|█████████████████▊ | 9361/10001 [4:48:41<21:40,  2.03s/batch]Batch 9300/10001 Done, mean position loss: 20.61279822587967\n",
      "Training NF2:  95%|█████████████████▉ | 9465/10001 [4:48:41<14:46,  1.65s/batch]Batch 9400/10001 Done, mean position loss: 20.434625911712647\n",
      "Training NF2:  94%|█████████████████▉ | 9417/10001 [4:48:51<18:20,  1.89s/batch]Batch 9400/10001 Done, mean position loss: 20.836433098316192\n",
      "Training NF2:  94%|█████████████████▊ | 9379/10001 [4:48:54<18:47,  1.81s/batch]Batch 9300/10001 Done, mean position loss: 20.6324569773674\n",
      "Training NF2:  93%|█████████████████▊ | 9348/10001 [4:49:02<21:44,  2.00s/batch]Batch 9300/10001 Done, mean position loss: 20.816069889068604\n",
      "Training NF2:  94%|█████████████████▊ | 9378/10001 [4:49:06<17:02,  1.64s/batch]Batch 9400/10001 Done, mean position loss: 20.537254643440246\n",
      "Training NF2:  94%|█████████████████▉ | 9439/10001 [4:49:13<17:16,  1.84s/batch]Batch 9500/10001 Done, mean position loss: 21.063100883960722\n",
      "Training NF2:  93%|█████████████████▋ | 9300/10001 [4:49:13<22:43,  1.95s/batch]Batch 9400/10001 Done, mean position loss: 20.5896981048584\n",
      "Training NF2:  93%|█████████████████▋ | 9312/10001 [4:49:15<23:22,  2.04s/batch]Batch 9300/10001 Done, mean position loss: 20.563559226989746\n",
      "Training NF2:  94%|█████████████████▉ | 9436/10001 [4:49:29<15:39,  1.66s/batch]Batch 9400/10001 Done, mean position loss: 20.359583163261416\n",
      "Training NF2:  95%|█████████████████▉ | 9471/10001 [4:49:38<19:07,  2.17s/batch]Batch 9400/10001 Done, mean position loss: 20.439998066425325\n",
      "Training NF2:  94%|█████████████████▉ | 9431/10001 [4:49:43<20:05,  2.12s/batch]Batch 9500/10001 Done, mean position loss: 20.607722024917603\n",
      "Training NF2:  94%|█████████████████▊ | 9395/10001 [4:49:44<17:51,  1.77s/batch]Batch 9500/10001 Done, mean position loss: 20.481015577316285\n",
      "Training NF2:  94%|█████████████████▉ | 9444/10001 [4:49:47<20:07,  2.17s/batch]Batch 9400/10001 Done, mean position loss: 20.614200818538663\n",
      "Training NF2:  93%|█████████████████▋ | 9329/10001 [4:49:46<18:35,  1.66s/batch]Batch 9400/10001 Done, mean position loss: 20.630193183422087\n",
      "Training NF2:  94%|█████████████████▉ | 9412/10001 [4:49:47<17:04,  1.74s/batch]Batch 9400/10001 Done, mean position loss: 20.381815972328184\n",
      "Training NF2:  94%|█████████████████▉ | 9440/10001 [4:49:48<16:14,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.4041099691391\n",
      "Training NF2:  95%|██████████████████ | 9508/10001 [4:49:55<14:17,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.364579999446867\n",
      "Training NF2:  93%|█████████████████▋ | 9323/10001 [4:49:57<25:55,  2.29s/batch]Batch 9400/10001 Done, mean position loss: 20.422976758480075\n",
      "Training NF2:  95%|██████████████████ | 9483/10001 [4:50:03<13:37,  1.58s/batch]Batch 9400/10001 Done, mean position loss: 20.296911859512328\n",
      "Training NF2:  94%|█████████████████▉ | 9411/10001 [4:50:03<15:23,  1.56s/batch]Batch 9400/10001 Done, mean position loss: 20.647772426605226\n",
      "Training NF2:  94%|█████████████████▉ | 9411/10001 [4:50:05<19:46,  2.01s/batch]Batch 9400/10001 Done, mean position loss: 20.951111874580384\n",
      "Training NF2:  94%|█████████████████▉ | 9413/10001 [4:50:07<17:17,  1.76s/batch]Batch 9500/10001 Done, mean position loss: 20.336591215133666\n",
      "Training NF2:  94%|█████████████████▉ | 9420/10001 [4:50:12<17:43,  1.83s/batch]Batch 9400/10001 Done, mean position loss: 20.706690649986264\n",
      "Training NF2:  95%|██████████████████ | 9505/10001 [4:50:15<15:58,  1.93s/batch]Batch 9500/10001 Done, mean position loss: 20.702585663795467\n",
      "Training NF2:  93%|█████████████████▋ | 9341/10001 [4:50:18<21:40,  1.97s/batch]Batch 9400/10001 Done, mean position loss: 20.777949204444887\n",
      "Training NF2:  95%|██████████████████ | 9479/10001 [4:50:19<17:22,  2.00s/batch]Batch 9300/10001 Done, mean position loss: 20.532301330566405\n",
      "Training NF2:  95%|█████████████████▉ | 9454/10001 [4:50:31<17:19,  1.90s/batch]Batch 9500/10001 Done, mean position loss: 20.533099179267886\n",
      "Training NF2:  95%|██████████████████ | 9528/10001 [4:50:34<14:43,  1.87s/batch]Batch 9500/10001 Done, mean position loss: 20.291861107349398\n",
      "Training NF2:  95%|██████████████████ | 9475/10001 [4:50:39<17:37,  2.01s/batch]Batch 9500/10001 Done, mean position loss: 20.666833462715147\n",
      "Training NF2:  95%|██████████████████ | 9522/10001 [4:50:44<15:06,  1.89s/batch]Batch 9400/10001 Done, mean position loss: 20.779693977832796\n",
      "Training NF2:  94%|█████████████████▊ | 9372/10001 [4:50:56<23:13,  2.21s/batch]Batch 9500/10001 Done, mean position loss: 21.422862997055056\n",
      "Training NF2:  95%|██████████████████ | 9486/10001 [4:51:00<15:21,  1.79s/batch]Batch 9500/10001 Done, mean position loss: 20.478568623065946\n",
      "Training NF2:  94%|█████████████████▉ | 9430/10001 [4:51:11<18:49,  1.98s/batch]Batch 9500/10001 Done, mean position loss: 20.522318930625914\n",
      "Training NF2:  94%|█████████████████▉ | 9437/10001 [4:51:24<17:01,  1.81s/batch]Batch 9400/10001 Done, mean position loss: 20.331483163833617\n",
      "Training NF2:  95%|██████████████████ | 9483/10001 [4:51:28<14:59,  1.74s/batch]Batch 9500/10001 Done, mean position loss: 20.410027821064\n",
      "Training NF2:  95%|██████████████████ | 9499/10001 [4:51:29<12:44,  1.52s/batch]Batch 9500/10001 Done, mean position loss: 20.503798842430115\n",
      "Training NF2:  95%|█████████████████▉ | 9473/10001 [4:51:33<16:08,  1.83s/batch]Batch 9500/10001 Done, mean position loss: 20.999137790203097\n",
      "Training NF2:  95%|█████████████████▉ | 9451/10001 [4:51:42<14:48,  1.62s/batch]Batch 9500/10001 Done, mean position loss: 20.804968621730804\n",
      "Training NF2:  94%|█████████████████▊ | 9387/10001 [4:51:49<19:47,  1.93s/batch]Batch 9500/10001 Done, mean position loss: 20.915675265789034\n",
      "Training NF2:  95%|██████████████████ | 9479/10001 [4:51:52<14:31,  1.67s/batch]Batch 9400/10001 Done, mean position loss: 20.633679935932157\n",
      "Training NF2:  95%|██████████████████ | 9519/10001 [4:52:02<14:22,  1.79s/batch]Batch 9500/10001 Done, mean position loss: 20.44717468500137\n",
      "Training NF2:  95%|█████████████████▉ | 9470/10001 [4:52:03<18:57,  2.14s/batch]Batch 9500/10001 Done, mean position loss: 20.86857901573181\n",
      "Training NF2:  95%|██████████████████ | 9499/10001 [4:52:14<14:19,  1.71s/batch]Batch 9400/10001 Done, mean position loss: 20.623587851524352\n",
      "Training NF2:  95%|██████████████████ | 9523/10001 [4:52:18<16:33,  2.08s/batch]Batch 9400/10001 Done, mean position loss: 20.853059711456297\n",
      "Training NF2:  95%|██████████████████ | 9519/10001 [4:52:18<15:52,  1.98s/batch]Batch 9500/10001 Done, mean position loss: 20.52290479183197\n",
      "Training NF2:  96%|██████████████████▏| 9559/10001 [4:52:24<15:43,  2.14s/batch]Batch 9400/10001 Done, mean position loss: 20.52861495733261\n",
      "Training NF2:  94%|█████████████████▊ | 9401/10001 [4:52:25<22:15,  2.23s/batch]Batch 9500/10001 Done, mean position loss: 20.605952904224395\n",
      "Batch 9600/10001 Done, mean position loss: 21.085831761360172\n",
      "Training NF2:  95%|██████████████████ | 9480/10001 [4:52:35<15:40,  1.81s/batch]Batch 9500/10001 Done, mean position loss: 20.346760985851287\n",
      "Training NF2:  94%|█████████████████▉ | 9418/10001 [4:52:47<17:20,  1.79s/batch]Batch 9600/10001 Done, mean position loss: 20.60065099954605\n",
      "Training NF2:  95%|█████████████████▉ | 9468/10001 [4:52:52<13:16,  1.49s/batch]Batch 9500/10001 Done, mean position loss: 20.449214038848876\n",
      "Training NF2:  95%|██████████████████▏| 9543/10001 [4:52:56<14:19,  1.88s/batch]Batch 9500/10001 Done, mean position loss: 20.41935246229172\n",
      "Training NF2:  94%|█████████████████▉ | 9417/10001 [4:52:56<20:11,  2.07s/batch]Batch 9600/10001 Done, mean position loss: 20.412339930534362\n",
      "Training NF2:  95%|██████████████████ | 9530/10001 [4:52:57<15:01,  1.91s/batch]Batch 9500/10001 Done, mean position loss: 20.375967404842378\n",
      "Training NF2:  95%|██████████████████▏| 9542/10001 [4:52:58<13:16,  1.73s/batch]Batch 9500/10001 Done, mean position loss: 20.624526488780976\n",
      "Training NF2:  96%|██████████████████▎| 9608/10001 [4:52:59<10:43,  1.64s/batch]Batch 9600/10001 Done, mean position loss: 20.49227205038071\n",
      "Training NF2:  95%|██████████████████ | 9518/10001 [4:53:07<15:12,  1.89s/batch]Batch 9500/10001 Done, mean position loss: 20.6197220826149\n",
      "Training NF2:  96%|██████████████████▎| 9607/10001 [4:53:08<12:59,  1.98s/batch]Batch 9600/10001 Done, mean position loss: 20.36269520044327\n",
      "Training NF2:  95%|██████████████████ | 9496/10001 [4:53:15<16:54,  2.01s/batch]Batch 9600/10001 Done, mean position loss: 20.331533865928648\n",
      "Training NF2:  95%|█████████████████▉ | 9462/10001 [4:53:16<17:10,  1.91s/batch]Batch 9500/10001 Done, mean position loss: 20.680542705059054\n",
      "Training NF2:  96%|██████████████████▏| 9568/10001 [4:53:19<12:03,  1.67s/batch]Batch 9500/10001 Done, mean position loss: 20.77582033634186\n",
      "Training NF2:  94%|█████████████████▉ | 9448/10001 [4:53:21<16:30,  1.79s/batch]Batch 9500/10001 Done, mean position loss: 20.9858279466629\n",
      "Training NF2:  96%|██████████████████▎| 9607/10001 [4:53:26<11:53,  1.81s/batch]Batch 9500/10001 Done, mean position loss: 20.310168607234957\n",
      "Training NF2:  96%|██████████████████▏| 9561/10001 [4:53:32<14:05,  1.92s/batch]Batch 9500/10001 Done, mean position loss: 20.70460744857788\n",
      "Training NF2:  95%|██████████████████ | 9509/10001 [4:53:35<15:37,  1.90s/batch]Batch 9600/10001 Done, mean position loss: 20.716749057769775\n",
      "Training NF2:  95%|██████████████████ | 9476/10001 [4:53:41<15:40,  1.79s/batch]Batch 9400/10001 Done, mean position loss: 20.528749651908875\n",
      "Training NF2:  96%|██████████████████▏| 9566/10001 [4:53:41<13:44,  1.89s/batch]Batch 9600/10001 Done, mean position loss: 20.53265238761902\n",
      "Training NF2:  96%|██████████████████▏| 9597/10001 [4:53:49<13:05,  1.94s/batch]Batch 9600/10001 Done, mean position loss: 20.29831001520157\n",
      "Training NF2:  96%|██████████████████▎| 9634/10001 [4:53:56<09:17,  1.52s/batch]Batch 9500/10001 Done, mean position loss: 20.763696773052217\n",
      "Training NF2:  96%|██████████████████▏| 9574/10001 [4:53:57<13:01,  1.83s/batch]Batch 9600/10001 Done, mean position loss: 20.67110471248627\n",
      "Training NF2:  94%|█████████████████▉ | 9412/10001 [4:54:01<18:01,  1.84s/batch]Batch 9600/10001 Done, mean position loss: 21.4489279294014\n",
      "Training NF2:  96%|██████████████████▏| 9571/10001 [4:54:18<12:50,  1.79s/batch]Batch 9600/10001 Done, mean position loss: 20.48815961122513\n",
      "Training NF2:  95%|█████████████████▉ | 9458/10001 [4:54:18<17:33,  1.94s/batch]Batch 9600/10001 Done, mean position loss: 20.53430640220642\n",
      "Training NF2:  95%|██████████████████ | 9536/10001 [4:54:30<15:47,  2.04s/batch]Batch 9500/10001 Done, mean position loss: 20.354755654335023\n",
      "Training NF2:  96%|██████████████████▏| 9553/10001 [4:54:38<11:27,  1.53s/batch]Batch 9600/10001 Done, mean position loss: 20.500296425819396\n",
      "Training NF2:  95%|██████████████████ | 9524/10001 [4:54:41<14:50,  1.87s/batch]Batch 9600/10001 Done, mean position loss: 20.41073976993561\n",
      "Training NF2:  96%|██████████████████▎| 9633/10001 [4:54:50<12:14,  2.00s/batch]Batch 9600/10001 Done, mean position loss: 20.981528608798982\n",
      "Training NF2:  96%|██████████████████▏| 9580/10001 [4:54:53<13:48,  1.97s/batch]Batch 9600/10001 Done, mean position loss: 20.9185448718071\n",
      "Training NF2:  95%|██████████████████ | 9485/10001 [4:54:55<15:30,  1.80s/batch]Batch 9600/10001 Done, mean position loss: 20.81084845304489\n",
      "Training NF2:  95%|██████████████████▏| 9547/10001 [4:55:00<12:44,  1.68s/batch]Batch 9500/10001 Done, mean position loss: 20.646089046001435\n",
      "Training NF2:  96%|██████████████████▎| 9613/10001 [4:55:16<10:52,  1.68s/batch]Batch 9600/10001 Done, mean position loss: 20.881627860069276\n",
      "Training NF2:  96%|██████████████████▏| 9595/10001 [4:55:21<11:29,  1.70s/batch]Batch 9600/10001 Done, mean position loss: 20.425688042640687\n",
      "Training NF2:  96%|██████████████████▏| 9564/10001 [4:55:26<14:10,  1.95s/batch]Batch 9500/10001 Done, mean position loss: 20.82123351573944\n",
      "Training NF2:  96%|██████████████████▎| 9607/10001 [4:55:28<13:13,  2.01s/batch]Batch 9500/10001 Done, mean position loss: 20.62952701330185\n",
      "Training NF2:  96%|██████████████████▎| 9648/10001 [4:55:33<10:41,  1.82s/batch]Batch 9600/10001 Done, mean position loss: 20.510047211647034\n",
      "Training NF2:  95%|██████████████████ | 9505/10001 [4:55:35<16:31,  2.00s/batch]Batch 9700/10001 Done, mean position loss: 21.092923781871797\n",
      "Training NF2:  96%|██████████████████▎| 9630/10001 [4:55:37<13:01,  2.11s/batch]Batch 9600/10001 Done, mean position loss: 20.59566175699234\n",
      "Training NF2:  95%|██████████████████ | 9523/10001 [4:55:44<17:04,  2.14s/batch]Batch 9500/10001 Done, mean position loss: 20.524112384319302\n",
      "Training NF2:  97%|██████████████████▍| 9680/10001 [4:55:48<09:32,  1.78s/batch]Batch 9700/10001 Done, mean position loss: 20.609838132858275\n",
      "Training NF2:  96%|██████████████████▏| 9586/10001 [4:55:52<11:17,  1.63s/batch]Batch 9600/10001 Done, mean position loss: 20.36805181026459\n",
      "Training NF2:  95%|██████████████████ | 9533/10001 [4:56:06<18:00,  2.31s/batch]Batch 9600/10001 Done, mean position loss: 20.442451872825625\n",
      "Training NF2:  97%|██████████████████▎| 9672/10001 [4:56:06<11:32,  2.10s/batch]Batch 9700/10001 Done, mean position loss: 20.49636399269104\n",
      "Training NF2:  97%|██████████████████▍| 9704/10001 [4:56:11<08:44,  1.76s/batch]Batch 9700/10001 Done, mean position loss: 20.417628521919248\n",
      "Training NF2:  96%|██████████████████▎| 9645/10001 [4:56:12<09:36,  1.62s/batch]Batch 9600/10001 Done, mean position loss: 20.392759540081023\n",
      "Training NF2:  97%|██████████████████▎| 9660/10001 [4:56:13<11:10,  1.97s/batch]Batch 9700/10001 Done, mean position loss: 20.353012611865996\n",
      "Training NF2:  97%|██████████████████▍| 9722/10001 [4:56:14<07:31,  1.62s/batch]Batch 9600/10001 Done, mean position loss: 20.411654832363126\n",
      "Training NF2:  97%|██████████████████▎| 9670/10001 [4:56:17<12:10,  2.21s/batch]Batch 9600/10001 Done, mean position loss: 20.612586455345152\n",
      "Training NF2:  97%|██████████████████▍| 9706/10001 [4:56:21<08:41,  1.77s/batch]Batch 9600/10001 Done, mean position loss: 20.624212889671323\n",
      "Training NF2:  97%|██████████████████▍| 9677/10001 [4:56:30<10:09,  1.88s/batch]Batch 9700/10001 Done, mean position loss: 20.335831685066225\n",
      "Training NF2:  96%|██████████████████▎| 9638/10001 [4:56:32<11:10,  1.85s/batch]Batch 9600/10001 Done, mean position loss: 20.78966588973999\n",
      "Training NF2:  96%|██████████████████▏| 9598/10001 [4:56:34<13:28,  2.01s/batch]Batch 9600/10001 Done, mean position loss: 21.006217532157898\n",
      "Training NF2:  97%|██████████████████▍| 9690/10001 [4:56:41<10:52,  2.10s/batch]Batch 9600/10001 Done, mean position loss: 20.64448648452759\n",
      "Training NF2:  97%|██████████████████▍| 9687/10001 [4:56:41<09:30,  1.82s/batch]Batch 9600/10001 Done, mean position loss: 20.30794786930084\n",
      "Training NF2:  97%|██████████████████▌| 9738/10001 [4:56:44<08:20,  1.90s/batch]Batch 9700/10001 Done, mean position loss: 20.703424551486968\n",
      "Training NF2:  96%|██████████████████▏| 9605/10001 [4:56:49<13:44,  2.08s/batch]Batch 9700/10001 Done, mean position loss: 20.549575242996216\n",
      "Training NF2:  96%|██████████████████▎| 9610/10001 [4:56:50<10:58,  1.69s/batch]Batch 9600/10001 Done, mean position loss: 20.678955895900724\n",
      "Training NF2:  97%|██████████████████▎| 9655/10001 [4:57:02<11:15,  1.95s/batch]Batch 9500/10001 Done, mean position loss: 20.541353290081027\n",
      "Training NF2:  97%|██████████████████▍| 9728/10001 [4:57:04<08:22,  1.84s/batch]Batch 9700/10001 Done, mean position loss: 20.667336206436158\n",
      "Training NF2:  96%|██████████████████▎| 9626/10001 [4:57:08<13:50,  2.22s/batch]Batch 9600/10001 Done, mean position loss: 20.768225324153903\n",
      "Training NF2:  97%|██████████████████▍| 9730/10001 [4:57:08<09:20,  2.07s/batch]Batch 9700/10001 Done, mean position loss: 20.291944477558136\n",
      "Training NF2:  97%|██████████████████▍| 9715/10001 [4:57:14<09:23,  1.97s/batch]Batch 9700/10001 Done, mean position loss: 21.420594444274904\n",
      "Training NF2:  96%|██████████████████▎| 9642/10001 [4:57:35<09:58,  1.67s/batch]Batch 9700/10001 Done, mean position loss: 20.532179286479952\n",
      "Training NF2:  98%|██████████████████▌| 9765/10001 [4:57:37<07:17,  1.85s/batch]Batch 9700/10001 Done, mean position loss: 20.474993600845337\n",
      "Training NF2:  98%|██████████████████▌| 9753/10001 [4:57:46<06:50,  1.65s/batch]Batch 9600/10001 Done, mean position loss: 20.348889234066007\n",
      "Training NF2:  97%|██████████████████▍| 9697/10001 [4:57:51<08:50,  1.75s/batch]Batch 9700/10001 Done, mean position loss: 20.51187937259674\n",
      "Training NF2:  97%|██████████████████▎| 9656/10001 [4:57:58<10:57,  1.91s/batch]Batch 9700/10001 Done, mean position loss: 20.41497222185135\n",
      "Training NF2:  96%|██████████████████▏| 9570/10001 [4:57:58<16:00,  2.23s/batch]Batch 9700/10001 Done, mean position loss: 20.913815667629244\n",
      "Training NF2:  97%|██████████████████▍| 9702/10001 [4:57:59<08:28,  1.70s/batch]Batch 9700/10001 Done, mean position loss: 21.027138323783873\n",
      "Batch 9700/10001 Done, mean position loss: 20.808282451629637\n",
      "Training NF2:  97%|██████████████████▍| 9735/10001 [4:58:20<09:25,  2.13s/batch]Batch 9600/10001 Done, mean position loss: 20.65533432006836\n",
      "Training NF2:  98%|██████████████████▌| 9790/10001 [4:58:23<06:29,  1.84s/batch]Batch 9700/10001 Done, mean position loss: 20.85395431280136\n",
      "Training NF2:  97%|██████████████████▎| 9671/10001 [4:58:30<09:43,  1.77s/batch]Batch 9700/10001 Done, mean position loss: 20.424024727344513\n",
      "Training NF2:  96%|██████████████████▏| 9589/10001 [4:58:35<13:17,  1.94s/batch]Batch 9600/10001 Done, mean position loss: 20.815485231876373\n",
      "Training NF2:  98%|██████████████████▌| 9776/10001 [4:58:38<08:15,  2.20s/batch]Batch 9600/10001 Done, mean position loss: 20.616151218414306\n",
      "Training NF2:  97%|██████████████████▍| 9684/10001 [4:58:46<07:11,  1.36s/batch]Batch 9800/10001 Done, mean position loss: 21.0794402885437\n",
      "Training NF2:  98%|██████████████████▌| 9801/10001 [4:58:45<06:41,  2.01s/batch]Batch 9700/10001 Done, mean position loss: 20.513515009880066\n",
      "Training NF2:  98%|██████████████████▌| 9771/10001 [4:58:55<06:41,  1.74s/batch]Batch 9700/10001 Done, mean position loss: 20.5961910200119\n",
      "Training NF2:  97%|██████████████████▎| 9660/10001 [4:58:58<12:27,  2.19s/batch]Batch 9600/10001 Done, mean position loss: 20.522289636135103\n",
      "Training NF2:  98%|██████████████████▌| 9773/10001 [4:58:59<06:29,  1.71s/batch]Batch 9800/10001 Done, mean position loss: 20.612674632072448\n",
      "Batch 9700/10001 Done, mean position loss: 20.346879467964172\n",
      "Training NF2:  97%|██████████████████▍| 9724/10001 [4:59:12<07:29,  1.62s/batch]Batch 9800/10001 Done, mean position loss: 20.414993720054625\n",
      "Training NF2:  97%|██████████████████▎| 9668/10001 [4:59:13<11:05,  2.00s/batch]Batch 9700/10001 Done, mean position loss: 20.43285155534744\n",
      "Training NF2:  98%|██████████████████▌| 9755/10001 [4:59:21<07:21,  1.80s/batch]Batch 9700/10001 Done, mean position loss: 20.37788588762283\n",
      "Training NF2:  97%|██████████████████▍| 9690/10001 [4:59:23<09:40,  1.87s/batch]Batch 9700/10001 Done, mean position loss: 20.59708936214447\n",
      "Training NF2:  97%|██████████████████▍| 9702/10001 [4:59:23<08:00,  1.61s/batch]Batch 9800/10001 Done, mean position loss: 20.48779270172119\n",
      "Training NF2:  97%|██████████████████▌| 9747/10001 [4:59:26<07:27,  1.76s/batch]Batch 9700/10001 Done, mean position loss: 20.61325002670288\n",
      "Training NF2:  97%|██████████████████▍| 9721/10001 [4:59:28<09:48,  2.10s/batch]Batch 9800/10001 Done, mean position loss: 20.36746877908707\n",
      "Training NF2:  96%|██████████████████▎| 9637/10001 [4:59:28<08:57,  1.48s/batch]Batch 9700/10001 Done, mean position loss: 20.41821180820465\n",
      "Training NF2:  97%|██████████████████▍| 9706/10001 [4:59:39<09:58,  2.03s/batch]Batch 9800/10001 Done, mean position loss: 20.339475927352904\n",
      "Training NF2:  98%|██████████████████▌| 9784/10001 [4:59:42<06:18,  1.74s/batch]Batch 9700/10001 Done, mean position loss: 20.975393137931825\n",
      "Training NF2:  98%|██████████████████▌| 9798/10001 [4:59:44<06:34,  1.94s/batch]Batch 9700/10001 Done, mean position loss: 20.798941802978515\n",
      "Training NF2:  98%|██████████████████▋| 9834/10001 [4:59:48<04:51,  1.75s/batch]Batch 9800/10001 Done, mean position loss: 20.542216262817384\n",
      "Training NF2:  97%|██████████████████▍| 9717/10001 [4:59:52<08:49,  1.86s/batch]Batch 9700/10001 Done, mean position loss: 20.689018898010254\n",
      "Training NF2:  97%|██████████████████▍| 9730/10001 [4:59:53<08:27,  1.87s/batch]Batch 9700/10001 Done, mean position loss: 20.303848707675932\n",
      "Training NF2:  98%|██████████████████▋| 9840/10001 [4:59:58<04:39,  1.74s/batch]Batch 9700/10001 Done, mean position loss: 20.638471374511717\n",
      "Training NF2:  97%|██████████████████▍| 9674/10001 [5:00:02<10:06,  1.85s/batch]Batch 9800/10001 Done, mean position loss: 20.699684720039368\n",
      "Training NF2:  98%|██████████████████▋| 9834/10001 [5:00:11<04:48,  1.73s/batch]Batch 9600/10001 Done, mean position loss: 20.547649908065797\n",
      "Training NF2:  98%|██████████████████▌| 9782/10001 [5:00:13<06:28,  1.77s/batch]Batch 9800/10001 Done, mean position loss: 20.66925144672394\n",
      "Training NF2:  98%|██████████████████▌| 9799/10001 [5:00:17<06:13,  1.85s/batch]Batch 9700/10001 Done, mean position loss: 20.764799427986144\n",
      "Training NF2:  98%|██████████████████▋| 9840/10001 [5:00:22<04:55,  1.84s/batch]Batch 9800/10001 Done, mean position loss: 20.284934785366058\n",
      "Training NF2:  98%|██████████████████▋| 9810/10001 [5:00:28<05:32,  1.74s/batch]Batch 9800/10001 Done, mean position loss: 21.42275561571121\n",
      "Training NF2:  97%|██████████████████▍| 9732/10001 [5:00:49<08:39,  1.93s/batch]Batch 9800/10001 Done, mean position loss: 20.523950655460357\n",
      "Training NF2:  97%|██████████████████▍| 9729/10001 [5:00:50<08:32,  1.88s/batch]Batch 9800/10001 Done, mean position loss: 20.47825530052185\n",
      "Training NF2:  98%|██████████████████▌| 9802/10001 [5:00:51<05:59,  1.81s/batch]Batch 9700/10001 Done, mean position loss: 20.3345463013649\n",
      "Training NF2:  98%|██████████████████▌| 9796/10001 [5:00:58<06:18,  1.85s/batch]Batch 9800/10001 Done, mean position loss: 20.407757284641267\n",
      "Training NF2:  98%|██████████████████▌| 9798/10001 [5:01:02<06:15,  1.85s/batch]Batch 9800/10001 Done, mean position loss: 20.504037764072415\n",
      "Training NF2:  97%|██████████████████▌| 9742/10001 [5:01:02<07:45,  1.80s/batch]Batch 9800/10001 Done, mean position loss: 21.025565712451936\n",
      "Training NF2:  98%|██████████████████▋| 9826/10001 [5:01:08<05:02,  1.73s/batch]Batch 9800/10001 Done, mean position loss: 20.807983529567718\n",
      "Training NF2:  99%|██████████████████▋| 9851/10001 [5:01:10<04:07,  1.65s/batch]Batch 9800/10001 Done, mean position loss: 20.93857714176178\n",
      "Training NF2:  98%|██████████████████▋| 9819/10001 [5:01:28<04:46,  1.58s/batch]Batch 9700/10001 Done, mean position loss: 20.637784349918363\n",
      "Training NF2:  98%|██████████████████▌| 9752/10001 [5:01:31<07:57,  1.92s/batch]Batch 9800/10001 Done, mean position loss: 20.871549313068392\n",
      "Training NF2:  98%|██████████████████▌| 9766/10001 [5:01:39<05:59,  1.53s/batch]Batch 9800/10001 Done, mean position loss: 20.43529449701309\n",
      "Training NF2:  98%|██████████████████▋| 9829/10001 [5:01:43<04:56,  1.72s/batch]Batch 9700/10001 Done, mean position loss: 20.828220465183257\n",
      "Training NF2:  98%|██████████████████▌| 9760/10001 [5:01:45<06:26,  1.60s/batch]Batch 9700/10001 Done, mean position loss: 20.621179699897766\n",
      "Training NF2:  98%|██████████████████▌| 9795/10001 [5:01:48<05:40,  1.65s/batch]Batch 9800/10001 Done, mean position loss: 20.53181577205658\n",
      "Training NF2:  98%|██████████████████▌| 9785/10001 [5:01:51<06:33,  1.82s/batch]Batch 9900/10001 Done, mean position loss: 21.08985418558121\n",
      "Training NF2:  98%|██████████████████▋| 9826/10001 [5:01:53<04:50,  1.66s/batch]Batch 9800/10001 Done, mean position loss: 20.59097613573074\n",
      "Training NF2:  98%|██████████████████▋| 9848/10001 [5:01:59<04:23,  1.72s/batch]Batch 9800/10001 Done, mean position loss: 20.32851227283478\n",
      "Training NF2:  98%|██████████████████▌| 9769/10001 [5:02:00<06:08,  1.59s/batch]Batch 9700/10001 Done, mean position loss: 20.521904597282408\n",
      "Training NF2:  98%|██████████████████▋| 9828/10001 [5:02:02<04:30,  1.56s/batch]Batch 9900/10001 Done, mean position loss: 20.606068189144136\n",
      "Training NF2:  99%|██████████████████▊| 9873/10001 [5:02:13<03:23,  1.59s/batch]Batch 9900/10001 Done, mean position loss: 20.42282739877701\n",
      "Training NF2:  98%|██████████████████▌| 9769/10001 [5:02:19<06:14,  1.62s/batch]Batch 9800/10001 Done, mean position loss: 20.437751767635348\n",
      "Training NF2:  99%|██████████████████▊| 9879/10001 [5:02:23<03:29,  1.72s/batch]Batch 9800/10001 Done, mean position loss: 20.607488887310026\n",
      "Training NF2:  98%|██████████████████▌| 9787/10001 [5:02:25<05:49,  1.63s/batch]Batch 9900/10001 Done, mean position loss: 20.35547689437866\n",
      "Training NF2:  99%|██████████████████▊| 9870/10001 [5:02:26<03:57,  1.81s/batch]Batch 9800/10001 Done, mean position loss: 20.384309272766114\n",
      "Training NF2:  99%|██████████████████▊| 9909/10001 [5:02:27<02:59,  1.95s/batch]Batch 9900/10001 Done, mean position loss: 20.48979506492615\n",
      "Training NF2:  98%|██████████████████▋| 9847/10001 [5:02:29<04:17,  1.68s/batch]Batch 9800/10001 Done, mean position loss: 20.613471970558166\n",
      "Training NF2:  99%|██████████████████▊| 9890/10001 [5:02:30<03:47,  2.05s/batch]Batch 9800/10001 Done, mean position loss: 20.41475019216537\n",
      "Training NF2:  98%|██████████████████▌| 9796/10001 [5:02:43<05:46,  1.69s/batch]Batch 9800/10001 Done, mean position loss: 21.016526095867157\n",
      "Training NF2:  99%|██████████████████▊| 9931/10001 [5:02:44<01:57,  1.68s/batch]Batch 9900/10001 Done, mean position loss: 20.331579966545107\n",
      "Training NF2:  98%|██████████████████▋| 9835/10001 [5:02:49<05:34,  2.01s/batch]Batch 9800/10001 Done, mean position loss: 20.79833471059799\n",
      "Training NF2:  97%|██████████████████▌| 9738/10001 [5:02:51<06:48,  1.55s/batch]Batch 9900/10001 Done, mean position loss: 20.539892122745513\n",
      "Training NF2:  98%|██████████████████▋| 9831/10001 [5:02:52<04:05,  1.44s/batch]Batch 9800/10001 Done, mean position loss: 20.303057293891904\n",
      "Training NF2:  97%|██████████████████▌| 9740/10001 [5:02:54<07:08,  1.64s/batch]Batch 9800/10001 Done, mean position loss: 20.694382886886597\n",
      "Training NF2:  98%|██████████████████▌| 9756/10001 [5:03:04<07:21,  1.80s/batch]Batch 9800/10001 Done, mean position loss: 20.642449100017547\n",
      "Training NF2:  98%|██████████████████▌| 9803/10001 [5:03:06<04:58,  1.51s/batch]Batch 9900/10001 Done, mean position loss: 20.691077749729157\n",
      "Training NF2:  99%|██████████████████▊| 9915/10001 [5:03:17<02:35,  1.80s/batch]Batch 9900/10001 Done, mean position loss: 20.660540025234223\n",
      "Training NF2:  98%|██████████████████▋| 9820/10001 [5:03:19<04:56,  1.64s/batch]Batch 9900/10001 Done, mean position loss: 20.28630380153656\n",
      "Training NF2:  99%|██████████████████▉| 9944/10001 [5:03:22<01:49,  1.92s/batch]Batch 9800/10001 Done, mean position loss: 20.757325263023375\n",
      "Training NF2:  99%|██████████████████▋| 9861/10001 [5:03:22<03:53,  1.67s/batch]Batch 9700/10001 Done, mean position loss: 20.526640937328338\n",
      "Training NF2:  99%|██████████████████▊| 9892/10001 [5:03:41<02:50,  1.56s/batch]Batch 9900/10001 Done, mean position loss: 21.412920293807986\n",
      "Training NF2:  98%|██████████████████▌| 9761/10001 [5:03:55<07:09,  1.79s/batch]Batch 9900/10001 Done, mean position loss: 20.526086523532868\n",
      "Training NF2:  98%|██████████████████▌| 9784/10001 [5:03:56<06:36,  1.83s/batch]Batch 9900/10001 Done, mean position loss: 20.46930976867676\n",
      "Training NF2:  99%|██████████████████▊| 9902/10001 [5:03:56<02:43,  1.66s/batch]Batch 9900/10001 Done, mean position loss: 20.419369032382967\n",
      "Training NF2:  99%|██████████████████▋| 9852/10001 [5:03:57<04:30,  1.82s/batch]Batch 9900/10001 Done, mean position loss: 21.023891179561616\n",
      "Training NF2:  99%|██████████████████▉| 9941/10001 [5:04:01<02:11,  2.19s/batch]Batch 9800/10001 Done, mean position loss: 20.33459411859512\n",
      "Training NF2:  98%|██████████████████▋| 9834/10001 [5:04:06<05:32,  1.99s/batch]Batch 9900/10001 Done, mean position loss: 20.81529442548752\n",
      "Training NF2: 100%|██████████████████▉| 9970/10001 [5:04:14<00:52,  1.70s/batch]Batch 9900/10001 Done, mean position loss: 20.532758467197418\n",
      "Training NF2:  99%|██████████████████▊| 9893/10001 [5:04:19<03:06,  1.73s/batch]Batch 9900/10001 Done, mean position loss: 20.92768287181854\n",
      "Training NF2:  99%|██████████████████▊| 9919/10001 [5:04:28<02:36,  1.91s/batch]Batch 9800/10001 Done, mean position loss: 20.62457684993744\n",
      "Training NF2:  99%|██████████████████▊| 9890/10001 [5:04:34<03:35,  1.94s/batch]Batch 9900/10001 Done, mean position loss: 20.83979260444641\n",
      "Training NF2:  98%|██████████████████▋| 9848/10001 [5:04:51<04:35,  1.80s/batch]Batch 9800/10001 Done, mean position loss: 20.838984711170195\n",
      "Training NF2:  98%|██████████████████▋| 9849/10001 [5:04:53<04:28,  1.77s/batch]Batch 9900/10001 Done, mean position loss: 20.4296993803978\n",
      "Training NF2: 100%|██████████████████▉| 9983/10001 [5:04:53<00:28,  1.60s/batch]Batch 9900/10001 Done, mean position loss: 20.592650434970857\n",
      "Training NF2:  99%|██████████████████▊| 9878/10001 [5:04:55<03:21,  1.64s/batch]Batch 9800/10001 Done, mean position loss: 20.627995486259458\n",
      "Training NF2:  98%|██████████████████▋| 9829/10001 [5:04:55<05:19,  1.86s/batch]Batch 9900/10001 Done, mean position loss: 20.52161630153656\n",
      "Training NF2:  99%|██████████████████▊| 9903/10001 [5:04:57<02:58,  1.82s/batch]Batch 10000/10001 Done, mean position loss: 21.084946732521058\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:04:57<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9957/10001 [5:05:03<01:24,  1.93s/batch]Batch 9900/10001 Done, mean position loss: 20.334297974109653\n",
      "Training NF2:  98%|██████████████████▋| 9812/10001 [5:05:12<05:40,  1.80s/batch]Batch 10000/10001 Done, mean position loss: 20.618923959732054\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:12<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9888/10001 [5:05:13<03:26,  1.83s/batch]Batch 10000/10001 Done, mean position loss: 20.416870682239534\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:14<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9890/10001 [5:05:17<03:14,  1.75s/batch]Batch 9800/10001 Done, mean position loss: 20.532860715389255\n",
      "Training NF2: 100%|██████████████████▉| 9985/10001 [5:05:26<00:30,  1.93s/batch]Batch 9900/10001 Done, mean position loss: 20.42964232444763\n",
      "Training NF2:  99%|██████████████████▉| 9949/10001 [5:05:28<01:39,  1.91s/batch]Batch 10000/10001 Done, mean position loss: 20.367050650119783\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:28<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9886/10001 [5:05:31<02:51,  1.49s/batch]Batch 10000/10001 Done, mean position loss: 20.49578119516373\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:31<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2:  98%|██████████████████▋| 9811/10001 [5:05:33<05:09,  1.63s/batch]Batch 9900/10001 Done, mean position loss: 20.602853333950044\n",
      "Training NF2: 100%|██████████████████▉| 9991/10001 [5:05:35<00:14,  1.50s/batch]Batch 9900/10001 Done, mean position loss: 20.42895128726959\n",
      "Training NF2:  98%|██████████████████▌| 9773/10001 [5:05:35<05:35,  1.47s/batch]Batch 9900/10001 Done, mean position loss: 20.39167730331421\n",
      "Training NF2:  98%|██████████████████▌| 9774/10001 [5:05:37<06:33,  1.73s/batch]Batch 9900/10001 Done, mean position loss: 20.6237615275383\n",
      "Training NF2:  99%|██████████████████▊| 9895/10001 [5:05:50<03:40,  2.08s/batch]Batch 9900/10001 Done, mean position loss: 21.02735935688019\n",
      "Training NF2: 100%|██████████████████▉| 9982/10001 [5:05:50<00:34,  1.82s/batch]Batch 10000/10001 Done, mean position loss: 20.333040926456455\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:51<00:00,  1.83s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9963/10001 [5:05:52<01:03,  1.67s/batch]Batch 10000/10001 Done, mean position loss: 20.534925832748414\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:05:51<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▋| 9863/10001 [5:05:53<03:39,  1.59s/batch]Batch 9900/10001 Done, mean position loss: 20.699757595062252\n",
      "Training NF2: 100%|██████████████████▉| 9964/10001 [5:05:53<01:06,  1.79s/batch]Batch 9900/10001 Done, mean position loss: 20.810828132629396\n",
      "Training NF2:  99%|██████████████████▊| 9935/10001 [5:06:00<01:46,  1.61s/batch]Batch 9900/10001 Done, mean position loss: 20.29652607679367\n",
      "Training NF2:  99%|██████████████████▊| 9911/10001 [5:06:08<02:03,  1.38s/batch]Batch 10000/10001 Done, mean position loss: 20.710234344005585\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:09<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9976/10001 [5:06:09<00:32,  1.31s/batch]Batch 9900/10001 Done, mean position loss: 20.663560948371888\n",
      "Training NF2:  99%|██████████████████▊| 9916/10001 [5:06:17<02:16,  1.60s/batch]Batch 10000/10001 Done, mean position loss: 20.658401241302492\n",
      "Training NF2:  99%|██████████████████▊| 9926/10001 [5:06:17<02:05,  1.67s/batch]Batch 10000/10001 Done, mean position loss: 20.28185342788696\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:17<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:17<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9980/10001 [5:06:19<00:31,  1.51s/batch]Batch 9800/10001 Done, mean position loss: 20.52409874200821\n",
      "Training NF2:  99%|██████████████████▊| 9914/10001 [5:06:20<02:33,  1.77s/batch]Batch 9900/10001 Done, mean position loss: 20.754433035850525\n",
      "Training NF2:  99%|██████████████████▉| 9944/10001 [5:06:39<01:16,  1.35s/batch]Batch 10000/10001 Done, mean position loss: 21.418477516174317\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:40<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  98%|██████████████████▋| 9821/10001 [5:06:47<04:06,  1.37s/batch]Batch 10000/10001 Done, mean position loss: 20.984284045696256\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:48<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▉| 9936/10001 [5:06:47<02:11,  2.02s/batch]Batch 10000/10001 Done, mean position loss: 20.522262847423555\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:48<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Batch 10000/10001 Done, mean position loss: 20.425154254436492\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:48<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9923/10001 [5:06:50<01:39,  1.27s/batch]Batch 10000/10001 Done, mean position loss: 20.448991186618805\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:50<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9926/10001 [5:06:54<01:32,  1.24s/batch]Batch 9900/10001 Done, mean position loss: 20.3358696269989\n",
      "Training NF2: 100%|██████████████████▉| 9951/10001 [5:06:55<01:15,  1.50s/batch]Batch 10000/10001 Done, mean position loss: 20.809345667362212\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:55<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9878/10001 [5:06:56<02:37,  1.28s/batch]Batch 10000/10001 Done, mean position loss: 20.505869793891907\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:06:56<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9910/10001 [5:07:03<01:41,  1.11s/batch]Batch 10000/10001 Done, mean position loss: 20.935703141689302\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:03<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9918/10001 [5:07:11<01:21,  1.01batch/s]Batch 10000/10001 Done, mean position loss: 20.86672386884689\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:11<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9962/10001 [5:07:14<00:40,  1.04s/batch]Batch 9900/10001 Done, mean position loss: 20.64138196706772\n",
      "Training NF2:  99%|██████████████████▊| 9904/10001 [5:07:18<01:37,  1.01s/batch]Batch 10000/10001 Done, mean position loss: 20.434632344245912\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:18<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9978/10001 [5:07:21<00:21,  1.06batch/s]Batch 10000/10001 Done, mean position loss: 20.527943415641783\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:21<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9976/10001 [5:07:22<00:23,  1.06batch/s]Batch 9900/10001 Done, mean position loss: 20.630483481884006\n",
      "Training NF2: 100%|██████████████████▉| 9968/10001 [5:07:23<00:34,  1.05s/batch]Batch 9900/10001 Done, mean position loss: 20.817547464370726\n",
      "Training NF2:  99%|██████████████████▊| 9902/10001 [5:07:22<01:41,  1.03s/batch]Batch 10000/10001 Done, mean position loss: 20.612834887504576\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:22<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9971/10001 [5:07:23<00:25,  1.16batch/s]Batch 10000/10001 Done, mean position loss: 20.355699746608735\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:23<00:00,  1.84s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9980/10001 [5:07:32<00:25,  1.20s/batch]Batch 9900/10001 Done, mean position loss: 20.498017897605898\n",
      "Training NF2: 100%|██████████████████▉| 9975/10001 [5:07:35<00:22,  1.16batch/s]Batch 10000/10001 Done, mean position loss: 20.43755083084106\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:35<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9984/10001 [5:07:37<00:16,  1.00batch/s]Batch 10000/10001 Done, mean position loss: 20.389865822792054\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:37<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9978/10001 [5:07:38<00:19,  1.20batch/s]Batch 10000/10001 Done, mean position loss: 20.414830737113952\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:38<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9926/10001 [5:07:39<00:42,  1.74batch/s]Batch 10000/10001 Done, mean position loss: 20.61142824888229\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:40<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9995/10001 [5:07:44<00:04,  1.29batch/s]Batch 10000/10001 Done, mean position loss: 20.59572097301483\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:44<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9997/10001 [5:07:45<00:02,  1.54batch/s]Batch 10000/10001 Done, mean position loss: 20.684497313499453\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:46<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9891/10001 [5:07:48<01:19,  1.39batch/s]Batch 10000/10001 Done, mean position loss: 20.296616332530974\n",
      "Batch 10000/10001 Done, mean position loss: 20.8220925450325\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:48<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:48<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▉| 9946/10001 [5:07:48<00:30,  1.79batch/s]Batch 10000/10001 Done, mean position loss: 20.64119633197784\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:49<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9998/10001 [5:07:49<00:01,  1.90batch/s]Batch 10000/10001 Done, mean position loss: 21.03068332195282\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:49<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▊| 9896/10001 [5:07:50<00:44,  2.37batch/s]Batch 10000/10001 Done, mean position loss: 20.76075259923935\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:50<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9982/10001 [5:07:52<00:05,  3.25batch/s]Batch 9900/10001 Done, mean position loss: 20.53958827972412\n",
      "Training NF2: 100%|██████████████████▉| 9973/10001 [5:07:57<00:07,  3.59batch/s]Batch 10000/10001 Done, mean position loss: 20.328375260829922\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:07:57<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2:  99%|██████████████████▉| 9945/10001 [5:08:04<00:14,  3.73batch/s]Batch 10000/10001 Done, mean position loss: 20.633630061149596\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:08:04<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9980/10001 [5:08:05<00:08,  2.46batch/s]Batch 10000/10001 Done, mean position loss: 20.805412669181827\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:08:05<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9951/10001 [5:08:05<00:12,  3.89batch/s]Batch 10000/10001 Done, mean position loss: 20.6319455909729\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:08:06<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|██████████████████▉| 9968/10001 [5:08:09<00:07,  4.58batch/s]Batch 10000/10001 Done, mean position loss: 20.497141032218934\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:08:09<00:00,  1.85s/batch]\n",
      "Done...\n",
      "Training NF2: 100%|█████████████████▉| 10000/10001 [5:08:15<00:00,  4.86batch/s]Batch 10000/10001 Done, mean position loss: 20.543508505821226\n",
      "Training NF2: 100%|██████████████████| 10001/10001 [5:08:16<00:00,  1.85s/batch]\n",
      "Done...\n",
      "model00...\n",
      "model01...\n",
      "model02...\n",
      "model03...\n",
      "model04...\n",
      "model05...\n",
      "model06...\n",
      "model07...\n",
      "model08...\n",
      "model09...\n",
      "model10...\n",
      "model11...\n",
      "model12...model13...\n",
      "\n",
      "model14...\n",
      "model15...\n",
      "model16...\n",
      "model17...\n",
      "model18...\n",
      "model19...\n",
      "model20...\n",
      "model21...\n",
      "model22...\n",
      "model23...\n",
      "model24...\n",
      "model25...\n",
      "model26...\n",
      "model27...\n",
      "model28...\n",
      "model29...\n",
      "model30...\n",
      "model31...\n",
      "model32...\n",
      "model33...\n",
      "model34...\n",
      "model35...\n",
      "model36...\n",
      "model37...\n",
      "model38...\n",
      "model39...\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:22<1:35:00,  1.84s/batch]Batch 100/3201 Done, mean position loss: 28.382504413127897\n",
      "Training FF2:   3%|▋                     | 97/3201 [03:23<1:43:19,  2.00s/batch]Batch 100/3201 Done, mean position loss: 28.0108017039299\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:25<1:50:55,  2.15s/batch]Batch 100/3201 Done, mean position loss: 28.16039752006531\n",
      "Training FF2:   3%|▋                    | 101/3201 [03:25<1:50:08,  2.13s/batch]Batch 100/3201 Done, mean position loss: 28.254702413082125\n",
      "Training FF2:   3%|▋                     | 92/3201 [03:25<2:03:15,  2.38s/batch]Batch 100/3201 Done, mean position loss: 30.015208024978637\n",
      "Training FF2:   3%|▋                    | 101/3201 [03:25<1:54:26,  2.21s/batch]Batch 100/3201 Done, mean position loss: 27.899170622825622\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:25<1:41:36,  1.97s/batch]Batch 100/3201 Done, mean position loss: 28.9958433508873\n",
      "Training FF2:   3%|▋                     | 94/3201 [03:25<1:57:45,  2.27s/batch]Batch 100/3201 Done, mean position loss: 29.099384839534757\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:26<1:43:40,  2.01s/batch]Batch 100/3201 Done, mean position loss: 27.75213176727295\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:27<1:31:42,  1.77s/batch]Batch 100/3201 Done, mean position loss: 28.554749212265016\n",
      "Training FF2:   3%|▋                     | 95/3201 [03:27<1:46:05,  2.05s/batch]Batch 100/3201 Done, mean position loss: 28.282449271678924\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:27<1:37:15,  1.88s/batch]Batch 100/3201 Done, mean position loss: 27.53010759353638\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:27<1:36:13,  1.86s/batch]Batch 100/3201 Done, mean position loss: 28.22632826089859\n",
      "Training FF2:   3%|▋                    | 101/3201 [03:27<1:40:55,  1.95s/batch]Batch 100/3201 Done, mean position loss: 28.807271564006804\n",
      "Training FF2:   3%|▋                     | 98/3201 [03:28<1:32:10,  1.78s/batch]Batch 100/3201 Done, mean position loss: 27.47580037355423\n",
      "Training FF2:   3%|▋                    | 101/3201 [03:29<1:35:50,  1.86s/batch]Batch 100/3201 Done, mean position loss: 28.520244629383086\n",
      "Training FF2:   3%|▋                     | 98/3201 [03:28<1:45:10,  2.03s/batch]Batch 100/3201 Done, mean position loss: 27.30631443977356\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:28<1:46:56,  2.07s/batch]Batch 100/3201 Done, mean position loss: 28.40801590681076\n",
      "Training FF2:   3%|▋                     | 98/3201 [03:28<1:46:17,  2.06s/batch]Batch 100/3201 Done, mean position loss: 28.515313663482665\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:29<1:34:05,  1.82s/batch]Batch 100/3201 Done, mean position loss: 27.761064867973328\n",
      "Training FF2:   3%|▋                    | 102/3201 [03:30<1:33:45,  1.82s/batch]Batch 100/3201 Done, mean position loss: 27.919467508792877\n",
      "Training FF2:   3%|▋                    | 104/3201 [03:31<1:38:39,  1.91s/batch]Batch 100/3201 Done, mean position loss: 27.825960140228272\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:31<1:48:51,  2.11s/batch]Batch 100/3201 Done, mean position loss: 28.106788783073426\n",
      "Training FF2:   3%|▋                    | 102/3201 [03:30<1:34:34,  1.83s/batch]Batch 100/3201 Done, mean position loss: 26.837836456298827\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:31<2:00:18,  2.33s/batch]Batch 100/3201 Done, mean position loss: 28.376055369377134\n",
      "Batch 100/3201 Done, mean position loss: 27.607582881450654\n",
      "Training FF2:   3%|▋                    | 104/3201 [03:33<1:34:29,  1.83s/batch]Batch 100/3201 Done, mean position loss: 27.43413141965866\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:33<1:45:01,  2.03s/batch]Batch 100/3201 Done, mean position loss: 27.79873561143875\n",
      "Training FF2:   3%|▋                     | 95/3201 [03:34<1:35:34,  1.85s/batch]Batch 100/3201 Done, mean position loss: 29.364230036735535\n",
      "Training FF2:   3%|▋                    | 100/3201 [03:34<1:44:04,  2.01s/batch]Batch 100/3201 Done, mean position loss: 27.491228263378144\n",
      "Training FF2:   3%|▋                    | 106/3201 [03:35<1:37:13,  1.88s/batch]Batch 100/3201 Done, mean position loss: 28.302528424263002\n",
      "Training FF2:   3%|▋                     | 99/3201 [03:35<1:40:00,  1.93s/batch]Batch 100/3201 Done, mean position loss: 28.947968912124637\n",
      "Training FF2:   3%|▋                    | 103/3201 [03:35<1:34:02,  1.82s/batch]Batch 100/3201 Done, mean position loss: 29.40630680322647\n",
      "Training FF2:   3%|▋                    | 107/3201 [03:36<1:33:38,  1.82s/batch]Batch 100/3201 Done, mean position loss: 27.48542364358902\n",
      "Training FF2:   3%|▋                     | 97/3201 [03:37<1:31:40,  1.77s/batch]Batch 100/3201 Done, mean position loss: 27.40576926469803\n",
      "Training FF2:   3%|▋                    | 103/3201 [03:38<1:31:50,  1.78s/batch]Batch 100/3201 Done, mean position loss: 28.14243406295776\n",
      "Training FF2:   3%|▋                    | 102/3201 [03:37<1:37:09,  1.88s/batch]Batch 100/3201 Done, mean position loss: 28.070036499500276\n",
      "Training FF2:   3%|▋                    | 108/3201 [03:39<1:45:47,  2.05s/batch]Batch 100/3201 Done, mean position loss: 28.74432450294495\n",
      "Training FF2:   3%|▋                    | 105/3201 [03:41<1:25:03,  1.65s/batch]Batch 100/3201 Done, mean position loss: 29.469963667392726\n",
      "Training FF2:   3%|▋                    | 108/3201 [03:46<1:57:03,  2.27s/batch]Batch 100/3201 Done, mean position loss: 28.500209095478056\n",
      "Training FF2:   6%|█▎                   | 198/3201 [06:26<1:42:10,  2.04s/batch]Batch 200/3201 Done, mean position loss: 24.350581998825074\n",
      "Training FF2:   6%|█▎                   | 193/3201 [06:26<1:30:55,  1.81s/batch]Batch 200/3201 Done, mean position loss: 25.106123580932618\n",
      "Training FF2:   6%|█▎                   | 193/3201 [06:28<1:37:17,  1.94s/batch]Batch 200/3201 Done, mean position loss: 25.12506290912628\n",
      "Training FF2:   6%|█▎                   | 201/3201 [06:28<1:43:14,  2.06s/batch]Batch 200/3201 Done, mean position loss: 25.61981176376343\n",
      "Training FF2:   6%|█▎                   | 194/3201 [06:30<1:46:00,  2.12s/batch]Batch 200/3201 Done, mean position loss: 24.758356578350067\n",
      "Training FF2:   6%|█▎                   | 203/3201 [06:31<1:15:37,  1.51s/batch]Batch 200/3201 Done, mean position loss: 24.839180326461793\n",
      "Training FF2:   6%|█▎                   | 198/3201 [06:32<1:39:54,  2.00s/batch]Batch 200/3201 Done, mean position loss: 24.853620448112487\n",
      "Training FF2:   6%|█▎                   | 194/3201 [06:31<1:33:37,  1.87s/batch]Batch 200/3201 Done, mean position loss: 25.922270851135256\n",
      "Training FF2:   6%|█▎                   | 196/3201 [06:32<1:46:17,  2.12s/batch]Batch 200/3201 Done, mean position loss: 24.617193961143492\n",
      "Training FF2:   6%|█▏                   | 190/3201 [06:33<1:48:54,  2.17s/batch]Batch 200/3201 Done, mean position loss: 24.280954163074494\n",
      "Training FF2:   6%|█▎                   | 201/3201 [06:33<1:47:41,  2.15s/batch]Batch 200/3201 Done, mean position loss: 25.068412630558015\n",
      "Training FF2:   6%|█▎                   | 202/3201 [06:34<1:28:50,  1.78s/batch]Batch 200/3201 Done, mean position loss: 25.815120246410373\n",
      "Training FF2:   6%|█▎                   | 197/3201 [06:34<1:39:32,  1.99s/batch]Batch 200/3201 Done, mean position loss: 25.510423843860625\n",
      "Training FF2:   6%|█▎                   | 198/3201 [06:36<1:35:57,  1.92s/batch]Batch 200/3201 Done, mean position loss: 25.231201095581056\n",
      "Training FF2:   6%|█▎                   | 197/3201 [06:36<1:22:32,  1.65s/batch]Batch 200/3201 Done, mean position loss: 24.461302473545075\n",
      "Training FF2:   6%|█▎                   | 199/3201 [06:37<1:31:03,  1.82s/batch]Batch 200/3201 Done, mean position loss: 24.15780457496643\n",
      "Training FF2:   6%|█▎                   | 201/3201 [06:37<1:43:40,  2.07s/batch]Batch 200/3201 Done, mean position loss: 24.560734751224516\n",
      "Training FF2:   6%|█▎                   | 192/3201 [06:37<1:52:03,  2.23s/batch]Batch 200/3201 Done, mean position loss: 24.60231355190277\n",
      "Training FF2:   6%|█▎                   | 195/3201 [06:37<1:21:15,  1.62s/batch]Batch 200/3201 Done, mean position loss: 25.433695113658906\n",
      "Training FF2:   6%|█▎                   | 202/3201 [06:37<1:10:18,  1.41s/batch]Batch 200/3201 Done, mean position loss: 24.914277713298798\n",
      "Training FF2:   6%|█▎                   | 200/3201 [06:38<1:25:36,  1.71s/batch]Batch 200/3201 Done, mean position loss: 24.97085569858551\n",
      "Training FF2:   6%|█▎                   | 200/3201 [06:38<1:29:54,  1.80s/batch]Batch 200/3201 Done, mean position loss: 25.006113843917845\n",
      "Training FF2:   6%|█▎                   | 202/3201 [06:39<1:23:36,  1.67s/batch]Batch 200/3201 Done, mean position loss: 24.69665448665619\n",
      "Training FF2:   6%|█▎                   | 203/3201 [06:39<1:10:07,  1.40s/batch]Batch 200/3201 Done, mean position loss: 24.325008101463318\n",
      "Training FF2:   6%|█▎                   | 200/3201 [06:41<1:40:40,  2.01s/batch]Batch 200/3201 Done, mean position loss: 23.6933559012413\n",
      "Training FF2:   6%|█▎                   | 204/3201 [06:41<1:20:07,  1.60s/batch]Batch 200/3201 Done, mean position loss: 25.295305187702176\n",
      "Batch 200/3201 Done, mean position loss: 24.36603162765503\n",
      "Training FF2:   7%|█▎                   | 209/3201 [06:41<1:16:03,  1.53s/batch]Batch 200/3201 Done, mean position loss: 23.898369500637056\n",
      "Batch 200/3201 Done, mean position loss: 24.886356134414672\n",
      "Training FF2:   7%|█▍                   | 211/3201 [06:43<1:18:00,  1.57s/batch]Batch 200/3201 Done, mean position loss: 24.900196828842162\n",
      "Training FF2:   6%|█▎                   | 205/3201 [06:44<1:27:01,  1.74s/batch]Batch 200/3201 Done, mean position loss: 23.83340233325958\n",
      "Training FF2:   6%|█▎                   | 207/3201 [06:44<1:35:17,  1.91s/batch]Batch 200/3201 Done, mean position loss: 24.16666344642639\n",
      "Training FF2:   6%|█▎                   | 198/3201 [06:44<1:33:37,  1.87s/batch]Batch 200/3201 Done, mean position loss: 25.317808074951174\n",
      "Training FF2:   6%|█▎                   | 204/3201 [06:47<1:18:28,  1.57s/batch]Batch 200/3201 Done, mean position loss: 26.356837069988252\n",
      "Training FF2:   6%|█▎                   | 207/3201 [06:48<1:40:20,  2.01s/batch]Batch 200/3201 Done, mean position loss: 25.93818687200546\n",
      "Training FF2:   6%|█▎                   | 208/3201 [06:49<1:22:33,  1.66s/batch]Batch 200/3201 Done, mean position loss: 24.391517543792723\n",
      "Training FF2:   7%|█▍                   | 215/3201 [06:50<1:28:05,  1.77s/batch]Batch 200/3201 Done, mean position loss: 24.577005372047424\n",
      "Training FF2:   7%|█▍                   | 214/3201 [06:51<1:39:59,  2.01s/batch]Batch 200/3201 Done, mean position loss: 24.45600301980972\n",
      "Training FF2:   6%|█▎                   | 208/3201 [06:56<1:25:25,  1.71s/batch]Batch 200/3201 Done, mean position loss: 26.0549484372139\n",
      "Training FF2:   7%|█▍                   | 212/3201 [07:11<1:30:20,  1.81s/batch]Batch 200/3201 Done, mean position loss: 25.067635312080384\n",
      "Training FF2:   9%|█▉                   | 296/3201 [09:31<1:13:34,  1.52s/batch]Batch 300/3201 Done, mean position loss: 23.734581575393676\n",
      "Training FF2:   9%|█▉                   | 299/3201 [09:37<1:31:18,  1.89s/batch]Batch 300/3201 Done, mean position loss: 23.19654311656952\n",
      "Batch 300/3201 Done, mean position loss: 23.81617947101593\n",
      "Training FF2:   9%|█▉                   | 294/3201 [09:38<1:31:05,  1.88s/batch]Batch 300/3201 Done, mean position loss: 23.35952747106552\n",
      "Training FF2:   9%|█▉                   | 297/3201 [09:37<1:22:42,  1.71s/batch]Batch 300/3201 Done, mean position loss: 23.46302554130554\n",
      "Training FF2:   9%|█▉                   | 300/3201 [09:38<1:27:35,  1.81s/batch]Batch 300/3201 Done, mean position loss: 24.080723202228548\n",
      "Training FF2:   9%|█▉                   | 294/3201 [09:39<1:29:51,  1.85s/batch]Batch 300/3201 Done, mean position loss: 23.655228748321534\n",
      "Training FF2:   9%|█▉                   | 300/3201 [09:39<1:22:55,  1.71s/batch]Batch 300/3201 Done, mean position loss: 23.40143064260483\n",
      "Training FF2:   9%|█▉                   | 301/3201 [09:39<1:22:06,  1.70s/batch]Batch 300/3201 Done, mean position loss: 22.72315473556519\n",
      "Training FF2:   9%|█▉                   | 290/3201 [09:39<1:27:40,  1.81s/batch]Batch 300/3201 Done, mean position loss: 22.934398913383482\n",
      "Training FF2:   9%|█▉                   | 297/3201 [09:39<1:18:16,  1.62s/batch]Batch 300/3201 Done, mean position loss: 23.031844177246093\n",
      "Training FF2:   9%|█▉                   | 303/3201 [09:41<1:15:54,  1.57s/batch]Batch 300/3201 Done, mean position loss: 23.43105736017227\n",
      "Training FF2:   9%|█▉                   | 291/3201 [09:42<1:44:20,  2.15s/batch]Batch 300/3201 Done, mean position loss: 22.827782452106476\n",
      "Training FF2:   9%|█▉                   | 300/3201 [09:44<1:17:24,  1.60s/batch]Batch 300/3201 Done, mean position loss: 23.27763123035431\n",
      "Training FF2:   9%|█▉                   | 294/3201 [09:44<1:38:36,  2.04s/batch]Batch 300/3201 Done, mean position loss: 23.144721922874453\n",
      "Training FF2:  10%|██                   | 305/3201 [09:44<1:16:20,  1.58s/batch]Batch 300/3201 Done, mean position loss: 23.8070639872551\n",
      "Training FF2:  10%|██                   | 305/3201 [09:45<1:14:20,  1.54s/batch]Batch 300/3201 Done, mean position loss: 23.200083904266357\n",
      "Training FF2:   9%|█▉                   | 297/3201 [09:45<1:24:00,  1.74s/batch]Batch 300/3201 Done, mean position loss: 24.170450189113616\n",
      "Training FF2:   9%|█▉                   | 298/3201 [09:46<1:26:11,  1.78s/batch]Batch 300/3201 Done, mean position loss: 22.743461706638335\n",
      "Training FF2:   9%|█▉                   | 301/3201 [09:47<1:12:39,  1.50s/batch]Batch 300/3201 Done, mean position loss: 23.485856924057007\n",
      "Training FF2:  10%|██                   | 306/3201 [09:48<1:29:22,  1.85s/batch]Batch 300/3201 Done, mean position loss: 23.16931290388107\n",
      "Training FF2:  10%|██                   | 307/3201 [09:48<1:24:55,  1.76s/batch]Batch 300/3201 Done, mean position loss: 22.98002918243408\n",
      "Training FF2:   9%|█▉                   | 300/3201 [09:48<1:19:27,  1.64s/batch]Batch 300/3201 Done, mean position loss: 23.607677335739133\n",
      "Batch 300/3201 Done, mean position loss: 23.78140311717987\n",
      "Training FF2:  10%|██                   | 307/3201 [09:50<1:38:30,  2.04s/batch]Batch 300/3201 Done, mean position loss: 23.64434799194336\n",
      "Training FF2:  10%|██                   | 306/3201 [09:49<1:19:44,  1.65s/batch]Batch 300/3201 Done, mean position loss: 24.040449447631836\n",
      "Training FF2:  10%|██                   | 307/3201 [09:50<1:23:40,  1.73s/batch]Batch 300/3201 Done, mean position loss: 23.983248620033265\n",
      "Training FF2:   9%|█▉                   | 295/3201 [09:50<1:45:26,  2.18s/batch]Batch 300/3201 Done, mean position loss: 22.558936004638674\n",
      "Training FF2:  10%|██                   | 309/3201 [09:50<1:09:13,  1.44s/batch]Batch 300/3201 Done, mean position loss: 23.47813097000122\n",
      "Training FF2:  10%|██                   | 305/3201 [09:51<1:30:44,  1.88s/batch]Batch 300/3201 Done, mean position loss: 24.423871331214905\n",
      "Training FF2:  10%|██                   | 305/3201 [09:52<1:28:17,  1.83s/batch]Batch 300/3201 Done, mean position loss: 23.661288425922393\n",
      "Training FF2:  10%|██                   | 305/3201 [09:53<1:15:02,  1.55s/batch]Batch 300/3201 Done, mean position loss: 22.478491933345794\n",
      "Training FF2:   9%|█▉                   | 296/3201 [09:52<1:50:30,  2.28s/batch]Batch 300/3201 Done, mean position loss: 24.065043520927432\n",
      "Batch 300/3201 Done, mean position loss: 24.54209363222122\n",
      "Training FF2:  10%|██                   | 308/3201 [09:58<1:23:25,  1.73s/batch]Batch 300/3201 Done, mean position loss: 22.928272690773014\n",
      "Training FF2:  10%|██                   | 310/3201 [10:00<1:24:11,  1.75s/batch]Batch 300/3201 Done, mean position loss: 23.42861584663391\n",
      "Training FF2:  10%|██                   | 308/3201 [10:00<1:27:56,  1.82s/batch]Batch 300/3201 Done, mean position loss: 23.021160855293274\n",
      "Training FF2:  10%|██                   | 316/3201 [10:01<1:26:25,  1.80s/batch]Batch 300/3201 Done, mean position loss: 23.107185797691344\n",
      "Training FF2:  10%|██                   | 311/3201 [10:04<1:30:20,  1.88s/batch]Batch 300/3201 Done, mean position loss: 24.359479961395266\n",
      "Training FF2:  10%|██▏                  | 324/3201 [10:21<1:25:17,  1.78s/batch]Batch 300/3201 Done, mean position loss: 23.381964168548585\n",
      "Training FF2:  12%|██▌                  | 394/3201 [12:28<1:19:59,  1.71s/batch]Batch 400/3201 Done, mean position loss: 23.303617956638334\n",
      "Training FF2:  12%|██▌                  | 388/3201 [12:33<1:36:52,  2.07s/batch]Batch 400/3201 Done, mean position loss: 23.120752527713776\n",
      "Training FF2:  12%|██▌                  | 397/3201 [12:36<1:22:10,  1.76s/batch]Batch 400/3201 Done, mean position loss: 21.862683475017548\n",
      "Training FF2:  12%|██▌                  | 399/3201 [12:38<1:29:29,  1.92s/batch]Batch 400/3201 Done, mean position loss: 22.77612165689468\n",
      "Training FF2:  12%|██▌                  | 392/3201 [12:39<1:35:10,  2.03s/batch]Batch 400/3201 Done, mean position loss: 22.999643666744234\n",
      "Training FF2:  12%|██▌                  | 391/3201 [12:40<1:31:10,  1.95s/batch]Batch 400/3201 Done, mean position loss: 22.723360648155214\n",
      "Training FF2:  12%|██▌                  | 395/3201 [12:40<1:23:13,  1.78s/batch]Batch 400/3201 Done, mean position loss: 22.483567616939546\n",
      "Training FF2:  12%|██▌                  | 397/3201 [12:40<1:19:24,  1.70s/batch]Batch 400/3201 Done, mean position loss: 23.155586583614348\n",
      "Training FF2:  13%|██▋                  | 402/3201 [12:41<1:21:07,  1.74s/batch]Batch 400/3201 Done, mean position loss: 22.95168431997299\n",
      "Training FF2:  13%|██▋                  | 406/3201 [12:41<1:19:40,  1.71s/batch]Batch 400/3201 Done, mean position loss: 22.478687098026274\n",
      "Training FF2:  12%|██▌                  | 396/3201 [12:42<1:26:33,  1.85s/batch]Batch 400/3201 Done, mean position loss: 22.490651865005493\n",
      "Training FF2:  13%|██▋                  | 403/3201 [12:42<1:10:30,  1.51s/batch]Batch 400/3201 Done, mean position loss: 22.28987764120102\n",
      "Training FF2:  12%|██▌                  | 396/3201 [12:43<1:34:21,  2.02s/batch]Batch 400/3201 Done, mean position loss: 22.29402246952057\n",
      "Training FF2:  12%|██▌                  | 385/3201 [12:44<1:42:38,  2.19s/batch]Batch 400/3201 Done, mean position loss: 22.60641370534897\n",
      "Training FF2:  13%|██▋                  | 404/3201 [12:44<1:15:51,  1.63s/batch]Batch 400/3201 Done, mean position loss: 22.566699497699737\n",
      "Training FF2:  12%|██▌                  | 399/3201 [12:47<1:30:31,  1.94s/batch]Batch 400/3201 Done, mean position loss: 22.743681883811952\n",
      "Training FF2:  12%|██▌                  | 396/3201 [12:47<1:37:38,  2.09s/batch]Batch 400/3201 Done, mean position loss: 21.912086589336397\n",
      "Training FF2:  13%|██▋                  | 404/3201 [12:49<1:33:52,  2.01s/batch]Batch 400/3201 Done, mean position loss: 22.939920539855954\n",
      "Training FF2:  12%|██▌                  | 399/3201 [12:49<1:31:24,  1.96s/batch]Batch 400/3201 Done, mean position loss: 22.252921347618102\n",
      "Training FF2:  12%|██▌                  | 399/3201 [12:49<1:36:41,  2.07s/batch]Batch 400/3201 Done, mean position loss: 23.502525806427002\n",
      "Training FF2:  13%|██▋                  | 403/3201 [12:50<1:20:11,  1.72s/batch]Batch 400/3201 Done, mean position loss: 22.48915822982788\n",
      "Training FF2:  13%|██▋                  | 402/3201 [12:52<1:09:15,  1.48s/batch]Batch 400/3201 Done, mean position loss: 22.43497974872589\n",
      "Training FF2:  12%|██▌                  | 389/3201 [12:51<1:26:29,  1.85s/batch]Batch 400/3201 Done, mean position loss: 22.767313044071198\n",
      "Training FF2:  13%|██▋                  | 408/3201 [12:51<1:21:56,  1.76s/batch]Batch 400/3201 Done, mean position loss: 23.499890446662903\n",
      "Training FF2:  13%|██▋                  | 406/3201 [12:53<1:28:03,  1.89s/batch]Batch 400/3201 Done, mean position loss: 23.229317009449005\n",
      "Training FF2:  13%|██▋                  | 401/3201 [12:53<1:15:58,  1.63s/batch]Batch 400/3201 Done, mean position loss: 22.360859518051146\n",
      "Batch 400/3201 Done, mean position loss: 23.040665013790132\n",
      "Training FF2:  13%|██▋                  | 409/3201 [12:53<1:25:47,  1.84s/batch]Batch 400/3201 Done, mean position loss: 22.894592437744137\n",
      "Training FF2:  13%|██▋                  | 412/3201 [12:52<1:24:42,  1.82s/batch]Batch 400/3201 Done, mean position loss: 23.55378512144089\n",
      "Training FF2:  12%|██▌                  | 393/3201 [12:53<1:36:03,  2.05s/batch]Batch 400/3201 Done, mean position loss: 22.170420818328857\n",
      "Training FF2:  13%|██▋                  | 405/3201 [12:54<1:19:31,  1.71s/batch]Batch 400/3201 Done, mean position loss: 23.45918511390686\n",
      "Training FF2:  13%|██▋                  | 411/3201 [12:54<1:13:10,  1.57s/batch]Batch 400/3201 Done, mean position loss: 22.920627992153168\n",
      "Training FF2:  13%|██▋                  | 411/3201 [12:55<1:14:43,  1.61s/batch]Batch 400/3201 Done, mean position loss: 22.540546889305112\n",
      "Training FF2:  13%|██▋                  | 406/3201 [12:56<1:09:42,  1.50s/batch]Batch 400/3201 Done, mean position loss: 23.185682523250577\n",
      "Training FF2:  13%|██▋                  | 414/3201 [12:58<1:47:55,  2.32s/batch]Batch 400/3201 Done, mean position loss: 21.79695256233215\n",
      "Training FF2:  13%|██▋                  | 408/3201 [13:02<1:32:50,  1.99s/batch]Batch 400/3201 Done, mean position loss: 22.881671109199523\n",
      "Training FF2:  13%|██▋                  | 417/3201 [13:06<1:20:48,  1.74s/batch]Batch 400/3201 Done, mean position loss: 22.674932658672333\n",
      "Training FF2:  13%|██▋                  | 416/3201 [13:10<1:30:27,  1.95s/batch]Batch 400/3201 Done, mean position loss: 22.47190914154053\n",
      "Training FF2:  13%|██▋                  | 418/3201 [13:13<1:19:27,  1.71s/batch]Batch 400/3201 Done, mean position loss: 23.55666131258011\n",
      "Training FF2:  13%|██▋                  | 419/3201 [13:30<1:35:14,  2.05s/batch]Batch 400/3201 Done, mean position loss: 22.84288353204727\n",
      "Training FF2:  15%|███▏                 | 489/3201 [15:41<1:41:08,  2.24s/batch]Batch 500/3201 Done, mean position loss: 22.716483378410338\n",
      "Training FF2:  15%|███▏                 | 490/3201 [15:42<1:28:06,  1.95s/batch]Batch 500/3201 Done, mean position loss: 22.865441198349\n",
      "Training FF2:  15%|███▏                 | 487/3201 [15:43<1:43:18,  2.28s/batch]Batch 500/3201 Done, mean position loss: 22.452877004146576\n",
      "Training FF2:  15%|███▏                 | 489/3201 [15:48<1:38:21,  2.18s/batch]Batch 500/3201 Done, mean position loss: 21.60860992908478\n",
      "Training FF2:  16%|███▎                 | 499/3201 [15:47<1:14:17,  1.65s/batch]Batch 500/3201 Done, mean position loss: 22.362272732257843\n",
      "Training FF2:  15%|███▎                 | 496/3201 [15:49<1:11:32,  1.59s/batch]Batch 500/3201 Done, mean position loss: 22.722514336109164\n",
      "Training FF2:  16%|███▎                 | 501/3201 [15:49<1:27:02,  1.93s/batch]Batch 500/3201 Done, mean position loss: 22.294320628643035\n",
      "Training FF2:  16%|███▎                 | 506/3201 [15:49<1:17:29,  1.73s/batch]Batch 500/3201 Done, mean position loss: 21.976088917255403\n",
      "Training FF2:  15%|███▏                 | 493/3201 [15:51<1:22:19,  1.82s/batch]Batch 500/3201 Done, mean position loss: 23.021573927402496\n",
      "Training FF2:  15%|███▏                 | 491/3201 [15:51<1:33:23,  2.07s/batch]Batch 500/3201 Done, mean position loss: 22.285680985450746\n",
      "Training FF2:  16%|███▎                 | 507/3201 [15:51<1:21:39,  1.82s/batch]Batch 500/3201 Done, mean position loss: 22.11124354362488\n",
      "Training FF2:  16%|███▎                 | 498/3201 [15:52<1:10:55,  1.57s/batch]Batch 500/3201 Done, mean position loss: 22.060222511291506\n",
      "Training FF2:  15%|███▏                 | 495/3201 [15:54<1:32:24,  2.05s/batch]Batch 500/3201 Done, mean position loss: 22.109744975566862\n",
      "Training FF2:  15%|███▏                 | 495/3201 [15:55<1:28:05,  1.95s/batch]Batch 500/3201 Done, mean position loss: 22.644028782844543\n",
      "Training FF2:  16%|███▎                 | 497/3201 [15:58<1:19:51,  1.77s/batch]Batch 500/3201 Done, mean position loss: 22.667159912586214\n",
      "Training FF2:  16%|███▎                 | 497/3201 [15:57<1:21:23,  1.81s/batch]Batch 500/3201 Done, mean position loss: 22.080402066707613\n",
      "Training FF2:  15%|███▏                 | 494/3201 [15:58<1:31:27,  2.03s/batch]Batch 500/3201 Done, mean position loss: 22.06740930557251\n",
      "Training FF2:  16%|███▎                 | 499/3201 [15:58<1:31:43,  2.04s/batch]Batch 500/3201 Done, mean position loss: 22.342888288497925\n",
      "Training FF2:  16%|███▎                 | 501/3201 [15:58<1:14:41,  1.66s/batch]Batch 500/3201 Done, mean position loss: 21.889295358657836\n",
      "Training FF2:  16%|███▎                 | 498/3201 [15:58<1:14:26,  1.65s/batch]Batch 500/3201 Done, mean position loss: 21.605389909744265\n",
      "Training FF2:  16%|███▎                 | 502/3201 [16:00<1:19:05,  1.76s/batch]Batch 500/3201 Done, mean position loss: 22.149578285217288\n",
      "Training FF2:  16%|███▎                 | 506/3201 [16:01<1:16:12,  1.70s/batch]Batch 500/3201 Done, mean position loss: 22.011080586910246\n",
      "Training FF2:  16%|███▎                 | 499/3201 [16:00<1:14:30,  1.65s/batch]Batch 500/3201 Done, mean position loss: 21.99224967479706\n",
      "Training FF2:  16%|███▎                 | 500/3201 [16:02<1:44:39,  2.32s/batch]Batch 500/3201 Done, mean position loss: 22.812813408374787\n",
      "Training FF2:  16%|███▎                 | 501/3201 [16:02<1:25:36,  1.90s/batch]Batch 500/3201 Done, mean position loss: 23.114600176811216\n",
      "Training FF2:  16%|███▎                 | 508/3201 [16:03<1:39:43,  2.22s/batch]Batch 500/3201 Done, mean position loss: 23.04040509939194\n",
      "Training FF2:  16%|███▎                 | 502/3201 [16:03<1:12:25,  1.61s/batch]Batch 500/3201 Done, mean position loss: 22.45621633529663\n",
      "Training FF2:  16%|███▎                 | 509/3201 [16:03<1:18:22,  1.75s/batch]Batch 500/3201 Done, mean position loss: 22.60183375120163\n",
      "Training FF2:  16%|███▎                 | 498/3201 [16:04<1:25:35,  1.90s/batch]Batch 500/3201 Done, mean position loss: 22.858745129108428\n",
      "Training FF2:  16%|███▎                 | 504/3201 [16:04<1:12:15,  1.61s/batch]Batch 500/3201 Done, mean position loss: 22.01232753276825\n",
      "Training FF2:  16%|███▎                 | 501/3201 [16:05<1:24:24,  1.88s/batch]Batch 500/3201 Done, mean position loss: 21.79772917985916\n",
      "Training FF2:  16%|███▎                 | 505/3201 [16:06<1:21:11,  1.81s/batch]Batch 500/3201 Done, mean position loss: 22.513976674079892\n",
      "Training FF2:  16%|███▎                 | 501/3201 [16:06<1:22:41,  1.84s/batch]Batch 500/3201 Done, mean position loss: 21.361051466465\n",
      "Training FF2:  16%|███▎                 | 512/3201 [16:11<1:25:40,  1.91s/batch]Batch 500/3201 Done, mean position loss: 22.324760818481447\n",
      "Training FF2:  16%|███▎                 | 505/3201 [16:10<1:12:49,  1.62s/batch]Batch 500/3201 Done, mean position loss: 22.16836092948914\n",
      "Training FF2:  16%|███▎                 | 513/3201 [16:11<1:22:01,  1.83s/batch]Batch 500/3201 Done, mean position loss: 22.330283682346348\n",
      "Training FF2:  16%|███▍                 | 517/3201 [16:12<1:25:54,  1.92s/batch]Batch 500/3201 Done, mean position loss: 22.13058028459549\n",
      "Training FF2:  16%|███▎                 | 507/3201 [16:18<1:34:11,  2.10s/batch]Batch 500/3201 Done, mean position loss: 22.51152503967285\n",
      "Training FF2:  16%|███▎                 | 512/3201 [16:24<1:13:21,  1.64s/batch]Batch 500/3201 Done, mean position loss: 23.054118323326108\n",
      "Training FF2:  16%|███▍                 | 526/3201 [16:51<1:27:51,  1.97s/batch]Batch 500/3201 Done, mean position loss: 22.56099671125412\n",
      "Training FF2:  19%|███▉                 | 593/3201 [18:50<1:37:03,  2.23s/batch]Batch 600/3201 Done, mean position loss: 21.521502006053925\n",
      "Training FF2:  18%|███▊                 | 588/3201 [18:51<1:38:25,  2.26s/batch]Batch 600/3201 Done, mean position loss: 22.45986161470413\n",
      "Training FF2:  18%|███▊                 | 586/3201 [18:52<1:24:31,  1.94s/batch]Batch 600/3201 Done, mean position loss: 22.598576309680936\n",
      "Training FF2:  18%|███▉                 | 591/3201 [18:55<1:07:33,  1.55s/batch]Batch 600/3201 Done, mean position loss: 22.38872965335846\n",
      "Training FF2:  19%|███▉                 | 594/3201 [18:56<1:26:10,  1.98s/batch]Batch 600/3201 Done, mean position loss: 22.079369716644287\n",
      "Training FF2:  18%|███▊                 | 589/3201 [18:56<1:30:57,  2.09s/batch]Batch 600/3201 Done, mean position loss: 21.793021197319028\n",
      "Training FF2:  18%|███▊                 | 585/3201 [18:58<1:37:11,  2.23s/batch]Batch 600/3201 Done, mean position loss: 22.01629426240921\n",
      "Training FF2:  19%|███▉                 | 601/3201 [18:58<1:32:50,  2.14s/batch]Batch 600/3201 Done, mean position loss: 21.730753672122958\n",
      "Training FF2:  19%|███▉                 | 602/3201 [19:00<1:21:05,  1.87s/batch]Batch 600/3201 Done, mean position loss: 21.890824241638185\n",
      "Training FF2:  19%|███▉                 | 600/3201 [19:01<1:12:39,  1.68s/batch]Batch 600/3201 Done, mean position loss: 22.062840552330016\n",
      "Training FF2:  19%|███▉                 | 594/3201 [19:01<1:26:26,  1.99s/batch]Batch 600/3201 Done, mean position loss: 22.445396902561185\n",
      "Training FF2:  19%|███▉                 | 599/3201 [19:01<1:17:21,  1.78s/batch]Batch 600/3201 Done, mean position loss: 21.93319804906845\n",
      "Training FF2:  19%|███▉                 | 607/3201 [19:02<1:11:02,  1.64s/batch]Batch 600/3201 Done, mean position loss: 21.468991947174075\n",
      "Training FF2:  19%|███▉                 | 607/3201 [19:03<1:43:30,  2.39s/batch]Batch 600/3201 Done, mean position loss: 22.633208661079408\n",
      "Training FF2:  19%|███▉                 | 605/3201 [19:04<1:13:19,  1.69s/batch]Batch 600/3201 Done, mean position loss: 21.71375169277191\n",
      "Training FF2:  19%|███▉                 | 602/3201 [19:05<1:07:01,  1.55s/batch]Batch 600/3201 Done, mean position loss: 22.329243695735933\n",
      "Training FF2:  18%|███▉                 | 592/3201 [19:05<1:31:38,  2.11s/batch]Batch 600/3201 Done, mean position loss: 22.159607045650482\n",
      "Training FF2:  19%|███▉                 | 594/3201 [19:05<1:28:49,  2.04s/batch]Batch 600/3201 Done, mean position loss: 21.85619448184967\n",
      "Batch 600/3201 Done, mean position loss: 22.00155937433243\n",
      "Training FF2:  19%|███▉                 | 608/3201 [19:07<1:12:04,  1.67s/batch]Batch 600/3201 Done, mean position loss: 21.91232110738754\n",
      "Training FF2:  19%|███▉                 | 602/3201 [19:08<1:10:35,  1.63s/batch]Batch 600/3201 Done, mean position loss: 21.809860289096832\n",
      "Training FF2:  19%|███▉                 | 603/3201 [19:09<1:17:55,  1.80s/batch]Batch 600/3201 Done, mean position loss: 21.21942574739456\n",
      "Training FF2:  19%|███▉                 | 599/3201 [19:09<1:14:02,  1.71s/batch]Batch 600/3201 Done, mean position loss: 21.660050601959227\n",
      "Training FF2:  19%|███▉                 | 598/3201 [19:10<1:51:44,  2.58s/batch]Batch 600/3201 Done, mean position loss: 22.675382730960845\n",
      "Training FF2:  19%|████                 | 612/3201 [19:11<1:16:35,  1.78s/batch]Batch 600/3201 Done, mean position loss: 21.858750863075258\n",
      "Training FF2:  19%|███▉                 | 604/3201 [19:11<1:19:00,  1.83s/batch]Batch 600/3201 Done, mean position loss: 22.767042152881622\n",
      "Training FF2:  19%|███▉                 | 604/3201 [19:12<1:16:25,  1.77s/batch]Batch 600/3201 Done, mean position loss: 21.84656722545624\n",
      "Training FF2:  19%|███▉                 | 608/3201 [19:12<1:18:09,  1.81s/batch]Batch 600/3201 Done, mean position loss: 22.431983346939084\n",
      "Training FF2:  19%|███▉                 | 605/3201 [19:13<1:17:21,  1.79s/batch]Batch 600/3201 Done, mean position loss: 21.723825511932375\n",
      "Training FF2:  19%|████                 | 613/3201 [19:15<1:25:15,  1.98s/batch]Batch 600/3201 Done, mean position loss: 22.785065252780914\n",
      "Training FF2:  19%|███▉                 | 609/3201 [19:14<1:14:10,  1.72s/batch]Batch 600/3201 Done, mean position loss: 21.763556253910064\n",
      "Training FF2:  19%|███▉                 | 608/3201 [19:18<1:14:06,  1.71s/batch]Batch 600/3201 Done, mean position loss: 21.882575759887697\n",
      "Training FF2:  19%|███▉                 | 608/3201 [19:19<1:32:20,  2.14s/batch]Batch 600/3201 Done, mean position loss: 21.898179829120636\n",
      "Training FF2:  19%|███▉                 | 607/3201 [19:18<1:15:48,  1.75s/batch]Batch 600/3201 Done, mean position loss: 22.44843864440918\n",
      "Training FF2:  19%|████                 | 610/3201 [19:22<1:30:20,  2.09s/batch]Batch 600/3201 Done, mean position loss: 22.183527858257293\n",
      "Training FF2:  19%|████                 | 615/3201 [19:25<1:24:44,  1.97s/batch]Batch 600/3201 Done, mean position loss: 22.044016819000245\n",
      "Training FF2:  19%|████                 | 613/3201 [19:28<1:25:32,  1.98s/batch]Batch 600/3201 Done, mean position loss: 22.292541313171384\n",
      "Training FF2:  19%|████                 | 624/3201 [19:35<1:19:42,  1.86s/batch]Batch 600/3201 Done, mean position loss: 22.632474002838133\n",
      "Training FF2:  19%|████                 | 619/3201 [19:34<1:32:31,  2.15s/batch]Batch 600/3201 Done, mean position loss: 21.90048320531845\n",
      "Training FF2:  20%|████                 | 628/3201 [20:07<1:14:41,  1.74s/batch]Batch 600/3201 Done, mean position loss: 22.369817094802855\n",
      "Training FF2:  21%|████▌                | 688/3201 [21:55<1:24:10,  2.01s/batch]Batch 700/3201 Done, mean position loss: 22.23978294610977\n",
      "Training FF2:  22%|████▌                | 689/3201 [21:58<1:33:02,  2.22s/batch]Batch 700/3201 Done, mean position loss: 22.352873733043673\n",
      "Training FF2:  21%|████▍                | 684/3201 [22:01<1:16:20,  1.82s/batch]Batch 700/3201 Done, mean position loss: 21.57372498512268\n",
      "Training FF2:  22%|████▋                | 705/3201 [22:05<1:17:14,  1.86s/batch]Batch 700/3201 Done, mean position loss: 21.90619312286377\n",
      "Training FF2:  22%|████▌                | 704/3201 [22:06<1:11:43,  1.72s/batch]Batch 700/3201 Done, mean position loss: 22.06362873315811\n",
      "Training FF2:  22%|████▌                | 702/3201 [22:06<1:13:31,  1.77s/batch]Batch 700/3201 Done, mean position loss: 21.457171938419343\n",
      "Training FF2:  22%|████▌                | 699/3201 [22:08<1:20:25,  1.93s/batch]Batch 700/3201 Done, mean position loss: 22.311650791168212\n",
      "Training FF2:  21%|████▎                | 664/3201 [22:08<1:14:08,  1.75s/batch]Batch 700/3201 Done, mean position loss: 21.966376321315764\n",
      "Training FF2:  22%|████▌                | 699/3201 [22:08<1:11:44,  1.72s/batch]Batch 700/3201 Done, mean position loss: 21.749346899986268\n",
      "Training FF2:  21%|████▌                | 688/3201 [22:08<1:17:06,  1.84s/batch]Batch 700/3201 Done, mean position loss: 21.657756507396698\n",
      "Training FF2:  22%|████▌                | 701/3201 [22:08<1:13:04,  1.75s/batch]Batch 700/3201 Done, mean position loss: 22.27763383626938\n",
      "Training FF2:  22%|████▌                | 703/3201 [22:10<1:19:33,  1.91s/batch]Batch 700/3201 Done, mean position loss: 21.661751251220704\n",
      "Training FF2:  22%|████▌                | 703/3201 [22:11<1:17:46,  1.87s/batch]Batch 700/3201 Done, mean position loss: 21.72652389526367\n",
      "Training FF2:  21%|████▎                | 666/3201 [22:12<1:14:10,  1.76s/batch]Batch 700/3201 Done, mean position loss: 22.516085147857666\n",
      "Training FF2:  22%|████▌                | 704/3201 [22:12<1:16:46,  1.84s/batch]Batch 700/3201 Done, mean position loss: 21.8232577252388\n",
      "Training FF2:  21%|████▍                | 667/3201 [22:13<1:09:10,  1.64s/batch]Batch 700/3201 Done, mean position loss: 21.343558576107025\n",
      "Training FF2:  22%|████▋                | 709/3201 [22:15<1:29:10,  2.15s/batch]Batch 700/3201 Done, mean position loss: 21.787305476665495\n",
      "Training FF2:  22%|████▋                | 711/3201 [22:14<1:21:40,  1.97s/batch]Batch 700/3201 Done, mean position loss: 21.710332167148593\n",
      "Training FF2:  22%|████▌                | 704/3201 [22:15<1:05:12,  1.57s/batch]Batch 700/3201 Done, mean position loss: 21.909115579128265\n",
      "Training FF2:  22%|█████                  | 704/3201 [22:20<59:47,  1.44s/batch]Batch 700/3201 Done, mean position loss: 21.644136095046996\n",
      "Training FF2:  22%|████▋                | 714/3201 [22:19<1:15:20,  1.82s/batch]Batch 700/3201 Done, mean position loss: 22.34988112449646\n",
      "Training FF2:  22%|████▌                | 696/3201 [22:20<1:16:22,  1.83s/batch]Batch 700/3201 Done, mean position loss: 21.62925871372223\n",
      "Training FF2:  22%|████▌                | 695/3201 [22:21<1:29:48,  2.15s/batch]Batch 700/3201 Done, mean position loss: 22.391429793834686\n",
      "Training FF2:  22%|████▌                | 699/3201 [22:22<1:22:50,  1.99s/batch]Batch 700/3201 Done, mean position loss: 22.421846253871916\n",
      "Training FF2:  21%|████▌                | 688/3201 [22:23<1:24:51,  2.03s/batch]Batch 700/3201 Done, mean position loss: 22.512491238117217\n",
      "Training FF2:  22%|████▌                | 692/3201 [22:23<1:31:15,  2.18s/batch]Batch 700/3201 Done, mean position loss: 21.73591016292572\n",
      "Training FF2:  22%|████▋                | 716/3201 [22:24<1:24:10,  2.03s/batch]Batch 700/3201 Done, mean position loss: 21.582080380916594\n",
      "Training FF2:  22%|████▌                | 692/3201 [22:25<1:26:14,  2.06s/batch]Batch 700/3201 Done, mean position loss: 21.180969166755677\n",
      "Training FF2:  22%|████▌                | 703/3201 [22:26<1:13:03,  1.75s/batch]Batch 700/3201 Done, mean position loss: 21.742047727108\n",
      "Training FF2:  22%|████▋                | 705/3201 [22:27<1:18:52,  1.90s/batch]Batch 700/3201 Done, mean position loss: 21.665531356334686\n",
      "Training FF2:  22%|████▋                | 705/3201 [22:31<1:07:11,  1.62s/batch]Batch 700/3201 Done, mean position loss: 21.48582925319672\n",
      "Training FF2:  22%|████▌                | 699/3201 [22:33<1:15:28,  1.81s/batch]Batch 700/3201 Done, mean position loss: 21.742904136180876\n",
      "Training FF2:  22%|████▋                | 707/3201 [22:33<1:09:30,  1.67s/batch]Batch 700/3201 Done, mean position loss: 21.85718942642212\n",
      "Training FF2:  22%|████▌                | 702/3201 [22:35<1:21:32,  1.96s/batch]Batch 700/3201 Done, mean position loss: 21.771727826595306\n",
      "Training FF2:  22%|████▋                | 710/3201 [22:36<1:17:11,  1.86s/batch]Batch 700/3201 Done, mean position loss: 22.193104701042174\n",
      "Training FF2:  22%|████▋                | 711/3201 [22:38<1:19:20,  1.91s/batch]Batch 700/3201 Done, mean position loss: 21.97626356124878\n",
      "Training FF2:  22%|████▋                | 711/3201 [22:41<1:07:02,  1.62s/batch]Batch 700/3201 Done, mean position loss: 21.61206114768982\n",
      "Training FF2:  22%|████▋                | 716/3201 [22:42<1:14:43,  1.80s/batch]Batch 700/3201 Done, mean position loss: 22.060133457183838\n",
      "Training FF2:  22%|████▋                | 713/3201 [22:46<1:29:25,  2.16s/batch]Batch 700/3201 Done, mean position loss: 22.260945591926575\n",
      "Training FF2:  23%|████▊                | 730/3201 [23:17<1:21:57,  1.99s/batch]Batch 700/3201 Done, mean position loss: 22.194172418117525\n",
      "Training FF2:  25%|█████▏               | 799/3201 [25:05<1:12:06,  1.80s/batch]Batch 800/3201 Done, mean position loss: 22.130130803585054\n",
      "Training FF2:  25%|█████▏               | 796/3201 [25:05<1:18:14,  1.95s/batch]Batch 800/3201 Done, mean position loss: 22.06583574295044\n",
      "Training FF2:  25%|█████▎               | 803/3201 [25:08<1:02:32,  1.56s/batch]Batch 800/3201 Done, mean position loss: 21.41637028694153\n",
      "Training FF2:  24%|█████                | 780/3201 [25:09<1:29:07,  2.21s/batch]Batch 800/3201 Done, mean position loss: 22.141841523647308\n",
      "Training FF2:  25%|█████▏               | 788/3201 [25:10<1:15:32,  1.88s/batch]Batch 800/3201 Done, mean position loss: 22.217009229660036\n",
      "Training FF2:  24%|█████▏               | 782/3201 [25:12<1:06:51,  1.66s/batch]Batch 800/3201 Done, mean position loss: 21.65259034872055\n",
      "Training FF2:  25%|█████▏               | 795/3201 [25:14<1:38:36,  2.46s/batch]Batch 800/3201 Done, mean position loss: 21.850208745002746\n",
      "Training FF2:  25%|█████▏               | 799/3201 [25:14<1:12:53,  1.82s/batch]Batch 800/3201 Done, mean position loss: 21.56538510799408\n",
      "Training FF2:  24%|████▉                | 760/3201 [25:15<1:04:57,  1.60s/batch]Batch 800/3201 Done, mean position loss: 21.833488528728484\n",
      "Training FF2:  25%|█████▏               | 798/3201 [25:17<1:27:57,  2.20s/batch]Batch 800/3201 Done, mean position loss: 21.777602250576017\n",
      "Training FF2:  24%|█████                | 778/3201 [25:18<1:15:34,  1.87s/batch]Batch 800/3201 Done, mean position loss: 21.575261113643645\n",
      "Training FF2:  25%|█████▎               | 804/3201 [25:18<1:10:41,  1.77s/batch]Batch 800/3201 Done, mean position loss: 21.597968974113464\n",
      "Training FF2:  25%|█████▏               | 795/3201 [25:21<1:09:25,  1.73s/batch]Batch 800/3201 Done, mean position loss: 21.586965241432193\n",
      "Training FF2:  25%|█████▋                 | 797/3201 [25:22<56:07,  1.40s/batch]Batch 800/3201 Done, mean position loss: 21.7653622174263\n",
      "Training FF2:  25%|█████▏               | 789/3201 [25:24<1:15:07,  1.87s/batch]Batch 800/3201 Done, mean position loss: 21.445491423606875\n",
      "Training FF2:  25%|█████▏               | 796/3201 [25:24<1:11:20,  1.78s/batch]Batch 800/3201 Done, mean position loss: 21.60814476251602\n",
      "Training FF2:  25%|█████▏               | 789/3201 [25:26<1:32:57,  2.31s/batch]Batch 800/3201 Done, mean position loss: 22.37789218664169\n",
      "Training FF2:  25%|█████▎               | 812/3201 [25:27<1:23:40,  2.10s/batch]Batch 800/3201 Done, mean position loss: 21.81160229682922\n",
      "Training FF2:  25%|█████▎               | 801/3201 [25:27<1:38:08,  2.45s/batch]Batch 800/3201 Done, mean position loss: 21.641273338794708\n",
      "Training FF2:  25%|█████▎               | 808/3201 [25:28<1:14:31,  1.87s/batch]Batch 800/3201 Done, mean position loss: 22.261120331287383\n",
      "Training FF2:  25%|█████▏               | 798/3201 [25:28<1:19:40,  1.99s/batch]Batch 800/3201 Done, mean position loss: 21.47486925840378\n",
      "Training FF2:  25%|█████▎               | 814/3201 [25:29<1:13:01,  1.84s/batch]Batch 800/3201 Done, mean position loss: 22.314419679641723\n",
      "Training FF2:  25%|█████▏               | 791/3201 [25:30<1:17:42,  1.93s/batch]Batch 800/3201 Done, mean position loss: 21.207362179756167\n",
      "Training FF2:  25%|█████▏               | 797/3201 [25:29<1:20:17,  2.00s/batch]Batch 800/3201 Done, mean position loss: 21.643348813056946\n",
      "Training FF2:  25%|█████▎               | 802/3201 [25:32<1:07:55,  1.70s/batch]Batch 800/3201 Done, mean position loss: 22.23507853746414\n",
      "Training FF2:  24%|█████                | 772/3201 [25:34<1:16:36,  1.89s/batch]Batch 800/3201 Done, mean position loss: 21.17873297929764\n",
      "Training FF2:  25%|█████▎               | 812/3201 [25:34<1:14:52,  1.88s/batch]Batch 800/3201 Done, mean position loss: 21.58103103876114\n",
      "Training FF2:  25%|█████▎               | 801/3201 [25:34<1:26:35,  2.16s/batch]Batch 800/3201 Done, mean position loss: 22.260560762882236\n",
      "Training FF2:  25%|█████▏               | 795/3201 [25:35<1:11:54,  1.79s/batch]Batch 800/3201 Done, mean position loss: 21.65726606607437\n",
      "Training FF2:  25%|█████▎               | 812/3201 [25:39<1:34:40,  2.38s/batch]Batch 800/3201 Done, mean position loss: 21.602079801559448\n",
      "Training FF2:  26%|█████▎               | 817/3201 [25:41<1:05:10,  1.64s/batch]Batch 800/3201 Done, mean position loss: 21.6882359957695\n",
      "Training FF2:  25%|█████▎               | 810/3201 [25:43<1:12:57,  1.83s/batch]Batch 800/3201 Done, mean position loss: 21.649812376499177\n",
      "Training FF2:  25%|█████▎               | 804/3201 [25:46<1:06:03,  1.65s/batch]Batch 800/3201 Done, mean position loss: 21.840840394496915\n",
      "Training FF2:  25%|█████▎               | 810/3201 [25:49<1:17:22,  1.94s/batch]Batch 800/3201 Done, mean position loss: 21.489615662097933\n",
      "Batch 800/3201 Done, mean position loss: 21.31254173517227\n",
      "Training FF2:  25%|█████▎               | 814/3201 [25:49<1:16:03,  1.91s/batch]Batch 800/3201 Done, mean position loss: 21.854260122776033\n",
      "Training FF2:  25%|█████▎               | 810/3201 [25:51<1:24:43,  2.13s/batch]Batch 800/3201 Done, mean position loss: 21.720172419548035\n",
      "Batch 800/3201 Done, mean position loss: 21.987932703495026\n",
      "Training FF2:  25%|█████▊                 | 811/3201 [26:05<57:51,  1.45s/batch]Batch 800/3201 Done, mean position loss: 22.072277550697326\n",
      "Training FF2:  26%|█████▍               | 834/3201 [26:30<1:14:05,  1.88s/batch]Batch 800/3201 Done, mean position loss: 22.04519021511078\n",
      "Training FF2:  28%|█████▊               | 884/3201 [28:06<1:27:58,  2.28s/batch]Batch 900/3201 Done, mean position loss: 22.02858107328415\n",
      "Training FF2:  28%|█████▉               | 904/3201 [28:11<1:11:00,  1.85s/batch]Batch 900/3201 Done, mean position loss: 21.315132763385773\n",
      "Training FF2:  28%|█████▉               | 900/3201 [28:14<1:10:17,  1.83s/batch]Batch 900/3201 Done, mean position loss: 21.963855900764464\n",
      "Training FF2:  28%|█████▊               | 892/3201 [28:15<1:11:06,  1.85s/batch]Batch 900/3201 Done, mean position loss: 21.937148993015292\n",
      "Training FF2:  28%|██████▍                | 903/3201 [28:16<59:17,  1.55s/batch]Batch 900/3201 Done, mean position loss: 22.183572309017183\n",
      "Training FF2:  28%|█████▉               | 900/3201 [28:20<1:18:01,  2.03s/batch]Batch 900/3201 Done, mean position loss: 21.46236157178879\n",
      "Training FF2:  28%|█████▊               | 894/3201 [28:22<1:08:05,  1.77s/batch]Batch 900/3201 Done, mean position loss: 21.709090626239778\n",
      "Training FF2:  28%|█████▉               | 896/3201 [28:23<1:09:10,  1.80s/batch]Batch 900/3201 Done, mean position loss: 21.568276538848878\n",
      "Training FF2:  28%|██████▎                | 881/3201 [28:27<58:11,  1.51s/batch]Batch 900/3201 Done, mean position loss: 21.58066491127014\n",
      "Training FF2:  28%|█████▉               | 897/3201 [28:31<1:08:17,  1.78s/batch]Batch 900/3201 Done, mean position loss: 21.427711505889892\n",
      "Training FF2:  28%|█████▉               | 899/3201 [28:31<1:15:11,  1.96s/batch]Batch 900/3201 Done, mean position loss: 21.71036146402359\n",
      "Training FF2:  27%|█████▋               | 862/3201 [28:33<1:12:38,  1.86s/batch]Batch 900/3201 Done, mean position loss: 21.666196520328523\n",
      "Training FF2:  28%|█████▉               | 898/3201 [28:33<1:04:15,  1.67s/batch]Batch 900/3201 Done, mean position loss: 21.52738189458847\n",
      "Training FF2:  28%|█████▉               | 902/3201 [28:33<1:04:39,  1.69s/batch]Batch 900/3201 Done, mean position loss: 21.717942056655886\n",
      "Training FF2:  28%|█████▊               | 892/3201 [28:34<1:02:59,  1.64s/batch]Batch 900/3201 Done, mean position loss: 21.430178973674774\n",
      "Training FF2:  28%|█████▊               | 886/3201 [28:35<1:14:26,  1.93s/batch]Batch 900/3201 Done, mean position loss: 21.594501390457154\n",
      "Training FF2:  28%|█████▊               | 887/3201 [28:38<1:03:39,  1.65s/batch]Batch 900/3201 Done, mean position loss: 21.12549247264862\n",
      "Training FF2:  28%|█████▉               | 905/3201 [28:38<1:12:07,  1.88s/batch]Batch 900/3201 Done, mean position loss: 21.442316179275515\n",
      "Training FF2:  28%|█████▉               | 910/3201 [28:38<1:14:38,  1.95s/batch]Batch 900/3201 Done, mean position loss: 22.241268661022186\n",
      "Training FF2:  28%|█████▉               | 901/3201 [28:37<1:14:45,  1.95s/batch]Batch 900/3201 Done, mean position loss: 22.209233405590055\n",
      "Training FF2:  28%|█████▉               | 900/3201 [28:38<1:17:02,  2.01s/batch]Batch 900/3201 Done, mean position loss: 22.21599769115448\n",
      "Training FF2:  28%|█████▊               | 895/3201 [28:39<1:06:45,  1.74s/batch]Batch 900/3201 Done, mean position loss: 21.376606600284575\n",
      "Training FF2:  28%|█████▉               | 911/3201 [28:40<1:16:55,  2.02s/batch]Batch 900/3201 Done, mean position loss: 21.71509969711304\n",
      "Training FF2:  28%|█████▊               | 894/3201 [28:41<1:20:56,  2.11s/batch]Batch 900/3201 Done, mean position loss: 22.12731828927994\n",
      "Training FF2:  29%|██████               | 916/3201 [28:41<1:22:04,  2.16s/batch]Batch 900/3201 Done, mean position loss: 21.544989306926727\n",
      "Training FF2:  28%|█████▉               | 896/3201 [28:41<1:11:18,  1.86s/batch]Batch 900/3201 Done, mean position loss: 22.213935477733614\n",
      "Training FF2:  28%|█████▉               | 904/3201 [28:43<1:04:42,  1.69s/batch]Batch 900/3201 Done, mean position loss: 21.592997450828552\n",
      "Training FF2:  28%|█████▊               | 893/3201 [28:49<1:16:52,  2.00s/batch]Batch 900/3201 Done, mean position loss: 21.57683322906494\n",
      "Training FF2:  28%|█████▉               | 902/3201 [28:51<1:04:04,  1.67s/batch]Batch 900/3201 Done, mean position loss: 21.70765290737152\n",
      "Training FF2:  29%|██████               | 925/3201 [28:51<1:08:58,  1.82s/batch]Batch 900/3201 Done, mean position loss: 21.208503489494323\n",
      "Training FF2:  29%|█████▉               | 914/3201 [28:51<1:12:41,  1.91s/batch]Batch 900/3201 Done, mean position loss: 21.59700686454773\n",
      "Training FF2:  29%|█████▉               | 914/3201 [28:54<1:05:04,  1.71s/batch]Batch 900/3201 Done, mean position loss: 21.550481719970705\n",
      "Training FF2:  28%|█████▉               | 909/3201 [28:55<1:15:08,  1.97s/batch]Batch 900/3201 Done, mean position loss: 21.50354208469391\n",
      "Training FF2:  29%|██████               | 929/3201 [28:58<1:12:36,  1.92s/batch]Batch 900/3201 Done, mean position loss: 21.160474495887755\n",
      "Training FF2:  29%|█████▉               | 914/3201 [29:02<1:05:50,  1.73s/batch]Batch 900/3201 Done, mean position loss: 21.43242402791977\n",
      "Training FF2:  29%|██████▌                | 915/3201 [29:03<58:32,  1.54s/batch]Batch 900/3201 Done, mean position loss: 21.66546800136566\n",
      "Training FF2:  29%|█████▉               | 914/3201 [29:08<1:10:03,  1.84s/batch]Batch 900/3201 Done, mean position loss: 21.659153079986574\n",
      "Training FF2:  29%|██████               | 924/3201 [29:10<1:11:28,  1.88s/batch]Batch 900/3201 Done, mean position loss: 21.96879646539688\n",
      "Training FF2:  29%|██████               | 920/3201 [29:12<1:14:31,  1.96s/batch]Batch 900/3201 Done, mean position loss: 21.832009539604186\n",
      "Training FF2:  30%|██████▏              | 951/3201 [29:44<1:10:57,  1.89s/batch]Batch 900/3201 Done, mean position loss: 21.899373495578764\n",
      "Training FF2:  30%|██████▎              | 964/3201 [31:08<1:06:59,  1.80s/batch]Batch 1000/3201 Done, mean position loss: 21.978209807872773\n",
      "Training FF2:  31%|██████▌              | 997/3201 [31:10<1:12:47,  1.98s/batch]Batch 1000/3201 Done, mean position loss: 21.804746239185334\n",
      "Training FF2:  30%|██████▎              | 970/3201 [31:15<1:12:23,  1.95s/batch]Batch 1000/3201 Done, mean position loss: 22.12318986415863\n",
      "Training FF2:  31%|██████▌              | 992/3201 [31:17<1:07:48,  1.84s/batch]Batch 1000/3201 Done, mean position loss: 21.21099838733673\n",
      "Training FF2:  31%|██████▌              | 997/3201 [31:19<1:08:45,  1.87s/batch]Batch 1000/3201 Done, mean position loss: 21.354020726680755\n",
      "Training FF2:  31%|██████▌              | 994/3201 [31:20<1:03:01,  1.71s/batch]Batch 1000/3201 Done, mean position loss: 21.823901262283325\n",
      "Training FF2:  31%|███████                | 983/3201 [31:23<59:57,  1.62s/batch]Batch 1000/3201 Done, mean position loss: 21.587619323730465\n",
      "Training FF2:  31%|██████▌              | 998/3201 [31:26<1:22:31,  2.25s/batch]Batch 1000/3201 Done, mean position loss: 21.624958658218382\n",
      "Training FF2:  31%|██████▌              | 996/3201 [31:28<1:02:25,  1.70s/batch]Batch 1000/3201 Done, mean position loss: 21.461366388797757\n",
      "Training FF2:  31%|██████▌              | 992/3201 [31:30<1:08:54,  1.87s/batch]Batch 1000/3201 Done, mean position loss: 21.516285002231598\n",
      "Training FF2:  31%|██████▎             | 1008/3201 [31:32<1:04:39,  1.77s/batch]Batch 1000/3201 Done, mean position loss: 21.6045769572258\n",
      "Training FF2:  31%|██████▌              | 998/3201 [31:34<1:12:25,  1.97s/batch]Batch 1000/3201 Done, mean position loss: 21.347373299598694\n",
      "Training FF2:  31%|██████▌              | 994/3201 [31:34<1:12:36,  1.97s/batch]Batch 1000/3201 Done, mean position loss: 21.578490099906922\n",
      "Training FF2:  32%|██████▎             | 1010/3201 [31:35<1:03:33,  1.74s/batch]Batch 1000/3201 Done, mean position loss: 22.06501805782318\n",
      "Training FF2:  31%|██████▍              | 978/3201 [31:36<1:07:31,  1.82s/batch]Batch 1000/3201 Done, mean position loss: 21.30333081960678\n",
      "Training FF2:  31%|██████▎             | 1005/3201 [31:36<1:00:19,  1.65s/batch]Batch 1000/3201 Done, mean position loss: 21.667825405597686\n",
      "Training FF2:  31%|██████▌              | 993/3201 [31:36<1:12:26,  1.97s/batch]Batch 1000/3201 Done, mean position loss: 21.632818593978882\n",
      "Training FF2:  31%|███████▏               | 999/3201 [31:38<58:46,  1.60s/batch]Batch 1000/3201 Done, mean position loss: 22.115351207256317\n",
      "Batch 1000/3201 Done, mean position loss: 21.659139342308045\n",
      "Training FF2:  31%|██████▍              | 984/3201 [31:38<1:07:01,  1.81s/batch]Batch 1000/3201 Done, mean position loss: 22.160948448181152\n",
      "Training FF2:  32%|██████▎             | 1013/3201 [31:38<1:10:42,  1.94s/batch]Batch 1000/3201 Done, mean position loss: 21.082063274383543\n",
      "Training FF2:  31%|██████▍              | 986/3201 [31:42<1:13:21,  1.99s/batch]Batch 1000/3201 Done, mean position loss: 22.065798754692075\n",
      "Training FF2:  32%|███████               | 1019/3201 [31:42<57:49,  1.59s/batch]Batch 1000/3201 Done, mean position loss: 21.420060970783233\n",
      "Training FF2:  31%|██████▌              | 996/3201 [31:43<1:23:01,  2.26s/batch]Batch 1000/3201 Done, mean position loss: 22.18440500497818\n",
      "Training FF2:  31%|██████▎             | 1005/3201 [31:45<1:07:29,  1.84s/batch]Batch 1000/3201 Done, mean position loss: 21.42576646327972\n",
      "Training FF2:  32%|██████▎             | 1012/3201 [31:47<1:13:02,  2.00s/batch]Batch 1000/3201 Done, mean position loss: 21.328963072299956\n",
      "Training FF2:  31%|██████▎             | 1006/3201 [31:47<1:08:28,  1.87s/batch]Batch 1000/3201 Done, mean position loss: 21.51273079395294\n",
      "Training FF2:  32%|██████▉               | 1011/3201 [31:48<55:24,  1.52s/batch]Batch 1000/3201 Done, mean position loss: 21.538777427673338\n",
      "Training FF2:  31%|██████▍              | 988/3201 [31:50<1:19:32,  2.16s/batch]Batch 1000/3201 Done, mean position loss: 21.551282019615172\n",
      "Training FF2:  31%|██████▎             | 1004/3201 [31:51<1:06:47,  1.82s/batch]Batch 1000/3201 Done, mean position loss: 21.650895168781283\n",
      "Training FF2:  31%|██████▎             | 1007/3201 [31:55<1:03:04,  1.73s/batch]Batch 1000/3201 Done, mean position loss: 21.551806206703187\n",
      "Training FF2:  31%|██████▎             | 1008/3201 [31:57<1:13:58,  2.02s/batch]Batch 1000/3201 Done, mean position loss: 21.49250538110733\n",
      "Training FF2:  32%|██████▎             | 1015/3201 [31:57<1:20:19,  2.20s/batch]Batch 1000/3201 Done, mean position loss: 21.194772357940675\n",
      "Training FF2:  32%|██████▎             | 1011/3201 [32:04<1:01:42,  1.69s/batch]Batch 1000/3201 Done, mean position loss: 21.1717818236351\n",
      "Training FF2:  32%|██████▎             | 1016/3201 [32:08<1:06:38,  1.83s/batch]Batch 1000/3201 Done, mean position loss: 21.583308646678923\n",
      "Training FF2:  32%|██████▎             | 1012/3201 [32:09<1:09:31,  1.91s/batch]Batch 1000/3201 Done, mean position loss: 21.38834191083908\n",
      "Training FF2:  32%|██████▍             | 1030/3201 [32:11<1:09:31,  1.92s/batch]Batch 1000/3201 Done, mean position loss: 21.87227950334549\n",
      "Training FF2:  32%|██████▎             | 1015/3201 [32:15<1:09:41,  1.91s/batch]Batch 1000/3201 Done, mean position loss: 21.6797136592865\n",
      "Training FF2:  32%|██████▍             | 1035/3201 [32:20<1:02:26,  1.73s/batch]Batch 1000/3201 Done, mean position loss: 21.574283819198605\n",
      "Training FF2:  32%|██████▍             | 1039/3201 [32:38<1:07:55,  1.89s/batch]Batch 1000/3201 Done, mean position loss: 21.778431658744815\n",
      "Training FF2:  33%|██████▋             | 1068/3201 [34:13<1:21:01,  2.28s/batch]Batch 1100/3201 Done, mean position loss: 21.948713936805724\n",
      "Training FF2:  34%|██████▊             | 1083/3201 [34:16<1:11:00,  2.01s/batch]Batch 1100/3201 Done, mean position loss: 21.71355816602707\n",
      "Training FF2:  33%|██████▋             | 1066/3201 [34:18<1:06:29,  1.87s/batch]Batch 1100/3201 Done, mean position loss: 22.13715968847275\n",
      "Training FF2:  34%|██████▊             | 1094/3201 [34:25<1:06:06,  1.88s/batch]Batch 1100/3201 Done, mean position loss: 21.312672908306123\n",
      "Training FF2:  34%|██████▊             | 1085/3201 [34:27<1:14:07,  2.10s/batch]Batch 1100/3201 Done, mean position loss: 21.588160297870637\n",
      "Training FF2:  34%|███████▌              | 1097/3201 [34:28<57:21,  1.64s/batch]Batch 1100/3201 Done, mean position loss: 21.148002030849458\n",
      "Training FF2:  34%|██████▊             | 1095/3201 [34:34<1:12:34,  2.07s/batch]Batch 1100/3201 Done, mean position loss: 21.56288893938065\n",
      "Training FF2:  34%|██████▊             | 1089/3201 [34:33<1:14:58,  2.13s/batch]Batch 1100/3201 Done, mean position loss: 21.709360272884368\n",
      "Training FF2:  34%|██████▊             | 1100/3201 [34:34<1:15:38,  2.16s/batch]Batch 1100/3201 Done, mean position loss: 21.591157765388488\n",
      "Training FF2:  34%|██████▊             | 1096/3201 [34:35<1:08:25,  1.95s/batch]Batch 1100/3201 Done, mean position loss: 21.402382063865662\n",
      "Training FF2:  34%|██████▊             | 1088/3201 [34:36<1:12:03,  2.05s/batch]Batch 1100/3201 Done, mean position loss: 21.22440116405487\n",
      "Training FF2:  34%|██████▊             | 1100/3201 [34:37<1:15:26,  2.15s/batch]Batch 1100/3201 Done, mean position loss: 21.96886906147003\n",
      "Training FF2:  33%|██████▋             | 1062/3201 [34:37<1:10:41,  1.98s/batch]Batch 1100/3201 Done, mean position loss: 21.51892942428589\n",
      "Training FF2:  34%|███████▌              | 1102/3201 [34:37<54:00,  1.54s/batch]Batch 1100/3201 Done, mean position loss: 21.473303821086883\n",
      "Training FF2:  34%|██████▊             | 1094/3201 [34:39<1:03:28,  1.81s/batch]Batch 1100/3201 Done, mean position loss: 22.10805315256119\n",
      "Training FF2:  35%|██████▉             | 1105/3201 [34:41<1:08:17,  1.96s/batch]Batch 1100/3201 Done, mean position loss: 21.549435584545137\n",
      "Training FF2:  34%|██████▉             | 1101/3201 [34:41<1:07:21,  1.92s/batch]Batch 1100/3201 Done, mean position loss: 21.258979794979098\n",
      "Training FF2:  35%|██████▉             | 1109/3201 [34:43<1:20:19,  2.30s/batch]Batch 1100/3201 Done, mean position loss: 21.658114922046664\n",
      "Training FF2:  34%|███████▌              | 1096/3201 [34:46<58:20,  1.66s/batch]Batch 1100/3201 Done, mean position loss: 21.583525145053866\n",
      "Training FF2:  35%|███████▌              | 1108/3201 [34:46<59:06,  1.69s/batch]Batch 1100/3201 Done, mean position loss: 21.085315299034118\n",
      "Training FF2:  34%|███████▌              | 1098/3201 [34:46<59:35,  1.70s/batch]Batch 1100/3201 Done, mean position loss: 21.330233035087588\n",
      "Training FF2:  35%|██████▉             | 1110/3201 [34:51<1:13:23,  2.11s/batch]Batch 1100/3201 Done, mean position loss: 22.002846615314485\n",
      "Training FF2:  35%|██████▉             | 1108/3201 [34:52<1:26:11,  2.47s/batch]Batch 1100/3201 Done, mean position loss: 22.002906694412232\n",
      "Training FF2:  34%|██████▊             | 1096/3201 [34:54<1:14:09,  2.11s/batch]Batch 1100/3201 Done, mean position loss: 21.36832948923111\n",
      "Training FF2:  34%|██████▊             | 1098/3201 [34:54<1:08:52,  1.97s/batch]Batch 1100/3201 Done, mean position loss: 21.273354635238647\n",
      "Training FF2:  34%|██████▊             | 1100/3201 [34:56<1:10:50,  2.02s/batch]Batch 1100/3201 Done, mean position loss: 21.490877635478974\n",
      "Training FF2:  34%|██████▊             | 1095/3201 [34:57<1:14:58,  2.14s/batch]Batch 1100/3201 Done, mean position loss: 22.177261197566985\n",
      "Training FF2:  34%|██████▊             | 1096/3201 [34:58<1:00:37,  1.73s/batch]Batch 1100/3201 Done, mean position loss: 21.416514468193057\n",
      "Training FF2:  34%|██████▊             | 1097/3201 [35:00<1:08:52,  1.96s/batch]Batch 1100/3201 Done, mean position loss: 21.622785651683806\n",
      "Training FF2:  34%|██████▊             | 1099/3201 [35:00<1:13:16,  2.09s/batch]Batch 1100/3201 Done, mean position loss: 21.41066171646118\n",
      "Training FF2:  35%|██████▉             | 1120/3201 [35:02<1:03:25,  1.83s/batch]Batch 1100/3201 Done, mean position loss: 21.506310963630675\n",
      "Training FF2:  35%|██████▉             | 1117/3201 [35:05<1:17:43,  2.24s/batch]Batch 1100/3201 Done, mean position loss: 21.51938323259354\n",
      "Training FF2:  35%|██████▉             | 1108/3201 [35:08<1:08:35,  1.97s/batch]Batch 1100/3201 Done, mean position loss: 21.172027733325958\n",
      "Training FF2:  35%|██████▉             | 1117/3201 [35:08<1:09:09,  1.99s/batch]Batch 1100/3201 Done, mean position loss: 21.132638342380524\n",
      "Training FF2:  35%|███████             | 1134/3201 [35:18<1:07:03,  1.95s/batch]Batch 1100/3201 Done, mean position loss: 21.3855535531044\n",
      "Training FF2:  35%|███████▋              | 1111/3201 [35:18<57:33,  1.65s/batch]Batch 1100/3201 Done, mean position loss: 21.810959262847902\n",
      "Training FF2:  35%|███████             | 1122/3201 [35:20<1:15:40,  2.18s/batch]Batch 1100/3201 Done, mean position loss: 21.489135303497314\n",
      "Training FF2:  35%|██████▉             | 1113/3201 [35:31<1:16:10,  2.19s/batch]Batch 1100/3201 Done, mean position loss: 21.556122047901155\n",
      "Training FF2:  35%|███████             | 1128/3201 [35:32<1:10:30,  2.04s/batch]Batch 1100/3201 Done, mean position loss: 21.538220281600953\n",
      "Training FF2:  36%|███████▏            | 1144/3201 [35:49<1:04:05,  1.87s/batch]Batch 1100/3201 Done, mean position loss: 21.70194200515747\n",
      "Training FF2:  37%|███████▎            | 1178/3201 [37:22<1:13:10,  2.17s/batch]Batch 1200/3201 Done, mean position loss: 21.868909962177277\n",
      "Training FF2:  37%|███████▍            | 1192/3201 [37:27<1:03:24,  1.89s/batch]Batch 1200/3201 Done, mean position loss: 21.64648283004761\n",
      "Training FF2:  37%|███████▍            | 1194/3201 [37:27<1:00:45,  1.82s/batch]Batch 1200/3201 Done, mean position loss: 22.133514544963838\n",
      "Training FF2:  38%|███████▌            | 1207/3201 [37:33<1:00:17,  1.81s/batch]Batch 1200/3201 Done, mean position loss: 21.59943233251572\n",
      "Training FF2:  37%|███████▍            | 1195/3201 [37:37<1:12:10,  2.16s/batch]Batch 1200/3201 Done, mean position loss: 21.09338361501694\n",
      "Training FF2:  37%|███████▎            | 1175/3201 [37:40<1:14:53,  2.22s/batch]Batch 1200/3201 Done, mean position loss: 21.57186590194702\n",
      "Training FF2:  37%|███████▍            | 1188/3201 [37:41<1:13:12,  2.18s/batch]Batch 1200/3201 Done, mean position loss: 21.246849992275237\n",
      "Training FF2:  37%|███████▍            | 1194/3201 [37:42<1:07:54,  2.03s/batch]Batch 1200/3201 Done, mean position loss: 21.505953776836396\n",
      "Training FF2:  37%|████████▏             | 1196/3201 [37:45<59:03,  1.77s/batch]Batch 1200/3201 Done, mean position loss: 21.96679150104523\n",
      "Training FF2:  37%|███████▍            | 1187/3201 [37:46<1:04:07,  1.91s/batch]Batch 1200/3201 Done, mean position loss: 21.15729093313217\n",
      "Training FF2:  37%|████████▏             | 1193/3201 [37:47<58:00,  1.73s/batch]Batch 1200/3201 Done, mean position loss: 22.14836520433426\n",
      "Training FF2:  37%|███████▍            | 1188/3201 [37:49<1:08:18,  2.04s/batch]Batch 1200/3201 Done, mean position loss: 21.194201760292053\n",
      "Training FF2:  38%|████████▎             | 1202/3201 [37:50<51:00,  1.53s/batch]Batch 1200/3201 Done, mean position loss: 21.666038987636565\n",
      "Training FF2:  37%|███████▍            | 1190/3201 [37:49<1:06:07,  1.97s/batch]Batch 1200/3201 Done, mean position loss: 21.505055482387544\n",
      "Training FF2:  37%|████████▏             | 1188/3201 [37:50<55:30,  1.65s/batch]Batch 1200/3201 Done, mean position loss: 21.365226488113404\n",
      "Training FF2:  36%|████████              | 1166/3201 [37:54<58:16,  1.72s/batch]Batch 1200/3201 Done, mean position loss: 21.486151742935178\n",
      "Training FF2:  38%|███████▌            | 1210/3201 [37:55<1:03:23,  1.91s/batch]Batch 1200/3201 Done, mean position loss: 21.44508043050766\n",
      "Training FF2:  36%|███████▎            | 1168/3201 [37:57<1:01:10,  1.81s/batch]Batch 1200/3201 Done, mean position loss: 21.36155870437622\n",
      "Training FF2:  37%|███████▍            | 1192/3201 [37:58<1:05:35,  1.96s/batch]Batch 1200/3201 Done, mean position loss: 21.29801424264908\n",
      "Training FF2:  37%|███████▍            | 1184/3201 [37:58<1:05:04,  1.94s/batch]Batch 1200/3201 Done, mean position loss: 21.617775828838347\n",
      "Training FF2:  37%|███████▍            | 1194/3201 [38:01<1:00:07,  1.80s/batch]Batch 1200/3201 Done, mean position loss: 21.058435144424436\n",
      "Training FF2:  38%|███████▌            | 1219/3201 [38:03<1:10:27,  2.13s/batch]Batch 1200/3201 Done, mean position loss: 21.348619456291196\n",
      "Training FF2:  37%|████████▏             | 1200/3201 [38:04<56:43,  1.70s/batch]Batch 1200/3201 Done, mean position loss: 21.538570227622984\n",
      "Training FF2:  37%|███████▍            | 1188/3201 [38:08<1:18:02,  2.33s/batch]Batch 1200/3201 Done, mean position loss: 21.239656200408938\n",
      "Training FF2:  37%|███████▎            | 1173/3201 [38:08<1:16:32,  2.26s/batch]Batch 1200/3201 Done, mean position loss: 21.925771009922027\n",
      "Training FF2:  38%|███████▌            | 1202/3201 [38:09<1:06:45,  2.00s/batch]Batch 1200/3201 Done, mean position loss: 21.45478054046631\n",
      "Training FF2:  38%|███████▌            | 1204/3201 [38:11<1:05:49,  1.98s/batch]Batch 1200/3201 Done, mean position loss: 21.605621373653413\n",
      "Training FF2:  38%|████████▎             | 1218/3201 [38:11<56:08,  1.70s/batch]Batch 1200/3201 Done, mean position loss: 22.175153059959413\n",
      "Training FF2:  38%|████████▎             | 1211/3201 [38:11<51:44,  1.56s/batch]Batch 1200/3201 Done, mean position loss: 21.958993010520935\n",
      "Training FF2:  38%|███████▌            | 1202/3201 [38:13<1:01:01,  1.83s/batch]Batch 1200/3201 Done, mean position loss: 21.474935970306397\n",
      "Training FF2:  37%|███████▎            | 1176/3201 [38:13<1:04:22,  1.91s/batch]Batch 1200/3201 Done, mean position loss: 21.370356638431552\n",
      "Training FF2:  38%|███████▌            | 1212/3201 [38:19<1:08:55,  2.08s/batch]Batch 1200/3201 Done, mean position loss: 21.492640917301177\n",
      "Training FF2:  38%|████████▍             | 1231/3201 [38:23<53:33,  1.63s/batch]Batch 1200/3201 Done, mean position loss: 21.1657674741745\n",
      "Training FF2:  38%|███████▌            | 1217/3201 [38:23<1:04:51,  1.96s/batch]Batch 1200/3201 Done, mean position loss: 21.150011599063873\n",
      "Training FF2:  38%|████████▍             | 1227/3201 [38:27<57:25,  1.75s/batch]Batch 1200/3201 Done, mean position loss: 21.4545135307312\n",
      "Training FF2:  38%|███████▋            | 1223/3201 [38:31<1:02:31,  1.90s/batch]Batch 1200/3201 Done, mean position loss: 21.770821862220764\n",
      "Training FF2:  38%|████████▎             | 1216/3201 [38:38<59:29,  1.80s/batch]Batch 1200/3201 Done, mean position loss: 21.411866140365603\n",
      "Training FF2:  38%|███████▌            | 1220/3201 [38:43<1:02:02,  1.88s/batch]Batch 1200/3201 Done, mean position loss: 21.465950264930726\n",
      "Training FF2:  38%|███████▋            | 1221/3201 [38:45<1:00:14,  1.83s/batch]Batch 1200/3201 Done, mean position loss: 21.531555786132813\n",
      "Training FF2:  38%|███████▋            | 1230/3201 [39:02<1:01:09,  1.86s/batch]Batch 1200/3201 Done, mean position loss: 21.658164899349213\n",
      "Training FF2:  40%|████████▊             | 1291/3201 [40:34<53:04,  1.67s/batch]Batch 1300/3201 Done, mean position loss: 21.844449286460875\n",
      "Training FF2:  40%|████████▊             | 1277/3201 [40:39<51:04,  1.59s/batch]Batch 1300/3201 Done, mean position loss: 21.560870921611787\n",
      "Training FF2:  40%|███████▉            | 1276/3201 [40:41<1:12:50,  2.27s/batch]Batch 1300/3201 Done, mean position loss: 22.07212460041046\n",
      "Training FF2:  40%|████████            | 1285/3201 [40:46<1:03:43,  2.00s/batch]Batch 1300/3201 Done, mean position loss: 21.527785947322847\n",
      "Training FF2:  41%|████████▉             | 1309/3201 [40:48<57:00,  1.81s/batch]Batch 1300/3201 Done, mean position loss: 21.588224720954898\n",
      "Training FF2:  40%|████████▊             | 1284/3201 [40:51<57:10,  1.79s/batch]Batch 1300/3201 Done, mean position loss: 21.07192676305771\n",
      "Training FF2:  40%|████████▊             | 1277/3201 [40:55<49:07,  1.53s/batch]Batch 1300/3201 Done, mean position loss: 22.13141040086746\n",
      "Training FF2:  40%|████████            | 1295/3201 [40:55<1:01:38,  1.94s/batch]Batch 1300/3201 Done, mean position loss: 21.199880027770995\n",
      "Training FF2:  40%|████████            | 1289/3201 [40:59<1:12:04,  2.26s/batch]Batch 1300/3201 Done, mean position loss: 21.091505925655365\n",
      "Batch 1300/3201 Done, mean position loss: 21.30797809123993\n",
      "Training FF2:  40%|████████            | 1292/3201 [41:00<1:02:00,  1.95s/batch]Batch 1300/3201 Done, mean position loss: 21.93409022808075\n",
      "Training FF2:  41%|████████            | 1297/3201 [41:01<1:02:55,  1.98s/batch]Batch 1300/3201 Done, mean position loss: 21.12894681215286\n",
      "Batch 1300/3201 Done, mean position loss: 21.482735722064973\n",
      "Training FF2:  41%|████████▏           | 1310/3201 [41:03<1:05:02,  2.06s/batch]Batch 1300/3201 Done, mean position loss: 21.32828889131546\n",
      "Training FF2:  40%|████████▊             | 1291/3201 [41:04<56:53,  1.79s/batch]Batch 1300/3201 Done, mean position loss: 21.58846012830734\n",
      "Training FF2:  41%|████████▉             | 1304/3201 [41:05<51:02,  1.61s/batch]Batch 1300/3201 Done, mean position loss: 21.48006671667099\n",
      "Training FF2:  41%|████████▏           | 1304/3201 [41:06<1:09:00,  2.18s/batch]Batch 1300/3201 Done, mean position loss: 21.57160610437393\n",
      "Training FF2:  41%|█████████             | 1312/3201 [41:06<57:17,  1.82s/batch]Batch 1300/3201 Done, mean position loss: 21.276700258255005\n",
      "Training FF2:  40%|████████▉             | 1295/3201 [41:08<59:26,  1.87s/batch]Batch 1300/3201 Done, mean position loss: 21.416376836299897\n",
      "Training FF2:  41%|████████▉             | 1304/3201 [41:09<55:54,  1.77s/batch]Batch 1300/3201 Done, mean position loss: 21.430709331035615\n",
      "Training FF2:  40%|████████            | 1288/3201 [41:13<1:05:47,  2.06s/batch]Batch 1300/3201 Done, mean position loss: 21.090018265247345\n",
      "Training FF2:  41%|████████            | 1299/3201 [41:16<1:04:04,  2.02s/batch]Batch 1300/3201 Done, mean position loss: 21.49267087697983\n",
      "Training FF2:  40%|████████            | 1290/3201 [41:17<1:09:20,  2.18s/batch]Batch 1300/3201 Done, mean position loss: 21.4416476392746\n",
      "Training FF2:  41%|████████▉             | 1302/3201 [41:18<53:56,  1.70s/batch]Batch 1300/3201 Done, mean position loss: 22.164901654720303\n",
      "Training FF2:  41%|█████████             | 1313/3201 [41:20<48:39,  1.55s/batch]Batch 1300/3201 Done, mean position loss: 21.299378912448883\n",
      "Training FF2:  40%|████████▊             | 1291/3201 [41:20<57:33,  1.81s/batch]Batch 1300/3201 Done, mean position loss: 21.867903594970706\n",
      "Training FF2:  41%|████████▉             | 1304/3201 [41:21<56:45,  1.80s/batch]Batch 1300/3201 Done, mean position loss: 21.585928213596347\n",
      "Training FF2:  40%|████████▉             | 1292/3201 [41:22<53:50,  1.69s/batch]Batch 1300/3201 Done, mean position loss: 21.235780696868897\n",
      "Training FF2:  40%|████████▊             | 1281/3201 [41:23<58:24,  1.83s/batch]Batch 1300/3201 Done, mean position loss: 21.889097852706907\n",
      "Training FF2:  41%|████████▉             | 1309/3201 [41:23<56:33,  1.79s/batch]Batch 1300/3201 Done, mean position loss: 21.42378761768341\n",
      "Training FF2:  41%|█████████             | 1314/3201 [41:25<58:43,  1.87s/batch]Batch 1300/3201 Done, mean position loss: 21.33642288684845\n",
      "Training FF2:  41%|████████▏           | 1315/3201 [41:33<1:06:49,  2.13s/batch]Batch 1300/3201 Done, mean position loss: 21.497006740570065\n",
      "Training FF2:  41%|████████▎           | 1323/3201 [41:36<1:03:56,  2.04s/batch]Batch 1300/3201 Done, mean position loss: 21.154869346618653\n",
      "Training FF2:  42%|█████████▏            | 1330/3201 [41:38<57:51,  1.86s/batch]Batch 1300/3201 Done, mean position loss: 21.73575989961624\n",
      "Training FF2:  41%|█████████             | 1312/3201 [41:40<56:18,  1.79s/batch]Batch 1300/3201 Done, mean position loss: 21.399059650897982\n",
      "Training FF2:  41%|████████▉             | 1303/3201 [41:40<43:35,  1.38s/batch]Batch 1300/3201 Done, mean position loss: 21.10134348154068\n",
      "Training FF2:  41%|█████████             | 1324/3201 [41:47<48:23,  1.55s/batch]Batch 1300/3201 Done, mean position loss: 21.407965991497036\n",
      "Training FF2:  42%|████████▍           | 1344/3201 [42:01<1:01:45,  2.00s/batch]Batch 1300/3201 Done, mean position loss: 21.41477745294571\n",
      "Training FF2:  41%|█████████             | 1327/3201 [42:05<53:15,  1.71s/batch]Batch 1300/3201 Done, mean position loss: 21.521222145557402\n",
      "Training FF2:  42%|█████████▏            | 1343/3201 [42:20<59:40,  1.93s/batch]Batch 1300/3201 Done, mean position loss: 21.5849933552742\n",
      "Training FF2:  43%|█████████▌            | 1383/3201 [43:42<58:44,  1.94s/batch]Batch 1400/3201 Done, mean position loss: 21.84068834066391\n",
      "Training FF2:  43%|████████▌           | 1374/3201 [43:44<1:02:29,  2.05s/batch]Batch 1400/3201 Done, mean position loss: 22.089596316814422\n",
      "Training FF2:  43%|████████▋           | 1392/3201 [43:50<1:01:29,  2.04s/batch]Batch 1400/3201 Done, mean position loss: 21.527032265663145\n",
      "Training FF2:  43%|█████████▌            | 1387/3201 [43:50<56:37,  1.87s/batch]Batch 1400/3201 Done, mean position loss: 21.573788952827456\n",
      "Training FF2:  43%|████████▌           | 1379/3201 [43:58<1:02:24,  2.06s/batch]Batch 1400/3201 Done, mean position loss: 21.506252295970917\n",
      "Training FF2:  44%|█████████▋            | 1410/3201 [44:01<53:36,  1.80s/batch]Batch 1400/3201 Done, mean position loss: 21.268841269016264\n",
      "Training FF2:  44%|█████████▌            | 1394/3201 [44:03<57:46,  1.92s/batch]Batch 1400/3201 Done, mean position loss: 21.039262454509736\n",
      "Training FF2:  44%|████████▊           | 1403/3201 [44:06<1:06:52,  2.23s/batch]Batch 1400/3201 Done, mean position loss: 22.118193371295927\n",
      "Training FF2:  44%|████████▊           | 1401/3201 [44:07<1:06:29,  2.22s/batch]Batch 1400/3201 Done, mean position loss: 21.14938742399216\n",
      "Training FF2:  43%|█████████▌            | 1391/3201 [44:06<59:25,  1.97s/batch]Batch 1400/3201 Done, mean position loss: 21.081660819053653\n",
      "Training FF2:  43%|█████████▌            | 1386/3201 [44:06<54:45,  1.81s/batch]Batch 1400/3201 Done, mean position loss: 21.54938529014587\n",
      "Training FF2:  44%|█████████▌            | 1396/3201 [44:06<56:46,  1.89s/batch]Batch 1400/3201 Done, mean position loss: 21.431383855342865\n",
      "Batch 1400/3201 Done, mean position loss: 21.953785893917086\n",
      "Training FF2:  43%|█████████▍            | 1378/3201 [44:10<57:45,  1.90s/batch]Batch 1400/3201 Done, mean position loss: 21.559962127208706\n",
      "Training FF2:  44%|█████████▋            | 1406/3201 [44:11<53:35,  1.79s/batch]Batch 1400/3201 Done, mean position loss: 21.445743851661682\n",
      "Training FF2:  43%|█████████▍            | 1379/3201 [44:13<44:48,  1.48s/batch]Batch 1400/3201 Done, mean position loss: 21.355971693992615\n",
      "Training FF2:  44%|█████████▋            | 1418/3201 [44:14<58:03,  1.95s/batch]Batch 1400/3201 Done, mean position loss: 21.30232691287994\n",
      "Training FF2:  44%|█████████▋            | 1404/3201 [44:15<51:45,  1.73s/batch]Batch 1400/3201 Done, mean position loss: 21.085990006923677\n",
      "Training FF2:  43%|████████▋           | 1383/3201 [44:17<1:06:52,  2.21s/batch]Batch 1400/3201 Done, mean position loss: 21.233701639175415\n",
      "Training FF2:  44%|████████▊           | 1402/3201 [44:20<1:09:44,  2.33s/batch]Batch 1400/3201 Done, mean position loss: 21.48263160228729\n",
      "Training FF2:  44%|████████▊           | 1409/3201 [44:24<1:06:35,  2.23s/batch]Batch 1400/3201 Done, mean position loss: 22.190041544437406\n",
      "Training FF2:  43%|████████▌           | 1367/3201 [44:24<1:00:22,  1.98s/batch]Batch 1400/3201 Done, mean position loss: 21.407756507396698\n",
      "Training FF2:  44%|████████▊           | 1405/3201 [44:24<1:01:55,  2.07s/batch]Batch 1400/3201 Done, mean position loss: 21.082672743797303\n",
      "Training FF2:  44%|████████▊           | 1414/3201 [44:26<1:00:29,  2.03s/batch]Batch 1400/3201 Done, mean position loss: 21.40565680027008\n",
      "Training FF2:  44%|████████▊           | 1411/3201 [44:29<1:04:50,  2.17s/batch]Batch 1400/3201 Done, mean position loss: 21.3965233707428\n",
      "Training FF2:  44%|████████▊           | 1418/3201 [44:30<1:01:46,  2.08s/batch]Batch 1400/3201 Done, mean position loss: 21.58389957666397\n",
      "Training FF2:  44%|████████▊           | 1414/3201 [44:33<1:02:58,  2.11s/batch]Batch 1400/3201 Done, mean position loss: 21.263024275302886\n",
      "Training FF2:  44%|████████▊           | 1408/3201 [44:33<1:07:35,  2.26s/batch]Batch 1400/3201 Done, mean position loss: 21.215583751201628\n",
      "Training FF2:  44%|█████████▋            | 1416/3201 [44:34<59:48,  2.01s/batch]Batch 1400/3201 Done, mean position loss: 21.328578379154205\n",
      "Training FF2:  45%|████████▉           | 1427/3201 [44:37<1:04:48,  2.19s/batch]Batch 1400/3201 Done, mean position loss: 21.78453217983246\n",
      "Training FF2:  44%|████████▊           | 1412/3201 [44:42<1:07:43,  2.27s/batch]Batch 1400/3201 Done, mean position loss: 21.858684844970703\n",
      "Training FF2:  44%|████████▊           | 1407/3201 [44:42<1:00:39,  2.03s/batch]Batch 1400/3201 Done, mean position loss: 21.490778770446777\n",
      "Training FF2:  44%|█████████▊            | 1423/3201 [44:54<57:54,  1.95s/batch]Batch 1400/3201 Done, mean position loss: 21.7061624789238\n",
      "Training FF2:  44%|█████████▋            | 1411/3201 [44:54<56:04,  1.88s/batch]Batch 1400/3201 Done, mean position loss: 21.341455178260805\n",
      "Training FF2:  44%|█████████▋            | 1415/3201 [44:56<45:52,  1.54s/batch]Batch 1400/3201 Done, mean position loss: 21.096772241592404\n",
      "Training FF2:  45%|█████████▊            | 1428/3201 [44:58<50:57,  1.72s/batch]Batch 1400/3201 Done, mean position loss: 21.20047705173492\n",
      "Training FF2:  45%|█████████▊            | 1432/3201 [45:05<52:54,  1.79s/batch]Batch 1400/3201 Done, mean position loss: 21.402815787792207\n",
      "Training FF2:  45%|█████████▉            | 1439/3201 [45:21<52:40,  1.79s/batch]Batch 1400/3201 Done, mean position loss: 21.516849403381347\n",
      "Training FF2:  45%|█████████▊            | 1429/3201 [45:25<52:09,  1.77s/batch]Batch 1400/3201 Done, mean position loss: 21.38680643320084\n",
      "Training FF2:  45%|█████████▊            | 1432/3201 [45:30<52:31,  1.78s/batch]Batch 1400/3201 Done, mean position loss: 21.545698673725127\n",
      "Training FF2:  47%|█████████▎          | 1489/3201 [46:54<1:05:36,  2.30s/batch]Batch 1500/3201 Done, mean position loss: 21.78527706861496\n",
      "Training FF2:  46%|██████████▏           | 1482/3201 [47:04<51:34,  1.80s/batch]Batch 1500/3201 Done, mean position loss: 22.07429366111755\n",
      "Training FF2:  47%|██████████▎           | 1501/3201 [47:04<58:10,  2.05s/batch]Batch 1500/3201 Done, mean position loss: 21.584079501628878\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [47:08<40:05,  1.40s/batch]Batch 1500/3201 Done, mean position loss: 21.46480277776718\n",
      "Training FF2:  47%|██████████▎           | 1496/3201 [47:10<51:04,  1.80s/batch]Batch 1500/3201 Done, mean position loss: 21.484863176345826\n",
      "Training FF2:  47%|██████████▎           | 1495/3201 [47:13<50:58,  1.79s/batch]Batch 1500/3201 Done, mean position loss: 21.034203145503994\n",
      "Training FF2:  46%|██████████▏           | 1487/3201 [47:14<46:49,  1.64s/batch]Batch 1500/3201 Done, mean position loss: 21.515485227108\n",
      "Training FF2:  47%|██████████▎           | 1497/3201 [47:14<49:24,  1.74s/batch]Batch 1500/3201 Done, mean position loss: 21.121636939048763\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [47:15<52:29,  1.84s/batch]Batch 1500/3201 Done, mean position loss: 21.914059312343596\n",
      "Training FF2:  47%|██████████▍           | 1514/3201 [47:17<44:07,  1.57s/batch]Batch 1500/3201 Done, mean position loss: 21.283516991138455\n",
      "Training FF2:  46%|██████████            | 1463/3201 [47:17<43:54,  1.52s/batch]Batch 1500/3201 Done, mean position loss: 21.050374324321748\n",
      "Training FF2:  47%|██████████▎           | 1492/3201 [47:18<48:00,  1.69s/batch]Batch 1500/3201 Done, mean position loss: 22.074952380657194\n",
      "Training FF2:  47%|██████████▎           | 1494/3201 [47:18<44:07,  1.55s/batch]Batch 1500/3201 Done, mean position loss: 21.321944229602813\n",
      "Training FF2:  47%|██████████▎           | 1503/3201 [47:20<52:22,  1.85s/batch]Batch 1500/3201 Done, mean position loss: 21.234100761413575\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [47:20<49:28,  1.73s/batch]Batch 1500/3201 Done, mean position loss: 21.413894684314727\n",
      "Training FF2:  47%|█████████▎          | 1491/3201 [47:21<1:02:08,  2.18s/batch]Batch 1500/3201 Done, mean position loss: 21.499569392204286\n",
      "Training FF2:  47%|██████████▎           | 1501/3201 [47:20<59:46,  2.11s/batch]Batch 1500/3201 Done, mean position loss: 21.398756775856018\n",
      "Training FF2:  46%|██████████▏           | 1474/3201 [47:25<59:12,  2.06s/batch]Batch 1500/3201 Done, mean position loss: 21.065655605792998\n",
      "Training FF2:  47%|██████████▍           | 1517/3201 [47:31<50:42,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 21.22178593158722\n",
      "Training FF2:  47%|██████████▎           | 1508/3201 [47:32<43:50,  1.55s/batch]Batch 1500/3201 Done, mean position loss: 21.06934873819351\n",
      "Training FF2:  46%|██████████▏           | 1488/3201 [47:32<45:31,  1.59s/batch]Batch 1500/3201 Done, mean position loss: 21.433017032146452\n",
      "Training FF2:  47%|██████████▎           | 1493/3201 [47:33<55:18,  1.94s/batch]Batch 1500/3201 Done, mean position loss: 21.35660829782486\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [47:38<59:59,  2.10s/batch]Batch 1500/3201 Done, mean position loss: 21.38160237073898\n",
      "Training FF2:  47%|██████████▍           | 1515/3201 [47:38<48:55,  1.74s/batch]Batch 1500/3201 Done, mean position loss: 21.23495670080185\n",
      "Training FF2:  47%|██████████▎           | 1502/3201 [47:40<51:08,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 22.18810683727264\n",
      "Training FF2:  47%|█████████▍          | 1513/3201 [47:41<1:01:33,  2.19s/batch]Batch 1500/3201 Done, mean position loss: 21.18912452697754\n",
      "Training FF2:  47%|██████████▍           | 1519/3201 [47:42<49:12,  1.76s/batch]Batch 1500/3201 Done, mean position loss: 21.55803147792816\n",
      "Training FF2:  46%|██████████▏           | 1488/3201 [47:42<51:44,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 21.32260278701782\n",
      "Batch 1500/3201 Done, mean position loss: 21.388918578624725\n",
      "Training FF2:  48%|██████████▌           | 1533/3201 [47:49<55:45,  2.01s/batch]Batch 1500/3201 Done, mean position loss: 21.743953063488007\n",
      "Training FF2:  47%|█████████▍          | 1512/3201 [47:52<1:02:05,  2.21s/batch]Batch 1500/3201 Done, mean position loss: 21.44530522108078\n",
      "Training FF2:  47%|██████████▍           | 1510/3201 [48:02<55:33,  1.97s/batch]Batch 1500/3201 Done, mean position loss: 21.81632367372513\n",
      "Training FF2:  46%|██████████▏           | 1485/3201 [48:07<51:47,  1.81s/batch]Batch 1500/3201 Done, mean position loss: 21.111793041229248\n",
      "Training FF2:  47%|██████████▍           | 1515/3201 [48:09<46:17,  1.65s/batch]Batch 1500/3201 Done, mean position loss: 21.695357406139372\n",
      "Training FF2:  47%|██████████▎           | 1501/3201 [48:09<56:34,  2.00s/batch]Batch 1500/3201 Done, mean position loss: 21.150779287815094\n",
      "Training FF2:  48%|██████████▌           | 1529/3201 [48:11<46:14,  1.66s/batch]Batch 1500/3201 Done, mean position loss: 21.323051936626435\n",
      "Training FF2:  48%|█████████▌          | 1533/3201 [48:18<1:02:19,  2.24s/batch]Batch 1500/3201 Done, mean position loss: 21.41278313398361\n",
      "Training FF2:  48%|██████████▌           | 1540/3201 [48:26<58:02,  2.10s/batch]Batch 1500/3201 Done, mean position loss: 21.439631485939024\n",
      "Training FF2:  48%|██████████▌           | 1545/3201 [48:35<51:30,  1.87s/batch]Batch 1500/3201 Done, mean position loss: 21.3440847325325\n",
      "Training FF2:  48%|██████████▌           | 1529/3201 [48:37<55:29,  1.99s/batch]Batch 1500/3201 Done, mean position loss: 21.488927228450773\n",
      "Training FF2:  49%|██████████▊           | 1579/3201 [49:55<38:49,  1.44s/batch]Batch 1600/3201 Done, mean position loss: 21.724371359348297\n",
      "Training FF2:  50%|██████████▉           | 1591/3201 [50:09<44:20,  1.65s/batch]Batch 1600/3201 Done, mean position loss: 22.055284321308136\n",
      "Training FF2:  50%|██████████▉           | 1600/3201 [50:14<45:05,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.52717127084732\n",
      "Training FF2:  49%|██████████▊           | 1581/3201 [50:14<45:40,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.23776144504547\n",
      "Training FF2:  50%|██████████▉           | 1594/3201 [50:16<55:19,  2.07s/batch]Batch 1600/3201 Done, mean position loss: 21.517801954746247\n",
      "Training FF2:  50%|███████████           | 1603/3201 [50:17<42:07,  1.58s/batch]Batch 1600/3201 Done, mean position loss: 21.440143010616303\n",
      "Training FF2:  50%|███████████           | 1606/3201 [50:17<49:20,  1.86s/batch]Batch 1600/3201 Done, mean position loss: 21.440066001415254\n",
      "Training FF2:  50%|██████████▉           | 1598/3201 [50:18<55:00,  2.06s/batch]Batch 1600/3201 Done, mean position loss: 20.97607813835144\n",
      "Training FF2:  49%|██████████▊           | 1575/3201 [50:20<53:26,  1.97s/batch]Batch 1600/3201 Done, mean position loss: 21.296617727279664\n",
      "Training FF2:  49%|██████████▋           | 1559/3201 [50:24<52:24,  1.91s/batch]Batch 1600/3201 Done, mean position loss: 21.079509184360504\n",
      "Training FF2:  50%|███████████           | 1605/3201 [50:25<50:03,  1.88s/batch]Batch 1600/3201 Done, mean position loss: 21.46152981758118\n",
      "Training FF2:  50%|███████████           | 1602/3201 [50:25<50:57,  1.91s/batch]Batch 1600/3201 Done, mean position loss: 21.912405343055724\n",
      "Training FF2:  50%|██████████▉           | 1586/3201 [50:27<50:01,  1.86s/batch]Batch 1600/3201 Done, mean position loss: 21.365783994197848\n",
      "Training FF2:  49%|██████████▊           | 1579/3201 [50:27<53:14,  1.97s/batch]Batch 1600/3201 Done, mean position loss: 21.055512347221374\n",
      "Training FF2:  49%|██████████▋           | 1561/3201 [50:29<49:05,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.39717026233673\n",
      "Training FF2:  50%|███████████           | 1603/3201 [50:30<39:45,  1.49s/batch]Batch 1600/3201 Done, mean position loss: 21.200419547557832\n",
      "Training FF2:  50%|██████████▉           | 1599/3201 [50:30<48:00,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.032661745548246\n",
      "Training FF2:  50%|█████████▉          | 1591/3201 [50:32<1:01:30,  2.29s/batch]Batch 1600/3201 Done, mean position loss: 22.022739000320435\n",
      "Training FF2:  50%|██████████▉           | 1594/3201 [50:34<41:56,  1.57s/batch]Batch 1600/3201 Done, mean position loss: 21.35455501794815\n",
      "Training FF2:  50%|███████████           | 1616/3201 [50:34<42:20,  1.60s/batch]Batch 1600/3201 Done, mean position loss: 21.22655610322952\n",
      "Training FF2:  50%|██████████▉           | 1585/3201 [50:42<45:32,  1.69s/batch]Batch 1600/3201 Done, mean position loss: 21.42629021644592\n",
      "Training FF2:  51%|███████████           | 1618/3201 [50:45<47:06,  1.79s/batch]Batch 1600/3201 Done, mean position loss: 21.063176572322845\n",
      "Training FF2:  50%|███████████           | 1610/3201 [50:48<54:35,  2.06s/batch]Batch 1600/3201 Done, mean position loss: 21.1979527926445\n",
      "Batch 1600/3201 Done, mean position loss: 21.529603798389434\n",
      "Training FF2:  51%|███████████▏          | 1619/3201 [50:48<55:32,  2.11s/batch]Batch 1600/3201 Done, mean position loss: 21.188277287483217\n",
      "Training FF2:  50%|██████████▉           | 1588/3201 [50:48<51:16,  1.91s/batch]Batch 1600/3201 Done, mean position loss: 21.30603036403656\n",
      "Training FF2:  50%|██████████▉           | 1585/3201 [50:51<53:50,  2.00s/batch]Batch 1600/3201 Done, mean position loss: 21.696140732765194\n",
      "Training FF2:  51%|███████████▏          | 1619/3201 [50:51<45:23,  1.72s/batch]Batch 1600/3201 Done, mean position loss: 21.324975783824918\n",
      "Training FF2:  50%|██████████▉           | 1586/3201 [50:52<52:33,  1.95s/batch]Batch 1600/3201 Done, mean position loss: 22.18363559961319\n",
      "Training FF2:  51%|███████████▏          | 1622/3201 [50:56<43:37,  1.66s/batch]Batch 1600/3201 Done, mean position loss: 21.331206347942352\n",
      "Training FF2:  49%|██████████▊           | 1579/3201 [51:03<47:55,  1.77s/batch]Batch 1600/3201 Done, mean position loss: 21.461699793338774\n",
      "Training FF2:  51%|███████████▏          | 1622/3201 [51:08<47:23,  1.80s/batch]Batch 1600/3201 Done, mean position loss: 21.79036255121231\n",
      "Training FF2:  51%|███████████▏          | 1626/3201 [51:09<46:44,  1.78s/batch]Batch 1600/3201 Done, mean position loss: 21.314556324481963\n",
      "Training FF2:  51%|███████████▏          | 1626/3201 [51:10<50:20,  1.92s/batch]Batch 1600/3201 Done, mean position loss: 21.15654325246811\n",
      "Training FF2:  51%|███████████▏          | 1627/3201 [51:21<43:06,  1.64s/batch]Batch 1600/3201 Done, mean position loss: 21.101698648929595\n",
      "Training FF2:  50%|██████████▉           | 1591/3201 [51:24<47:38,  1.78s/batch]Batch 1600/3201 Done, mean position loss: 21.663278696537017\n",
      "Training FF2:  51%|███████████▏          | 1627/3201 [51:28<44:36,  1.70s/batch]Batch 1600/3201 Done, mean position loss: 21.391131184101106\n",
      "Training FF2:  51%|███████████▏          | 1636/3201 [51:30<54:19,  2.08s/batch]Batch 1600/3201 Done, mean position loss: 21.43251543521881\n",
      "Training FF2:  51%|███████████▎          | 1643/3201 [51:42<59:18,  2.28s/batch]Batch 1600/3201 Done, mean position loss: 21.45915224790573\n",
      "Training FF2:  52%|███████████▎          | 1650/3201 [51:42<45:36,  1.76s/batch]Batch 1600/3201 Done, mean position loss: 21.314731600284574\n",
      "Training FF2:  52%|███████████▍          | 1662/3201 [52:59<49:30,  1.93s/batch]Batch 1700/3201 Done, mean position loss: 21.69246682405472\n",
      "Training FF2:  53%|███████████▋          | 1692/3201 [53:12<51:48,  2.06s/batch]Batch 1700/3201 Done, mean position loss: 22.043231356143952\n",
      "Training FF2:  53%|███████████▌          | 1683/3201 [53:15<49:19,  1.95s/batch]Batch 1700/3201 Done, mean position loss: 21.207858984470366\n",
      "Training FF2:  53%|███████████▋          | 1694/3201 [53:17<40:02,  1.59s/batch]Batch 1700/3201 Done, mean position loss: 21.53155224084854\n",
      "Training FF2:  53%|███████████▋          | 1693/3201 [53:18<43:33,  1.73s/batch]Batch 1700/3201 Done, mean position loss: 21.459583640098572\n",
      "Training FF2:  53%|███████████▋          | 1704/3201 [53:20<43:22,  1.74s/batch]Batch 1700/3201 Done, mean position loss: 20.953886497020722\n",
      "Training FF2:  53%|███████████▋          | 1693/3201 [53:25<48:15,  1.92s/batch]Batch 1700/3201 Done, mean position loss: 21.43163780450821\n",
      "Training FF2:  53%|███████████▋          | 1705/3201 [53:25<48:53,  1.96s/batch]Batch 1700/3201 Done, mean position loss: 21.28156232595444\n",
      "Training FF2:  52%|███████████▎          | 1655/3201 [53:26<47:18,  1.84s/batch]Batch 1700/3201 Done, mean position loss: 21.0449876499176\n",
      "Training FF2:  53%|███████████▋          | 1700/3201 [53:28<44:57,  1.80s/batch]Batch 1700/3201 Done, mean position loss: 21.361190381050108\n",
      "Training FF2:  54%|███████████▊          | 1718/3201 [53:29<42:50,  1.73s/batch]Batch 1700/3201 Done, mean position loss: 21.420088164806366\n",
      "Training FF2:  53%|███████████▌          | 1689/3201 [53:29<44:51,  1.78s/batch]Batch 1700/3201 Done, mean position loss: 21.369138205051424\n",
      "Training FF2:  52%|███████████▍          | 1657/3201 [53:30<46:52,  1.82s/batch]Batch 1700/3201 Done, mean position loss: 21.826676974296568\n",
      "Training FF2:  52%|███████████▍          | 1659/3201 [53:33<43:05,  1.68s/batch]Batch 1700/3201 Done, mean position loss: 21.177643876075745\n",
      "Training FF2:  53%|███████████▊          | 1712/3201 [53:32<43:33,  1.76s/batch]Batch 1700/3201 Done, mean position loss: 21.013277842998505\n",
      "Training FF2:  53%|███████████▌          | 1691/3201 [53:35<43:17,  1.72s/batch]Batch 1700/3201 Done, mean position loss: 21.412431104183195\n",
      "Training FF2:  53%|███████████▊          | 1712/3201 [53:37<56:12,  2.27s/batch]Batch 1700/3201 Done, mean position loss: 21.017678842544555\n",
      "Training FF2:  53%|███████████▋          | 1704/3201 [53:39<44:44,  1.79s/batch]Batch 1700/3201 Done, mean position loss: 21.40127320051193\n",
      "Training FF2:  53%|███████████▌          | 1691/3201 [53:40<57:38,  2.29s/batch]Batch 1700/3201 Done, mean position loss: 21.99258880853653\n",
      "Training FF2:  53%|███████████▋          | 1702/3201 [53:41<47:14,  1.89s/batch]Batch 1700/3201 Done, mean position loss: 21.30915777206421\n",
      "Training FF2:  53%|███████████▌          | 1690/3201 [53:44<46:58,  1.87s/batch]Batch 1700/3201 Done, mean position loss: 21.23137140274048\n",
      "Training FF2:  53%|███████████▊          | 1712/3201 [53:50<45:42,  1.84s/batch]Batch 1700/3201 Done, mean position loss: 21.314981050491333\n",
      "Training FF2:  52%|███████████▌          | 1680/3201 [53:50<48:00,  1.89s/batch]Batch 1700/3201 Done, mean position loss: 21.29087776184082\n",
      "Training FF2:  53%|███████████▌          | 1688/3201 [53:50<56:10,  2.23s/batch]Batch 1700/3201 Done, mean position loss: 21.072316522598268\n",
      "Training FF2:  53%|███████████▋          | 1699/3201 [53:50<49:30,  1.98s/batch]Batch 1700/3201 Done, mean position loss: 21.164952750205995\n",
      "Training FF2:  54%|███████████▊          | 1714/3201 [53:52<43:47,  1.77s/batch]Batch 1700/3201 Done, mean position loss: 22.156415214538573\n",
      "Training FF2:  54%|███████████▊          | 1716/3201 [53:51<38:47,  1.57s/batch]Batch 1700/3201 Done, mean position loss: 21.63932845592499\n",
      "Training FF2:  54%|███████████▉          | 1731/3201 [53:54<46:16,  1.89s/batch]Batch 1700/3201 Done, mean position loss: 21.51930170059204\n",
      "Batch 1700/3201 Done, mean position loss: 21.29432603597641\n",
      "Training FF2:  52%|███████████▌          | 1678/3201 [54:00<59:08,  2.33s/batch]Batch 1700/3201 Done, mean position loss: 21.164577136039735\n",
      "Training FF2:  54%|███████████▊          | 1716/3201 [54:02<41:47,  1.69s/batch]Batch 1700/3201 Done, mean position loss: 21.301541447639465\n",
      "Training FF2:  54%|███████████▉          | 1740/3201 [54:09<44:06,  1.81s/batch]Batch 1700/3201 Done, mean position loss: 21.741926329135897\n",
      "Training FF2:  53%|███████████▋          | 1693/3201 [54:13<43:28,  1.73s/batch]Batch 1700/3201 Done, mean position loss: 21.477412276268005\n",
      "Training FF2:  54%|███████████▊          | 1716/3201 [54:13<50:24,  2.04s/batch]Batch 1700/3201 Done, mean position loss: 21.128188359737397\n",
      "Training FF2:  54%|███████████▉          | 1732/3201 [54:31<54:45,  2.24s/batch]Batch 1700/3201 Done, mean position loss: 21.390575664043425\n",
      "Training FF2:  54%|███████████▉          | 1729/3201 [54:31<50:13,  2.05s/batch]Batch 1700/3201 Done, mean position loss: 21.60084088087082\n",
      "Training FF2:  54%|███████████▊          | 1726/3201 [54:35<47:09,  1.92s/batch]Batch 1700/3201 Done, mean position loss: 21.05652281522751\n",
      "Training FF2:  54%|███████████▉          | 1729/3201 [54:45<52:47,  2.15s/batch]Batch 1700/3201 Done, mean position loss: 21.397261102199554\n",
      "Training FF2:  54%|███████████▉          | 1742/3201 [54:48<51:42,  2.13s/batch]Batch 1700/3201 Done, mean position loss: 21.265124726295472\n",
      "Training FF2:  55%|████████████          | 1754/3201 [55:02<45:43,  1.90s/batch]Batch 1700/3201 Done, mean position loss: 21.391615657806398\n",
      "Training FF2:  56%|████████████▎         | 1785/3201 [56:08<44:28,  1.88s/batch]Batch 1800/3201 Done, mean position loss: 21.690943546295166\n",
      "Training FF2:  56%|████████████▎         | 1792/3201 [56:20<48:56,  2.08s/batch]Batch 1800/3201 Done, mean position loss: 22.006337509155273\n",
      "Training FF2:  56%|████████████▎         | 1797/3201 [56:29<38:56,  1.66s/batch]Batch 1800/3201 Done, mean position loss: 21.448087623119356\n",
      "Training FF2:  55%|████████████          | 1764/3201 [56:30<43:55,  1.83s/batch]Batch 1800/3201 Done, mean position loss: 21.48527973413467\n",
      "Training FF2:  57%|████████████▍         | 1812/3201 [56:31<45:21,  1.96s/batch]Batch 1800/3201 Done, mean position loss: 20.932300815582273\n",
      "Training FF2:  55%|████████████▏         | 1765/3201 [56:37<33:34,  1.40s/batch]Batch 1800/3201 Done, mean position loss: 21.340010969638826\n",
      "Training FF2:  56%|████████████▎         | 1792/3201 [56:38<46:26,  1.98s/batch]Batch 1800/3201 Done, mean position loss: 21.38440953016281\n",
      "Training FF2:  56%|████████████▎         | 1787/3201 [56:39<50:33,  2.15s/batch]Batch 1800/3201 Done, mean position loss: 21.013906145095824\n",
      "Training FF2:  56%|████████████▎         | 1800/3201 [56:40<47:52,  2.05s/batch]Batch 1800/3201 Done, mean position loss: 21.18702870607376\n",
      "Training FF2:  55%|████████████          | 1764/3201 [56:40<56:10,  2.35s/batch]Batch 1800/3201 Done, mean position loss: 21.284913313388824\n",
      "Training FF2:  56%|████████████▍         | 1802/3201 [56:42<46:52,  2.01s/batch]Batch 1800/3201 Done, mean position loss: 21.79723630666733\n",
      "Training FF2:  56%|████████████▎         | 1796/3201 [56:43<47:06,  2.01s/batch]Batch 1800/3201 Done, mean position loss: 21.367051181793215\n",
      "Training FF2:  56%|████████████▍         | 1808/3201 [56:43<51:21,  2.21s/batch]Batch 1800/3201 Done, mean position loss: 20.99961272716522\n",
      "Training FF2:  56%|████████████▎         | 1787/3201 [56:48<46:43,  1.98s/batch]Batch 1800/3201 Done, mean position loss: 21.397890918254852\n",
      "Training FF2:  56%|████████████▍         | 1807/3201 [56:50<38:59,  1.68s/batch]Batch 1800/3201 Done, mean position loss: 21.38565641880035\n",
      "Training FF2:  57%|████████████▍         | 1809/3201 [56:52<42:41,  1.84s/batch]Batch 1800/3201 Done, mean position loss: 21.1523858332634\n",
      "Training FF2:  56%|████████████▎         | 1794/3201 [56:54<47:31,  2.03s/batch]Batch 1800/3201 Done, mean position loss: 21.373368887901304\n",
      "Training FF2:  56%|████████████▎         | 1797/3201 [56:54<48:19,  2.07s/batch]Batch 1800/3201 Done, mean position loss: 21.99481943845749\n",
      "Training FF2:  56%|████████████▎         | 1785/3201 [56:57<35:48,  1.52s/batch]Batch 1800/3201 Done, mean position loss: 21.001148300170897\n",
      "Batch 1800/3201 Done, mean position loss: 21.269956929683687\n",
      "Training FF2:  57%|████████████▍         | 1812/3201 [57:00<38:56,  1.68s/batch]Batch 1800/3201 Done, mean position loss: 21.23517887353897\n",
      "Training FF2:  56%|████████████▎         | 1793/3201 [57:02<48:06,  2.05s/batch]Batch 1800/3201 Done, mean position loss: 21.278001828193666\n",
      "Training FF2:  57%|████████████▍         | 1814/3201 [57:04<43:39,  1.89s/batch]Batch 1800/3201 Done, mean position loss: 21.06837825536728\n",
      "Training FF2:  57%|████████████▍         | 1816/3201 [57:06<46:44,  2.02s/batch]Batch 1800/3201 Done, mean position loss: 22.10933132648468\n",
      "Training FF2:  56%|████████████▍         | 1802/3201 [57:08<45:59,  1.97s/batch]Batch 1800/3201 Done, mean position loss: 21.603980436325074\n",
      "Training FF2:  57%|████████████▍         | 1817/3201 [57:10<45:02,  1.95s/batch]Batch 1800/3201 Done, mean position loss: 21.2911745762825\n",
      "Training FF2:  56%|████████████▍         | 1801/3201 [57:10<44:43,  1.92s/batch]Batch 1800/3201 Done, mean position loss: 21.25943146944046\n",
      "Training FF2:  57%|████████████▍         | 1812/3201 [57:10<41:30,  1.79s/batch]Batch 1800/3201 Done, mean position loss: 21.143634462356566\n",
      "Training FF2:  57%|████████████▍         | 1811/3201 [57:13<36:17,  1.57s/batch]Batch 1800/3201 Done, mean position loss: 21.503242897987366\n",
      "Training FF2:  55%|████████████▏         | 1767/3201 [57:17<55:40,  2.33s/batch]Batch 1800/3201 Done, mean position loss: 21.135641155242922\n",
      "Training FF2:  57%|████████████▌         | 1830/3201 [57:21<41:39,  1.82s/batch]Batch 1800/3201 Done, mean position loss: 21.28539969921112\n",
      "Training FF2:  57%|████████████▍         | 1818/3201 [57:26<45:11,  1.96s/batch]Batch 1800/3201 Done, mean position loss: 21.442623085975647\n",
      "Training FF2:  56%|████████████▎         | 1790/3201 [57:28<49:31,  2.11s/batch]Batch 1800/3201 Done, mean position loss: 21.721642279624938\n",
      "Training FF2:  57%|████████████▌         | 1827/3201 [57:35<47:05,  2.06s/batch]Batch 1800/3201 Done, mean position loss: 21.114229557514193\n",
      "Training FF2:  56%|████████████▎         | 1797/3201 [57:40<38:01,  1.63s/batch]Batch 1800/3201 Done, mean position loss: 21.427300822734836\n",
      "Training FF2:  57%|████████████▌         | 1822/3201 [57:47<43:40,  1.90s/batch]Batch 1800/3201 Done, mean position loss: 21.613133447170256\n",
      "Training FF2:  58%|████████████▋         | 1854/3201 [57:52<35:46,  1.59s/batch]Batch 1800/3201 Done, mean position loss: 21.07630798816681\n",
      "Training FF2:  57%|████████████▌         | 1834/3201 [58:00<52:05,  2.29s/batch]Batch 1800/3201 Done, mean position loss: 21.349844026565552\n",
      "Training FF2:  56%|████████████▎         | 1793/3201 [58:04<43:00,  1.83s/batch]Batch 1800/3201 Done, mean position loss: 21.22274686098099\n",
      "Training FF2:  58%|████████████▊         | 1858/3201 [58:18<47:36,  2.13s/batch]Batch 1800/3201 Done, mean position loss: 21.39274756193161\n",
      "Training FF2:  59%|████████████▉         | 1888/3201 [59:19<48:23,  2.21s/batch]Batch 1900/3201 Done, mean position loss: 21.688087253570558\n",
      "Training FF2:  59%|█████████████         | 1899/3201 [59:42<37:30,  1.73s/batch]Batch 1900/3201 Done, mean position loss: 22.005343952178954\n",
      "Training FF2:  59%|████████████▉         | 1888/3201 [59:43<36:52,  1.69s/batch]Batch 1900/3201 Done, mean position loss: 21.470546224117278\n",
      "Training FF2:  59%|████████████▉         | 1877/3201 [59:42<48:31,  2.20s/batch]Batch 1900/3201 Done, mean position loss: 21.43076808452606\n",
      "Training FF2:  59%|█████████████         | 1899/3201 [59:43<38:17,  1.76s/batch]Batch 1900/3201 Done, mean position loss: 20.906746809482577\n",
      "Training FF2:  58%|████████████▊         | 1865/3201 [59:45<41:57,  1.88s/batch]Batch 1900/3201 Done, mean position loss: 21.40165758609772\n",
      "Training FF2:  59%|████████████▉         | 1884/3201 [59:46<39:39,  1.81s/batch]Batch 1900/3201 Done, mean position loss: 21.166978280544278\n",
      "Training FF2:  59%|████████████▉         | 1888/3201 [59:49<35:20,  1.62s/batch]Batch 1900/3201 Done, mean position loss: 21.31539196014404\n",
      "Training FF2:  59%|█████████████         | 1894/3201 [59:52<41:15,  1.89s/batch]Batch 1900/3201 Done, mean position loss: 21.03405443429947\n",
      "Training FF2:  58%|████████████▊         | 1870/3201 [59:55<45:28,  2.05s/batch]Batch 1900/3201 Done, mean position loss: 21.374591743946077\n",
      "Training FF2:  59%|█████████████         | 1896/3201 [59:56<43:41,  2.01s/batch]Batch 1900/3201 Done, mean position loss: 21.228505232334136\n",
      "Training FF2:  59%|███████████▉        | 1903/3201 [1:00:00<41:47,  1.93s/batch]Batch 1900/3201 Done, mean position loss: 20.97962314605713\n",
      "Training FF2:  58%|███████████▋        | 1863/3201 [1:00:00<41:42,  1.87s/batch]Batch 1900/3201 Done, mean position loss: 21.77398045063019\n",
      "Training FF2:  59%|███████████▊        | 1894/3201 [1:00:05<47:21,  2.17s/batch]Batch 1900/3201 Done, mean position loss: 21.961288332939148\n",
      "Training FF2:  60%|███████████▉        | 1907/3201 [1:00:05<34:58,  1.62s/batch]Batch 1900/3201 Done, mean position loss: 21.38012981414795\n",
      "Training FF2:  60%|███████████▉        | 1912/3201 [1:00:05<42:20,  1.97s/batch]Batch 1900/3201 Done, mean position loss: 21.017453618049622\n",
      "Batch 1900/3201 Done, mean position loss: 21.151626901626585\n",
      "Training FF2:  60%|███████████▉        | 1905/3201 [1:00:06<30:38,  1.42s/batch]Batch 1900/3201 Done, mean position loss: 21.345607538223266\n",
      "Training FF2:  59%|███████████▊        | 1900/3201 [1:00:08<41:17,  1.90s/batch]Batch 1900/3201 Done, mean position loss: 21.24200874567032\n",
      "Training FF2:  60%|███████████▉        | 1913/3201 [1:00:08<45:18,  2.11s/batch]Batch 1900/3201 Done, mean position loss: 21.060561833381655\n",
      "Training FF2:  59%|███████████▉        | 1902/3201 [1:00:09<46:01,  2.13s/batch]Batch 1900/3201 Done, mean position loss: 21.221780564785007\n",
      "Training FF2:  59%|███████████▊        | 1885/3201 [1:00:11<42:03,  1.92s/batch]Batch 1900/3201 Done, mean position loss: 21.3592410326004\n",
      "Training FF2:  59%|███████████▊        | 1886/3201 [1:00:13<43:48,  2.00s/batch]Batch 1900/3201 Done, mean position loss: 21.582797088623046\n",
      "Training FF2:  59%|███████████▊        | 1900/3201 [1:00:16<38:57,  1.80s/batch]Batch 1900/3201 Done, mean position loss: 21.2687583398819\n",
      "Training FF2:  60%|███████████▉        | 1907/3201 [1:00:17<39:11,  1.82s/batch]Batch 1900/3201 Done, mean position loss: 21.229904572963715\n",
      "Training FF2:  60%|███████████▉        | 1915/3201 [1:00:18<53:44,  2.51s/batch]Batch 1900/3201 Done, mean position loss: 22.071318469047547\n",
      "Training FF2:  59%|███████████▊        | 1891/3201 [1:00:22<41:16,  1.89s/batch]Batch 1900/3201 Done, mean position loss: 21.105540549755098\n",
      "Training FF2:  60%|███████████▉        | 1916/3201 [1:00:25<39:12,  1.83s/batch]Batch 1900/3201 Done, mean position loss: 21.48688230037689\n",
      "Training FF2:  59%|███████████▊        | 1892/3201 [1:00:26<54:02,  2.48s/batch]Batch 1900/3201 Done, mean position loss: 21.305973045825958\n",
      "Training FF2:  59%|███████████▊        | 1883/3201 [1:00:27<46:31,  2.12s/batch]Batch 1900/3201 Done, mean position loss: 21.259559586048127\n",
      "Training FF2:  59%|███████████▊        | 1892/3201 [1:00:28<45:22,  2.08s/batch]Batch 1900/3201 Done, mean position loss: 21.132079606056216\n",
      "Training FF2:  61%|████████████▏       | 1948/3201 [1:00:43<39:25,  1.89s/batch]Batch 1900/3201 Done, mean position loss: 21.09106249332428\n",
      "Training FF2:  60%|███████████▉        | 1909/3201 [1:00:43<44:09,  2.05s/batch]Batch 1900/3201 Done, mean position loss: 21.425028386116026\n",
      "Training FF2:  60%|███████████▉        | 1911/3201 [1:00:45<53:43,  2.50s/batch]Batch 1900/3201 Done, mean position loss: 21.42292676448822\n",
      "Training FF2:  60%|████████████        | 1926/3201 [1:00:47<44:05,  2.07s/batch]Batch 1900/3201 Done, mean position loss: 21.68402654647827\n",
      "Training FF2:  60%|███████████▉        | 1917/3201 [1:00:59<43:59,  2.06s/batch]Batch 1900/3201 Done, mean position loss: 21.592470006942747\n",
      "Training FF2:  60%|████████████        | 1924/3201 [1:01:02<40:02,  1.88s/batch]Batch 1900/3201 Done, mean position loss: 21.07406630754471\n",
      "Training FF2:  61%|████████████▎       | 1964/3201 [1:01:17<46:34,  2.26s/batch]Batch 1900/3201 Done, mean position loss: 21.316803352832796\n",
      "Training FF2:  60%|███████████▉        | 1911/3201 [1:01:19<36:16,  1.69s/batch]Batch 1900/3201 Done, mean position loss: 21.204960384368896\n",
      "Training FF2:  60%|████████████        | 1927/3201 [1:01:37<41:42,  1.96s/batch]Batch 1900/3201 Done, mean position loss: 21.377752463817597\n",
      "Training FF2:  62%|████████████▍       | 1982/3201 [1:02:25<38:57,  1.92s/batch]Batch 2000/3201 Done, mean position loss: 21.650847070217132\n",
      "Training FF2:  62%|████████████▍       | 1993/3201 [1:02:54<42:15,  2.10s/batch]Batch 2000/3201 Done, mean position loss: 21.4351852273941\n",
      "Training FF2:  62%|████████████▍       | 1999/3201 [1:02:58<43:43,  2.18s/batch]Batch 2000/3201 Done, mean position loss: 21.906708571910862\n",
      "Training FF2:  61%|████████████▎       | 1965/3201 [1:03:02<41:43,  2.03s/batch]Batch 2000/3201 Done, mean position loss: 21.141858983039857\n",
      "Training FF2:  62%|████████████▎       | 1971/3201 [1:03:03<42:33,  2.08s/batch]Batch 2000/3201 Done, mean position loss: 21.421489644050595\n",
      "Training FF2:  62%|████████████▍       | 1998/3201 [1:03:04<41:59,  2.09s/batch]Batch 2000/3201 Done, mean position loss: 20.860136561393738\n",
      "Training FF2:  62%|████████████▍       | 1994/3201 [1:03:05<39:47,  1.98s/batch]Batch 2000/3201 Done, mean position loss: 21.286259162425996\n",
      "Training FF2:  62%|████████████▍       | 1982/3201 [1:03:06<39:04,  1.92s/batch]Batch 2000/3201 Done, mean position loss: 21.366323421001432\n",
      "Training FF2:  62%|████████████▎       | 1979/3201 [1:03:05<39:37,  1.95s/batch]Batch 2000/3201 Done, mean position loss: 21.064950375556943\n",
      "Training FF2:  63%|████████████▌       | 2005/3201 [1:03:08<31:59,  1.60s/batch]Batch 2000/3201 Done, mean position loss: 20.97427510261536\n",
      "Training FF2:  62%|████████████▍       | 1999/3201 [1:03:10<31:48,  1.59s/batch]Batch 2000/3201 Done, mean position loss: 21.3677646112442\n",
      "Training FF2:  63%|████████████▌       | 2005/3201 [1:03:11<34:17,  1.72s/batch]Batch 2000/3201 Done, mean position loss: 21.688519237041476\n",
      "Training FF2:  61%|████████████▏       | 1956/3201 [1:03:12<40:56,  1.97s/batch]Batch 2000/3201 Done, mean position loss: 21.360232133865356\n",
      "Training FF2:  62%|████████████▍       | 1992/3201 [1:03:14<34:04,  1.69s/batch]Batch 2000/3201 Done, mean position loss: 21.316991922855376\n",
      "Training FF2:  63%|████████████▌       | 2007/3201 [1:03:13<35:22,  1.78s/batch]Batch 2000/3201 Done, mean position loss: 21.20843119621277\n",
      "Training FF2:  62%|████████████▍       | 1988/3201 [1:03:17<34:39,  1.71s/batch]Batch 2000/3201 Done, mean position loss: 20.97465716838837\n",
      "Training FF2:  61%|████████████▏       | 1957/3201 [1:03:22<43:58,  2.12s/batch]Batch 2000/3201 Done, mean position loss: 21.91971461057663\n",
      "Training FF2:  62%|████████████▍       | 1981/3201 [1:03:22<41:04,  2.02s/batch]Batch 2000/3201 Done, mean position loss: 21.29571672201157\n",
      "Training FF2:  63%|████████████▌       | 2015/3201 [1:03:26<36:22,  1.84s/batch]Batch 2000/3201 Done, mean position loss: 21.233259429931643\n",
      "Training FF2:  63%|████████████▌       | 2018/3201 [1:03:28<29:05,  1.48s/batch]Batch 2000/3201 Done, mean position loss: 21.093656771183014\n",
      "Training FF2:  63%|████████████▌       | 2014/3201 [1:03:30<40:47,  2.06s/batch]Batch 2000/3201 Done, mean position loss: 21.263728229999543\n",
      "Training FF2:  63%|████████████▌       | 2012/3201 [1:03:31<33:57,  1.71s/batch]Batch 2000/3201 Done, mean position loss: 21.058034083843232\n",
      "Training FF2:  63%|████████████▋       | 2021/3201 [1:03:31<31:41,  1.61s/batch]Batch 2000/3201 Done, mean position loss: 21.19959060907364\n",
      "Training FF2:  62%|████████████▍       | 1993/3201 [1:03:32<39:28,  1.96s/batch]Batch 2000/3201 Done, mean position loss: 21.529034876823424Batch 2000/3201 Done, mean position loss: 22.037898700237275\n",
      "\n",
      "Training FF2:  63%|████████████▌       | 2001/3201 [1:03:32<40:59,  2.05s/batch]Batch 2000/3201 Done, mean position loss: 21.21056772708893\n",
      "Training FF2:  62%|████████████▍       | 1988/3201 [1:03:37<38:24,  1.90s/batch]Batch 2000/3201 Done, mean position loss: 21.095731954574585\n",
      "Training FF2:  63%|████████████▌       | 2017/3201 [1:03:40<36:18,  1.84s/batch]Batch 2000/3201 Done, mean position loss: 21.286313796043395\n",
      "Training FF2:  63%|████████████▌       | 2009/3201 [1:03:46<46:14,  2.33s/batch]Batch 2000/3201 Done, mean position loss: 21.244806263446808\n",
      "Training FF2:  63%|████████████▋       | 2022/3201 [1:03:48<33:43,  1.72s/batch]Batch 2000/3201 Done, mean position loss: 21.477184669971464\n",
      "Training FF2:  63%|████████████▌       | 2011/3201 [1:03:48<34:47,  1.75s/batch]Batch 2000/3201 Done, mean position loss: 21.11184356212616\n",
      "Training FF2:  63%|████████████▋       | 2032/3201 [1:04:00<34:42,  1.78s/batch]Batch 2000/3201 Done, mean position loss: 21.435859546661376\n",
      "Training FF2:  62%|████████████▍       | 2000/3201 [1:04:01<36:37,  1.83s/batch]Batch 2000/3201 Done, mean position loss: 21.679969084262847\n",
      "Training FF2:  63%|████████████▌       | 2017/3201 [1:04:02<38:05,  1.93s/batch]Batch 2000/3201 Done, mean position loss: 21.415494203567505\n",
      "Training FF2:  63%|████████████▌       | 2003/3201 [1:04:03<34:40,  1.74s/batch]Batch 2000/3201 Done, mean position loss: 21.107372367382048\n",
      "Training FF2:  62%|████████████▍       | 1987/3201 [1:04:12<40:28,  2.00s/batch]Batch 2000/3201 Done, mean position loss: 21.613944573402406\n",
      "Training FF2:  63%|████████████▋       | 2021/3201 [1:04:20<39:53,  2.03s/batch]Batch 2000/3201 Done, mean position loss: 21.039087755680082\n",
      "Training FF2:  63%|████████████▋       | 2027/3201 [1:04:28<29:26,  1.50s/batch]Batch 2000/3201 Done, mean position loss: 21.319260087013244\n",
      "Training FF2:  64%|████████████▋       | 2037/3201 [1:04:38<38:05,  1.96s/batch]Batch 2000/3201 Done, mean position loss: 21.163271522521974\n",
      "Training FF2:  64%|████████████▊       | 2052/3201 [1:04:46<37:15,  1.95s/batch]Batch 2000/3201 Done, mean position loss: 21.33289942741394\n",
      "Training FF2:  65%|████████████▉       | 2067/3201 [1:05:33<32:31,  1.72s/batch]Batch 2100/3201 Done, mean position loss: 21.616024246215822\n",
      "Training FF2:  65%|█████████████       | 2089/3201 [1:06:00<34:44,  1.87s/batch]Batch 2100/3201 Done, mean position loss: 21.480113983154297\n",
      "Training FF2:  65%|████████████▉       | 2070/3201 [1:06:07<38:23,  2.04s/batch]Batch 2100/3201 Done, mean position loss: 21.345468006134034\n",
      "Training FF2:  65%|█████████████       | 2094/3201 [1:06:08<31:01,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.27232763528824\n",
      "Training FF2:  65%|█████████████       | 2086/3201 [1:06:09<34:54,  1.88s/batch]Batch 2100/3201 Done, mean position loss: 21.89247124195099\n",
      "Training FF2:  65%|█████████████       | 2095/3201 [1:06:10<31:06,  1.69s/batch]Batch 2100/3201 Done, mean position loss: 20.83542235612869\n",
      "Training FF2:  65%|████████████▉       | 2073/3201 [1:06:12<32:19,  1.72s/batch]Batch 2100/3201 Done, mean position loss: 21.40209666967392\n",
      "Training FF2:  66%|█████████████▏      | 2102/3201 [1:06:13<25:51,  1.41s/batch]Batch 2100/3201 Done, mean position loss: 21.11439067602158\n",
      "Training FF2:  66%|█████████████       | 2098/3201 [1:06:16<41:33,  2.26s/batch]Batch 2100/3201 Done, mean position loss: 20.977685909271237\n",
      "Training FF2:  65%|█████████████       | 2087/3201 [1:06:16<38:10,  2.06s/batch]Batch 2100/3201 Done, mean position loss: 21.310128514766696\n",
      "Training FF2:  66%|█████████████       | 2098/3201 [1:06:20<35:06,  1.91s/batch]Batch 2100/3201 Done, mean position loss: 21.03635015010834\n",
      "Training FF2:  65%|█████████████       | 2093/3201 [1:06:22<38:50,  2.10s/batch]Batch 2100/3201 Done, mean position loss: 20.98067605495453\n",
      "Training FF2:  65%|████████████▉       | 2078/3201 [1:06:22<38:05,  2.03s/batch]Batch 2100/3201 Done, mean position loss: 21.35218464851379\n",
      "Batch 2100/3201 Done, mean position loss: 21.20891351699829\n",
      "Training FF2:  66%|█████████████▏      | 2101/3201 [1:06:22<40:28,  2.21s/batch]Batch 2100/3201 Done, mean position loss: 21.711952321529388\n",
      "Training FF2:  65%|████████████▉       | 2079/3201 [1:06:25<34:41,  1.86s/batch]Batch 2100/3201 Done, mean position loss: 21.25975270032883\n",
      "Training FF2:  64%|████████████▉       | 2063/3201 [1:06:26<38:36,  2.04s/batch]Batch 2100/3201 Done, mean position loss: 21.357375385761262\n",
      "Training FF2:  65%|█████████████       | 2082/3201 [1:06:33<30:13,  1.62s/batch]Batch 2100/3201 Done, mean position loss: 21.223418948650362\n",
      "Training FF2:  65%|█████████████       | 2084/3201 [1:06:34<38:23,  2.06s/batch]Batch 2100/3201 Done, mean position loss: 21.078995168209076\n",
      "Training FF2:  66%|█████████████▏      | 2108/3201 [1:06:34<31:30,  1.73s/batch]Batch 2100/3201 Done, mean position loss: 21.894902336597443\n",
      "Training FF2:  65%|█████████████       | 2085/3201 [1:06:37<35:56,  1.93s/batch]Batch 2100/3201 Done, mean position loss: 21.203901562690735\n",
      "Training FF2:  65%|█████████████       | 2082/3201 [1:06:38<33:46,  1.81s/batch]Batch 2100/3201 Done, mean position loss: 21.16053039073944\n",
      "Training FF2:  66%|█████████████▏      | 2109/3201 [1:06:39<31:40,  1.74s/batch]Batch 2100/3201 Done, mean position loss: 21.022671358585356\n",
      "Training FF2:  66%|█████████████▏      | 2103/3201 [1:06:40<31:12,  1.71s/batch]Batch 2100/3201 Done, mean position loss: 21.21841064453125\n",
      "Training FF2:  66%|█████████████▏      | 2106/3201 [1:06:42<34:05,  1.87s/batch]Batch 2100/3201 Done, mean position loss: 21.52642368555069\n",
      "Training FF2:  66%|█████████████▏      | 2113/3201 [1:06:43<33:48,  1.86s/batch]Batch 2100/3201 Done, mean position loss: 22.08375602483749\n",
      "Training FF2:  66%|█████████████▎      | 2122/3201 [1:06:49<30:10,  1.68s/batch]Batch 2100/3201 Done, mean position loss: 21.280590884685516\n",
      "Training FF2:  66%|█████████████▏      | 2118/3201 [1:06:52<33:32,  1.86s/batch]Batch 2100/3201 Done, mean position loss: 21.069641466140748\n",
      "Training FF2:  65%|█████████████       | 2090/3201 [1:06:52<32:08,  1.74s/batch]Batch 2100/3201 Done, mean position loss: 21.239108328819277\n",
      "Training FF2:  65%|████████████▉       | 2078/3201 [1:06:52<36:10,  1.93s/batch]Batch 2100/3201 Done, mean position loss: 21.452312021255494\n",
      "Training FF2:  65%|█████████████       | 2083/3201 [1:07:02<36:13,  1.94s/batch]Batch 2100/3201 Done, mean position loss: 21.668704764842985\n",
      "Training FF2:  67%|█████████████▎      | 2131/3201 [1:07:05<35:25,  1.99s/batch]Batch 2100/3201 Done, mean position loss: 21.111130907535554\n",
      "Training FF2:  66%|█████████████▏      | 2111/3201 [1:07:06<27:52,  1.53s/batch]Batch 2100/3201 Done, mean position loss: 21.448698296546937\n",
      "Training FF2:  66%|█████████████▏      | 2112/3201 [1:07:07<27:39,  1.52s/batch]Batch 2100/3201 Done, mean position loss: 21.393956441879272\n",
      "Training FF2:  65%|█████████████       | 2081/3201 [1:07:12<38:04,  2.04s/batch]Batch 2100/3201 Done, mean position loss: 21.062186415195466\n",
      "Training FF2:  67%|█████████████▎      | 2130/3201 [1:07:16<38:05,  2.13s/batch]Batch 2100/3201 Done, mean position loss: 21.57926701307297\n",
      "Training FF2:  67%|█████████████▎      | 2129/3201 [1:07:28<37:04,  2.07s/batch]Batch 2100/3201 Done, mean position loss: 21.029316630363468\n",
      "Training FF2:  67%|█████████████▎      | 2131/3201 [1:07:35<35:54,  2.01s/batch]Batch 2100/3201 Done, mean position loss: 21.293847115039824\n",
      "Training FF2:  67%|█████████████▎      | 2135/3201 [1:07:51<32:30,  1.83s/batch]Batch 2100/3201 Done, mean position loss: 21.128653116226197\n",
      "Training FF2:  67%|█████████████▍      | 2142/3201 [1:08:04<31:42,  1.80s/batch]Batch 2100/3201 Done, mean position loss: 21.30982006788254\n",
      "Training FF2:  68%|█████████████▌      | 2162/3201 [1:08:33<28:56,  1.67s/batch]Batch 2200/3201 Done, mean position loss: 21.563227908611296\n",
      "Training FF2:  68%|█████████████▌      | 2173/3201 [1:08:59<29:02,  1.70s/batch]Batch 2200/3201 Done, mean position loss: 21.415575015544892\n",
      "Training FF2:  68%|█████████████▋      | 2191/3201 [1:09:06<28:30,  1.69s/batch]Batch 2200/3201 Done, mean position loss: 21.234865379333495\n",
      "Training FF2:  68%|█████████████▋      | 2183/3201 [1:09:08<35:18,  2.08s/batch]Batch 2200/3201 Done, mean position loss: 20.84992254972458\n",
      "Training FF2:  68%|█████████████▌      | 2164/3201 [1:09:08<33:27,  1.94s/batch]Batch 2200/3201 Done, mean position loss: 21.34354803085327\n",
      "Training FF2:  68%|█████████████▋      | 2191/3201 [1:09:09<27:49,  1.65s/batch]Batch 2200/3201 Done, mean position loss: 21.892870311737063\n",
      "Training FF2:  67%|█████████████▍      | 2147/3201 [1:09:11<31:03,  1.77s/batch]Batch 2200/3201 Done, mean position loss: 21.372177278995515\n",
      "Training FF2:  67%|█████████████▍      | 2159/3201 [1:09:18<31:41,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 21.27350482702255\n",
      "Training FF2:  69%|█████████████▊      | 2202/3201 [1:09:19<27:57,  1.68s/batch]Batch 2200/3201 Done, mean position loss: 21.331432287693023\n",
      "Training FF2:  69%|█████████████▊      | 2209/3201 [1:09:19<30:03,  1.82s/batch]Batch 2200/3201 Done, mean position loss: 21.10051457643509\n",
      "Training FF2:  70%|█████████████▉      | 2227/3201 [1:09:21<33:27,  2.06s/batch]Batch 2200/3201 Done, mean position loss: 21.154062592983244\n",
      "Training FF2:  69%|█████████████▊      | 2203/3201 [1:09:22<20:43,  1.25s/batch]Batch 2200/3201 Done, mean position loss: 20.954105818271636\n",
      "Training FF2:  68%|█████████████▌      | 2173/3201 [1:09:25<29:02,  1.70s/batch]Batch 2200/3201 Done, mean position loss: 21.69450565338135\n",
      "Training FF2:  68%|█████████████▌      | 2178/3201 [1:09:24<28:16,  1.66s/batch]Batch 2200/3201 Done, mean position loss: 21.037309670448302\n",
      "Training FF2:  67%|█████████████▍      | 2146/3201 [1:09:27<26:40,  1.52s/batch]Batch 2200/3201 Done, mean position loss: 21.335920860767363\n",
      "Training FF2:  69%|█████████████▋      | 2193/3201 [1:09:29<31:53,  1.90s/batch]Batch 2200/3201 Done, mean position loss: 21.24029799938202\n",
      "Training FF2:  69%|█████████████▊      | 2214/3201 [1:09:28<27:49,  1.69s/batch]Batch 2200/3201 Done, mean position loss: 21.06421627283096\n",
      "Training FF2:  68%|█████████████▌      | 2179/3201 [1:09:30<31:52,  1.87s/batch]Batch 2200/3201 Done, mean position loss: 20.974748392105102\n",
      "Training FF2:  68%|█████████████▋      | 2183/3201 [1:09:33<36:44,  2.17s/batch]Batch 2200/3201 Done, mean position loss: 21.197897245883944\n",
      "Training FF2:  68%|█████████████▌      | 2171/3201 [1:09:37<27:28,  1.60s/batch]Batch 2200/3201 Done, mean position loss: 21.02720846414566\n",
      "Training FF2:  70%|█████████████▉      | 2236/3201 [1:09:38<32:22,  2.01s/batch]Batch 2200/3201 Done, mean position loss: 21.219563653469088\n",
      "Training FF2:  69%|█████████████▊      | 2206/3201 [1:09:39<32:45,  1.98s/batch]Batch 2200/3201 Done, mean position loss: 21.199433457851413\n",
      "Training FF2:  67%|█████████████▍      | 2154/3201 [1:09:40<29:14,  1.68s/batch]Batch 2200/3201 Done, mean position loss: 22.028791227340697\n",
      "Training FF2:  70%|█████████████▉      | 2238/3201 [1:09:41<27:55,  1.74s/batch]Batch 2200/3201 Done, mean position loss: 21.877811274528504\n",
      "Training FF2:  69%|█████████████▋      | 2197/3201 [1:09:44<33:08,  1.98s/batch]Batch 2200/3201 Done, mean position loss: 21.14508718252182\n",
      "Training FF2:  68%|█████████████▋      | 2188/3201 [1:09:47<26:38,  1.58s/batch]Batch 2200/3201 Done, mean position loss: 21.527022235393524\n",
      "Training FF2:  69%|█████████████▊      | 2219/3201 [1:09:51<24:45,  1.51s/batch]Batch 2200/3201 Done, mean position loss: 21.21645313024521\n",
      "Training FF2:  69%|█████████████▊      | 2208/3201 [1:09:51<30:54,  1.87s/batch]Batch 2200/3201 Done, mean position loss: 21.251615543365478\n",
      "Training FF2:  69%|█████████████▊      | 2216/3201 [1:09:51<28:44,  1.75s/batch]Batch 2200/3201 Done, mean position loss: 21.45010295391083\n",
      "Training FF2:  69%|█████████████▊      | 2204/3201 [1:09:57<28:30,  1.72s/batch]Batch 2200/3201 Done, mean position loss: 21.052645921707153\n",
      "Training FF2:  69%|█████████████▊      | 2217/3201 [1:10:05<28:00,  1.71s/batch]Batch 2200/3201 Done, mean position loss: 21.620110309123994\n",
      "Training FF2:  69%|█████████████▉      | 2223/3201 [1:10:06<32:57,  2.02s/batch]Batch 2200/3201 Done, mean position loss: 21.094322230815887\n",
      "Training FF2:  69%|█████████████▋      | 2200/3201 [1:10:11<27:20,  1.64s/batch]Batch 2200/3201 Done, mean position loss: 21.37483540058136\n",
      "Training FF2:  69%|█████████████▊      | 2202/3201 [1:10:12<30:43,  1.85s/batch]Batch 2200/3201 Done, mean position loss: 21.456953282356263\n",
      "Training FF2:  70%|█████████████▉      | 2226/3201 [1:10:13<33:59,  2.09s/batch]Batch 2200/3201 Done, mean position loss: 21.531718678474427\n",
      "Training FF2:  70%|█████████████▉      | 2234/3201 [1:10:16<26:30,  1.64s/batch]Batch 2200/3201 Done, mean position loss: 21.025330600738524\n",
      "Training FF2:  68%|█████████████▋      | 2181/3201 [1:10:32<31:59,  1.88s/batch]Batch 2200/3201 Done, mean position loss: 21.266834731101987\n",
      "Training FF2:  70%|█████████████▉      | 2239/3201 [1:10:35<33:40,  2.10s/batch]Batch 2200/3201 Done, mean position loss: 21.042613825798036\n",
      "Training FF2:  70%|██████████████      | 2243/3201 [1:10:52<34:38,  2.17s/batch]Batch 2200/3201 Done, mean position loss: 21.128661820888517\n",
      "Training FF2:  71%|██████████████▏     | 2265/3201 [1:11:08<30:43,  1.97s/batch]Batch 2200/3201 Done, mean position loss: 21.287455904483792\n",
      "Training FF2:  71%|██████████████▎     | 2281/3201 [1:11:35<28:31,  1.86s/batch]Batch 2300/3201 Done, mean position loss: 21.567213962078092\n",
      "Training FF2:  71%|██████████████      | 2260/3201 [1:12:01<29:33,  1.88s/batch]Batch 2300/3201 Done, mean position loss: 21.395195360183713\n",
      "Training FF2:  72%|██████████████▎     | 2296/3201 [1:12:07<26:48,  1.78s/batch]Batch 2300/3201 Done, mean position loss: 21.345538885593413\n",
      "Training FF2:  72%|██████████████▎     | 2289/3201 [1:12:15<26:40,  1.75s/batch]Batch 2300/3201 Done, mean position loss: 21.2683469247818\n",
      "Training FF2:  72%|██████████████▎     | 2300/3201 [1:12:17<29:02,  1.93s/batch]Batch 2300/3201 Done, mean position loss: 20.834974682331087\n",
      "Training FF2:  72%|██████████████▍     | 2309/3201 [1:12:17<32:53,  2.21s/batch]Batch 2300/3201 Done, mean position loss: 21.214313921928408\n",
      "Training FF2:  70%|██████████████      | 2256/3201 [1:12:18<31:58,  2.03s/batch]Batch 2300/3201 Done, mean position loss: 21.28608681201935\n",
      "Training FF2:  71%|██████████████▏     | 2272/3201 [1:12:18<31:29,  2.03s/batch]Batch 2300/3201 Done, mean position loss: 21.06641848564148\n",
      "Training FF2:  72%|██████████████▍     | 2301/3201 [1:12:19<29:44,  1.98s/batch]Batch 2300/3201 Done, mean position loss: 21.883624675273897\n",
      "Training FF2:  71%|██████████████▏     | 2271/3201 [1:12:23<36:11,  2.33s/batch]Batch 2300/3201 Done, mean position loss: 21.137736575603483\n",
      "Training FF2:  71%|██████████████▏     | 2271/3201 [1:12:24<30:45,  1.98s/batch]Batch 2300/3201 Done, mean position loss: 21.630540556907654\n",
      "Training FF2:  72%|██████████████▎     | 2291/3201 [1:12:27<29:39,  1.96s/batch]Batch 2300/3201 Done, mean position loss: 21.34437169790268\n",
      "Training FF2:  72%|██████████████▍     | 2308/3201 [1:12:28<29:24,  1.98s/batch]Batch 2300/3201 Done, mean position loss: 21.297233281135558\n",
      "Training FF2:  70%|██████████████      | 2251/3201 [1:12:28<30:55,  1.95s/batch]Batch 2300/3201 Done, mean position loss: 20.95812655687332\n",
      "Training FF2:  72%|██████████████▍     | 2307/3201 [1:12:32<35:12,  2.36s/batch]Batch 2300/3201 Done, mean position loss: 21.025189390182497\n",
      "Training FF2:  72%|██████████████▎     | 2300/3201 [1:12:36<28:26,  1.89s/batch]Batch 2300/3201 Done, mean position loss: 21.167934436798095\n",
      "Training FF2:  73%|██████████████▌     | 2331/3201 [1:12:37<30:42,  2.12s/batch]Batch 2300/3201 Done, mean position loss: 21.058910477161408\n",
      "Training FF2:  72%|██████████████▍     | 2311/3201 [1:12:38<24:56,  1.68s/batch]Batch 2300/3201 Done, mean position loss: 21.191993231773374\n",
      "Training FF2:  71%|██████████████▏     | 2279/3201 [1:12:39<28:49,  1.88s/batch]Batch 2300/3201 Done, mean position loss: 20.92564379453659\n",
      "Training FF2:  72%|██████████████▍     | 2315/3201 [1:12:40<26:11,  1.77s/batch]Batch 2300/3201 Done, mean position loss: 21.011822419166567\n",
      "Training FF2:  72%|██████████████▍     | 2312/3201 [1:12:42<34:12,  2.31s/batch]Batch 2300/3201 Done, mean position loss: 21.219851534366605\n",
      "Training FF2:  72%|██████████████▍     | 2318/3201 [1:12:48<26:18,  1.79s/batch]Batch 2300/3201 Done, mean position loss: 22.09264544963837\n",
      "Training FF2:  72%|██████████████▍     | 2314/3201 [1:12:53<34:29,  2.33s/batch]Batch 2300/3201 Done, mean position loss: 21.52787827968597\n",
      "Training FF2:  71%|██████████████▏     | 2273/3201 [1:12:55<34:21,  2.22s/batch]Batch 2300/3201 Done, mean position loss: 21.171836400032042\n",
      "Training FF2:  73%|██████████████▌     | 2325/3201 [1:12:55<31:41,  2.17s/batch]Batch 2300/3201 Done, mean position loss: 21.883925223350523\n",
      "Training FF2:  72%|██████████████▍     | 2303/3201 [1:12:59<29:40,  1.98s/batch]Batch 2300/3201 Done, mean position loss: 21.14077624320984\n",
      "Training FF2:  72%|██████████████▍     | 2311/3201 [1:13:00<33:01,  2.23s/batch]Batch 2300/3201 Done, mean position loss: 21.227400913238522\n",
      "Training FF2:  73%|██████████████▌     | 2331/3201 [1:13:03<29:15,  2.02s/batch]Batch 2300/3201 Done, mean position loss: 21.18814864873886\n",
      "Training FF2:  72%|██████████████▎     | 2291/3201 [1:13:04<27:17,  1.80s/batch]Batch 2300/3201 Done, mean position loss: 21.41291136741638\n",
      "Training FF2:  73%|██████████████▌     | 2328/3201 [1:13:08<29:37,  2.04s/batch]Batch 2300/3201 Done, mean position loss: 21.054370296001434\n",
      "Training FF2:  73%|██████████████▌     | 2327/3201 [1:13:13<26:09,  1.80s/batch]Batch 2300/3201 Done, mean position loss: 21.08831925153732\n",
      "Training FF2:  73%|██████████████▌     | 2325/3201 [1:13:18<27:00,  1.85s/batch]Batch 2300/3201 Done, mean position loss: 21.510211534500122\n",
      "Training FF2:  71%|██████████████▏     | 2270/3201 [1:13:22<25:37,  1.65s/batch]Batch 2300/3201 Done, mean position loss: 21.620573823451995\n",
      "Training FF2:  71%|██████████████▏     | 2279/3201 [1:13:24<33:19,  2.17s/batch]Batch 2300/3201 Done, mean position loss: 21.026349203586577\n",
      "Training FF2:  73%|██████████████▌     | 2334/3201 [1:13:26<28:02,  1.94s/batch]Batch 2300/3201 Done, mean position loss: 21.359687855243685\n",
      "Training FF2:  73%|██████████████▌     | 2328/3201 [1:13:32<26:15,  1.81s/batch]Batch 2300/3201 Done, mean position loss: 21.442071018218993\n",
      "Training FF2:  73%|██████████████▌     | 2336/3201 [1:13:47<27:28,  1.91s/batch]Batch 2300/3201 Done, mean position loss: 21.034611082077028\n",
      "Training FF2:  73%|██████████████▌     | 2337/3201 [1:13:47<24:25,  1.70s/batch]Batch 2300/3201 Done, mean position loss: 21.244788808822634\n",
      "Training FF2:  73%|██████████████▋     | 2352/3201 [1:14:05<27:15,  1.93s/batch]Batch 2300/3201 Done, mean position loss: 21.09953906059265\n",
      "Training FF2:  73%|██████████████▌     | 2335/3201 [1:14:18<21:59,  1.52s/batch]Batch 2300/3201 Done, mean position loss: 21.27602724313736\n",
      "Training FF2:  74%|██████████████▊     | 2371/3201 [1:14:54<26:08,  1.89s/batch]Batch 2400/3201 Done, mean position loss: 21.505590946674346\n",
      "Training FF2:  75%|██████████████▉     | 2387/3201 [1:15:15<24:24,  1.80s/batch]Batch 2400/3201 Done, mean position loss: 21.381565868854523\n",
      "Training FF2:  75%|██████████████▉     | 2387/3201 [1:15:25<22:48,  1.68s/batch]Batch 2400/3201 Done, mean position loss: 21.31195412874222\n",
      "Training FF2:  74%|██████████████▊     | 2375/3201 [1:15:30<25:22,  1.84s/batch]Batch 2400/3201 Done, mean position loss: 21.24765149116516\n",
      "Training FF2:  74%|██████████████▉     | 2382/3201 [1:15:32<24:09,  1.77s/batch]Batch 2400/3201 Done, mean position loss: 20.826253786087037\n",
      "Training FF2:  74%|██████████████▉     | 2383/3201 [1:15:32<28:24,  2.08s/batch]Batch 2400/3201 Done, mean position loss: 21.30048752069473\n",
      "Training FF2:  75%|██████████████▉     | 2400/3201 [1:15:32<23:08,  1.73s/batch]Batch 2400/3201 Done, mean position loss: 21.823252103328706\n",
      "Training FF2:  73%|██████████████▌     | 2338/3201 [1:15:35<33:43,  2.35s/batch]Batch 2400/3201 Done, mean position loss: 21.191681883335114\n",
      "Training FF2:  74%|██████████████▊     | 2378/3201 [1:15:36<27:27,  2.00s/batch]Batch 2400/3201 Done, mean position loss: 21.330823209285736\n",
      "Training FF2:  74%|██████████████▉     | 2383/3201 [1:15:39<27:06,  1.99s/batch]Batch 2400/3201 Done, mean position loss: 21.590470154285434\n",
      "Training FF2:  73%|██████████████▋     | 2351/3201 [1:15:42<26:01,  1.84s/batch]Batch 2400/3201 Done, mean position loss: 21.302837660312655\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:15:44<27:32,  2.06s/batch]Batch 2400/3201 Done, mean position loss: 21.12905670642853\n",
      "Training FF2:  74%|██████████████▉     | 2383/3201 [1:15:44<26:22,  1.94s/batch]Batch 2400/3201 Done, mean position loss: 21.053505678176883\n",
      "Training FF2:  74%|██████████████▉     | 2382/3201 [1:15:47<29:35,  2.17s/batch]Batch 2400/3201 Done, mean position loss: 21.02001244068146\n",
      "Training FF2:  74%|██████████████▊     | 2371/3201 [1:15:47<27:20,  1.98s/batch]Batch 2400/3201 Done, mean position loss: 20.954678065776825\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:15:49<23:08,  1.73s/batch]Batch 2400/3201 Done, mean position loss: 21.02766443014145\n",
      "Training FF2:  75%|██████████████▉     | 2399/3201 [1:15:52<32:45,  2.45s/batch]Batch 2400/3201 Done, mean position loss: 20.91395830154419\n",
      "Training FF2:  74%|██████████████▊     | 2376/3201 [1:15:52<30:40,  2.23s/batch]Batch 2400/3201 Done, mean position loss: 21.16100772380829\n",
      "Training FF2:  75%|███████████████     | 2404/3201 [1:15:54<26:32,  2.00s/batch]Batch 2400/3201 Done, mean position loss: 21.204677789211274\n",
      "Training FF2:  75%|███████████████     | 2413/3201 [1:15:56<24:44,  1.88s/batch]Batch 2400/3201 Done, mean position loss: 21.151432075500487\n",
      "Training FF2:  75%|███████████████     | 2413/3201 [1:15:58<24:00,  1.83s/batch]Batch 2400/3201 Done, mean position loss: 20.987683188915256\n",
      "Training FF2:  75%|███████████████     | 2411/3201 [1:16:06<23:18,  1.77s/batch]Batch 2400/3201 Done, mean position loss: 21.522521352767946\n",
      "Training FF2:  76%|███████████████▏    | 2422/3201 [1:16:08<26:31,  2.04s/batch]Batch 2400/3201 Done, mean position loss: 21.84047650814056\n",
      "Training FF2:  76%|███████████████     | 2419/3201 [1:16:10<25:35,  1.96s/batch]Batch 2400/3201 Done, mean position loss: 21.1338476228714\n",
      "Training FF2:  76%|███████████████▏    | 2422/3201 [1:16:11<21:30,  1.66s/batch]Batch 2400/3201 Done, mean position loss: 21.209595851898193\n",
      "Training FF2:  76%|███████████████     | 2420/3201 [1:16:14<28:57,  2.23s/batch]Batch 2400/3201 Done, mean position loss: 22.081409337520597\n",
      "Training FF2:  75%|██████████████▉     | 2400/3201 [1:16:17<26:48,  2.01s/batch]Batch 2400/3201 Done, mean position loss: 21.176876554489137\n",
      "Training FF2:  75%|██████████████▉     | 2400/3201 [1:16:19<24:03,  1.80s/batch]Batch 2400/3201 Done, mean position loss: 21.07986094236374\n",
      "Training FF2:  75%|██████████████▉     | 2400/3201 [1:16:21<31:02,  2.33s/batch]Batch 2400/3201 Done, mean position loss: 21.04889619112015\n",
      "Training FF2:  75%|██████████████▉     | 2398/3201 [1:16:22<26:34,  1.99s/batch]Batch 2400/3201 Done, mean position loss: 21.164496023654937\n",
      "Training FF2:  76%|███████████████▏    | 2421/3201 [1:16:23<20:45,  1.60s/batch]Batch 2400/3201 Done, mean position loss: 21.391392250061035\n",
      "Training FF2:  75%|███████████████     | 2406/3201 [1:16:28<23:13,  1.75s/batch]Batch 2400/3201 Done, mean position loss: 21.509989597797393\n",
      "Training FF2:  77%|███████████████▎    | 2453/3201 [1:16:36<23:04,  1.85s/batch]Batch 2400/3201 Done, mean position loss: 21.34169305562973\n",
      "Training FF2:  76%|███████████████▏    | 2428/3201 [1:16:39<28:40,  2.23s/batch]Batch 2400/3201 Done, mean position loss: 21.573891837596893\n",
      "Training FF2:  75%|██████████████▉     | 2389/3201 [1:16:39<31:15,  2.31s/batch]Batch 2400/3201 Done, mean position loss: 21.0041317486763\n",
      "Training FF2:  75%|███████████████     | 2414/3201 [1:16:47<23:17,  1.78s/batch]Batch 2400/3201 Done, mean position loss: 21.43109428167343\n",
      "Training FF2:  76%|███████████████▏    | 2436/3201 [1:17:02<21:51,  1.71s/batch]Batch 2400/3201 Done, mean position loss: 21.207662785053255\n",
      "Training FF2:  77%|███████████████▎    | 2449/3201 [1:17:04<24:13,  1.93s/batch]Batch 2400/3201 Done, mean position loss: 21.012739627361295\n",
      "Training FF2:  76%|███████████████▏    | 2431/3201 [1:17:22<27:19,  2.13s/batch]Batch 2400/3201 Done, mean position loss: 21.094058372974395\n",
      "Training FF2:  77%|███████████████▎    | 2460/3201 [1:17:41<26:04,  2.11s/batch]Batch 2400/3201 Done, mean position loss: 21.281932239532473\n",
      "Training FF2:  77%|███████████████▍    | 2479/3201 [1:18:09<23:30,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.466927378177644\n",
      "Training FF2:  78%|███████████████▌    | 2489/3201 [1:18:18<20:28,  1.72s/batch]Batch 2500/3201 Done, mean position loss: 21.345403983592988\n",
      "Training FF2:  78%|███████████████▌    | 2490/3201 [1:18:36<20:58,  1.77s/batch]Batch 2500/3201 Done, mean position loss: 21.85563637971878\n",
      "Training FF2:  77%|███████████████▎    | 2459/3201 [1:18:39<27:00,  2.18s/batch]Batch 2500/3201 Done, mean position loss: 21.31576654434204\n",
      "Training FF2:  78%|███████████████▋    | 2512/3201 [1:18:41<23:05,  2.01s/batch]Batch 2500/3201 Done, mean position loss: 21.28706937551498\n",
      "Training FF2:  78%|███████████████▌    | 2497/3201 [1:18:42<23:23,  1.99s/batch]Batch 2500/3201 Done, mean position loss: 21.321483380794525\n",
      "Training FF2:  78%|███████████████▌    | 2492/3201 [1:18:43<24:48,  2.10s/batch]Batch 2500/3201 Done, mean position loss: 21.23585283756256\n",
      "Training FF2:  78%|███████████████▌    | 2495/3201 [1:18:44<19:44,  1.68s/batch]Batch 2500/3201 Done, mean position loss: 20.79700042963028\n",
      "Training FF2:  78%|███████████████▌    | 2481/3201 [1:18:48<22:14,  1.85s/batch]Batch 2500/3201 Done, mean position loss: 21.27850217580795\n",
      "Training FF2:  78%|███████████████▌    | 2493/3201 [1:18:49<26:59,  2.29s/batch]Batch 2500/3201 Done, mean position loss: 21.21267106294632\n",
      "Training FF2:  78%|███████████████▌    | 2492/3201 [1:18:56<28:07,  2.38s/batch]Batch 2500/3201 Done, mean position loss: 21.07969383239746\n",
      "Training FF2:  77%|███████████████▎    | 2458/3201 [1:18:56<24:17,  1.96s/batch]Batch 2500/3201 Done, mean position loss: 21.01184182405472\n",
      "Batch 2500/3201 Done, mean position loss: 21.59546485185623\n",
      "Training FF2:  77%|███████████████▎    | 2459/3201 [1:18:58<26:15,  2.12s/batch]Batch 2500/3201 Done, mean position loss: 21.014097743034363\n",
      "Training FF2:  78%|███████████████▌    | 2489/3201 [1:18:58<21:29,  1.81s/batch]Batch 2500/3201 Done, mean position loss: 20.984921581745148\n",
      "Training FF2:  79%|███████████████▋    | 2513/3201 [1:19:01<20:01,  1.75s/batch]Batch 2500/3201 Done, mean position loss: 20.927993247509\n",
      "Training FF2:  78%|███████████████▋    | 2504/3201 [1:19:04<22:11,  1.91s/batch]Batch 2500/3201 Done, mean position loss: 21.14565024614334\n",
      "Training FF2:  78%|███████████████▌    | 2488/3201 [1:19:08<20:53,  1.76s/batch]Batch 2500/3201 Done, mean position loss: 21.18958314657211\n",
      "Training FF2:  78%|███████████████▌    | 2489/3201 [1:19:10<20:19,  1.71s/batch]Batch 2500/3201 Done, mean position loss: 20.970474824905395\n",
      "Training FF2:  77%|███████████████▍    | 2465/3201 [1:19:11<23:57,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.119587762355806\n",
      "Training FF2:  78%|███████████████▌    | 2481/3201 [1:19:11<23:32,  1.96s/batch]Batch 2500/3201 Done, mean position loss: 20.913330767154694\n",
      "Training FF2:  78%|███████████████▌    | 2492/3201 [1:19:11<21:35,  1.83s/batch]Batch 2500/3201 Done, mean position loss: 21.104973793029785\n",
      "Training FF2:  78%|███████████████▌    | 2488/3201 [1:19:22<21:29,  1.81s/batch]Batch 2500/3201 Done, mean position loss: 21.48911384344101\n",
      "Training FF2:  78%|███████████████▋    | 2508/3201 [1:19:26<23:33,  2.04s/batch]Batch 2500/3201 Done, mean position loss: 22.002075350284578\n",
      "Training FF2:  78%|███████████████▋    | 2511/3201 [1:19:26<21:15,  1.85s/batch]Batch 2500/3201 Done, mean position loss: 21.841297986507417\n",
      "Training FF2:  79%|███████████████▋    | 2516/3201 [1:19:27<21:15,  1.86s/batch]Batch 2500/3201 Done, mean position loss: 21.20462770462036\n",
      "Training FF2:  78%|███████████████▌    | 2482/3201 [1:19:29<22:52,  1.91s/batch]Batch 2500/3201 Done, mean position loss: 21.06515216350555\n",
      "Training FF2:  78%|███████████████▌    | 2497/3201 [1:19:30<23:22,  1.99s/batch]Batch 2500/3201 Done, mean position loss: 21.139406208992007\n",
      "Training FF2:  79%|███████████████▊    | 2521/3201 [1:19:32<18:47,  1.66s/batch]Batch 2500/3201 Done, mean position loss: 21.37666868925095\n",
      "Training FF2:  79%|███████████████▊    | 2529/3201 [1:19:36<19:02,  1.70s/batch]Batch 2500/3201 Done, mean position loss: 21.024160652160646\n",
      "Training FF2:  79%|███████████████▊    | 2522/3201 [1:19:38<20:11,  1.78s/batch]Batch 2500/3201 Done, mean position loss: 21.15246046066284\n",
      "Training FF2:  77%|███████████████▍    | 2480/3201 [1:19:41<25:10,  2.09s/batch]Batch 2500/3201 Done, mean position loss: 21.535539038181305\n",
      "Training FF2:  79%|███████████████▊    | 2540/3201 [1:19:45<20:10,  1.83s/batch]Batch 2500/3201 Done, mean position loss: 21.322832236289976\n",
      "Training FF2:  78%|███████████████▋    | 2510/3201 [1:19:46<19:45,  1.72s/batch]Batch 2500/3201 Done, mean position loss: 21.539506800174713\n",
      "Training FF2:  78%|███████████████▋    | 2503/3201 [1:19:50<21:08,  1.82s/batch]Batch 2500/3201 Done, mean position loss: 20.977340095043182\n",
      "Training FF2:  78%|███████████████▋    | 2511/3201 [1:20:08<20:27,  1.78s/batch]Batch 2500/3201 Done, mean position loss: 21.40403170347214\n",
      "Training FF2:  80%|███████████████▉    | 2553/3201 [1:20:23<21:01,  1.95s/batch]Batch 2500/3201 Done, mean position loss: 21.177565233707426\n",
      "Training FF2:  79%|███████████████▊    | 2530/3201 [1:20:24<21:55,  1.96s/batch]Batch 2500/3201 Done, mean position loss: 21.014581928253172\n",
      "Training FF2:  80%|███████████████▉    | 2552/3201 [1:20:31<20:10,  1.87s/batch]Batch 2500/3201 Done, mean position loss: 21.070351731777194\n",
      "Training FF2:  80%|████████████████    | 2572/3201 [1:20:59<20:48,  1.98s/batch]Batch 2500/3201 Done, mean position loss: 21.236126112937928\n",
      "Training FF2:  80%|███████████████▉    | 2556/3201 [1:21:22<18:11,  1.69s/batch]Batch 2600/3201 Done, mean position loss: 21.45936719417572\n",
      "Training FF2:  80%|████████████████    | 2573/3201 [1:21:28<18:26,  1.76s/batch]Batch 2600/3201 Done, mean position loss: 21.322651426792145\n",
      "Training FF2:  80%|████████████████    | 2571/3201 [1:21:43<19:51,  1.89s/batch]Batch 2600/3201 Done, mean position loss: 21.835855894088745\n",
      "Training FF2:  81%|████████████████▏   | 2590/3201 [1:21:47<18:24,  1.81s/batch]Batch 2600/3201 Done, mean position loss: 21.309233462810514\n",
      "Training FF2:  81%|████████████████▏   | 2586/3201 [1:21:55<20:19,  1.98s/batch]Batch 2600/3201 Done, mean position loss: 20.802281882762912\n",
      "Training FF2:  81%|████████████████▎   | 2608/3201 [1:21:56<19:00,  1.92s/batch]Batch 2600/3201 Done, mean position loss: 21.28765481710434\n",
      "Training FF2:  81%|████████████████▏   | 2592/3201 [1:21:58<19:53,  1.96s/batch]Batch 2600/3201 Done, mean position loss: 21.27658164262772\n",
      "Training FF2:  81%|████████████████▏   | 2586/3201 [1:22:00<18:48,  1.83s/batch]Batch 2600/3201 Done, mean position loss: 21.252797453403474\n",
      "Training FF2:  81%|████████████████▎   | 2603/3201 [1:22:04<17:44,  1.78s/batch]Batch 2600/3201 Done, mean position loss: 21.001006598472596\n",
      "Training FF2:  81%|████████████████▏   | 2590/3201 [1:22:04<19:25,  1.91s/batch]Batch 2600/3201 Done, mean position loss: 21.210375201702117\n",
      "Training FF2:  82%|████████████████▎   | 2611/3201 [1:22:06<17:29,  1.78s/batch]Batch 2600/3201 Done, mean position loss: 21.577803363800047\n",
      "Training FF2:  82%|████████████████▎   | 2612/3201 [1:22:05<21:01,  2.14s/batch]Batch 2600/3201 Done, mean position loss: 21.17340582370758\n",
      "Training FF2:  81%|████████████████▏   | 2591/3201 [1:22:10<19:19,  1.90s/batch]Batch 2600/3201 Done, mean position loss: 20.989075438976286\n",
      "Training FF2:  81%|████████████████▏   | 2587/3201 [1:22:11<19:23,  1.89s/batch]Batch 2600/3201 Done, mean position loss: 21.05451900243759\n",
      "Training FF2:  81%|████████████████▏   | 2584/3201 [1:22:15<19:00,  1.85s/batch]Batch 2600/3201 Done, mean position loss: 20.975355594158174\n",
      "Training FF2:  82%|████████████████▍   | 2627/3201 [1:22:17<18:38,  1.95s/batch]Batch 2600/3201 Done, mean position loss: 21.12686982154846\n",
      "Training FF2:  81%|████████████████▏   | 2589/3201 [1:22:18<22:29,  2.20s/batch]Batch 2600/3201 Done, mean position loss: 20.927686693668367\n",
      "Training FF2:  81%|████████████████▏   | 2587/3201 [1:22:21<19:35,  1.91s/batch]Batch 2600/3201 Done, mean position loss: 21.195685987472537\n",
      "Training FF2:  82%|████████████████▎   | 2610/3201 [1:22:24<20:47,  2.11s/batch]Batch 2600/3201 Done, mean position loss: 20.89195674419403\n",
      "Training FF2:  81%|████████████████▏   | 2600/3201 [1:22:27<18:49,  1.88s/batch]Batch 2600/3201 Done, mean position loss: 21.100833494663238\n",
      "Training FF2:  80%|███████████████▉    | 2546/3201 [1:22:27<18:09,  1.66s/batch]Batch 2600/3201 Done, mean position loss: 21.06966125488281\n",
      "Training FF2:  81%|████████████████▎   | 2607/3201 [1:22:31<17:14,  1.74s/batch]Batch 2600/3201 Done, mean position loss: 20.948333823680876\n",
      "Training FF2:  82%|████████████████▎   | 2610/3201 [1:22:35<20:34,  2.09s/batch]Batch 2600/3201 Done, mean position loss: 21.116139113903046\n",
      "Training FF2:  82%|████████████████▍   | 2638/3201 [1:22:37<21:24,  2.28s/batch]Batch 2600/3201 Done, mean position loss: 21.468121559619902\n",
      "Batch 2600/3201 Done, mean position loss: 21.35657883644104\n",
      "Training FF2:  82%|████████████████▎   | 2619/3201 [1:22:38<21:18,  2.20s/batch]Batch 2600/3201 Done, mean position loss: 21.79489756822586\n",
      "Training FF2:  81%|████████████████▏   | 2590/3201 [1:22:42<21:43,  2.13s/batch]Batch 2600/3201 Done, mean position loss: 21.063669905662536\n",
      "Training FF2:  81%|████████████████▏   | 2584/3201 [1:22:43<19:12,  1.87s/batch]Batch 2600/3201 Done, mean position loss: 21.9946236038208\n",
      "Training FF2:  83%|████████████████▌   | 2641/3201 [1:22:44<15:28,  1.66s/batch]Batch 2600/3201 Done, mean position loss: 21.20864772796631\n",
      "Training FF2:  82%|████████████████▍   | 2625/3201 [1:22:47<19:55,  2.08s/batch]Batch 2600/3201 Done, mean position loss: 21.132168457508087\n",
      "Training FF2:  81%|████████████████▏   | 2588/3201 [1:22:49<18:11,  1.78s/batch]Batch 2600/3201 Done, mean position loss: 21.50957042455673\n",
      "Training FF2:  82%|████████████████▍   | 2630/3201 [1:22:52<19:33,  2.06s/batch]Batch 2600/3201 Done, mean position loss: 21.0078714966774\n",
      "Training FF2:  81%|████████████████    | 2580/3201 [1:22:56<22:06,  2.14s/batch]Batch 2600/3201 Done, mean position loss: 21.300367367267608\n",
      "Training FF2:  82%|████████████████▍   | 2637/3201 [1:23:01<20:11,  2.15s/batch]Batch 2600/3201 Done, mean position loss: 21.001456520557404\n",
      "Training FF2:  80%|████████████████    | 2568/3201 [1:23:06<19:46,  1.87s/batch]Batch 2600/3201 Done, mean position loss: 21.521527025699616\n",
      "Training FF2:  82%|████████████████▎   | 2619/3201 [1:23:13<18:44,  1.93s/batch]Batch 2600/3201 Done, mean position loss: 21.38575988769531\n",
      "Training FF2:  82%|████████████████▍   | 2634/3201 [1:23:35<14:42,  1.56s/batch]Batch 2600/3201 Done, mean position loss: 21.164266374111175\n",
      "Training FF2:  81%|████████████████▎   | 2603/3201 [1:23:38<17:07,  1.72s/batch]Batch 2600/3201 Done, mean position loss: 21.000048270225523\n",
      "Training FF2:  83%|████████████████▋   | 2664/3201 [1:23:52<20:59,  2.35s/batch]Batch 2600/3201 Done, mean position loss: 21.06451932191849\n",
      "Training FF2:  84%|████████████████▊   | 2682/3201 [1:24:17<16:09,  1.87s/batch]Batch 2600/3201 Done, mean position loss: 21.240321254730226\n",
      "Training FF2:  83%|████████████████▋   | 2672/3201 [1:24:31<14:10,  1.61s/batch]Batch 2700/3201 Done, mean position loss: 21.457740013599395\n",
      "Training FF2:  83%|████████████████▋   | 2665/3201 [1:24:41<15:45,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 21.34511072397232\n",
      "Training FF2:  85%|████████████████▉   | 2714/3201 [1:24:52<13:45,  1.69s/batch]Batch 2700/3201 Done, mean position loss: 21.802745449543\n",
      "Training FF2:  84%|████████████████▋   | 2679/3201 [1:24:59<19:02,  2.19s/batch]Batch 2700/3201 Done, mean position loss: 20.772162399291993\n",
      "Training FF2:  84%|████████████████▊   | 2683/3201 [1:25:06<15:15,  1.77s/batch]Batch 2700/3201 Done, mean position loss: 21.276178972721098\n",
      "Training FF2:  84%|████████████████▋   | 2680/3201 [1:25:12<15:28,  1.78s/batch]Batch 2700/3201 Done, mean position loss: 21.267721068859103\n",
      "Training FF2:  84%|████████████████▊   | 2691/3201 [1:25:15<16:47,  1.97s/batch]Batch 2700/3201 Done, mean position loss: 21.26715049266815\n",
      "Training FF2:  84%|████████████████▊   | 2682/3201 [1:25:16<13:45,  1.59s/batch]Batch 2700/3201 Done, mean position loss: 21.226780316829682\n",
      "Training FF2:  84%|████████████████▊   | 2695/3201 [1:25:18<15:32,  1.84s/batch]Batch 2700/3201 Done, mean position loss: 20.97927335977554\n",
      "Training FF2:  83%|████████████████▋   | 2670/3201 [1:25:18<18:13,  2.06s/batch]Batch 2700/3201 Done, mean position loss: 21.033429393768312\n",
      "Training FF2:  85%|█████████████████   | 2729/3201 [1:25:19<13:40,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 21.21371364593506\n",
      "Training FF2:  84%|████████████████▊   | 2684/3201 [1:25:22<16:24,  1.90s/batch]Batch 2700/3201 Done, mean position loss: 21.112503442764282\n",
      "Training FF2:  84%|████████████████▉   | 2704/3201 [1:25:23<14:14,  1.72s/batch]Batch 2700/3201 Done, mean position loss: 20.993146500587464\n",
      "Training FF2:  84%|████████████████▉   | 2702/3201 [1:25:23<14:29,  1.74s/batch]Batch 2700/3201 Done, mean position loss: 21.15502945661545\n",
      "Training FF2:  85%|████████████████▉   | 2715/3201 [1:25:24<15:16,  1.89s/batch]Batch 2700/3201 Done, mean position loss: 21.583597519397735\n",
      "Training FF2:  85%|█████████████████   | 2733/3201 [1:25:28<16:09,  2.07s/batch]Batch 2700/3201 Done, mean position loss: 20.99878237009048\n",
      "Training FF2:  84%|████████████████▊   | 2700/3201 [1:25:28<15:29,  1.85s/batch]Batch 2700/3201 Done, mean position loss: 20.92091543674469\n",
      "Training FF2:  85%|████████████████▉   | 2709/3201 [1:25:31<13:59,  1.71s/batch]Batch 2700/3201 Done, mean position loss: 21.1812395119667\n",
      "Training FF2:  85%|████████████████▉   | 2712/3201 [1:25:33<13:02,  1.60s/batch]Batch 2700/3201 Done, mean position loss: 21.082301032543185\n",
      "Training FF2:  85%|████████████████▉   | 2715/3201 [1:25:35<13:43,  1.69s/batch]Batch 2700/3201 Done, mean position loss: 21.110789737701417\n",
      "Training FF2:  85%|█████████████████   | 2726/3201 [1:25:36<13:18,  1.68s/batch]Batch 2700/3201 Done, mean position loss: 20.903209676742556\n",
      "Training FF2:  83%|████████████████▌   | 2643/3201 [1:25:38<17:07,  1.84s/batch]Batch 2700/3201 Done, mean position loss: 21.057967946529388\n",
      "Training FF2:  85%|█████████████████   | 2730/3201 [1:25:43<12:54,  1.64s/batch]Batch 2700/3201 Done, mean position loss: 21.799260263442992\n",
      "Training FF2:  83%|████████████████▋   | 2669/3201 [1:25:43<14:21,  1.62s/batch]Batch 2700/3201 Done, mean position loss: 20.94191065788269\n",
      "Training FF2:  85%|████████████████▉   | 2712/3201 [1:25:46<17:31,  2.15s/batch]Batch 2700/3201 Done, mean position loss: 21.369861810207368\n",
      "Training FF2:  83%|████████████████▌   | 2650/3201 [1:25:50<15:50,  1.72s/batch]Batch 2700/3201 Done, mean position loss: 21.441874790191648\n",
      "Training FF2:  84%|████████████████▉   | 2702/3201 [1:25:51<12:42,  1.53s/batch]Batch 2700/3201 Done, mean position loss: 21.039241523742675\n",
      "Training FF2:  85%|████████████████▉   | 2720/3201 [1:25:52<15:34,  1.94s/batch]Batch 2700/3201 Done, mean position loss: 21.202241365909575\n",
      "Training FF2:  84%|████████████████▊   | 2690/3201 [1:25:54<14:43,  1.73s/batch]Batch 2700/3201 Done, mean position loss: 21.9550749373436\n",
      "Training FF2:  84%|████████████████▊   | 2693/3201 [1:25:58<13:36,  1.61s/batch]Batch 2700/3201 Done, mean position loss: 21.11971873998642\n",
      "Training FF2:  86%|█████████████████   | 2739/3201 [1:25:59<12:48,  1.66s/batch]Batch 2700/3201 Done, mean position loss: 21.275683455467224\n",
      "Training FF2:  85%|█████████████████   | 2726/3201 [1:26:04<13:59,  1.77s/batch]Batch 2700/3201 Done, mean position loss: 21.017548050880432\n",
      "Training FF2:  85%|████████████████▉   | 2708/3201 [1:26:04<16:14,  1.98s/batch]Batch 2700/3201 Done, mean position loss: 21.5103409910202\n",
      "Training FF2:  85%|█████████████████   | 2735/3201 [1:26:12<12:31,  1.61s/batch]Batch 2700/3201 Done, mean position loss: 20.990299160480497\n",
      "Training FF2:  85%|█████████████████   | 2733/3201 [1:26:14<15:19,  1.97s/batch]Batch 2700/3201 Done, mean position loss: 21.39672683954239\n",
      "Training FF2:  85%|████████████████▉   | 2705/3201 [1:26:21<14:39,  1.77s/batch]Batch 2700/3201 Done, mean position loss: 21.529550735950473\n",
      "Training FF2:  85%|█████████████████   | 2732/3201 [1:26:43<14:44,  1.89s/batch]Batch 2700/3201 Done, mean position loss: 21.169495551586152\n",
      "Training FF2:  85%|████████████████▉   | 2719/3201 [1:26:46<14:09,  1.76s/batch]Batch 2700/3201 Done, mean position loss: 21.010640308856964\n",
      "Training FF2:  85%|████████████████▉   | 2711/3201 [1:27:04<12:44,  1.56s/batch]Batch 2700/3201 Done, mean position loss: 21.028754119873046\n",
      "Training FF2:  86%|█████████████████▎  | 2763/3201 [1:27:26<10:36,  1.45s/batch]Batch 2800/3201 Done, mean position loss: 21.433602733612062\n",
      "Training FF2:  87%|█████████████████▍  | 2793/3201 [1:27:29<12:52,  1.89s/batch]Batch 2700/3201 Done, mean position loss: 21.207298617362977\n",
      "Training FF2:  87%|█████████████████▎  | 2774/3201 [1:27:44<13:32,  1.90s/batch]Batch 2800/3201 Done, mean position loss: 21.318009226322175\n",
      "Training FF2:  86%|█████████████████▏  | 2750/3201 [1:27:49<14:07,  1.88s/batch]Batch 2800/3201 Done, mean position loss: 21.792486197948456\n",
      "Training FF2:  85%|█████████████████   | 2732/3201 [1:27:59<16:25,  2.10s/batch]Batch 2800/3201 Done, mean position loss: 20.78522031068802\n",
      "Training FF2:  87%|█████████████████▍  | 2785/3201 [1:28:10<12:34,  1.81s/batch]Batch 2800/3201 Done, mean position loss: 21.197586810588838\n",
      "Training FF2:  88%|█████████████████▌  | 2802/3201 [1:28:12<12:22,  1.86s/batch]Batch 2800/3201 Done, mean position loss: 21.28269607067108\n",
      "Training FF2:  87%|█████████████████▍  | 2793/3201 [1:28:17<14:44,  2.17s/batch]Batch 2800/3201 Done, mean position loss: 21.013364512920376\n",
      "Training FF2:  86%|█████████████████▏  | 2751/3201 [1:28:18<13:54,  1.85s/batch]Batch 2800/3201 Done, mean position loss: 21.268312363624574\n",
      "Training FF2:  85%|█████████████████   | 2728/3201 [1:28:20<15:23,  1.95s/batch]Batch 2800/3201 Done, mean position loss: 21.247209753990173\n",
      "Training FF2:  87%|█████████████████▍  | 2785/3201 [1:28:22<15:08,  2.18s/batch]Batch 2800/3201 Done, mean position loss: 21.176942107677462\n",
      "Training FF2:  87%|█████████████████▎  | 2779/3201 [1:28:22<12:49,  1.82s/batch]Batch 2800/3201 Done, mean position loss: 21.131145741939545\n",
      "Training FF2:  88%|█████████████████▌  | 2804/3201 [1:28:24<12:15,  1.85s/batch]Batch 2800/3201 Done, mean position loss: 20.962458012104033\n",
      "Training FF2:  87%|█████████████████▍  | 2797/3201 [1:28:26<12:43,  1.89s/batch]Batch 2800/3201 Done, mean position loss: 21.09893145561218\n",
      "Training FF2:  86%|█████████████████▏  | 2755/3201 [1:28:26<14:53,  2.00s/batch]Batch 2800/3201 Done, mean position loss: 20.98140003204346\n",
      "Training FF2:  88%|█████████████████▌  | 2806/3201 [1:28:29<11:31,  1.75s/batch]Batch 2800/3201 Done, mean position loss: 21.179888608455656\n",
      "Training FF2:  87%|█████████████████▎  | 2775/3201 [1:28:32<13:16,  1.87s/batch]Batch 2800/3201 Done, mean position loss: 21.544203877449036\n",
      "Training FF2:  88%|█████████████████▋  | 2828/3201 [1:28:33<11:34,  1.86s/batch]Batch 2800/3201 Done, mean position loss: 21.06271070957184\n",
      "Training FF2:  87%|█████████████████▍  | 2786/3201 [1:28:33<12:29,  1.81s/batch]Batch 2800/3201 Done, mean position loss: 20.98351104736328\n",
      "Training FF2:  88%|█████████████████▌  | 2807/3201 [1:28:34<09:30,  1.45s/batch]Batch 2800/3201 Done, mean position loss: 20.92367394924164\n",
      "Training FF2:  87%|█████████████████▍  | 2782/3201 [1:28:40<12:04,  1.73s/batch]Batch 2800/3201 Done, mean position loss: 20.883095524311067\n",
      "Training FF2:  87%|█████████████████▍  | 2792/3201 [1:28:42<13:34,  1.99s/batch]Batch 2800/3201 Done, mean position loss: 20.932690346240996\n",
      "Training FF2:  88%|█████████████████▌  | 2814/3201 [1:28:43<12:56,  2.01s/batch]Batch 2800/3201 Done, mean position loss: 21.113193619251252\n",
      "Training FF2:  87%|█████████████████▍  | 2793/3201 [1:28:44<13:26,  1.98s/batch]Batch 2800/3201 Done, mean position loss: 21.0443479681015\n",
      "Training FF2:  89%|█████████████████▋  | 2833/3201 [1:28:49<11:06,  1.81s/batch]Batch 2800/3201 Done, mean position loss: 21.74102659702301\n",
      "Training FF2:  87%|█████████████████▍  | 2795/3201 [1:28:52<12:23,  1.83s/batch]Batch 2800/3201 Done, mean position loss: 21.455097296237945\n",
      "Training FF2:  87%|█████████████████▍  | 2798/3201 [1:28:52<11:40,  1.74s/batch]Batch 2800/3201 Done, mean position loss: 21.042906222343444\n",
      "Training FF2:  88%|█████████████████▌  | 2819/3201 [1:28:57<11:35,  1.82s/batch]Batch 2800/3201 Done, mean position loss: 21.115333967208862\n",
      "Training FF2:  88%|█████████████████▌  | 2815/3201 [1:28:59<11:29,  1.79s/batch]Batch 2800/3201 Done, mean position loss: 21.37263783454895\n",
      "Training FF2:  88%|█████████████████▌  | 2806/3201 [1:29:01<14:05,  2.14s/batch]Batch 2800/3201 Done, mean position loss: 21.963401889801027\n",
      "Training FF2:  88%|█████████████████▌  | 2802/3201 [1:29:00<11:39,  1.75s/batch]Batch 2800/3201 Done, mean position loss: 20.995319185256957\n",
      "Training FF2:  88%|█████████████████▌  | 2810/3201 [1:29:02<13:46,  2.11s/batch]Batch 2800/3201 Done, mean position loss: 21.243020420074465\n",
      "Training FF2:  88%|█████████████████▋  | 2832/3201 [1:29:10<11:13,  1.82s/batch]Batch 2800/3201 Done, mean position loss: 21.193246808052063\n",
      "Training FF2:  88%|█████████████████▋  | 2827/3201 [1:29:15<13:41,  2.20s/batch]Batch 2800/3201 Done, mean position loss: 20.97274613380432\n",
      "Training FF2:  88%|█████████████████▌  | 2804/3201 [1:29:15<12:01,  1.82s/batch]Batch 2800/3201 Done, mean position loss: 21.52433707237244\n",
      "Training FF2:  88%|█████████████████▌  | 2814/3201 [1:29:18<14:13,  2.21s/batch]Batch 2800/3201 Done, mean position loss: 21.396111965179443\n",
      "Training FF2:  89%|█████████████████▋  | 2835/3201 [1:29:35<10:12,  1.67s/batch]Batch 2800/3201 Done, mean position loss: 21.50529326438904\n",
      "Training FF2:  90%|█████████████████▉  | 2868/3201 [1:29:53<10:30,  1.89s/batch]Batch 2800/3201 Done, mean position loss: 21.164682285785673\n",
      "Training FF2:  89%|█████████████████▊  | 2848/3201 [1:30:02<11:50,  2.01s/batch]Batch 2800/3201 Done, mean position loss: 21.012089495658877\n",
      "Training FF2:  89%|█████████████████▊  | 2854/3201 [1:30:13<12:01,  2.08s/batch]Batch 2800/3201 Done, mean position loss: 21.001003246307373\n",
      "Training FF2:  90%|█████████████████▉  | 2876/3201 [1:30:40<10:18,  1.90s/batch]Batch 2800/3201 Done, mean position loss: 21.211284103393552\n",
      "Training FF2:  89%|█████████████████▊  | 2853/3201 [1:30:42<11:38,  2.01s/batch]Batch 2900/3201 Done, mean position loss: 21.461394610404966\n",
      "Training FF2:  90%|█████████████████▉  | 2877/3201 [1:30:55<11:21,  2.10s/batch]Batch 2900/3201 Done, mean position loss: 21.307048645019535\n",
      "Training FF2:  90%|█████████████████▉  | 2872/3201 [1:31:01<10:10,  1.86s/batch]Batch 2900/3201 Done, mean position loss: 21.76009491443634\n",
      "Training FF2:  90%|█████████████████▉  | 2879/3201 [1:31:11<08:33,  1.59s/batch]Batch 2900/3201 Done, mean position loss: 20.778354818820954\n",
      "Training FF2:  89%|█████████████████▉  | 2863/3201 [1:31:24<12:00,  2.13s/batch]Batch 2900/3201 Done, mean position loss: 20.987668011188507\n",
      "Training FF2:  90%|██████████████████  | 2890/3201 [1:31:25<09:19,  1.80s/batch]Batch 2900/3201 Done, mean position loss: 21.183801009655\n",
      "Training FF2:  91%|██████████████████▏ | 2904/3201 [1:31:29<08:52,  1.79s/batch]Batch 2900/3201 Done, mean position loss: 21.233089325428008\n",
      "Training FF2:  91%|██████████████████▏ | 2918/3201 [1:31:30<09:57,  2.11s/batch]Batch 2900/3201 Done, mean position loss: 21.244921495914458\n",
      "Training FF2:  90%|██████████████████  | 2881/3201 [1:31:32<09:57,  1.87s/batch]Batch 2900/3201 Done, mean position loss: 21.113077762126924\n",
      "Training FF2:  88%|█████████████████▋  | 2832/3201 [1:31:40<10:46,  1.75s/batch]Batch 2900/3201 Done, mean position loss: 21.173823685646056\n",
      "Training FF2:  91%|██████████████████▏ | 2901/3201 [1:31:39<09:40,  1.93s/batch]Batch 2900/3201 Done, mean position loss: 21.23337869644165\n",
      "Training FF2:  91%|██████████████████▏ | 2908/3201 [1:31:42<09:50,  2.01s/batch]Batch 2900/3201 Done, mean position loss: 21.073429505825043\n",
      "Training FF2:  90%|██████████████████  | 2894/3201 [1:31:43<08:09,  1.60s/batch]Batch 2900/3201 Done, mean position loss: 21.171609528064728\n",
      "Training FF2:  91%|██████████████████  | 2900/3201 [1:31:43<08:58,  1.79s/batch]Batch 2900/3201 Done, mean position loss: 20.960696997642515\n",
      "Training FF2:  91%|██████████████████▏ | 2920/3201 [1:31:44<07:45,  1.66s/batch]Batch 2900/3201 Done, mean position loss: 20.89801288843155\n",
      "Batch 2900/3201 Done, mean position loss: 21.519160788059235\n",
      "Training FF2:  90%|█████████████████▉  | 2868/3201 [1:31:46<10:00,  1.80s/batch]Batch 2900/3201 Done, mean position loss: 20.9279354429245\n",
      "Training FF2:  91%|██████████████████  | 2897/3201 [1:31:47<10:41,  2.11s/batch]Batch 2900/3201 Done, mean position loss: 21.043546392917634\n",
      "Training FF2:  90%|██████████████████  | 2890/3201 [1:31:47<08:04,  1.56s/batch]Batch 2900/3201 Done, mean position loss: 20.950220320224762\n",
      "Training FF2:  91%|██████████████████▏ | 2912/3201 [1:31:53<09:28,  1.97s/batch]Batch 2900/3201 Done, mean position loss: 21.087511303424833\n",
      "Training FF2:  90%|██████████████████  | 2894/3201 [1:31:55<08:59,  1.76s/batch]Batch 2900/3201 Done, mean position loss: 20.920456035137178\n",
      "Training FF2:  90%|██████████████████  | 2894/3201 [1:31:56<10:15,  2.01s/batch]Batch 2900/3201 Done, mean position loss: 21.023699803352358\n",
      "Training FF2:  90%|█████████████████▉  | 2865/3201 [1:31:58<13:07,  2.34s/batch]Batch 2900/3201 Done, mean position loss: 20.88365104675293\n",
      "Training FF2:  89%|█████████████████▊  | 2845/3201 [1:32:04<10:53,  1.83s/batch]Batch 2900/3201 Done, mean position loss: 21.10828471183777\n",
      "Training FF2:  91%|██████████████████  | 2898/3201 [1:32:06<08:30,  1.68s/batch]Batch 2900/3201 Done, mean position loss: 21.02583300590515\n",
      "Training FF2:  91%|██████████████████  | 2900/3201 [1:32:06<08:49,  1.76s/batch]Batch 2900/3201 Done, mean position loss: 21.214910378456118\n",
      "Training FF2:  91%|██████████████████▏ | 2909/3201 [1:32:08<08:17,  1.70s/batch]Batch 2900/3201 Done, mean position loss: 21.00599958896637\n",
      "Training FF2:  92%|██████████████████▎ | 2933/3201 [1:32:08<08:02,  1.80s/batch]Batch 2900/3201 Done, mean position loss: 21.77957926750183\n",
      "Training FF2:  91%|██████████████████▏ | 2909/3201 [1:32:09<09:37,  1.98s/batch]Batch 2900/3201 Done, mean position loss: 21.35545163393021\n",
      "Training FF2:  91%|██████████████████▏ | 2903/3201 [1:32:11<08:49,  1.78s/batch]Batch 2900/3201 Done, mean position loss: 21.42157743215561\n",
      "Training FF2:  91%|██████████████████▎ | 2923/3201 [1:32:12<10:44,  2.32s/batch]Batch 2900/3201 Done, mean position loss: 21.96460277557373\n",
      "Training FF2:  91%|██████████████████▏ | 2910/3201 [1:32:24<08:50,  1.82s/batch]Batch 2900/3201 Done, mean position loss: 21.156720275878904\n",
      "Training FF2:  92%|██████████████████▍ | 2946/3201 [1:32:27<09:48,  2.31s/batch]Batch 2900/3201 Done, mean position loss: 20.933949048519132\n",
      "Training FF2:  91%|██████████████████▏ | 2912/3201 [1:32:28<08:44,  1.81s/batch]Batch 2900/3201 Done, mean position loss: 21.48898673772812\n",
      "Training FF2:  91%|██████████████████▎ | 2928/3201 [1:32:37<09:20,  2.05s/batch]Batch 2900/3201 Done, mean position loss: 21.377287769317626\n",
      "Training FF2:  91%|██████████████████▏ | 2912/3201 [1:32:49<09:46,  2.03s/batch]Batch 2900/3201 Done, mean position loss: 21.48582437992096\n",
      "Training FF2:  92%|██████████████████▎ | 2934/3201 [1:33:13<09:02,  2.03s/batch]Batch 2900/3201 Done, mean position loss: 21.120551502704622\n",
      "Training FF2:  92%|██████████████████▍ | 2951/3201 [1:33:16<07:46,  1.87s/batch]Batch 2900/3201 Done, mean position loss: 20.99589158296585\n",
      "Training FF2:  92%|██████████████████▍ | 2951/3201 [1:33:24<07:50,  1.88s/batch]Batch 2900/3201 Done, mean position loss: 21.00046959400177\n",
      "Training FF2:  93%|██████████████████▌ | 2974/3201 [1:33:51<07:00,  1.85s/batch]Batch 2900/3201 Done, mean position loss: 21.190905311107635\n",
      "Training FF2:  93%|██████████████████▌ | 2973/3201 [1:34:03<08:14,  2.17s/batch]Batch 3000/3201 Done, mean position loss: 21.46237828969955\n",
      "Training FF2:  93%|██████████████████▌ | 2974/3201 [1:34:05<07:03,  1.87s/batch]Batch 3000/3201 Done, mean position loss: 21.76425119161606\n",
      "Training FF2:  93%|██████████████████▌ | 2980/3201 [1:34:11<06:49,  1.85s/batch]Batch 3000/3201 Done, mean position loss: 21.284116003513336\n",
      "Training FF2:  93%|██████████████████▋ | 2982/3201 [1:34:22<05:43,  1.57s/batch]Batch 3000/3201 Done, mean position loss: 20.776165385246276\n",
      "Training FF2:  94%|██████████████████▋ | 3000/3201 [1:34:31<06:17,  1.88s/batch]Batch 3000/3201 Done, mean position loss: 20.98564929962158\n",
      "Training FF2:  91%|██████████████████▎ | 2923/3201 [1:34:33<08:18,  1.79s/batch]Batch 3000/3201 Done, mean position loss: 21.147923543453217\n",
      "Training FF2:  93%|██████████████████▌ | 2979/3201 [1:34:42<07:16,  1.97s/batch]Batch 3000/3201 Done, mean position loss: 21.09257396221161\n",
      "Training FF2:  94%|██████████████████▉ | 3021/3201 [1:34:43<05:50,  1.95s/batch]Batch 3000/3201 Done, mean position loss: 21.259767842292785\n",
      "Training FF2:  94%|██████████████████▋ | 2996/3201 [1:34:45<05:53,  1.72s/batch]Batch 3000/3201 Done, mean position loss: 21.211183423995973\n",
      "Training FF2:  94%|██████████████████▊ | 3006/3201 [1:34:52<06:15,  1.93s/batch]Batch 3000/3201 Done, mean position loss: 21.216071643829345\n",
      "Training FF2:  94%|██████████████████▊ | 3002/3201 [1:34:54<06:18,  1.90s/batch]Batch 3000/3201 Done, mean position loss: 21.02573615074158\n",
      "Training FF2:  94%|██████████████████▊ | 3008/3201 [1:34:54<05:43,  1.78s/batch]Batch 3000/3201 Done, mean position loss: 21.15321746349335\n",
      "Training FF2:  94%|██████████████████▊ | 3009/3201 [1:34:57<05:39,  1.77s/batch]Batch 3000/3201 Done, mean position loss: 21.158613221645354\n",
      "Training FF2:  94%|██████████████████▋ | 2999/3201 [1:34:58<06:45,  2.01s/batch]Batch 3000/3201 Done, mean position loss: 21.0488933634758\n",
      "Training FF2:  93%|██████████████████▋ | 2989/3201 [1:35:00<07:18,  2.07s/batch]Batch 3000/3201 Done, mean position loss: 20.919982347488403\n",
      "Training FF2:  94%|██████████████████▊ | 3004/3201 [1:34:59<05:48,  1.77s/batch]Batch 3000/3201 Done, mean position loss: 21.51834232568741\n",
      "Training FF2:  94%|██████████████████▋ | 2994/3201 [1:35:01<07:13,  2.10s/batch]Batch 3000/3201 Done, mean position loss: 20.903270382881168\n",
      "Training FF2:  93%|██████████████████▋ | 2991/3201 [1:35:04<06:30,  1.86s/batch]Batch 3000/3201 Done, mean position loss: 20.90348859548569\n",
      "Training FF2:  92%|██████████████████▍ | 2952/3201 [1:35:04<09:33,  2.30s/batch]Batch 3000/3201 Done, mean position loss: 20.970859096050262\n",
      "Training FF2:  94%|██████████████████▋ | 2999/3201 [1:35:06<07:33,  2.25s/batch]Batch 3000/3201 Done, mean position loss: 20.952056934833525\n",
      "Training FF2:  94%|██████████████████▊ | 3013/3201 [1:35:10<06:41,  2.14s/batch]Batch 3000/3201 Done, mean position loss: 21.05141862630844\n",
      "Training FF2:  94%|██████████████████▊ | 3011/3201 [1:35:12<05:59,  1.89s/batch]Batch 3000/3201 Done, mean position loss: 21.01725575208664\n",
      "Training FF2:  95%|██████████████████▉ | 3029/3201 [1:35:14<06:07,  2.14s/batch]Batch 3000/3201 Done, mean position loss: 21.197937791347503\n",
      "Training FF2:  94%|██████████████████▉ | 3024/3201 [1:35:15<05:54,  2.01s/batch]Batch 3000/3201 Done, mean position loss: 20.8607759642601\n",
      "Training FF2:  94%|██████████████████▊ | 3015/3201 [1:35:14<06:44,  2.17s/batch]Batch 3000/3201 Done, mean position loss: 21.00601508617401\n",
      "Training FF2:  94%|██████████████████▊ | 3019/3201 [1:35:16<06:04,  2.00s/batch]Batch 3000/3201 Done, mean position loss: 21.340260994434356\n",
      "Training FF2:  92%|██████████████████▍ | 2960/3201 [1:35:19<07:34,  1.89s/batch]Batch 3000/3201 Done, mean position loss: 21.09309966802597\n",
      "Training FF2:  94%|██████████████████▊ | 3013/3201 [1:35:23<05:51,  1.87s/batch]Batch 3000/3201 Done, mean position loss: 21.734040725231168\n",
      "Training FF2:  95%|███████████████████ | 3042/3201 [1:35:23<04:40,  1.76s/batch]Batch 3000/3201 Done, mean position loss: 21.45086827993393\n",
      "Training FF2:  93%|██████████████████▌ | 2971/3201 [1:35:24<06:21,  1.66s/batch]Batch 3000/3201 Done, mean position loss: 21.91878490447998\n",
      "Training FF2:  94%|██████████████████▊ | 3009/3201 [1:35:25<05:18,  1.66s/batch]Batch 3000/3201 Done, mean position loss: 20.99078302383423\n",
      "Training FF2:  95%|██████████████████▉ | 3033/3201 [1:35:42<05:26,  1.94s/batch]Batch 3000/3201 Done, mean position loss: 21.14229877948761\n",
      "Training FF2:  94%|██████████████████▊ | 3001/3201 [1:35:43<06:06,  1.83s/batch]Batch 3000/3201 Done, mean position loss: 21.53308510303497\n",
      "Training FF2:  95%|██████████████████▉ | 3027/3201 [1:35:43<05:27,  1.88s/batch]Batch 3000/3201 Done, mean position loss: 20.95478005409241\n",
      "Training FF2:  95%|██████████████████▉ | 3037/3201 [1:35:49<04:49,  1.77s/batch]Batch 3000/3201 Done, mean position loss: 21.358912863731383\n",
      "Training FF2:  94%|██████████████████▉ | 3022/3201 [1:36:05<06:15,  2.10s/batch]Batch 3000/3201 Done, mean position loss: 21.430730714797974\n",
      "Training FF2:  95%|███████████████████ | 3049/3201 [1:36:20<05:21,  2.11s/batch]Batch 3000/3201 Done, mean position loss: 21.13073026418686\n",
      "Training FF2:  95%|███████████████████ | 3049/3201 [1:36:36<05:00,  1.98s/batch]Batch 3000/3201 Done, mean position loss: 20.96170725107193\n",
      "Training FF2:  96%|███████████████████▏| 3080/3201 [1:36:36<03:59,  1.98s/batch]Batch 3000/3201 Done, mean position loss: 20.984757430553437\n",
      "Training FF2:  96%|███████████████████▏| 3061/3201 [1:37:05<04:21,  1.87s/batch]Batch 3000/3201 Done, mean position loss: 21.138637404441834\n",
      "Training FF2:  96%|███████████████████▏| 3071/3201 [1:37:16<03:50,  1.77s/batch]Batch 3100/3201 Done, mean position loss: 21.393052468299867\n",
      "Training FF2:  96%|███████████████████▏| 3079/3201 [1:37:20<04:08,  2.04s/batch]Batch 3100/3201 Done, mean position loss: 21.752686419487\n",
      "Training FF2:  96%|███████████████████▏| 3077/3201 [1:37:21<04:00,  1.94s/batch]Batch 3100/3201 Done, mean position loss: 21.260739154815674\n",
      "Training FF2:  96%|███████████████████▎| 3087/3201 [1:37:36<03:58,  2.09s/batch]Batch 3100/3201 Done, mean position loss: 20.774380633831026\n",
      "Training FF2:  96%|███████████████████▏| 3080/3201 [1:37:40<03:43,  1.85s/batch]Batch 3100/3201 Done, mean position loss: 20.95610277414322\n",
      "Training FF2:  96%|███████████████████▎| 3085/3201 [1:37:45<04:04,  2.11s/batch]Batch 3100/3201 Done, mean position loss: 21.14084396362305\n",
      "Training FF2:  97%|███████████████████▍| 3107/3201 [1:37:53<03:20,  2.13s/batch]Batch 3100/3201 Done, mean position loss: 21.19902344226837\n",
      "Training FF2:  96%|███████████████████▏| 3069/3201 [1:37:53<05:07,  2.33s/batch]Batch 3100/3201 Done, mean position loss: 21.0702148270607\n",
      "Training FF2:  97%|███████████████████▍| 3113/3201 [1:38:00<02:51,  1.95s/batch]Batch 3100/3201 Done, mean position loss: 21.14937473535538\n",
      "Training FF2:  97%|███████████████████▍| 3111/3201 [1:38:01<03:22,  2.25s/batch]Batch 3100/3201 Done, mean position loss: 21.19342124700546\n",
      "Training FF2:  96%|███████████████████▎| 3081/3201 [1:38:02<04:10,  2.09s/batch]Batch 3100/3201 Done, mean position loss: 20.988414680957796\n",
      "Training FF2:  97%|███████████████████▎| 3089/3201 [1:38:07<03:43,  2.00s/batch]Batch 3100/3201 Done, mean position loss: 21.15967885494232\n",
      "Training FF2:  96%|███████████████████▏| 3076/3201 [1:38:08<04:08,  1.98s/batch]Batch 3100/3201 Done, mean position loss: 21.473187119960784\n",
      "Training FF2:  97%|███████████████████▎| 3089/3201 [1:38:09<03:28,  1.86s/batch]Batch 3100/3201 Done, mean position loss: 21.20186080932617\n",
      "Training FF2:  97%|███████████████████▎| 3095/3201 [1:38:09<03:08,  1.78s/batch]Batch 3100/3201 Done, mean position loss: 20.876464407444\n",
      "Training FF2:  96%|███████████████████▏| 3077/3201 [1:38:13<04:24,  2.13s/batch]Batch 3100/3201 Done, mean position loss: 20.895146508216857\n",
      "Training FF2:  96%|███████████████████▎| 3087/3201 [1:38:13<04:34,  2.41s/batch]Batch 3100/3201 Done, mean position loss: 21.0347740483284\n",
      "Training FF2:  95%|███████████████████ | 3051/3201 [1:38:16<05:03,  2.02s/batch]Batch 3100/3201 Done, mean position loss: 20.969984436035155\n",
      "Training FF2:  95%|██████████████████▉ | 3038/3201 [1:38:17<05:29,  2.02s/batch]Batch 3100/3201 Done, mean position loss: 20.89801192045212\n",
      "Training FF2:  96%|███████████████████▎| 3083/3201 [1:38:21<03:56,  2.00s/batch]Batch 3100/3201 Done, mean position loss: 20.986955387592317\n",
      "Training FF2:  97%|███████████████████▍| 3112/3201 [1:38:21<03:11,  2.15s/batch]Batch 3100/3201 Done, mean position loss: 21.060308115482332\n",
      "Training FF2:  97%|███████████████████▎| 3092/3201 [1:38:22<03:34,  1.97s/batch]Batch 3100/3201 Done, mean position loss: 21.203798413276672\n",
      "Training FF2:  96%|███████████████████▎| 3082/3201 [1:38:24<03:59,  2.01s/batch]Batch 3100/3201 Done, mean position loss: 20.941050584316255\n",
      "Training FF2:  96%|███████████████████▏| 3066/3201 [1:38:23<03:55,  1.75s/batch]Batch 3100/3201 Done, mean position loss: 20.8602290225029\n",
      "Training FF2:  98%|███████████████████▌| 3138/3201 [1:38:27<01:42,  1.62s/batch]Batch 3100/3201 Done, mean position loss: 21.004463398456572\n",
      "Training FF2:  98%|███████████████████▌| 3126/3201 [1:38:31<02:29,  1.99s/batch]Batch 3100/3201 Done, mean position loss: 21.90989368200302\n",
      "Training FF2:  97%|███████████████████▍| 3106/3201 [1:38:31<02:40,  1.69s/batch]Batch 3100/3201 Done, mean position loss: 21.339564015865328\n",
      "Training FF2:  97%|███████████████████▍| 3115/3201 [1:38:35<02:44,  1.91s/batch]Batch 3100/3201 Done, mean position loss: 21.101312584877014\n",
      "Training FF2:  98%|███████████████████▌| 3123/3201 [1:38:38<02:16,  1.75s/batch]Batch 3100/3201 Done, mean position loss: 21.378552527427672\n",
      "Training FF2:  97%|███████████████████▍| 3116/3201 [1:38:40<02:35,  1.83s/batch]Batch 3100/3201 Done, mean position loss: 20.974847390651703\n",
      "Training FF2:  97%|███████████████████▍| 3113/3201 [1:38:40<02:52,  1.96s/batch]Batch 3100/3201 Done, mean position loss: 21.742312200069428\n",
      "Training FF2:  97%|███████████████████▍| 3118/3201 [1:38:56<02:30,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 20.931188910007478\n",
      "Training FF2:  97%|███████████████████▍| 3101/3201 [1:38:56<03:56,  2.37s/batch]Batch 3100/3201 Done, mean position loss: 21.13161257266998\n",
      "Training FF2:  97%|███████████████████▍| 3113/3201 [1:38:57<02:54,  1.98s/batch]Batch 3100/3201 Done, mean position loss: 21.371877717971802\n",
      "Training FF2:  98%|███████████████████▌| 3121/3201 [1:39:00<02:38,  1.98s/batch]Batch 3100/3201 Done, mean position loss: 21.466800010204317\n",
      "Training FF2:  97%|███████████████████▍| 3119/3201 [1:39:15<02:15,  1.65s/batch]Batch 3100/3201 Done, mean position loss: 21.467052688598635\n",
      "Training FF2:  99%|███████████████████▋| 3157/3201 [1:39:36<01:43,  2.36s/batch]Batch 3100/3201 Done, mean position loss: 21.070777640342712\n",
      "Training FF2:  99%|███████████████████▋| 3158/3201 [1:39:48<01:37,  2.26s/batch]Batch 3100/3201 Done, mean position loss: 20.976786971092224\n",
      "Training FF2:  99%|███████████████████▉| 3182/3201 [1:39:52<00:34,  1.82s/batch]Batch 3100/3201 Done, mean position loss: 20.939306526184083\n",
      "Training FF2:  98%|███████████████████▌| 3134/3201 [1:40:20<02:15,  2.02s/batch]Batch 3100/3201 Done, mean position loss: 21.130530831813815\n",
      "Training FF2:  99%|███████████████████▊| 3171/3201 [1:40:28<00:59,  1.98s/batch]Batch 3200/3201 Done, mean position loss: 21.256274156570434\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:28<00:00,  1.88s/batch]\n",
      "Done...\n",
      "Training FF2:  98%|███████████████████▋| 3147/3201 [1:40:33<02:04,  2.31s/batch]Batch 3200/3201 Done, mean position loss: 21.373250737190247\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:33<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3159/3201 [1:40:39<01:25,  2.03s/batch]Batch 3200/3201 Done, mean position loss: 21.73711834192276\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:38<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3171/3201 [1:40:52<01:01,  2.04s/batch]Batch 3200/3201 Done, mean position loss: 21.179067673683164\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:53<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3160/3201 [1:40:56<01:13,  1.80s/batch]Batch 3200/3201 Done, mean position loss: 20.754348287582395\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:56<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Batch 3200/3201 Done, mean position loss: 20.936237890720367\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:57<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▉| 3181/3201 [1:40:58<00:35,  1.79s/batch]Batch 3200/3201 Done, mean position loss: 21.126691024303433\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:40:58<00:00,  1.89s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3199/3201 [1:41:07<00:03,  1.55s/batch]Batch 3200/3201 Done, mean position loss: 21.16327718734741\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:06<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3192/3201 [1:41:07<00:14,  1.63s/batch]Batch 3200/3201 Done, mean position loss: 21.065257377624512\n",
      "Training FF2: 100%|███████████████████▉| 3195/3201 [1:41:06<00:09,  1.63s/batch]Batch 3200/3201 Done, mean position loss: 21.138002610206605\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:06<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:06<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3174/3201 [1:41:10<00:47,  1.76s/batch]Batch 3200/3201 Done, mean position loss: 21.491787860393522\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:10<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3187/3201 [1:41:10<00:18,  1.31s/batch]Batch 3200/3201 Done, mean position loss: 21.15780980348587\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:10<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3188/3201 [1:41:12<00:23,  1.80s/batch]Batch 3200/3201 Done, mean position loss: 20.96752063035965\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:11<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3174/3201 [1:41:15<00:34,  1.29s/batch]Batch 3200/3201 Done, mean position loss: 21.20772566795349\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:15<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3191/3201 [1:41:15<00:13,  1.40s/batch]Batch 3200/3201 Done, mean position loss: 20.974550120830536\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:15<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3188/3201 [1:41:16<00:21,  1.69s/batch]Batch 3200/3201 Done, mean position loss: 20.870453417301178\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:17<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3168/3201 [1:41:17<00:41,  1.25s/batch]Batch 3200/3201 Done, mean position loss: 20.87358046770096\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:17<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3194/3201 [1:41:18<00:10,  1.55s/batch]Batch 3200/3201 Done, mean position loss: 20.866685988903047\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:18<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3194/3201 [1:41:18<00:08,  1.26s/batch]Batch 3200/3201 Done, mean position loss: 21.028632640838623\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:18<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3159/3201 [1:41:24<00:45,  1.08s/batch]Batch 3200/3201 Done, mean position loss: 20.997889976501465\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:25<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:41:24<00:01,  1.05s/batch]Batch 3200/3201 Done, mean position loss: 21.21226381778717\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:25<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3197/3201 [1:41:26<00:04,  1.05s/batch]Batch 3200/3201 Done, mean position loss: 20.865397019386293\n",
      "Training FF2:  99%|███████████████████▉| 3184/3201 [1:41:26<00:16,  1.04batch/s]Batch 3200/3201 Done, mean position loss: 21.030697493553163\n",
      "Batch 3200/3201 Done, mean position loss: 20.994714231491088\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:26<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:26<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:26<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3185/3201 [1:41:26<00:13,  1.18batch/s]Batch 3200/3201 Done, mean position loss: 21.079096159935\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:27<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3157/3201 [1:41:27<00:41,  1.05batch/s]Batch 3200/3201 Done, mean position loss: 20.951418843269348\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:27<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3187/3201 [1:41:28<00:12,  1.14batch/s]Batch 3200/3201 Done, mean position loss: 21.89337033987045\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:28<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  98%|███████████████████▋| 3151/3201 [1:41:30<00:22,  2.24batch/s]Batch 3200/3201 Done, mean position loss: 20.962789812088012\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:29<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3193/3201 [1:41:30<00:03,  2.42batch/s]Batch 3200/3201 Done, mean position loss: 21.3231728720665\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:30<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3197/3201 [1:41:31<00:02,  1.46batch/s]Batch 3200/3201 Done, mean position loss: 21.39524648427963\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:30<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▋| 3158/3201 [1:41:32<00:15,  2.72batch/s]Batch 3200/3201 Done, mean position loss: 21.721641974449156\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:33<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:41:33<00:00,  2.11batch/s]Batch 3200/3201 Done, mean position loss: 20.905514562129976\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:33<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3188/3201 [1:41:33<00:07,  1.71batch/s]Batch 3200/3201 Done, mean position loss: 21.325721249580383\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:34<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3198/3201 [1:41:34<00:01,  2.93batch/s]Batch 3200/3201 Done, mean position loss: 21.09890941143036\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:35<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3168/3201 [1:41:35<00:11,  2.98batch/s]Batch 3200/3201 Done, mean position loss: 21.491022198200227\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:36<00:00,  1.90s/batch]\n",
      "Done...\n",
      "Training FF2:  99%|███████████████████▊| 3175/3201 [1:41:38<00:08,  3.21batch/s]Batch 3200/3201 Done, mean position loss: 21.440370235443115\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:38<00:00,  1.91s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3190/3201 [1:41:39<00:02,  3.83batch/s]Batch 3200/3201 Done, mean position loss: 21.102812674045563\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:39<00:00,  1.91s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3192/3201 [1:41:42<00:01,  4.57batch/s]Batch 3200/3201 Done, mean position loss: 20.92310585260391\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:42<00:00,  1.91s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3193/3201 [1:41:42<00:01,  4.14batch/s]Batch 3200/3201 Done, mean position loss: 20.945169413089754\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:42<00:00,  1.91s/batch]\n",
      "Done...\n",
      "Training FF2: 100%|███████████████████▉| 3200/3201 [1:41:43<00:00,  5.13batch/s]Batch 3200/3201 Done, mean position loss: 21.155393562316895\n",
      "Training FF2: 100%|████████████████████| 3201/3201 [1:41:44<00:00,  1.91s/batch]\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "for n_s in sizes: # sizes\n",
    "    network_siz = n_s\n",
    "    !python ../model.py {1} {network_siz} {n_model} {0} {1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1: Learing curves for all phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = return_ignore(folder_name,n_model,ff_coef=-8) # sometimes some of cpus fail so we want to ignore those models\n",
    "loss = get_loss(folder_name,n_model,w=1,target=None,ignore=ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss['FF2'])):\n",
    "    loss['FF2'][i] = list(-1*np.array(loss['FF2'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning\n",
    "fig, ax = plot_learning(loss,figsize=(10,6),show_saving=True,gap=5000) # (3.8,2.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#fig.savefig(os.path.join(save_fig,'learning_curve2.pdf'),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_loss(loss):\n",
    "    \n",
    "    T = pd.DataFrame()\n",
    "\n",
    "    # get initial loss\n",
    "    data = {'NF1':[],'FF1':[],'NF2':[],'FF2':[]}\n",
    "    for p in list(data.keys()):\n",
    "        index=0\n",
    "        if p=='NF1' or p=='NF2':\n",
    "            # index=np.arange(-10,0)\n",
    "            index = -1\n",
    "        dat = np.array(loss[p])[:,index]\n",
    "        if dat.ndim==1:\n",
    "            dat = dat[:,None]\n",
    "        data[p] = list(np.mean(dat,axis=1))\n",
    "\n",
    "    T = create_dataframe(data)\n",
    "    T['feature'] = 'init'\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2: Do we have savings? \n",
    "Get the initial values of the loss and also the learning rate. It saves the result as a dataframe so that we don't need to run it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFF2\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFF2\u001b[39m\u001b[38;5;124m'\u001b[39m][i]))\n\u001b[1;32m     14\u001b[0m R \u001b[38;5;241m=\u001b[39m get_rate(loss,w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,check_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[43mget_initial_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m R[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m network_siz\n\u001b[1;32m     18\u001b[0m I[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m network_siz\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mget_initial_loss\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNF1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNF2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# index=np.arange(-10,0)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 12\u001b[0m dat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dat\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m     dat \u001b[38;5;241m=\u001b[39m dat[:,\u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "D = pd.DataFrame()\n",
    "\n",
    "for n_s in sizes:\n",
    "    network_siz = n_s\n",
    "    fn = f'Sim_fixed_{network_siz}'\n",
    "\n",
    "    ignore = return_ignore(fn,n_model,ff_coef=-8)\n",
    "    loss = get_loss(fn,n_model,w=1,target=None,ignore=ignore)\n",
    "\n",
    "    for i in range(len(loss['FF2'])):\n",
    "        loss['FF2'][i] = list(-1*np.array(loss['FF2'][i]))\n",
    "\n",
    "    R = get_rate(loss,w=5,check_fit=False)\n",
    "    I = get_initial_loss(loss)\n",
    "\n",
    "    R['size'] = network_siz\n",
    "    I['size'] = network_siz\n",
    "\n",
    "    T = pd.concat([R,I], ignore_index=True)\n",
    "\n",
    "    D = pd.concat([D,T], ignore_index=True)\n",
    "\n",
    "D.to_csv(base_dir+'/model_loss.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the result only for network of size 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-821.9861192628492\n",
      "Observed Difference: -0.8219861192628493, p-value: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define bootstrap function\n",
    "def bootstrap_paired_test(a, b, n_bootstrap=10000):\n",
    "    \"\"\"Perform a bootstrap test for paired samples.\"\"\"\n",
    "    diffs = a - b  # Compute pairwise differences\n",
    "    observed_mean = np.median(diffs)  # Observed mean difference\n",
    "\n",
    "    # Bootstrap resampling: Shuffle sign of differences\n",
    "    boot_medians = np.array([np.median(diffs * np.random.choice([-1, 1], size=len(diffs))) for _ in range(n_bootstrap)])\n",
    "\n",
    "    # Compute p-value: Two-tailed test\n",
    "    p_value = np.mean(np.abs(boot_medians) >= np.abs(observed_mean))\n",
    "    \n",
    "    return observed_mean, p_value\n",
    "\n",
    "# Perform bootstrap paired test\n",
    "obs_diff, p_value = bootstrap_paired_test(a, b)\n",
    "print(obs_diff *1000)\n",
    "\n",
    "# Display results\n",
    "print(f'Observed Difference: {obs_diff}, p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -4.918637576543184, p-value: 1.618502722286073e-05\n",
      "-908.3546494366577\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "a = T[T['phase']=='FF1']['value'].values\n",
    "b = T[T['phase']=='FF2']['value'].values\n",
    "\n",
    "t_stat, p_value = ttest_rel(a, b, nan_policy='omit')\n",
    "print(f't-statistic: {t_stat}, p-value: {p_value}')\n",
    "print(np.mean(a-b)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  2.,  4., 13.,  6.,  8.,  4.,  1.,  1.]),\n",
       " array([-1.33680178e-03, -1.07676449e-03, -8.16727203e-04, -5.56689913e-04,\n",
       "        -2.96652623e-04, -3.66153326e-05,  2.23421957e-04,  4.83459247e-04,\n",
       "         7.43496537e-04,  1.00353383e-03,  1.26357112e-03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1106694851841855"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(b-a)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 80x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the plots\n",
    "D = pd.read_csv(base_dir+'/model_loss.csv')\n",
    "feature = 'init' # you can select 'init' or 'rate'\n",
    "\n",
    "filter_condition = (\n",
    "    (D['phase'].isin(['FF1', 'FF2'])) &\n",
    "    (D['size'] == 128) &\n",
    "    (D['feature'] == feature) # you can select 'init' or 'rate'\n",
    ")\n",
    "T = D[filter_condition].reset_index(drop=True)\n",
    "\n",
    "T['value'] = T['value']*1000\n",
    "# plot \n",
    "ax = my_pointplot(T,x='phase',y='value',hue='phase',linewidth=3,ax=None)\n",
    "fig=ax.get_figure()\n",
    "if feature == 'rate':\n",
    "    fig.savefig(os.path.join(save_fig, 'saving_rate_control.pdf'), dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    fig.savefig(os.path.join(save_fig, 'saving_init_control.pdf'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the result only for networks of all sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 270x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = pd.read_csv(base_dir+'/model_loss.csv')\n",
    "feature = 'rate' # you can select 'init' or 'rate'\n",
    "\n",
    "filter_condition = (\n",
    "    (D['phase'].isin(['FF1', 'FF2'])) &\n",
    "    (D['feature'] == feature) # you can select 'init' or 'rate'\n",
    ")\n",
    "T = D[filter_condition].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig,ax = my_barplot(T,x='size',y='value',hue='phase')\n",
    "#fig.savefig(os.path.join(save_fig, 'perf_vs_size.pdf'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# % of network that showed saving vs. network size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = pd.read_csv(base_dir+'/model_loss.csv')\n",
    "fig = None\n",
    "ax = None\n",
    "\n",
    "linestyle=['-','--']\n",
    "\n",
    "for i,feature in enumerate(['rate','init']):\n",
    "#feature = 'init' # you can select 'init' or 'rate'\n",
    "\n",
    "    filter_condition = (\n",
    "        (D['phase'].isin(['FF1','FF2'])) &\n",
    "        (D['feature'] == feature) &\n",
    "        (D['size'] <512)  &\n",
    "        (D['size'] > 16)\n",
    "    )\n",
    "    T = D[filter_condition].reset_index(drop=True)\n",
    "\n",
    "    ff1_data = T[T['phase']=='FF1']\n",
    "    ff2_data = T[T['phase']=='FF2']\n",
    "    merged_data = pd.merge(ff1_data,ff2_data,on='size',suffixes=('_ff1', '_ff2'))\n",
    "    # Calculate the difference in 'loss'\n",
    "    if feature == 'init':\n",
    "        merged_data['loss_difference'] = (-merged_data['value_ff2']+merged_data['value_ff1']>0 ) # <0 \n",
    "    else:\n",
    "        merged_data['loss_difference'] = (merged_data['value_ff2']-merged_data['value_ff1']>0) # >0\n",
    "\n",
    "    # Plotting\n",
    "    ax = my_pointplot(merged_data,x='size',y='loss_difference',hue=None,figsize=(1.8, 1.8),ax=ax,linewidth=2,color='black',linestyle=linestyle[i],xlabel='# hidden units',ylabel='% of network with\\n savings')\n",
    "\n",
    "ax.axhline(0.5, color='black', linestyle='-', label='Chance level',linewidth=0.5)\n",
    "\n",
    "fig=ax.get_figure()\n",
    "fig.savefig(os.path.join(save_fig,'saving_vs_size.pdf'),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "savings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
